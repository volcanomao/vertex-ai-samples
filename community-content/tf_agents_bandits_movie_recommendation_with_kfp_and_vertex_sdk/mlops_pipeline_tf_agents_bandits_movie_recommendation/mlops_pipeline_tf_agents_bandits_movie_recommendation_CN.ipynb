{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zFWqF3Y4Gilg"
   },
   "source": [
    "构建端到端强化学习应用程序管道的指南，使用Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "<table align=\"left\">\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/tree/master/community-content/tf_agents_bandits_movie_recommendation_with_kfp_and_vertex_sdk/mlops_pipeline_tf_agents_bandits_movie_recommendation/mlops_pipeline_tf_agents_bandits_movie_recommendation.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> 在 Colab 中运行\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/tree/master/community-content/tf_agents_bandits_movie_recommendation_with_kfp_and_vertex_sdk/mlops_pipeline_tf_agents_bandits_movie_recommendation/mlops_pipeline_tf_agents_bandits_movie_recommendation.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      在 GitHub 上查看\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## 概述\n",
    "\n",
    "该演示展示了如何使用[TF-Agents](https://www.tensorflow.org/agents)、[Kubeflow Pipelines (KFP)](https://www.kubeflow.org/docs/components/pipelines/overview/pipelines-overview/)和[Vertex AI](https://cloud.google.com/vertex-ai)，特别是[Vertex Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines)，构建一个电影推荐系统的端到端强化学习（RL）管道。该演示适用于希望使用TensorFlow、TF-Agents和Vertex AI服务创建RL应用程序的开发人员，以及希望使用KFP和Vertex Pipelines构建端到端生产管道的开发人员。建议开发人员对RL和上下文匹配背包乐观算法的概念以及TF-Agents接口有一定了解。\n",
    "\n",
    "### 数据集\n",
    "\n",
    "该演示使用[MovieLens 100K](https://www.kaggle.com/prajitdatta/movielens-100k-dataset)数据集来模拟具有用户及其偏好的环境。该数据集可在`gs://cloud-samples-data/vertex-ai/community-content/tf_agents_bandits_movie_recommendation_with_kfp_and_vertex_sdk/u.data`中找到。\n",
    "\n",
    "### 目标\n",
    "\n",
    "在这个笔记本中，您将学习如何使用[KFP](https://www.kubeflow.org/docs/components/pipelines/overview/pipelines-overview/)、[Vertex AI](https://cloud.google.com/vertex-ai)和尤其是[Vertex Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines)构建一个端到端的RL管道，用于基于TF-Agents（特别是背包模块）的电影推荐系统，该管道是完全托管且高度可扩展的。\n",
    "\n",
    "该Vertex Pipeline包括以下组件：\n",
    "1. *生成器* 用于生成MovieLens模拟数据\n",
    "2. *摄取器* 用于摄取数据\n",
    "3. *训练器* 用于训练RL策略\n",
    "4. *部署器* 用于将训练过的策略部署到Vertex AI端点\n",
    "\n",
    "在管道构建之后，您可以(1)创建*模拟器*（利用Cloud Functions、Cloud Scheduler和Pub/Sub）来发送模拟的MovieLens预测请求，(2)创建*记录器*来异步记录预测输入和结果（利用Cloud Functions、Pub/Sub以及预测代码中的挂钩），以及(3)创建*触发器*来触发周期性重新训练。\n",
    "\n",
    "在[MLOps on Vertex AI](https://github.com/ksalama/ucaip-labs)中展示了一个更通用的ML管道。\n",
    "\n",
    "### 成本\n",
    "\n",
    "本教程使用Google Cloud的计费组件：\n",
    "\n",
    "* Vertex AI\n",
    "* BigQuery\n",
    "* Cloud Build\n",
    "* Cloud Functions\n",
    "* Cloud Scheduler\n",
    "* Cloud Storage\n",
    "* Pub/Sub\n",
    "\n",
    "了解[Vertex AI\n",
    "价格](https://cloud.google.com/vertex-ai/pricing)、[BigQuery价格](https://cloud.google.com/bigquery/pricing)、[Cloud Build价格](https://cloud.google.com/build/pricing)、[Cloud Functions价格](https://cloud.google.com/functions/pricing)、[Cloud Scheduler价格](https://cloud.google.com/scheduler/pricing)、[Cloud Storage\n",
    "价格](https://cloud.google.com/storage/pricing)和[Pub/Sub价格](https://cloud.google.com/pubsub/pricing)，并使用[Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "根据您的预期使用量生成成本估算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ze4-nDLfK4pw"
   },
   "source": [
    "### 设置您的本地开发环境\n",
    "\n",
    "**如果您正在使用Colab或Google云笔记本**，您的环境已经满足运行此笔记本的所有要求。您可以跳过此步骤。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCuSR8GkAgzl"
   },
   "source": [
    "否则，请确保您的环境符合此笔记本的要求。您需要以下内容：\n",
    "\n",
    "* Google Cloud SDK\n",
    "* Git\n",
    "* Python 3\n",
    "* virtualenv\n",
    "* 在使用Python 3的虚拟环境中运行的Jupyter笔记本\n",
    "\n",
    "Google Cloud的[设置Python开发环境指南](https://cloud.google.com/python/setup)和[Jupyter安装指南](https://jupyter.org/install)提供了满足这些要求的详细说明。以下步骤提供了简化的说明：\n",
    "\n",
    "1. [安装和初始化Cloud SDK。](https://cloud.google.com/sdk/docs/)\n",
    "\n",
    "2. [安装Python 3。](https://cloud.google.com/python/setup#installing_python)\n",
    "\n",
    "3. [安装virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv)，并创建一个使用Python 3的虚拟环境。激活虚拟环境。\n",
    "\n",
    "4. 要安装Jupyter，请在终端窗口中的命令行中运行`pip3 install jupyter`。\n",
    "\n",
    "5. 要启动Jupyter，请在终端窗口中的命令行中运行`jupyter notebook`。\n",
    "\n",
    "6. 在Jupyter Notebook仪表板中打开此笔记本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "安装额外的包\n",
    "\n",
    "安装未在您的笔记本环境中安装的额外包依赖项，如Kubeflow Pipelines（KFP）SDK。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2b4ef9b72d43"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Google Cloud Notebook product has specific requirements\n",
    "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "\n",
    "# Google Cloud Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wyy5Lbnzg5fi"
   },
   "outputs": [],
   "source": [
    "! pip3 install {USER_FLAG} google-cloud-aiplatform\n",
    "! pip3 install {USER_FLAG} google-cloud-pipeline-components\n",
    "! pip3 install {USER_FLAG} --upgrade kfp\n",
    "! pip3 install {USER_FLAG} numpy\n",
    "! pip3 install {USER_FLAG} --upgrade tensorflow\n",
    "! pip3 install {USER_FLAG} --upgrade pillow\n",
    "! pip3 install {USER_FLAG} --upgrade tf-agents\n",
    "! pip3 install {USER_FLAG} --upgrade fastapi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhq5zEbGg0XX"
   },
   "source": [
    "### 重新启动内核\n",
    "\n",
    "在安装附加包之后，您需要重新启动笔记本内核以便找到这些包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EzrelQZ22IZj"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWEdiXsJg0XY"
   },
   "source": [
    "在开始之前\n",
    "\n",
    "选择GPU运行时\n",
    "\n",
    "如果你有这个选项，请确保你在GPU运行时中运行这个笔记本。在Colab中，选择“Runtime --> Change runtime type > GPU”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF1j6f9HApxa"
   },
   "source": [
    "### 设置您的Google Cloud项目\n",
    "\n",
    "**无论您使用哪种笔记本环境，以下步骤都是必需的。**\n",
    "\n",
    "1. [选择或创建一个Google Cloud项目](https://console.cloud.google.com/cloud-resource-manager)。当您第一次创建帐户时，您将获得$300的免费信用额度，可用于计算/存储成本。\n",
    "\n",
    "1. [确保为您的项目启用计费](https://cloud.google.com/billing/docs/how-to/modify-project)。\n",
    "\n",
    "1. [启用Vertex AI API，BigQuery API，Cloud Build，Cloud Functions，Cloud Scheduler，Cloud Storage和Pub/Sub API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,bigquery.googleapis.com,build.googleapis.com,functions.googleapis.com,scheduler.googleapis.com,storage.googleapis.com,pubsub.googleapis.com)。\n",
    "\n",
    "1. 如果您在本地运行此笔记本，您需要安装[Cloud SDK](https://cloud.google.com/sdk)。\n",
    "\n",
    "1. 在下面的单元格中输入您的项目ID。然后运行单元格，确保Cloud SDK对本笔记本中的所有命令使用正确的项目。\n",
    "\n",
    "**注意**：Jupyter 运行以 `!` 开头的行作为shell命令，并插入以 `$` 开头的Python变量到这些命令中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WReHDGG5g0XY"
   },
   "source": [
    "#### 设置您的项目ID\n",
    "\n",
    "**如果您不知道您的项目ID**，您可以使用`gcloud`来获取您的项目ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oM1iC_MfAts1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_ID = \"\"\n",
    "\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJYoRfYng0XZ"
   },
   "source": [
    "否则，请在这里设置您的项目ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "riG_qUokg0XZ"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None:\n",
    "    PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06571eb4063b"
   },
   "source": [
    "时间戳\n",
    "\n",
    "如果您正在进行实时教程会话，您可能会使用一个共享的测试帐户或项目。为了避免在创建的资源中用户之间的名称冲突，您为每个实例会话创建一个时间戳，并将其附加到您在本教程中创建的资源的名称上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "697568e92bd6"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dr--iN2kAylZ"
   },
   "source": [
    "### 验证您的Google Cloud账户\n",
    "\n",
    "**如果您正在使用Google Cloud笔记本**，您的环境已经经过验证。请跳过此步骤。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBCra4QMA2wR"
   },
   "source": [
    "**如果您正在使用Colab**，请运行下面的单元格，并按提示执行验证您的帐户。\n",
    "\n",
    "**否则**，请按照以下步骤操作：\n",
    "\n",
    "1. 在Cloud Console中，转到[**创建服务帐户密钥**页面](https://console.cloud.google.com/apis/credentials/serviceaccountkey)。\n",
    "\n",
    "2. 单击**创建服务帐户**。\n",
    "\n",
    "3. 在**服务帐户名称**字段中输入一个名称，然后单击**创建**。\n",
    "\n",
    "4. 在**授予此服务帐户访问项目**部分，单击**角色**下拉列表。在过滤框中键入\"Vertex AI\"，并选择**Vertex AI管理员**。在过滤框中键入\"Storage Object Admin\"，并选择**存储对象管理员**。\n",
    "\n",
    "5. 单击*创建*。一个包含您密钥的JSON文件将下载到您的本地环境。\n",
    "\n",
    "6. 将您的服务帐户密钥的路径作为`GOOGLE_APPLICATION_CREDENTIALS`变量输入到下面的单元格中并运行该单元格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PyQmSRbKA8r-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "# The Google Cloud Notebook product has specific requirements\n",
    "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "\n",
    "# If on Google Cloud Notebooks, then don't execute this code\n",
    "if not IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\"):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "### 创建一个云存储桶\n",
    "\n",
    "**无论您使用哪种笔记本环境，以下步骤都是必需的。**\n",
    "\n",
    "在本教程中，一个云存储桶存储了用于模型训练的MovieLens数据集文件。Vertex AI还会将训练作业产生的训练模型保存在同一个存储桶中。通过使用这个模型工件，您可以创建Vertex AI模型和端点资源，以提供在线预测。\n",
    "\n",
    "请设置您的云存储桶的名称。它必须在所有云存储桶中是唯一的。\n",
    "\n",
    "您还可以更改`REGION`变量，该变量用于本笔记本其余部分的操作。请确保[选择一个Vertex AI服务可用的地区](https://cloud.google.com/vertex-ai/docs/general/locations#available_regions)。您不能使用多区域存储桶进行Vertex AI的训练。另外请注意，Vertex Pipelines目前仅在部分地区支持，如 \"us-central1\"（[参考](https://cloud.google.com/vertex-ai/docs/general/locations)）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"gs://[your-bucket-name]\"  # @param {type:\"string\"}\n",
    "REGION = \"[your-region]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cf221059d072"
   },
   "outputs": [],
   "source": [
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"gs://[your-bucket-name]\":\n",
    "    BUCKET_NAME = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EcIXiGsCePi"
   },
   "source": [
    "只有在您的存储桶不存在时才运行以下单元格以创建您的云存储存储桶。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIq7R4HZCfIc"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucvCsknMCims"
   },
   "source": [
    "最后，通过检查云存储桶的内容来验证对其的访问权限。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vhOb7YnwClBb"
   },
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "导入库并定义常量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRUOFELefqf1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from kfp.v2 import compiler, dsl\n",
    "from kfp.v2.google.client import AIPlatformClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eIMBwJpVVaU3"
   },
   "source": [
    "请填写以下配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PhWoA2uCVaU3"
   },
   "outputs": [],
   "source": [
    "# BigQuery parameters (used for the Generator, Ingester, Logger)\n",
    "BIGQUERY_DATASET_ID = f\"{PROJECT_ID}.movielens_dataset\"  # @param {type:\"string\"} BigQuery dataset ID as `project_id.dataset_id`.\n",
    "BIGQUERY_LOCATION = \"us\"  # @param {type:\"string\"} BigQuery dataset region.\n",
    "BIGQUERY_TABLE_ID = f\"{BIGQUERY_DATASET_ID}.training_dataset\"  # @param {type:\"string\"} BigQuery table ID as `project_id.dataset_id.table_id`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fuN1uU27VaU3"
   },
   "source": [
    "#### 设置额外配置\n",
    "\n",
    "您可以使用下面的默认值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "895ac243c125"
   },
   "outputs": [],
   "source": [
    "# Dataset parameters\n",
    "RAW_DATA_PATH = \"gs://[your-bucket-name]/raw_data/u.data\"   # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "62bfb9a820f6"
   },
   "outputs": [],
   "source": [
    "# Download the sample data into your RAW_DATA_PATH\n",
    "! gsutil cp \"gs://cloud-samples-data/vertex-ai/community-content/tf_agents_bandits_movie_recommendation_with_kfp_and_vertex_sdk/u.data\" $RAW_DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H3530hdGGilo"
   },
   "outputs": [],
   "source": [
    "# Pipeline parameters\n",
    "PIPELINE_NAME = \"movielens-pipeline\"  # Pipeline display name.\n",
    "ENABLE_CACHING = False  # Whether to enable execution caching for the pipeline.\n",
    "PIPELINE_ROOT = f\"{BUCKET_NAME}/pipeline\"  # Root directory for pipeline artifacts.\n",
    "PIPELINE_SPEC_PATH = \"metadata_pipeline.json\"  # Path to pipeline specification file.\n",
    "OUTPUT_COMPONENT_SPEC = \"output-component.yaml\"  # Output component specification file.\n",
    "\n",
    "# BigQuery parameters (used for the Generator, Ingester, Logger)\n",
    "BIGQUERY_TMP_FILE = (\n",
    "    \"tmp.json\"  # Temporary file for storing data to be loaded into BigQuery.\n",
    ")\n",
    "BIGQUERY_MAX_ROWS = 5  # Maximum number of rows of data in BigQuery to ingest.\n",
    "\n",
    "# Dataset parameters\n",
    "TFRECORD_FILE = (\n",
    "    f\"{BUCKET_NAME}/trainer_input_path/*\"  # TFRecord file to be used for training.\n",
    ")\n",
    "\n",
    "# Logger parameters (also used for the Logger hook in the prediction container)\n",
    "LOGGER_PUBSUB_TOPIC = \"logger-pubsub-topic\"  # Pub/Sub topic name for the Logger.\n",
    "LOGGER_CLOUD_FUNCTION = \"logger-cloud-function\"  # Cloud Functions name for the Logger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6ppE7imft-y"
   },
   "source": [
    "创建强化学习（RL）管道组件\n",
    "\n",
    "此部分包括以下步骤：\n",
    "1. 创建*Generator*以生成MovieLens仿真数据\n",
    "2. 创建*Ingester*以摄取数据\n",
    "3. 创建*Trainer*以训练RL策略\n",
    "4. 创建*Deployer*以部署训练好的策略到Vertex AI端点\n",
    "\n",
    "在管道构建完毕后，创建*Simulator*以发送模拟的MovieLens预测请求，创建*Logger*以异步记录预测输入和结果，并创建*Trigger*以触发重新训练。\n",
    "\n",
    "以下是整个工作流程：\n",
    "1. 初始管道包括以下组件：Generator --> Ingester --> Trainer --> Deployer。此管道只运行一次。\n",
    "2. 然后，Simulator生成预测请求（例如，每5分钟），Logger立即在每个预测请求时调用并异步记录每个预测请求到BigQuery中。Trigger每隔一段时间运行重新训练管道（例如，每30分钟），包括以下组件：Ingester --> Trainer --> Deploy。\n",
    "\n",
    "您可以在此处找到KFP SDK文档：[链接](https://www.kubeflow.org/docs/components/pipelines/sdk/sdk-overview/)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dxTLuuWEGilo"
   },
   "source": [
    "### 创建*生成器*以生成MovieLens模拟数据\n",
    "\n",
    "创建生成器组件，使用MovieLens模拟环境和随机数据采集策略生成初始的训练数据集。将生成的数据存储在BigQuery中。\n",
    "\n",
    "生成器的源代码位于[`src/generator/generator_component.py`](src/generator/generator_component.py)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ay1ztxwIGilo"
   },
   "source": [
    "在Generator组件上运行单元测试\n",
    "\n",
    "在运行命令之前，您应该更新[`src/generator/test_generator_component.py`](src/generator/test_generator_component.py)中的`RAW_DATA_PATH`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R9FQacKtGilo"
   },
   "outputs": [],
   "source": [
    "! python3 -m unittest src.generator.test_generator_component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gpYFPPBOWQP"
   },
   "source": [
    "创建*Ingester*组件以从BigQuery摄取数据，将其打包为`tf.train.Example`对象，并输出TFRecord文件。\n",
    "\n",
    "在[这里](https://www.tensorflow.org/tutorials/load_data/tfrecord)阅读更多关于`tf.train.Example`和TFRecord的信息。\n",
    "\n",
    "Ingester组件的源代码在[`src/ingester/ingester_component.py`](src/ingester/ingester_component.py)中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZQkLU7wyOWQP"
   },
   "source": [
    "在Ingester组件上运行单元测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ej4rNnnEOWQP"
   },
   "outputs": [],
   "source": [
    "! python3 -m unittest src.ingester.test_ingester_component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KFdSpGAWWkL9"
   },
   "source": [
    "创建*Trainer*组件来训练RL策略\n",
    "\n",
    "创建Trainer组件在训练数据集上训练RL策略，然后提交一个远程自定义训练作业到Vertex AI。该组件使用TF-Agents LinUCB代理在MovieLens模拟数据集上训练一个策略，并将训练好的策略保存为一个SavedModel。\n",
    "\n",
    "Trainer组件的源代码位于[`src/trainer/trainer_component.py`](src/trainer/trainer_component.py)。您可以在管道构建中使用其他Vertex AI平台代码，将Trainer中定义的训练代码提交为一个自定义训练作业到Vertex AI。（额外的代码类似于[`kfp.v2.google.experimental.run_as_aiplatform_custom_job`](https://github.com/kubeflow/pipelines/blob/master/sdk/python/kfp/v2/google/experimental/custom_job.py)的操作。您可以在这里找到一个示例笔记本[链接](https://github.com/GoogleCloudPlatform/ai-platform-samples/blob/master/ai-platform-unified/notebooks/official/pipelines/google_cloud_pipeline_components_model_train_upload_deploy.ipynb)，教您如何使用第一方Trainer组件。）\n",
    "\n",
    "Trainer执行离策略训练，即在一个静态的预先收集的数据记录集上训练策略，其中包含观察、动作和奖励等信息。在训练一个数据记录时，策略可能不会根据该数据记录中的观察输出相同的动作。\n",
    "\n",
    "如果您对管道指标感兴趣，请阅读关于[KFP管道指标](https://www.kubeflow.org/docs/components/pipelines/sdk/pipelines-metrics/)的介绍。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fh5S-bcCcHn3"
   },
   "outputs": [],
   "source": [
    "# Trainer parameters\n",
    "TRAINING_ARTIFACTS_DIR = (\n",
    "    f\"{BUCKET_NAME}/artifacts\"  # Root directory for training artifacts.\n",
    ")\n",
    "TRAINING_REPLICA_COUNT = 1  # Number of replica to run the custom training job.\n",
    "TRAINING_MACHINE_TYPE = (\n",
    "    \"n1-standard-4\"  # Type of machine to run the custom training job.\n",
    ")\n",
    "TRAINING_ACCELERATOR_TYPE = \"ACCELERATOR_TYPE_UNSPECIFIED\"  # Type of accelerators to run the custom training job.\n",
    "TRAINING_ACCELERATOR_COUNT = 0  # Number of accelerators for the custom training job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MH3UOVU8WkL9"
   },
   "source": [
    "在Trainer组件上运行单元测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CEJ7_ymvWkL9"
   },
   "outputs": [],
   "source": [
    "! python3 -m unittest src.trainer.test_trainer_component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5cz7h6V4ibYb"
   },
   "source": [
    "### 创建 *Deployer* 将训练好的策略部署到 Vertex AI 终端\n",
    "\n",
    "在管道构建过程中使用 [`google_cloud_pipeline_components.aiplatform`](https://cloud.google.com/vertex-ai/docs/pipelines/build-pipeline#google-cloud-components) 组件来：\n",
    "1. 上传训练好的策略\n",
    "2. 创建 Vertex AI 终端\n",
    "3. 将上传的训练好的策略部署到终端\n",
    "\n",
    "这三个组件形成了 Deployer。它们支持灵活的配置；例如，如果您想为终端设置流量分割以进行 A/B 测试，您可以将您的配置传递给 [google_cloud_pipeline_components.aiplatform.ModelDeployOp](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-0.1.3/google_cloud_pipeline_components.aiplatform.html#google_cloud_pipeline_components.aiplatform.ModelDeployOp)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E7dbkbDMcR-m"
   },
   "outputs": [],
   "source": [
    "# Deployer parameters\n",
    "TRAINED_POLICY_DISPLAY_NAME = (\n",
    "    \"movielens-trained-policy\"  # Display name of the uploaded and deployed policy.\n",
    ")\n",
    "TRAFFIC_SPLIT = {\"0\": 100}\n",
    "ENDPOINT_DISPLAY_NAME = \"movielens-endpoint\"  # Display name of the prediction endpoint.\n",
    "ENDPOINT_MACHINE_TYPE = \"n1-standard-4\"  # Type of machine of the prediction endpoint.\n",
    "ENDPOINT_REPLICA_COUNT = 1  # Number of replicas of the prediction endpoint.\n",
    "ENDPOINT_ACCELERATOR_TYPE = \"ACCELERATOR_TYPE_UNSPECIFIED\"  # Type of accelerators to run the custom training job.\n",
    "ENDPOINT_ACCELERATOR_COUNT = 0  # Number of accelerators for the custom training job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ldr0yDs6ibYb"
   },
   "source": [
    "### 使用 Cloud Build 创建自定义预测容器\n",
    "\n",
    "在设置部署器之前，首先要定义和构建一个自定义预测容器，该容器可以使用训练好的策略进行预测。源代码、Cloud Build YAML 配置文件和 Dockerfile 都位于 `src/prediction_container` 目录中。\n",
    "\n",
    "这个预测容器是用于部署的、经过训练的策略的服务容器。在这里查看有关构建预测自定义容器的更详细指南：[链接](https://github.com/GoogleCloudPlatform/vertex-ai-samples/tree/master/community-content/tf_agents_bandits_movie_recommendation_with_kfp_and_vertex_sdk/step_by_step_sdk_tf_agents_bandits_movie_recommendation/step_by_step_sdk_tf_agents_bandits_movie_recommendation.ipynb)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cKZd3Q_Pcfzo"
   },
   "outputs": [],
   "source": [
    "# Prediction container parameters\n",
    "PREDICTION_CONTAINER = \"prediction-container\"  # Name of the container image.\n",
    "PREDICTION_CONTAINER_DIR = \"src/prediction_container\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jkReLZUAibYb"
   },
   "source": [
    "使用Kaniko构建创建一个Cloud Build YAML文件\n",
    "\n",
    "注意：对于此应用程序，建议您使用E2_HIGHCPU_8或其他高资源机器配置，而不是列在[此处](https://cloud.google.com/build/docs/api/reference/rest/v1/projects.builds#Build.MachineType)的标准机器类型，以防止内存不足错误。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RUvEWUucibYc"
   },
   "outputs": [],
   "source": [
    "cloudbuild_yaml = \"\"\"steps:\n",
    "- name: \"gcr.io/kaniko-project/executor:latest\"\n",
    "  args: [\"--destination=gcr.io/{PROJECT_ID}/{PREDICTION_CONTAINER}:latest\",\n",
    "         \"--cache=true\",\n",
    "         \"--cache-ttl=99h\"]\n",
    "  env: [\"AIP_STORAGE_URI={ARTIFACTS_DIR}\",\n",
    "        \"PROJECT_ID={PROJECT_ID}\",\n",
    "        \"LOGGER_PUBSUB_TOPIC={LOGGER_PUBSUB_TOPIC}\"]\n",
    "options:\n",
    "  machineType: \"E2_HIGHCPU_8\"\n",
    "\"\"\".format(\n",
    "    PROJECT_ID=PROJECT_ID,\n",
    "    PREDICTION_CONTAINER=PREDICTION_CONTAINER,\n",
    "    ARTIFACTS_DIR=TRAINING_ARTIFACTS_DIR,\n",
    "    LOGGER_PUBSUB_TOPIC=LOGGER_PUBSUB_TOPIC,\n",
    ")\n",
    "\n",
    "with open(f\"{PREDICTION_CONTAINER_DIR}/cloudbuild.yaml\", \"w\") as fp:\n",
    "    fp.write(cloudbuild_yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kByjVm5yibYc"
   },
   "source": [
    "在预测代码上运行单元测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tzLU1V6fibYc"
   },
   "outputs": [],
   "source": [
    "! python3 -m unittest src.prediction_container.test_main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RelRBSFvibYc"
   },
   "source": [
    "#### 构建自定义预测容器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9uHbODeXibYd"
   },
   "outputs": [],
   "source": [
    "! gcloud builds submit --config $PREDICTION_CONTAINER_DIR/cloudbuild.yaml $PREDICTION_CONTAINER_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nW154IyqGilq"
   },
   "source": [
    "您可以使用之前部分构建的自定义KFP组件来编写管道，并使用Vertex Pipelines创建管道运行。您可以阅读有关是否启用执行缓存的更多信息，也可以针对训练专门配置工作池规范，例如如果您想要以更大规模和/或更高速度训练，则可以调整副本计数、机器类型、加速器类型和计数以及许多其他规范。\n",
    "\n",
    "在这里，您构建一个“启动”管道，该管道以生成随机抽样的训练数据（通过生成器）作为第一步。此管道仅运行一次。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cvXJzhSSGilq"
   },
   "outputs": [],
   "source": [
    "from google_cloud_pipeline_components.experimental.custom_job import utils\n",
    "from kfp.components import load_component_from_url\n",
    "\n",
    "generate_op = load_component_from_url(\n",
    "    \"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/62a2a7611499490b4b04d731d48a7ba87c2d636f/community-content/tf_agents_bandits_movie_recommendation_with_kfp_and_vertex_sdk/mlops_pipeline_tf_agents_bandits_movie_recommendation/src/generator/component.yaml\"\n",
    ")\n",
    "ingest_op = load_component_from_url(\n",
    "    \"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/62a2a7611499490b4b04d731d48a7ba87c2d636f/community-content/tf_agents_bandits_movie_recommendation_with_kfp_and_vertex_sdk/mlops_pipeline_tf_agents_bandits_movie_recommendation/src/ingester/component.yaml\"\n",
    ")\n",
    "train_op = load_component_from_url(\n",
    "    \"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/62a2a7611499490b4b04d731d48a7ba87c2d636f/community-content/tf_agents_bandits_movie_recommendation_with_kfp_and_vertex_sdk/mlops_pipeline_tf_agents_bandits_movie_recommendation/src/trainer/component.yaml\"\n",
    ")\n",
    "\n",
    "\n",
    "@dsl.pipeline(pipeline_root=PIPELINE_ROOT, name=f\"{PIPELINE_NAME}-startup\")\n",
    "def pipeline(\n",
    "    # Pipeline configs\n",
    "    project_id: str,\n",
    "    raw_data_path: str,\n",
    "    training_artifacts_dir: str,\n",
    "    # BigQuery configs\n",
    "    bigquery_dataset_id: str,\n",
    "    bigquery_location: str,\n",
    "    bigquery_table_id: str,\n",
    "    bigquery_max_rows: int = 10000,\n",
    "    # TF-Agents RL configs\n",
    "    batch_size: int = 8,\n",
    "    rank_k: int = 20,\n",
    "    num_actions: int = 20,\n",
    "    driver_steps: int = 3,\n",
    "    num_epochs: int = 5,\n",
    "    tikhonov_weight: float = 0.01,\n",
    "    agent_alpha: float = 10,\n",
    ") -> None:\n",
    "    \"\"\"Authors a RL pipeline for MovieLens movie recommendation system.\n",
    "\n",
    "    Integrates the Generator, Ingester, Trainer and Deployer components. This\n",
    "    pipeline generates initial training data with a random policy and runs once\n",
    "    as the initiation of the system.\n",
    "\n",
    "    Args:\n",
    "      project_id: GCP project ID. This is required because otherwise the BigQuery\n",
    "        client will use the ID of the tenant GCP project created as a result of\n",
    "        KFP, which doesn't have proper access to BigQuery.\n",
    "      raw_data_path: Path to MovieLens 100K's \"u.data\" file.\n",
    "      training_artifacts_dir: Path to store the Trainer artifacts (trained policy).\n",
    "\n",
    "      bigquery_dataset: A string of the BigQuery dataset ID in the format of\n",
    "        \"project.dataset\".\n",
    "      bigquery_location: A string of the BigQuery dataset location.\n",
    "      bigquery_table_id: A string of the BigQuery table ID in the format of\n",
    "        \"project.dataset.table\".\n",
    "      bigquery_max_rows: Optional; maximum number of rows to ingest.\n",
    "\n",
    "      batch_size: Optional; batch size of environment generated quantities eg.\n",
    "        rewards.\n",
    "      rank_k: Optional; rank for matrix factorization in the MovieLens environment;\n",
    "        also the observation dimension.\n",
    "      num_actions: Optional; number of actions (movie items) to choose from.\n",
    "      driver_steps: Optional; number of steps to run per batch.\n",
    "      num_epochs: Optional; number of training epochs.\n",
    "      tikhonov_weight: Optional; LinUCB Tikhonov regularization weight of the\n",
    "        Trainer.\n",
    "      agent_alpha: Optional; LinUCB exploration parameter that multiplies the\n",
    "        confidence intervals of the Trainer.\n",
    "    \"\"\"\n",
    "    # Run the Generator component.\n",
    "    generate_task = generate_op(\n",
    "        project_id=project_id,\n",
    "        raw_data_path=raw_data_path,\n",
    "        batch_size=batch_size,\n",
    "        rank_k=rank_k,\n",
    "        num_actions=num_actions,\n",
    "        driver_steps=driver_steps,\n",
    "        bigquery_tmp_file=BIGQUERY_TMP_FILE,\n",
    "        bigquery_dataset_id=bigquery_dataset_id,\n",
    "        bigquery_location=bigquery_location,\n",
    "        bigquery_table_id=bigquery_table_id,\n",
    "    )\n",
    "    \n",
    "    # Run the Ingester component.\n",
    "    ingest_task = ingest_op(\n",
    "        project_id=project_id,\n",
    "        bigquery_table_id=generate_task.outputs[\"bigquery_table_id\"],\n",
    "        bigquery_max_rows=bigquery_max_rows,\n",
    "        tfrecord_file=TFRECORD_FILE,\n",
    "    )\n",
    "\n",
    "    # Run the Trainer component and submit custom job to Vertex AI.\n",
    "    # Convert the train_op component into a Vertex AI Custom Job pre-built component\n",
    "    custom_job_training_op = utils.create_custom_training_job_op_from_component(\n",
    "        component_spec=train_op,\n",
    "        replica_count=TRAINING_REPLICA_COUNT,\n",
    "        machine_type=TRAINING_MACHINE_TYPE,\n",
    "        accelerator_type=TRAINING_ACCELERATOR_TYPE,\n",
    "        accelerator_count=TRAINING_ACCELERATOR_COUNT,\n",
    "    )\n",
    "\n",
    "    train_task = custom_job_training_op(\n",
    "        training_artifacts_dir=training_artifacts_dir,\n",
    "        tfrecord_file=ingest_task.outputs[\"tfrecord_file\"],\n",
    "        num_epochs=num_epochs,\n",
    "        rank_k=rank_k,\n",
    "        num_actions=num_actions,\n",
    "        tikhonov_weight=tikhonov_weight,\n",
    "        agent_alpha=agent_alpha,\n",
    "        project=PROJECT_ID,\n",
    "        location=REGION,\n",
    "    )\n",
    "\n",
    "    # Run the Deployer components.\n",
    "    # Upload the trained policy as a model.\n",
    "    model_upload_op = gcc_aip.ModelUploadOp(\n",
    "        project=project_id,\n",
    "        display_name=TRAINED_POLICY_DISPLAY_NAME,\n",
    "        artifact_uri=train_task.outputs[\"training_artifacts_dir\"],\n",
    "        serving_container_image_uri=f\"gcr.io/{PROJECT_ID}/{PREDICTION_CONTAINER}:latest\",\n",
    "    )\n",
    "    # Create a Vertex AI endpoint. (This operation can occur in parallel with\n",
    "    # the Generator, Ingester, Trainer components.)\n",
    "    endpoint_create_op = gcc_aip.EndpointCreateOp(\n",
    "        project=project_id, display_name=ENDPOINT_DISPLAY_NAME\n",
    "    )\n",
    "    # Deploy the uploaded, trained policy to the created endpoint. (This operation\n",
    "    # has to occur after both model uploading and endpoint creation complete.)\n",
    "    gcc_aip.ModelDeployOp(\n",
    "        endpoint=endpoint_create_op.outputs[\"endpoint\"],\n",
    "        model=model_upload_op.outputs[\"model\"],\n",
    "        deployed_model_display_name=TRAINED_POLICY_DISPLAY_NAME,\n",
    "        traffic_split=TRAFFIC_SPLIT,\n",
    "        dedicated_resources_machine_type=ENDPOINT_MACHINE_TYPE,\n",
    "        dedicated_resources_accelerator_type=ENDPOINT_ACCELERATOR_TYPE,\n",
    "        dedicated_resources_accelerator_count=ENDPOINT_ACCELERATOR_COUNT,\n",
    "        dedicated_resources_min_replica_count=ENDPOINT_REPLICA_COUNT,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "icYK0WoRGilr"
   },
   "outputs": [],
   "source": [
    "# Compile the authored pipeline.\n",
    "compiler.Compiler().compile(pipeline_func=pipeline, package_path=PIPELINE_SPEC_PATH)\n",
    "\n",
    "# Create a pipeline run job.\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=f\"{PIPELINE_NAME}-startup\",\n",
    "    template_path=PIPELINE_SPEC_PATH,\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values={\n",
    "        # Pipeline configs\n",
    "        \"project_id\": PROJECT_ID,\n",
    "        \"raw_data_path\": RAW_DATA_PATH,\n",
    "        \"training_artifacts_dir\": TRAINING_ARTIFACTS_DIR,\n",
    "        # BigQuery configs\n",
    "        \"bigquery_dataset_id\": BIGQUERY_DATASET_ID,\n",
    "        \"bigquery_location\": BIGQUERY_LOCATION,\n",
    "        \"bigquery_table_id\": BIGQUERY_TABLE_ID,\n",
    "    },\n",
    "    enable_caching=ENABLE_CACHING,\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_YDDhAx5i1UL"
   },
   "source": [
    "创建*模拟器*来发送模拟的MovieLens预测请求\n",
    "\n",
    "创建模拟器从MovieLens模拟环境中[获取观测](https://github.com/tensorflow/agents/blob/v0.8.0/tf_agents/bandits/environments/movielens_py_environment.py#L118-L125)，对其进行格式化，并向Vertex AI端点发送预测请求。\n",
    "\n",
    "工作流程是：Cloud Scheduler --> Pub/Sub --> Cloud Functions --> Endpoint\n",
    "\n",
    "在生产环境中，这个模拟器逻辑可以修改为收集真实世界输入特征作为观测，从端点获取预测结果，并将这些结果传达给真实世界用户。\n",
    "\n",
    "模拟器源代码是[`src/simulator/main.py`](src/simulator/main.py)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sxz6T0yjcoX2"
   },
   "outputs": [],
   "source": [
    "# Simulator parameters\n",
    "SIMULATOR_PUBSUB_TOPIC = (\n",
    "    \"simulator-pubsub-topic\"  # Pub/Sub topic name for the Simulator.\n",
    ")\n",
    "SIMULATOR_CLOUD_FUNCTION = (\n",
    "    \"simulator-cloud-function\"  # Cloud Functions name for the Simulator.\n",
    ")\n",
    "SIMULATOR_SCHEDULER_JOB = (\n",
    "    \"simulator-scheduler-job\"  # Cloud Scheduler cron job name for the Simulator.\n",
    ")\n",
    "SIMULATOR_SCHEDULE = \"*/5 * * * *\"  # Cloud Scheduler cron job schedule for the Simulator. Eg. \"*/5 * * * *\" means every 5 mins.\n",
    "SIMULATOR_SCHEDULER_MESSAGE = (\n",
    "    \"simulator-message\"  # Cloud Scheduler message for the Simulator.\n",
    ")\n",
    "# TF-Agents RL configs\n",
    "BATCH_SIZE = 8\n",
    "RANK_K = 20\n",
    "NUM_ACTIONS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6JkcpQmpi1UL"
   },
   "source": [
    "在模拟器上运行单元测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GoNH1VS_i1UL"
   },
   "outputs": [],
   "source": [
    "! python3 -m unittest src.simulator.test_main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0g8bl_pCi1UL"
   },
   "source": [
    "创建一个发布/订阅主题\n",
    "\n",
    "- 请点击此处阅读有关创建发布/订阅主题的更多信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5apj2cEri1UL"
   },
   "outputs": [],
   "source": [
    "! gcloud pubsub topics create $SIMULATOR_PUBSUB_TOPIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jA8gMvqXi1UM"
   },
   "source": [
    "###为Pub/Sub话题设置一个定期的Cloud Scheduler作业\n",
    "\n",
    "- 在[这里](https://cloud.google.com/scheduler/docs/creating#gcloud)了解更多关于创建cron作业的可能方式。\n",
    "- 在[这里](https://man7.org/linux/man-pages/man5/crontab.5.html)了解cron作业调度格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "snTyjFkDi1UM"
   },
   "outputs": [],
   "source": [
    "scheduler_job_args = \" \".join(\n",
    "    [\n",
    "        SIMULATOR_SCHEDULER_JOB,\n",
    "        f\"--schedule='{SIMULATOR_SCHEDULE}'\",\n",
    "        f\"--topic={SIMULATOR_PUBSUB_TOPIC}\",\n",
    "        f\"--message-body={SIMULATOR_SCHEDULER_MESSAGE}\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "! echo $scheduler_job_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4pY_Gs_Di1UM"
   },
   "outputs": [],
   "source": [
    "! gcloud scheduler jobs create pubsub $scheduler_job_args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HjyV2Arei1UM"
   },
   "source": [
    "### 在Cloud Function中定义*模拟器*逻辑以便定期触发，并部署该函数\n",
    "\n",
    "- 在[`src/simulator/requirements.txt`](src/simulator/requirements.txt)中指定函数的依赖项。\n",
    "- 详细了解在部署函数时可用的可配置参数 [这里](https://cloud.google.com/sdk/gcloud/reference/functions/deploy)。例如，根据函数的复杂性，您可能需要调整其内存和超时设置。\n",
    "- 注意`ENV_VARS`中的环境变量应使用逗号分隔；之间不应有额外的空格或其他字符。了解更多关于设置/更新/删除环境变量的信息，请访问 这里](https://cloud.google.com/functions/docs/env-var)。\n",
    "- 了解有关将预测发送到Vertex端点的更多信息，请访问[这里](https://cloud.google.com/vertex-ai/docs/predictions/online-predictions-custom-models)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0LJ2C_pdibYg"
   },
   "outputs": [],
   "source": [
    "endpoints = ! gcloud ai endpoints list \\\n",
    "    --region=$REGION \\\n",
    "    --filter=display_name=$ENDPOINT_DISPLAY_NAME\n",
    "print(\"\\n\".join(endpoints), \"\\n\")\n",
    "\n",
    "ENDPOINT_ID = endpoints[2].split(\" \")[0]\n",
    "print(f\"ENDPOINT_ID={ENDPOINT_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V4xwXBBgi1UM"
   },
   "outputs": [],
   "source": [
    "ENV_VARS = \",\".join(\n",
    "    [\n",
    "        f\"PROJECT_ID={PROJECT_ID}\",\n",
    "        f\"REGION={REGION}\",\n",
    "        f\"ENDPOINT_ID={ENDPOINT_ID}\",\n",
    "        f\"RAW_DATA_PATH={RAW_DATA_PATH}\",\n",
    "        f\"BATCH_SIZE={BATCH_SIZE}\",\n",
    "        f\"RANK_K={RANK_K}\",\n",
    "        f\"NUM_ACTIONS={NUM_ACTIONS}\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "! echo $ENV_VARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0yiHxUMBi1UM"
   },
   "outputs": [],
   "source": [
    "! gcloud functions deploy $SIMULATOR_CLOUD_FUNCTION \\\n",
    "    --region=$REGION \\\n",
    "    --trigger-topic=$SIMULATOR_PUBSUB_TOPIC \\\n",
    "    --runtime=python37 \\\n",
    "    --memory=512MB \\\n",
    "    --timeout=200s \\\n",
    "    --source=src/simulator \\\n",
    "    --entry-point=simulate \\\n",
    "    --stage-bucket=$BUCKET_NAME \\\n",
    "    --update-env-vars=$ENV_VARS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "213JWEcLxAhN"
   },
   "source": [
    "创建*Logger*以异步记录预测输入和结果\n",
    "\n",
    "创建Logger来根据预测观察和预测行为从MovieLens模拟环境中获得环境反馈作为奖励，制定轨迹数据，并将数据存储回BigQuery。Logger关闭了从预测到训练数据的强化学习反馈循环，并允许在新的训练数据上重新训练策略。\n",
    "\n",
    "Logger由预测代码中的挂钩触发。在每个预测请求时，预测代码会向一个Pub/Sub主题发送消息，触发Logger代码。\n",
    "\n",
    "工作流程是：预测容器代码（在预测请求时）--> Pub/Sub --> 云函数（将预测记录回写到BigQuery）\n",
    "\n",
    "在生产环境中，此Logger逻辑可以修改为基于观察和预测行为收集现实世界反馈（奖励）。\n",
    "\n",
    "Logger源代码是[`src/logger/main.py`](src/logger/main.py)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gIuPCKRjxAhN"
   },
   "source": [
    "在Logger上运行单元测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-eVdF88gxAhN"
   },
   "outputs": [],
   "source": [
    "! python3 -m unittest src.logger.test_main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hs56EW17xAhO"
   },
   "source": [
    "创建一个pub/sub主题\n",
    "\n",
    "- 了解更多关于创建pub/sub主题的信息，请点击[这里](https://cloud.google.com/functions/docs/tutorials/pubsub)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ydoCTizJxAhO"
   },
   "outputs": [],
   "source": [
    "! gcloud pubsub topics create $LOGGER_PUBSUB_TOPIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FnlsMxfjxAhO"
   },
   "source": [
    "将*Logger*逻辑定义为Cloud Function，由Pub/Sub主题触发，该主题由每个预测请求时的预测代码触发。\n",
    "\n",
    "- 在[`src/logger/requirements.txt`](src/logger/requirements.txt)中指定函数的依赖关系。\n",
    "- 了解有关部署函数的可配置参数的更多信息，请点击[此处](https://cloud.google.com/sdk/gcloud/reference/functions/deploy)。例如，根据函数的复杂性，您可能需要调整其内存和超时设置。\n",
    "- 请注意，在`ENV_VARS`中的环境变量应以逗号分隔；中间不应有额外的空格或其他字符。请点击此处了解有关设置/更新/删除环境变量的更多信息： [在这里](https://cloud.google.com/functions/docs/env-var)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DwrukBPHxAhO"
   },
   "outputs": [],
   "source": [
    "ENV_VARS = \",\".join(\n",
    "    [\n",
    "        f\"PROJECT_ID={PROJECT_ID}\",\n",
    "        f\"RAW_DATA_PATH={RAW_DATA_PATH}\",\n",
    "        f\"BATCH_SIZE={BATCH_SIZE}\",\n",
    "        f\"RANK_K={RANK_K}\",\n",
    "        f\"NUM_ACTIONS={NUM_ACTIONS}\",\n",
    "        f\"BIGQUERY_TMP_FILE={BIGQUERY_TMP_FILE}\",\n",
    "        f\"BIGQUERY_DATASET_ID={BIGQUERY_DATASET_ID}\",\n",
    "        f\"BIGQUERY_LOCATION={BIGQUERY_LOCATION}\",\n",
    "        f\"BIGQUERY_TABLE_ID={BIGQUERY_TABLE_ID}\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "! echo $ENV_VARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OykKRkScxAhO"
   },
   "outputs": [],
   "source": [
    "! gcloud functions deploy $LOGGER_CLOUD_FUNCTION \\\n",
    "    --region=$REGION \\\n",
    "    --trigger-topic=$LOGGER_PUBSUB_TOPIC \\\n",
    "    --runtime=python37 \\\n",
    "    --memory=512MB \\\n",
    "    --timeout=200s \\\n",
    "    --source=src/logger \\\n",
    "    --entry-point=log \\\n",
    "    --stage-bucket=$BUCKET_NAME \\\n",
    "    --update-env-vars=$ENV_VARS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n0YSz3xtJcci"
   },
   "source": [
    "## 创建*触发器*来触发重新训练\n",
    "\n",
    "使用`kfp.v2.google.client.AIPlatformClient.create_schedule_from_job_spec`创建一个触发器，以周期性地重新运行管道，使用新的训练数据重新训练策略。您可以在Vertex Pipelines上创建一个用于编排的管道，以及一个Cloud Scheduler作业，用于定期触发管道。该方法还会自动创建一个作为调度器和管道之间中介的Cloud Function。您可以在[这里](https://github.com/kubeflow/pipelines/blob/v1.7.0-alpha.3/sdk/python/kfp/v2/google/client/client.py#L347-L391)找到源代码。\n",
    "\n",
    "当模拟器向端点发送预测请求时，Logger会被预测代码中的钩子触发，将预测结果记录到BigQuery中，作为新的训练数据。由于该管道有一个定期的调度，它会利用新的训练数据来训练新的策略，从而闭合反馈循环。从理论上讲，如果您将管道调度器设置为无限频繁，那么您将逼近实时的连续训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-YmLQ-ykJcci"
   },
   "outputs": [],
   "source": [
    "TRIGGER_SCHEDULE = \"*/30 * * * *\"  # Schedule to trigger the pipeline. Eg. \"*/30 * * * *\" means every 30 mins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ay1x-rgIJcci"
   },
   "outputs": [],
   "source": [
    "ingest_op = load_component_from_url(\n",
    "    \"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/62a2a7611499490b4b04d731d48a7ba87c2d636f/community-content/tf_agents_bandits_movie_recommendation_with_kfp_and_vertex_sdk/mlops_pipeline_tf_agents_bandits_movie_recommendation/src/ingester/component.yaml\"\n",
    ")\n",
    "train_op = load_component_from_url(\n",
    "    \"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/62a2a7611499490b4b04d731d48a7ba87c2d636f/community-content/tf_agents_bandits_movie_recommendation_with_kfp_and_vertex_sdk/mlops_pipeline_tf_agents_bandits_movie_recommendation/src/trainer/component.yaml\"\n",
    ")\n",
    "\n",
    "\n",
    "@dsl.pipeline(pipeline_root=PIPELINE_ROOT, name=f\"{PIPELINE_NAME}-retraining\")\n",
    "def pipeline(\n",
    "    # Pipeline configs\n",
    "    project_id: str,\n",
    "    training_artifacts_dir: str,\n",
    "    # BigQuery configs\n",
    "    bigquery_table_id: str,\n",
    "    bigquery_max_rows: int = 10000,\n",
    "    # TF-Agents RL configs\n",
    "    rank_k: int = 20,\n",
    "    num_actions: int = 20,\n",
    "    num_epochs: int = 5,\n",
    "    tikhonov_weight: float = 0.01,\n",
    "    agent_alpha: float = 10,\n",
    ") -> None:\n",
    "    \"\"\"Authors a re-training pipeline for MovieLens movie recommendation system.\n",
    "\n",
    "    Integrates the Ingester, Trainer and Deployer components.\n",
    "\n",
    "    Args:\n",
    "      project_id: GCP project ID. This is required because otherwise the BigQuery\n",
    "        client will use the ID of the tenant GCP project created as a result of\n",
    "        KFP, which doesn't have proper access to BigQuery.\n",
    "      training_artifacts_dir: Path to store the Trainer artifacts (trained policy).\n",
    "\n",
    "      bigquery_table_id: A string of the BigQuery table ID in the format of\n",
    "        \"project.dataset.table\".\n",
    "      bigquery_max_rows: Optional; maximum number of rows to ingest.\n",
    "\n",
    "      rank_k: Optional; rank for matrix factorization in the MovieLens environment;\n",
    "        also the observation dimension.\n",
    "      num_actions: Optional; number of actions (movie items) to choose from.\n",
    "      num_epochs: Optional; number of training epochs.\n",
    "      tikhonov_weight: Optional; LinUCB Tikhonov regularization weight of the\n",
    "        Trainer.\n",
    "      agent_alpha: Optional; LinUCB exploration parameter that multiplies the\n",
    "        confidence intervals of the Trainer.\n",
    "    \"\"\"\n",
    "    # Run the Ingester component.\n",
    "    ingest_task = ingest_op(\n",
    "        project_id=project_id,\n",
    "        bigquery_table_id=bigquery_table_id,\n",
    "        bigquery_max_rows=bigquery_max_rows,\n",
    "        tfrecord_file=TFRECORD_FILE,\n",
    "    )\n",
    "\n",
    "    # Run the Trainer component and submit custom job to Vertex AI.\n",
    "    # Convert the train_op component into a Vertex AI Custom Job pre-built component\n",
    "    custom_job_training_op = utils.create_custom_training_job_op_from_component(\n",
    "        component_spec=train_op,\n",
    "        replica_count=TRAINING_REPLICA_COUNT,\n",
    "        machine_type=TRAINING_MACHINE_TYPE,\n",
    "        accelerator_type=TRAINING_ACCELERATOR_TYPE,\n",
    "        accelerator_count=TRAINING_ACCELERATOR_COUNT,\n",
    "    )\n",
    "\n",
    "    train_task = custom_job_training_op(\n",
    "        training_artifacts_dir=training_artifacts_dir,\n",
    "        tfrecord_file=ingest_task.outputs[\"tfrecord_file\"],\n",
    "        num_epochs=num_epochs,\n",
    "        rank_k=rank_k,\n",
    "        num_actions=num_actions,\n",
    "        tikhonov_weight=tikhonov_weight,\n",
    "        agent_alpha=agent_alpha,\n",
    "        project=PROJECT_ID,\n",
    "        location=REGION,\n",
    "    )\n",
    "\n",
    "    # Run the Deployer components.\n",
    "    # Upload the trained policy as a model.\n",
    "    model_upload_op = gcc_aip.ModelUploadOp(\n",
    "        project=project_id,\n",
    "        display_name=TRAINED_POLICY_DISPLAY_NAME,\n",
    "        artifact_uri=train_task.outputs[\"training_artifacts_dir\"],\n",
    "        serving_container_image_uri=f\"gcr.io/{PROJECT_ID}/{PREDICTION_CONTAINER}:latest\",\n",
    "    )\n",
    "    # Create a Vertex AI endpoint. (This operation can occur in parallel with\n",
    "    # the Generator, Ingester, Trainer components.)\n",
    "    endpoint_create_op = gcc_aip.EndpointCreateOp(\n",
    "        project=project_id, display_name=ENDPOINT_DISPLAY_NAME\n",
    "    )\n",
    "    # Deploy the uploaded, trained policy to the created endpoint. (This operation\n",
    "    # has to occur after both model uploading and endpoint creation complete.)\n",
    "    gcc_aip.ModelDeployOp(\n",
    "        endpoint=endpoint_create_op.outputs[\"endpoint\"],\n",
    "        model=model_upload_op.outputs[\"model\"],\n",
    "        deployed_model_display_name=TRAINED_POLICY_DISPLAY_NAME,\n",
    "        dedicated_resources_machine_type=ENDPOINT_MACHINE_TYPE,\n",
    "        dedicated_resources_accelerator_type=ENDPOINT_ACCELERATOR_TYPE,\n",
    "        dedicated_resources_accelerator_count=ENDPOINT_ACCELERATOR_COUNT,\n",
    "        dedicated_resources_min_replica_count=ENDPOINT_REPLICA_COUNT,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9yPjcm75Jcci"
   },
   "outputs": [],
   "source": [
    "# Compile the authored pipeline.\n",
    "compiler.Compiler().compile(pipeline_func=pipeline, package_path=PIPELINE_SPEC_PATH)\n",
    "\n",
    "# Createa Vertex AI client.\n",
    "api_client = AIPlatformClient(project_id=PROJECT_ID, region=REGION)\n",
    "\n",
    "# Schedule a recurring pipeline.\n",
    "response = api_client.create_schedule_from_job_spec(\n",
    "    job_spec_path=PIPELINE_SPEC_PATH,\n",
    "    schedule=TRIGGER_SCHEDULE,\n",
    "    parameter_values={\n",
    "        # Pipeline configs\n",
    "        \"project_id\": PROJECT_ID,\n",
    "        \"training_artifacts_dir\": TRAINING_ARTIFACTS_DIR,\n",
    "        # BigQuery config\n",
    "        \"bigquery_table_id\": BIGQUERY_TABLE_ID,\n",
    "    },\n",
    ")\n",
    "response[\"name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "清理\n",
    "\n",
    "要清理此项目中使用的所有Google Cloud资源，您可以删除用于教程的[Google Cloud项目](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects)。\n",
    "\n",
    "否则，您可以删除在本教程中创建的各个资源（您还需要清理其他难以删除的资源，如BigQuery中的所有/部分数据，定期的流水线及其调度作业，上传的策略/模型等）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sx_vKniMq9ZX"
   },
   "outputs": [],
   "source": [
    "# Delete endpoint resource.\n",
    "! gcloud ai endpoints delete $ENDPOINT_ID --quiet --region $REGION\n",
    "\n",
    "# Delete Pub/Sub topics.\n",
    "! gcloud pubsub topics delete $SIMULATOR_PUBSUB_TOPIC --quiet\n",
    "! gcloud pubsub topics delete $LOGGER_PUBSUB_TOPIC --quiet\n",
    "\n",
    "# Delete Cloud Functions.\n",
    "! gcloud functions delete $SIMULATOR_CLOUD_FUNCTION --quiet\n",
    "! gcloud functions delete $LOGGER_CLOUD_FUNCTION --quiet\n",
    "\n",
    "# Delete Scheduler job.\n",
    "! gcloud scheduler jobs delete $SIMULATOR_SCHEDULER_JOB --quiet\n",
    "\n",
    "# Delete Cloud Storage objects that were created.\n",
    "! gsutil -m rm -r $PIPELINE_ROOT\n",
    "! gsutil -m rm -r $TRAINING_ARTIFACTS_DIR"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "mlops_pipeline_tf_agents_bandits_movie_recommendation.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
