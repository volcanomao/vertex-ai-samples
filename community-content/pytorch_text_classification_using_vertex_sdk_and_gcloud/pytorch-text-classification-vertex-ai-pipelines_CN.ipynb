{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "<!-- <table align=\"left\">\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/ai-platform-samples/blob/master/ai-platform-unified/notebooks/notebook_template.ipynb\"\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> 在 Colab 上运行\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/ai-platform-samples/blob/master/ai-platform-unified/notebooks/notebook_template.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      在 GitHub 上查看\n",
    "    </a>\n",
    "  </td>\n",
    "</table> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3yTsQctS8QLd"
   },
   "source": [
    "使用Vertex AI Pipelines协调ML工作流程，训练和部署PyTorch文本分类模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## 概述\n",
    "\n",
    "此笔记本是对[之前的笔记本](./pytorch-text-classification-vertex-ai-train-tune-deploy.ipynb)的扩展，用于微调和部署[HuggingFace Hub](https://huggingface.co/bert-base-cased)上的预训练的BERT模型，以进行情感分类任务。此笔记本展示了如何通过在无服务器方式下使用[Vertex AI Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines/introduction)编排流水线来自动化和监控基于PyTorch的ML工作流程。\n",
    "\n",
    "该笔记本使用[Kubeflow Pipelines v2 (`kfp.v2`) SDK](https://www.kubeflow.org/docs/components/pipelines/sdk-v2/)定义了一个流水线，并将该流水线提交到Vertex AI Pipelines服务中。\n",
    "\n",
    "### 数据集\n",
    "\n",
    "该笔记本使用来自[Hugging Face Datasets](https://huggingface.co/datasets)的[IMDB电影评论数据集](https://huggingface.co/datasets/imdb)。\n",
    "\n",
    "### 目标\n",
    "\n",
    "如何在[Vertex AI](https://cloud.google.com/vertex-ai)上编排PyTorch ML工作流程，并强调对在Vertex AI上训练、部署和编排PyTorch工作流程提供一流支持。\n",
    "\n",
    "### 目录\n",
    "\n",
    "此笔记本涵盖以下部分：\n",
    "\n",
    "---\n",
    "- [构建流水线的高级流程](#High-Level-Flow-of-Building-a-Pipeline): 理解流水线概念和流水线示意图\n",
    "- [定义流水线组件](#Define-the-Pipeline-Components-for-PyTorch-based-ML-Workflow): 为基于PyTorch的ML工作流程编写自定义流水线组件\n",
    "- [定义流水线规范](#Define-Pipeline-Specification): 使用KFP v2 SDK为基于PyTorch的ML工作流程编写流水线规范\n",
    "- [提交流水线](#Submit-Pipeline): 在Vertex AI Pipelines上编译和执行流水线\n",
    "- [监控流水线](#Monitoring-the-Pipeline): 监控流水线的进度，并查看日志、谱系、工件和流水线运行\n",
    "---\n",
    "\n",
    "### 成本\n",
    "\n",
    "本教程使用Google Cloud Platform (GCP) 的计费组件：\n",
    "\n",
    "* [Vertex AI Workbench](https://cloud.google.com/vertex-ai-workbench)\n",
    "* [Vertex AI Training](https://cloud.google.com/vertex-ai/docs/training/custom-training)\n",
    "* [Vertex AI Predictions](https://cloud.google.com/vertex-ai/docs/predictions/getting-predictions)\n",
    "* [Vertex AI Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines/introduction)\n",
    "* [Cloud Storage](https://cloud.google.com/storage)\n",
    "* [Container Registry](https://cloud.google.com/container-registry)\n",
    "* [Cloud Build](https://cloud.google.com/build) *[可选]*\n",
    "\n",
    "了解[Vertex AI Pricing](https://cloud.google.com/vertex-ai/pricing)、[Cloud Storage Pricing](https://cloud.google.com/storage/pricing)和[Cloud Build Pricing](https://cloud.google.com/build/pricing)以及使用[定价计算器](https://cloud.google.com/products/calculator/)根据您的预期使用生成费用估计。\n",
    "\n",
    "***\n",
    "**注意：** 此笔记本不需要GPU运行时。但是，您必须拥有足够的GPU配额来运行由流水线启动的带有GPU的作业。请检查[配额](https://console.cloud.google.com/iam-admin/quotas)页面，确保您的项目中有足够的GPU可用。如果GPU未在配额页面列出，或者您需要额外的GPU配额，请[请求增加配额](https://cloud.google.com/compute/quotas#requesting_additional_quota)。免费试用账户默认不会收到GPU配额。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ze4-nDLfK4pw"
   },
   "source": [
    "### 设置您的本地开发环境\n",
    "\n",
    "**如果您正在使用Colab或Google云笔记本**，您的环境已经符合运行此笔记本的所有要求。您可以跳过这一步。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCuSR8GkAgzl"
   },
   "source": [
    "否则，请确保您的环境符合此笔记本的要求。\n",
    "您需要以下内容：\n",
    "\n",
    "* Google Cloud SDK\n",
    "* Git\n",
    "* Python 3\n",
    "* virtualenv\n",
    "* 在使用Python 3的虚拟环境中运行的Jupyter笔记本\n",
    "\n",
    "Google Cloud的[设置Python开发环境指南](https://cloud.google.com/python/setup)和[Jupyter安装指南](https://jupyter.org/install)提供了详细的说明以满足这些要求。以下步骤提供了一套简明的说明：\n",
    "\n",
    "1. [安装并初始化Cloud SDK。](https://cloud.google.com/sdk/docs/)\n",
    "2. [安装Python 3。](https://cloud.google.com/python/setup#installing_python)\n",
    "3. [安装virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv)并创建一个使用Python 3的虚拟环境。激活虚拟环境。\n",
    "4. 要安装Jupyter，请在终端shell中运行`pip3 install jupyter`命令。\n",
    "5. 要启动Jupyter，请在终端shell中运行`jupyter notebook`命令。\n",
    "6. 在Jupyter Notebook Dashboard中打开此笔记本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "### 安装额外的包\n",
    "\n",
    "以下是此笔记本所需的 Python 依赖项，将在笔记本实例中安装。\n",
    "\n",
    "- [Kubeflow Pipelines v2 SDK](https://pypi.org/project/kfp/)\n",
    "- [Google Cloud Pipeline Components](https://pypi.org/project/google-cloud-pipeline-components/) \n",
    "- [Vertex AI SDK for Python](https://pypi.org/project/google-cloud-aiplatform/) \n",
    "\n",
    "---\n",
    "该笔记本已使用以下版本的 Kubeflow Pipelines SDK 和 Google Cloud Pipeline Components 进行测试\n",
    "\n",
    "```\n",
    "kfp 版本: 1.8.10\n",
    "google_cloud_pipeline_components 版本: 0.2.2\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IaYsrh0Tc17L"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Google Cloud Notebook product has specific requirements\n",
    "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "\n",
    "# Google Cloud Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "34243b3c5bae"
   },
   "outputs": [],
   "source": [
    "!pip -q install {USER_FLAG} --upgrade kfp\n",
    "!pip -q install {USER_FLAG} --upgrade google-cloud-pipeline-components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76ed78b69ae0"
   },
   "source": [
    "安装Python的Vertex AI SDK。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0a96cc9f4156"
   },
   "source": [
    "这个笔记本使用[Python的Vertex AI SDK](https://cloud.google.com/vertex-ai/docs/start/client-libraries#python)与Vertex AI服务进行交互。这个高级别的`google-cloud-aiplatform`库旨在通过使用包装类和明确的默认值来简化常见的数据科学工作流程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aR7LNYMUCVKc"
   },
   "outputs": [],
   "source": [
    "!pip -q install {USER_FLAG} --upgrade google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhq5zEbGg0XX"
   },
   "source": [
    "### 重新启动内核\n",
    "\n",
    "在安装完附加软件包后，您需要重新启动笔记本内核，以便它可以找到这些软件包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EzrelQZ22IZj"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6GPgNN7eeX1l"
   },
   "source": [
    "请检查您安装的软件包版本。KFP SDK 版本应该大于等于1.6。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NN0mULkEeb84"
   },
   "outputs": [],
   "source": [
    "!python3 -c \"import kfp; print('kfp version: {}'.format(kfp.__version__))\"\n",
    "!python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWEdiXsJg0XY"
   },
   "source": [
    "在开始之前\n",
    "\n",
    "这本笔记本不需要GPU运行时。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF1j6f9HApxa"
   },
   "source": [
    "### 设置您的Google Cloud项目\n",
    "\n",
    "**无论您的笔记本环境如何，都需要按照以下步骤操作。**\n",
    "\n",
    "1. [选择或创建一个Google Cloud项目](https://console.cloud.google.com/cloud-resource-manager)。当您首次创建账号时，您将获得$300的免费信用额，可用于支付计算/存储成本。\n",
    "1. [确保您的项目已启用计费](https://cloud.google.com/billing/docs/how-to/modify-project)。\n",
    "1. 在您的项目中启用以下运行本教程所需的API\n",
    "    - [Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com)\n",
    "    - [Cloud Storage API](https://console.cloud.google.com/flows/enableapi?apiid=storage.googleapis.com)\n",
    "    - [Container Registry API](https://console.cloud.google.com/flows/enableapi?apiid=containerregistry.googleapis.com)\n",
    "    - [Cloud Build API](https://console.cloud.google.com/flows/enableapi?apiid=cloudbuild.googleapis.com)\n",
    "1. 如果您在本地运行此笔记本，您需要安装[Cloud SDK](https://cloud.google.com/sdk)。\n",
    "1. 在下面的单元格中输入您的项目ID。然后运行该单元格，以确保Cloud SDK在本笔记本中的所有命令中使用正确的项目。\n",
    "\n",
    "**注意**：Jupyter在以`!`前缀开头的行中运行shell命令，并将以`$`前缀开头的Python变量插入这些命令中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WReHDGG5g0XY"
   },
   "source": [
    "#### 设置您的项目ID\n",
    "\n",
    "**如果您不知道您的项目ID**，您可以使用`gcloud`来获取您的项目ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ab5f23615e4a"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # <---CHANGE THIS TO YOUR PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oM1iC_MfAts1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get your Google Cloud project ID using google.auth\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    import google.auth\n",
    "\n",
    "    _, PROJECT_ID = google.auth.default()\n",
    "    print(\"Project ID: \", PROJECT_ID)\n",
    "\n",
    "# validate PROJECT_ID\n",
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    print(\n",
    "        f\"Please set your project id before proceeding to next step. Currently it's set as {PROJECT_ID}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06571eb4063b"
   },
   "source": [
    "时间戳\n",
    "\n",
    "如果您正在参加实时教程会话，您可能正在使用共享的测试账号或项目。为了避免用户之间在创建的资源上发生名称冲突，您可以为每个实例会话创建一个时间戳，并将其附加到您在本教程中创建的资源的名称上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "697568e92bd6"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def get_timestamp():\n",
    "    return datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "\n",
    "TIMESTAMP = get_timestamp()\n",
    "print(f\"TIMESTAMP = {TIMESTAMP}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dr--iN2kAylZ"
   },
   "source": [
    "### 验证您的谷歌云账户\n",
    "\n",
    "---\n",
    "\n",
    "**如果您正在使用谷歌云笔记本**，则您的环境已经经过验证。请跳过此步骤。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBCra4QMA2wR"
   },
   "source": [
    "如果您正在使用Colab，请运行下面的单元格并按照提示进行oAuth身份验证。\n",
    "\n",
    "否则，请按照以下步骤操作：\n",
    "\n",
    "1. 在Cloud Console中，转到[**创建服务帐号密钥**页面](https://console.cloud.google.com/apis/credentials/serviceaccountkey)。\n",
    "2. 点击**创建服务帐号**。\n",
    "3. 在**服务帐号名称**字段中输入一个名称，然后点击**创建**。\n",
    "4. 在**授予此服务帐号对项目的访问权限**部分，点击**角色**下拉列表。在过滤框中键入“Vertex AI”，然后选择**Vertex AI管理员**。在过滤框中键入“Storage Object Admin”，然后选择**存储对象管理员**。\n",
    "5. 点击*创建*。包含您密钥的JSON文件将下载到本地环境中。\n",
    "6. 在下面的单元格中将您的服务帐号密钥路径输入为`GOOGLE_APPLICATION_CREDENTIALS`变量，并运行该单元格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PyQmSRbKA8r-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "# The Google Cloud Notebook product has specific requirements\n",
    "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "\n",
    "# If on Google Cloud Notebooks, then don't execute this code\n",
    "if not IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\"):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NxhCPW6e46EF"
   },
   "source": [
    "### 创建一个云存储桶\n",
    "\n",
    "**无论您使用哪种笔记本环境，以下步骤是必需的。**\n",
    "\n",
    "使用 Cloud SDK 提交训练作业时，您需要将包含训练代码的 Python 包上传到一个云存储桶中。Vertex AI 将从该包中运行代码。在本教程中，Vertex AI 还会将您作业的训练模型保存在同一个云存储桶中。使用此模型工件，您可以创建 Vertex AI 模型和端点资源，以便提供在线预测。\n",
    "\n",
    "在下方设置您的云存储桶的名称。它必须在所有云存储桶中是唯一的。\n",
    "\n",
    "您还可以更改 `REGION` 变量，该变量在笔记本的后续操作中使用。请确保 [选择 Vertex AI 服务可用的区域](https://cloud.google.com/vertex-ai/docs/general/locations#available_regions)。您不能使用多区域存储桶进行 Vertex AI 训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kZPew6MljTcP"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"gs://[your-bucket-name]\"  # @param {type:\"string\"}\n",
    "REGION = \"us-central1\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cf221059d072"
   },
   "outputs": [],
   "source": [
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"gs://[your-bucket-name]\":\n",
    "    BUCKET_NAME = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EcIXiGsCePi"
   },
   "source": [
    "只有在您的存储桶尚不存在时，才运行以下单元格以创建您的云存储存储桶。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIq7R4HZCfIc"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucvCsknMCims"
   },
   "source": [
    "最后，通过检查云存储桶的内容来验证访问权限。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vhOb7YnwClBb"
   },
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e3EQyqZiEMmf"
   },
   "source": [
    "###导入库并定义常量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lNV3Jd8BEMmj"
   },
   "source": [
    "导入运行管道所需的python库，并定义常量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "99c88db5877c"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lnz2aQ_EEMmk"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import NamedTuple\n",
    "\n",
    "import google_cloud_pipeline_components\n",
    "import kfp\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import gapic as aip\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "from google_cloud_pipeline_components import aiplatform as aip_components\n",
    "from google_cloud_pipeline_components.experimental import custom_job\n",
    "from kfp.v2 import compiler, dsl\n",
    "from kfp.v2.dsl import Input, Metrics, Model, Output, component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4366d8966fc7"
   },
   "outputs": [],
   "source": [
    "APP_NAME = \"finetuned-bert-classifier\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UwT_YZM6W5Pj"
   },
   "outputs": [],
   "source": [
    "PATH = %env PATH\n",
    "%env PATH={PATH}:/home/jupyter/.local/bin\n",
    "\n",
    "# Pipeline root is the GCS path to store the artifacts from the pipeline runs\n",
    "PIPELINE_ROOT = f\"{BUCKET_NAME}/pipeline_root/{APP_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7da19337f842"
   },
   "outputs": [],
   "source": [
    "print(f\"Kubeflow Pipelines SDK version = {kfp.__version__}\")\n",
    "print(\n",
    "    f\"Google Cloud Pipeline Components version = {google_cloud_pipeline_components.__version__}\"\n",
    ")\n",
    "print(f\"Pipeline Root = {PIPELINE_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ba9ab90f9590"
   },
   "source": [
    "## 构建流水线的高级流程\n",
    "\n",
    "以下是在 Vertex AI Pipelines 上定义和提交流水线的高级流程：\n",
    "\n",
    "1. 定义涉及训练和部署 PyTorch 模型的流水线组件\n",
    "2. 通过将预构建的 [Google Cloud 流水线组件](https://cloud.google.com/vertex-ai/docs/pipelines/components-introduction) 和自定义组件串联起来，定义一个流水线\n",
    "3. 编译并提交流水线到 Vertex AI Pipelines 服务以运行工作流程\n",
    "4. 监视流水线并分析生成的指标和工件\n",
    "\n",
    "![流水线的高级流程](./images/pipelines-high-level-flow.png)\n",
    "\n",
    "本笔记构建在先前开发的训练和服务代码之上，可在此 [笔记本](../pytorch-text-classification-vertex-ai-train-tune-deploy.ipynb)中找到。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e9e6c2c62ac9"
   },
   "source": [
    "### Pipeline的概念\n",
    "\n",
    "让我们来看看[Kubeflow Pipelines SDK v2](https://www.kubeflow.org/docs/components/pipelines/sdk-v2/)中使用的术语和概念。\n",
    "\n",
    "![pipeline的概念](./images/concepts-of-a-pipeline.png)\n",
    "\n",
    "- **组件：** 组件是在ML工作流中执行单个任务的一组自包含代码。 例如，训练一个模型。组件接口由输入、输出和容器镜像组成，该组件的代码在其中运行 - 包括可执行代码和环境定义。\n",
    "- **Pipeline：** 流水线由模块化任务组成，这些任务定义为通过输入和输出链接在一起的组件。 Pipeline定义包括运行管道所需的参数等配置。 流水线中的每个组件都是独立执行的，数据（输入和输出）以序列化格式在组件之间传递。\n",
    "- **输入和输出：** 组件的输入和输出必须用数据类型进行标注，这使得输入或输出成为参数或工件。\n",
    "    - **参数：** 参数是支持简单数据类型的输入或输出，例如 `str`、`int`、`float`、`bool`、`dict`、`list`。 输入参数始终在组件之间按值传递，并存储在[Vertex ML Metadata](https://cloud.google.com/vertex-ai/docs/ml-metadata/introduction)服务中。\n",
    "    - **工件：** 工件是流水线运行生成的对象或文件的引用，作为输入或输出传递。 工件支持各种较为复杂或较大的数据类型，如数据集、模型、指标、可视化，这些数据以文件或对象的形式写入。 工件由名称、URI和元数据定义，元数据自动存储在Vertex ML Metadata服务中，工件的实际内容引用云存储桶中的路径。 输入工件始终通过引用传递。\n",
    "\n",
    "在这里了解更多关于KFP SDK v2的概念。 [此处](https://www.kubeflow.org/docs/components/pipelines/sdk-v2/)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99dbf92cbad3"
   },
   "source": [
    "### 管道示意图\n",
    "\n",
    "以下是基于PyTorch的文本分类模型管道的高级示意图，包括管道中涉及的任务和输入输出：\n",
    "\n",
    "![PyTorch文本分类模型的管道示意图](./images/pipeline-schematic-pytorch-text-classification.png)\n",
    "\n",
    "- **构建自定义训练镜像：** 此步骤从训练应用代码和相关Dockerfile以及依赖项构建自定义训练容器镜像。此步骤的输出是自定义训练容器的容器或工件注册表URI。\n",
    "- **运行自定义训练任务以训练和评估模型：** 此步骤从HuggingFace上下载和预处理IMDB情感分类数据集的训练数据，然后在前一步的自定义训练容器上训练和评估模型。该步骤的输出是训练模型工件的Cloud Storage路径和模型性能指标。\n",
    "- **打包模型工件：** 此步骤使用Torch Model Archiver工具打包训练过的模型工件，包括自定义预测处理程序，以创建一个模型存档（.mar）文件。此步骤的输出是GCS上模型存档（.mar）文件的位置。\n",
    "- **构建自定义服务镜像：** 该步骤构建一个自定义服务容器，运行TorchServe HTTP服务器以为挂载的模型提供预测请求服务。此步骤的输出是自定义服务容器的容器或工件注册表URI。\n",
    "- **使用自定义服务容器上传模型：** 此步骤使用前述步骤中的自定义服务镜像和MAR文件创建一个模型资源。\n",
    "- **创建端点：** 此步骤创建一个Vertex AI端点，提供一个服务URL，用于发送预测请求。\n",
    "- **将模型部署到端点提供服务：** 此步骤将模型部署到创建的端点，创建必要的计算资源（根据配置的机器规格）以提供在线预测请求的服务。\n",
    "- **验证部署：** 此步骤向端点发送测试请求并验证部署。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZzmFXOmeXLiT"
   },
   "source": [
    "## 定义基于PyTorch的ML工作流的管道组件\n",
    "\n",
    "该管道使用了来自[Google Cloud Pipeline Components SDK](https://cloud.google.com/vertex-ai/docs/pipelines/components-introduction)的预构建组件，与Google Cloud服务如Vertex AI进行交互，并针对管道中的某些步骤定义自定义组件。本笔记本的本节定义了使用[KFP SDK v2组件规范](https://www.kubeflow.org/docs/components/pipelines/sdk-v2/component-development/)执行管道中任务的自定义组件。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0be5d976b66"
   },
   "source": [
    "将管道目录创建在本地以保存组件和管道规范。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e456d56c58dc"
   },
   "outputs": [],
   "source": [
    "!mkdir -p ./pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7b335530b41"
   },
   "source": [
    "### 1. 组件：构建自定义训练容器镜像\n",
    "\n",
    "这一步使用Cloud Build构建自定义训练容器镜像。构建作业从GCS位置拉取训练应用代码和相关的`Dockerfile`，然后将自定义训练容器镜像构建/推送到容器注册表。\n",
    "\n",
    "- **输入**：该组件的输入是训练应用代码和Dockerfile的GCS路径。\n",
    "- **输出**：此步骤的输出是自定义训练容器的容器或Artifact注册表URI。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "897960d83aba"
   },
   "source": [
    "使用PyTorch GPU镜像作为基础创建`Dockerfile`，安装所需的依赖项并复制训练应用程序代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d7d97b0bfcac"
   },
   "outputs": [],
   "source": [
    "%%writefile ./custom_container/Dockerfile\n",
    "\n",
    "# Use pytorch GPU base image\n",
    "# FROM gcr.io/cloud-aiplatform/training/pytorch-gpu.1-7\n",
    "FROM us-docker.pkg.dev/vertex-ai/training/pytorch-gpu.1-10:latest\n",
    "\n",
    "# set working directory\n",
    "WORKDIR /app\n",
    "\n",
    "# Install required packages\n",
    "RUN pip install google-cloud-storage transformers datasets tqdm cloudml-hypertune\n",
    "\n",
    "# Copies the trainer code to the docker image.\n",
    "COPY ./trainer/__init__.py /app/trainer/__init__.py\n",
    "COPY ./trainer/experiment.py /app/trainer/experiment.py\n",
    "COPY ./trainer/utils.py /app/trainer/utils.py\n",
    "COPY ./trainer/metadata.py /app/trainer/metadata.py\n",
    "COPY ./trainer/model.py /app/trainer/model.py\n",
    "COPY ./trainer/task.py /app/trainer/task.py\n",
    "\n",
    "# Set up the entry point to invoke the trainer.\n",
    "ENTRYPOINT [\"python\", \"-m\", \"trainer.task\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7c0f5c0ecf56"
   },
   "source": [
    "将培训应用程序代码和 `Dockerfile` 从本地路径复制到 GCS 位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4f72a2298ba8"
   },
   "outputs": [],
   "source": [
    "# copy training Dockerfile\n",
    "!gsutil cp ./custom_container/Dockerfile {BUCKET_NAME}/{APP_NAME}/train/\n",
    "\n",
    "# copy training application code\n",
    "!gsutil cp -r ./python_package/trainer/ {BUCKET_NAME}/{APP_NAME}/train/\n",
    "\n",
    "# list copied files from GCS location\n",
    "!gsutil ls -Rl {BUCKET_NAME}/{APP_NAME}/train/\n",
    "\n",
    "print(\n",
    "    f\"Copied training application code and Dockerfile to {BUCKET_NAME}/{APP_NAME}/train/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6d95036b54b6"
   },
   "source": [
    "定义自定义管道组件来构建自定义训练容器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1f08f04d404b"
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"gcr.io/google.com/cloudsdktool/cloud-sdk:latest\",\n",
    "    packages_to_install=[\"google-cloud-build\"],\n",
    "    output_component_file=\"./pipelines/build_custom_train_image.yaml\",\n",
    ")\n",
    "def build_custom_train_image(\n",
    "    project: str, gs_train_src_path: str, training_image_uri: str\n",
    ") -> NamedTuple(\"Outputs\", [(\"training_image_uri\", str)]):\n",
    "    \"\"\"custom pipeline component to build custom training image using\n",
    "    Cloud Build and the training application code and dependencies\n",
    "    defined in the Dockerfile\n",
    "    \"\"\"\n",
    "\n",
    "    import logging\n",
    "    import os\n",
    "\n",
    "    from google.cloud.devtools import cloudbuild_v1 as cloudbuild\n",
    "    from google.protobuf.duration_pb2 import Duration\n",
    "\n",
    "    # initialize client for cloud build\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    build_client = cloudbuild.services.cloud_build.CloudBuildClient()\n",
    "\n",
    "    # parse step inputs to get path to Dockerfile and training application code\n",
    "    gs_dockerfile_path = os.path.join(gs_train_src_path, \"Dockerfile\")\n",
    "    gs_train_src_path = os.path.join(gs_train_src_path, \"trainer/\")\n",
    "\n",
    "    logging.info(f\"training_image_uri: {training_image_uri}\")\n",
    "\n",
    "    # define build steps to pull the training code and Dockerfile\n",
    "    # and build/push the custom training container image\n",
    "    build = cloudbuild.Build()\n",
    "    build.steps = [\n",
    "        {\n",
    "            \"name\": \"gcr.io/cloud-builders/gsutil\",\n",
    "            \"args\": [\"cp\", \"-r\", gs_train_src_path, \".\"],\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"gcr.io/cloud-builders/gsutil\",\n",
    "            \"args\": [\"cp\", gs_dockerfile_path, \"Dockerfile\"],\n",
    "        },\n",
    "        # enabling Kaniko cache in a Docker build that caches intermediate\n",
    "        # layers and pushes image automatically to Container Registry\n",
    "        # https://cloud.google.com/build/docs/kaniko-cache\n",
    "        {\n",
    "            \"name\": \"gcr.io/kaniko-project/executor:latest\",\n",
    "            \"args\": [f\"--destination={training_image_uri}\", \"--cache=true\"],\n",
    "        },\n",
    "    ]\n",
    "    # override default timeout of 10min\n",
    "    timeout = Duration()\n",
    "    timeout.seconds = 7200\n",
    "    build.timeout = timeout\n",
    "\n",
    "    # create build\n",
    "    operation = build_client.create_build(project_id=project, build=build)\n",
    "    logging.info(\"IN PROGRESS:\")\n",
    "    logging.info(operation.metadata)\n",
    "\n",
    "    # get build status\n",
    "    result = operation.result()\n",
    "    logging.info(\"RESULT:\", result.status)\n",
    "\n",
    "    # return step outputs\n",
    "    return (training_image_uri,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0f33cfadfbb0"
   },
   "source": [
    "关于组件规范，有几点需要注意：\n",
    "- 定义的独立函数会被转换为一个流水线组件，使用[`@kfp.v2.dsl.component`](https://github.com/kubeflow/pipelines/blob/master/sdk/python/kfp/v2/components/component_decorator.py) 装饰器。\n",
    "- 所有独立函数中的参数必须有数据类型注释，因为KFP使用函数的输入和输出来定义组件的接口。\n",
    "- 默认情况下，Python 3.7被用作基础镜像来运行定义的代码。您可以[配置`@component`装饰器](https://www.kubeflow.org/docs/components/pipelines/sdk-v2/python-function-components/#building-python-function-based-components) 来覆盖默认镜像，通过指定`base_image`来安装额外的python包使用`packages_to_install`参数，并使用`output_component_file`将编译后的组件文件写成一个YAML文件以供共享或重用组件。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "39b9add646fe"
   },
   "source": [
    "### 2. 组件：从Vertex AI获取定制培训作业详情\n",
    "\n",
    "此步骤从Vertex AI获取定制培训作业的详情，包括培训经过的时间，模型性能指标，这些将在模型部署前的下一步中使用。该步骤还创建了一个[Model](https://github.com/kubeflow/pipelines/blob/master/sdk/python/kfp/v2/components/types/artifact_types.py#L77)工件，其中包含培训模型的工件。\n",
    "\n",
    "**注意：**管道中使用的预构建[custom job component](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-0.2.1/google_cloud_pipeline_components.experimental.custom_job.html)输出CustomJob资源，但不包括模型工件。\n",
    "\n",
    "- **输入**：\n",
    "    - **`job_resource`：** 由预构建的CustomJob组件返回的自定义作业资源\n",
    "    - **`project`：** 作业运行的项目ID\n",
    "    - **`region`：** 作业运行的区域\n",
    "    - **`eval_metric_key`：** 评估指标键名，例如eval_accuracy\n",
    "    - **`model_display_name`：** 保存模型工件的模型显示名称\n",
    "\n",
    "- **输出**： \n",
    "    - **`model`：** 通过培训作业创建的具有附加模型元数据的训练模型工件\n",
    "    - **`metrics`：** 从培训作业中捕获的模型性能指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b25a7b6a558c"
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\n",
    "        \"google-cloud-pipeline-components\",\n",
    "        \"google-cloud-aiplatform\",\n",
    "        \"pandas\",\n",
    "        \"fsspec\",\n",
    "    ],\n",
    "    output_component_file=\"./pipelines/get_training_job_details.yaml\",\n",
    ")\n",
    "def get_training_job_details(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    job_resource: str,\n",
    "    eval_metric_key: str,\n",
    "    model_display_name: str,\n",
    "    metrics: Output[Metrics],\n",
    "    model: Output[Model],\n",
    ") -> NamedTuple(\n",
    "    \"Outputs\", [(\"eval_metric\", float), (\"eval_loss\", float), (\"model_artifacts\", str)]\n",
    "):\n",
    "    \"\"\"custom pipeline component to get model artifacts and performance\n",
    "    metrics from custom training job\n",
    "    \"\"\"\n",
    "    import logging\n",
    "    import shutil\n",
    "    from collections import namedtuple\n",
    "\n",
    "    import pandas as pd\n",
    "    from google.cloud.aiplatform import gapic as aip\n",
    "    from google.protobuf.json_format import Parse\n",
    "    from google_cloud_pipeline_components.proto.gcp_resources_pb2 import \\\n",
    "        GcpResources\n",
    "\n",
    "    # parse training job resource\n",
    "    logging.info(f\"Custom job resource = {job_resource}\")\n",
    "    training_gcp_resources = Parse(job_resource, GcpResources())\n",
    "    custom_job_id = training_gcp_resources.resources[0].resource_uri\n",
    "    custom_job_name = \"/\".join(custom_job_id.split(\"/\")[-6:])\n",
    "    logging.info(f\"Custom job name parsed = {custom_job_name}\")\n",
    "\n",
    "    # get custom job information\n",
    "    API_ENDPOINT = \"{}-aiplatform.googleapis.com\".format(location)\n",
    "    client_options = {\"api_endpoint\": API_ENDPOINT}\n",
    "    job_client = aip.JobServiceClient(client_options=client_options)\n",
    "    job_resource = job_client.get_custom_job(name=custom_job_name)\n",
    "    job_base_dir = job_resource.job_spec.base_output_directory.output_uri_prefix\n",
    "    logging.info(f\"Custom job base output directory = {job_base_dir}\")\n",
    "\n",
    "    # copy model artifacts\n",
    "    logging.info(f\"Copying model artifacts to {model.path}\")\n",
    "    destination = shutil.copytree(job_base_dir.replace(\"gs://\", \"/gcs/\"), model.path)\n",
    "    logging.info(destination)\n",
    "    logging.info(f\"Model artifacts located at {model.uri}/model/{model_display_name}\")\n",
    "    logging.info(f\"Model artifacts located at model.uri = {model.uri}\")\n",
    "\n",
    "    # set model metadata\n",
    "    start, end = job_resource.start_time, job_resource.end_time\n",
    "    model.metadata[\"model_name\"] = model_display_name\n",
    "    model.metadata[\"framework\"] = \"pytorch\"\n",
    "    model.metadata[\"job_name\"] = custom_job_name\n",
    "    model.metadata[\"time_to_train_in_seconds\"] = (end - start).total_seconds()\n",
    "\n",
    "    # fetch metrics from the training job run\n",
    "    metrics_uri = f\"{model.path}/model/{model_display_name}/all_results.json\"\n",
    "    logging.info(f\"Reading and logging metrics from {metrics_uri}\")\n",
    "    metrics_df = pd.read_json(metrics_uri, typ=\"series\")\n",
    "    for k, v in metrics_df.items():\n",
    "        logging.info(f\"     {k} -> {v}\")\n",
    "        metrics.log_metric(k, v)\n",
    "\n",
    "    # capture eval metric and log to model metadata\n",
    "    eval_metric = (\n",
    "        metrics_df[eval_metric_key] if eval_metric_key in metrics_df.keys() else None\n",
    "    )\n",
    "    eval_loss = metrics_df[\"eval_loss\"] if \"eval_loss\" in metrics_df.keys() else None\n",
    "    logging.info(f\"     {eval_metric_key} -> {eval_metric}\")\n",
    "    logging.info(f'     \"eval_loss\" -> {eval_loss}')\n",
    "\n",
    "    model.metadata[eval_metric_key] = eval_metric\n",
    "    model.metadata[\"eval_loss\"] = eval_loss\n",
    "\n",
    "    # return output parameters\n",
    "    outputs = namedtuple(\"Outputs\", [\"eval_metric\", \"eval_loss\", \"model_artifacts\"])\n",
    "\n",
    "    return outputs(eval_metric, eval_loss, job_base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "578b7d62c023"
   },
   "source": [
    "### 3. 组件：使用Torch Model Archiver创建模型存档（MAR）文件\n",
    "\n",
    "此步骤使用[Torch Model Archiver](https://github.com/pytorch/serve/tree/master/model-archiver)工具打包经过训练的模型工件和自定义预测处理程序（在先前的笔记本中定义）作为模型存档（.mar）文件。\n",
    "\n",
    "- **输入**：\n",
    "    - **`model_display_name`：** 用于保存模型存档文件的模型显示名称\n",
    "    - **`model_version`：** 用于保存模型存档文件的模型版本\n",
    "    - **`handler`：** 自定义预测处理程序的位置\n",
    "    - **`model`：** 来自上一步的经过训练的模型工件\n",
    "\n",
    "- **输出**：\n",
    "    - **`model_mar`**：GCS上打包的模型存档文件（工件）\n",
    "    - **`mar_env`**：创建模型资源所需的环境变量列表\n",
    "    - **`mar_export_uri`**：模型存档文件的GCS路径"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13d4215ddabd"
   },
   "source": [
    "将自定义预测处理程序代码从本地路径复制到 GCS 位置\n",
    "\n",
    "**注意**：自定义预测处理程序在 [上一个笔记本](./pytorch-text-classification-vertex-ai-train-tune-deploy.ipynb) 中定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9b6c854aa4dd"
   },
   "outputs": [],
   "source": [
    "# copy custom prediction handler\n",
    "!gsutil cp ./predictor/custom_handler.py ./predictor/index_to_name.json {BUCKET_NAME}/{APP_NAME}/serve/predictor/\n",
    "\n",
    "# list copied files from GCS location\n",
    "!gsutil ls -lR {BUCKET_NAME}/{APP_NAME}/serve/\n",
    "\n",
    "print(f\"Copied custom prediction handler code to {BUCKET_NAME}/{APP_NAME}/serve/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5a77290959a"
   },
   "source": [
    "定义自定义管道组件以创建模型存档文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cbc6e4a0f9a3"
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\"torch-model-archiver\"],\n",
    "    output_component_file=\"./pipelines/generate_mar_file.yaml\",\n",
    ")\n",
    "def generate_mar_file(\n",
    "    model_display_name: str,\n",
    "    model_version: str,\n",
    "    handler: str,\n",
    "    model: Input[Model],\n",
    "    model_mar: Output[Model],\n",
    ") -> NamedTuple(\"Outputs\", [(\"mar_env_var\", list), (\"mar_export_uri\", str)]):\n",
    "    \"\"\"custom pipeline component to package model artifacts and custom\n",
    "    handler to a model archive file using Torch Model Archiver tool\n",
    "    \"\"\"\n",
    "    import logging\n",
    "    import os\n",
    "    import subprocess\n",
    "    import time\n",
    "    from collections import namedtuple\n",
    "    from pathlib import Path\n",
    "\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "    # create directory to save model archive file\n",
    "    model_output_root = model.path\n",
    "    mar_output_root = model_mar.path\n",
    "    export_path = f\"{mar_output_root}/model-store\"\n",
    "    try:\n",
    "        Path(export_path).mkdir(parents=True, exist_ok=True)\n",
    "    except Exception as e:\n",
    "        logging.warning(e)\n",
    "        # retry after pause\n",
    "        time.sleep(2)\n",
    "        Path(export_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # parse and configure paths for model archive config\n",
    "    handler_path = (\n",
    "        handler.replace(\"gs://\", \"/gcs/\") + \"predictor/custom_handler.py\"\n",
    "        if handler.startswith(\"gs://\")\n",
    "        else handler\n",
    "    )\n",
    "    model_artifacts_dir = f\"{model_output_root}/model/{model_display_name}\"\n",
    "    extra_files = [\n",
    "        os.path.join(model_artifacts_dir, f)\n",
    "        for f in os.listdir(model_artifacts_dir)\n",
    "        if f != \"pytorch_model.bin\"\n",
    "    ]\n",
    "\n",
    "    # define model archive config\n",
    "    mar_config = {\n",
    "        \"MODEL_NAME\": model_display_name,\n",
    "        \"HANDLER\": handler_path,\n",
    "        \"SERIALIZED_FILE\": f\"{model_artifacts_dir}/pytorch_model.bin\",\n",
    "        \"VERSION\": model_version,\n",
    "        \"EXTRA_FILES\": \",\".join(extra_files),\n",
    "        \"EXPORT_PATH\": f\"{model_mar.path}/model-store\",\n",
    "    }\n",
    "\n",
    "    # generate model archive command\n",
    "    archiver_cmd = (\n",
    "        \"torch-model-archiver --force \"\n",
    "        f\"--model-name {mar_config['MODEL_NAME']} \"\n",
    "        f\"--serialized-file {mar_config['SERIALIZED_FILE']} \"\n",
    "        f\"--handler {mar_config['HANDLER']} \"\n",
    "        f\"--version {mar_config['VERSION']}\"\n",
    "    )\n",
    "    if \"EXPORT_PATH\" in mar_config:\n",
    "        archiver_cmd += f\" --export-path {mar_config['EXPORT_PATH']}\"\n",
    "    if \"EXTRA_FILES\" in mar_config:\n",
    "        archiver_cmd += f\" --extra-files {mar_config['EXTRA_FILES']}\"\n",
    "    if \"REQUIREMENTS_FILE\" in mar_config:\n",
    "        archiver_cmd += f\" --requirements-file {mar_config['REQUIREMENTS_FILE']}\"\n",
    "\n",
    "    # run archiver command\n",
    "    logging.warning(\"Running archiver command: %s\", archiver_cmd)\n",
    "    with subprocess.Popen(\n",
    "        archiver_cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE\n",
    "    ) as p:\n",
    "        _, err = p.communicate()\n",
    "        if err:\n",
    "            raise ValueError(err)\n",
    "\n",
    "    # set output variables\n",
    "    mar_env_var = [{\"name\": \"MODEL_NAME\", \"value\": model_display_name}]\n",
    "    mar_export_uri = f\"{model_mar.uri}/model-store/\"\n",
    "\n",
    "    outputs = namedtuple(\"Outputs\", [\"mar_env_var\", \"mar_export_uri\"])\n",
    "    return outputs(mar_env_var, mar_export_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "895e91cb6487"
   },
   "source": [
    "### 4. 组件：创建自定义运行TorchServe的服务容器\n",
    "\n",
    "这一步构建一个[自定义服务容器](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements)，运行[TorchServe](https://pytorch.org/serve/) HTTP服务器，用于为挂载的模型提供预测请求。此步骤的输出是自定义服务容器的容器注册表URI。\n",
    "\n",
    "- **输入**：\n",
    "    - **`project`：** 要运行的项目ID\n",
    "    - **`serving_image_uri`：** 容器注册表中自定义服务容器的URI\n",
    "    - **`gs_serving_dependencies_path`：** 服务依赖项的位置 - Dockerfile\n",
    "- **输出**： \n",
    "    - **`serving_image_uri`：** 容器注册表中自定义服务容器的URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4366fa221967"
   },
   "source": [
    "将TorchServe CPU镜像作为基础创建`Dockerfile`，安装所需的依赖项并运行TorchServe serve命令。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "be7320901415"
   },
   "outputs": [],
   "source": [
    "%%bash -s $APP_NAME\n",
    "\n",
    "APP_NAME=$1\n",
    "\n",
    "cat << EOF > ./predictor/Dockerfile.serve\n",
    "\n",
    "FROM pytorch/torchserve:latest-cpu\n",
    "\n",
    "USER root\n",
    "# run and update some basic packages software packages, including security libs\n",
    "RUN apt-get update && \\\n",
    "    apt-get install -y software-properties-common && \\\n",
    "    add-apt-repository -y ppa:ubuntu-toolchain-r/test && \\\n",
    "    apt-get update && \\\n",
    "    apt-get install -y gcc-9 g++-9 apt-transport-https ca-certificates gnupg curl\n",
    "\n",
    "# Install gcloud tools for gsutil as well as debugging\n",
    "RUN echo \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] http://packages.cloud.google.com/apt cloud-sdk main\" | \\\n",
    "    tee -a /etc/apt/sources.list.d/google-cloud-sdk.list && \\\n",
    "    curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | \\\n",
    "    apt-key --keyring /usr/share/keyrings/cloud.google.gpg add - && \\\n",
    "    apt-get update -y && \\\n",
    "    apt-get install google-cloud-sdk -y\n",
    "\n",
    "USER model-server\n",
    "\n",
    "# install dependencies\n",
    "RUN python3 -m pip install --upgrade pip\n",
    "RUN pip3 install transformers\n",
    "\n",
    "ARG MODEL_NAME=$APP_NAME\n",
    "ENV MODEL_NAME=\"\\${MODEL_NAME}\"\n",
    "\n",
    "# health and prediction listener ports\n",
    "ARG AIP_HTTP_PORT=7080\n",
    "ENV AIP_HTTP_PORT=\"\\${AIP_HTTP_PORT}\"\n",
    "\n",
    "ARG MODEL_MGMT_PORT=7081\n",
    "\n",
    "# expose health and prediction listener ports from the image\n",
    "EXPOSE \"\\${AIP_HTTP_PORT}\"\n",
    "EXPOSE \"\\${MODEL_MGMT_PORT}\"\n",
    "EXPOSE 8080 8081 8082 7070 7071\n",
    "\n",
    "# create torchserve configuration file\n",
    "USER root\n",
    "RUN echo \"service_envelope=json\\n\" \\\n",
    "    \"inference_address=http://0.0.0.0:\\${AIP_HTTP_PORT}\\n\" \\\n",
    "    \"management_address=http://0.0.0.0:\\${MODEL_MGMT_PORT}\" >> \\\n",
    "    /home/model-server/config.properties\n",
    "USER model-server\n",
    "\n",
    "# run Torchserve HTTP serve to respond to prediction requests\n",
    "CMD [\"echo\", \"AIP_STORAGE_URI=\\${AIP_STORAGE_URI}\", \";\", \\\n",
    "    \"gsutil\", \"cp\", \"-r\", \"\\${AIP_STORAGE_URI}/\\${MODEL_NAME}.mar\", \"/home/model-server/model-store/\", \";\", \\\n",
    "    \"ls\", \"-ltr\", \"/home/model-server/model-store/\", \";\", \\\n",
    "    \"torchserve\", \"--start\", \"--ts-config=/home/model-server/config.properties\", \\\n",
    "    \"--models\", \"\\${MODEL_NAME}=\\${MODEL_NAME}.mar\", \\\n",
    "    \"--model-store\", \"/home/model-server/model-store\"]\n",
    "EOF\n",
    "\n",
    "echo \"Writing ./predictor/Dockerfile\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "16c3bd57152c"
   },
   "source": [
    "将`Dockerfile`从本地路径复制到GCS位置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bf8aced15cea"
   },
   "outputs": [],
   "source": [
    "# copy serving Dockerfile\n",
    "!gsutil cp ./predictor/Dockerfile.serve {BUCKET_NAME}/{APP_NAME}/serve/\n",
    "\n",
    "# list copied files from GCS location\n",
    "!gsutil ls -lR {BUCKET_NAME}/{APP_NAME}/serve/\n",
    "\n",
    "print(f\"Copied serving Dockerfile to {BUCKET_NAME}/{APP_NAME}/serve/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ccfc5380998e"
   },
   "source": [
    "定义自定义管道组件以构建自定义服务容器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5c65003f4d9c"
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\"google-cloud-build\"],\n",
    "    output_component_file=\"./pipelines/build_custom_serving_image.yaml\",\n",
    ")\n",
    "def build_custom_serving_image(\n",
    "    project: str, gs_serving_dependencies_path: str, serving_image_uri: str\n",
    ") -> NamedTuple(\"Outputs\", [(\"serving_image_uri\", str)],):\n",
    "    \"\"\"custom pipeline component to build custom serving image using\n",
    "    Cloud Build and dependencies defined in the Dockerfile\n",
    "    \"\"\"\n",
    "    import logging\n",
    "    import os\n",
    "\n",
    "    from google.cloud.devtools import cloudbuild_v1 as cloudbuild\n",
    "    from google.protobuf.duration_pb2 import Duration\n",
    "\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    build_client = cloudbuild.services.cloud_build.CloudBuildClient()\n",
    "\n",
    "    logging.info(f\"gs_serving_dependencies_path: {gs_serving_dependencies_path}\")\n",
    "    gs_dockerfile_path = os.path.join(gs_serving_dependencies_path, \"Dockerfile.serve\")\n",
    "\n",
    "    logging.info(f\"serving_image_uri: {serving_image_uri}\")\n",
    "    build = cloudbuild.Build()\n",
    "    build.steps = [\n",
    "        {\n",
    "            \"name\": \"gcr.io/cloud-builders/gsutil\",\n",
    "            \"args\": [\"cp\", gs_dockerfile_path, \"Dockerfile\"],\n",
    "        },\n",
    "        # enabling Kaniko cache in a Docker build that caches intermediate\n",
    "        # layers and pushes image automatically to Container Registry\n",
    "        # https://cloud.google.com/build/docs/kaniko-cache\n",
    "        {\n",
    "            \"name\": \"gcr.io/kaniko-project/executor:latest\",\n",
    "            \"args\": [f\"--destination={serving_image_uri}\", \"--cache=true\"],\n",
    "        },\n",
    "    ]\n",
    "    # override default timeout of 10min\n",
    "    timeout = Duration()\n",
    "    timeout.seconds = 7200\n",
    "    build.timeout = timeout\n",
    "\n",
    "    # create build\n",
    "    operation = build_client.create_build(project_id=project, build=build)\n",
    "    logging.info(\"IN PROGRESS:\")\n",
    "    logging.info(operation.metadata)\n",
    "\n",
    "    # get build status\n",
    "    result = operation.result()\n",
    "    logging.info(\"RESULT:\", result.status)\n",
    "\n",
    "    # return step outputs\n",
    "    return (serving_image_uri,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d26c6e423061"
   },
   "source": [
    "### 5. 组件：测试模型部署并进行在线预测请求\n",
    "\n",
    "此步骤将向 Vertex AI 端点发送测试请求，并通过发送测试预测请求来验证部署。当从模型服务器返回文本情感时，部署被认为是成功的。\n",
    "\n",
    "- **输入**：\n",
    "    - **`project`：** 要运行的项目 ID\n",
    "    - **`bucket`：** 暂存 GCS 存储桶路径\n",
    "    - **`endpoint`：** Vertex AI 端点的位置，来自端点创建任务\n",
    "    - **`instances`：** 测试预测请求的列表\n",
    "- **输出**：\n",
    "    - None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ebc57f1a6567"
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\"google-cloud-aiplatform\", \"google-cloud-pipeline-components\"],\n",
    "    output_component_file=\"./pipelines/make_prediction_request.yaml\",\n",
    ")\n",
    "def make_prediction_request(project: str, bucket: str, endpoint: str, instances: list):\n",
    "    \"\"\"custom pipeline component to pass prediction requests to Vertex AI\n",
    "    endpoint and get responses\n",
    "    \"\"\"\n",
    "    import base64\n",
    "    import logging\n",
    "\n",
    "    from google.cloud import aiplatform\n",
    "    from google.protobuf.json_format import Parse\n",
    "    from google_cloud_pipeline_components.proto.gcp_resources_pb2 import \\\n",
    "        GcpResources\n",
    "\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    aiplatform.init(project=project, staging_bucket=bucket)\n",
    "\n",
    "    # parse endpoint resource\n",
    "    logging.info(f\"Endpoint = {endpoint}\")\n",
    "    gcp_resources = Parse(endpoint, GcpResources())\n",
    "    endpoint_uri = gcp_resources.resources[0].resource_uri\n",
    "    endpoint_id = \"/\".join(endpoint_uri.split(\"/\")[-8:-2])\n",
    "    logging.info(f\"Endpoint ID = {endpoint_id}\")\n",
    "\n",
    "    # define endpoint client\n",
    "    _endpoint = aiplatform.Endpoint(endpoint_id)\n",
    "\n",
    "    # call prediction endpoint for each instance\n",
    "    for instance in instances:\n",
    "        if not isinstance(instance, (bytes, bytearray)):\n",
    "            instance = instance.encode()\n",
    "        logging.info(f\"Input text: {instance.decode('utf-8')}\")\n",
    "        b64_encoded = base64.b64encode(instance)\n",
    "        test_instance = [{\"data\": {\"b64\": f\"{str(b64_encoded.decode('utf-8'))}\"}}]\n",
    "        response = _endpoint.predict(instances=test_instance)\n",
    "        logging.info(f\"Prediction response: {response.predictions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1df6e7be96e"
   },
   "source": [
    "## 定义管道规范\n",
    "\n",
    "管道定义描述了输入和输出参数以及构件在步骤之间如何传递。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "811b8073ea11"
   },
   "source": [
    "设置环境变量\n",
    "\n",
    "这些环境变量将用于定义资源规格，如训练作业、模型资源等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1294bfa799cf"
   },
   "outputs": [],
   "source": [
    "os.environ[\"PROJECT_ID\"] = PROJECT_ID\n",
    "os.environ[\"BUCKET\"] = BUCKET_NAME\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"APP_NAME\"] = APP_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cdf5b473744f"
   },
   "source": [
    "创建管道配置文件\n",
    "\n",
    "管道配置文件有助于将管道模板化，从而使其可以使用不同的参数运行相同的管道。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6cf0b34d5edb"
   },
   "outputs": [],
   "source": [
    "%%writefile ./pipelines/pipeline_config.py\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "PROJECT_ID = os.getenv(\"PROJECT_ID\", \"\")\n",
    "BUCKET = os.getenv(\"BUCKET\", \"\")\n",
    "REGION = os.getenv(\"REGION\", \"us-central1\")\n",
    "\n",
    "APP_NAME = os.getenv(\"APP_NAME\", \"finetuned-bert-classifier\")\n",
    "VERSION = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "MODEL_NAME = APP_NAME\n",
    "MODEL_DISPLAY_NAME = f\"{MODEL_NAME}-{VERSION}\"\n",
    "\n",
    "PIPELINE_NAME = f\"pytorch-{APP_NAME}\"\n",
    "PIPELINE_ROOT = f\"{BUCKET}/pipeline_root/{MODEL_NAME}\"\n",
    "GCS_STAGING = f\"{BUCKET}/pipeline_root/{MODEL_NAME}\"\n",
    "\n",
    "TRAIN_IMAGE_URI = f\"gcr.io/{PROJECT_ID}/pytorch_gpu_train_{MODEL_NAME}\"\n",
    "SERVE_IMAGE_URI = f\"gcr.io/{PROJECT_ID}/pytorch_cpu_predict_{MODEL_NAME}\"\n",
    "\n",
    "MACHINE_TYPE = \"n1-standard-8\"\n",
    "REPLICA_COUNT = \"1\"\n",
    "ACCELERATOR_TYPE = \"NVIDIA_TESLA_T4\"\n",
    "ACCELERATOR_COUNT = \"1\"\n",
    "NUM_WORKERS = 1\n",
    "\n",
    "SERVING_HEALTH_ROUTE = \"/ping\"\n",
    "SERVING_PREDICT_ROUTE = f\"/predictions/{MODEL_NAME}\"\n",
    "SERVING_CONTAINER_PORT= [{\"containerPort\": 7080}]\n",
    "SERVING_MACHINE_TYPE = \"n1-standard-4\"\n",
    "SERVING_MIN_REPLICA_COUNT = 1\n",
    "SERVING_MAX_REPLICA_COUNT=1\n",
    "SERVING_TRAFFIC_SPLIT='{\"0\": 100}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12e5eeeb25c4"
   },
   "source": [
    "定义管道规范\n",
    "\n",
    "管道被定义为一个独立的Python函数，使用[`@kfp.dsl.pipeline`](https://github.com/kubeflow/pipelines/blob/master/sdk/python/kfp/v2/components/pipeline_context.py)修饰符标记，指定了管道的名称和存储管道工件的根路径。\n",
    "\n",
    "管道定义包括预构建的组件和自定义定义的组件：\n",
    "- 来自[Google Cloud Pipeline Components SDK](https://cloud.google.com/vertex-ai/docs/pipelines/components-introduction)的预构建组件用于调用Vertex AI服务的任务，如提交自定义训练作业（`custom_job.CustomTrainingJobOp`）、上传模型（`ModelUploadOp`）、创建端点（`EndpointCreateOp`）以及将模型部署到端点（`ModelDeployOp`）\n",
    "- 自定义组件用于构建用于训练的自定义容器任务（`build_custom_train_image`）、获取训练作业详细信息（`get_training_job_details`）、创建mar文件（`generate_mar_file`）和服务（`build_custom_serving_image`）以及验证模型部署任务（`ake_prediction_request`）。有关这些任务的自定义组件规范，请参考笔记本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7f7acf482906"
   },
   "outputs": [],
   "source": [
    "from pipelines import pipeline_config as cfg\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=cfg.PIPELINE_NAME,\n",
    "    pipeline_root=cfg.PIPELINE_ROOT,\n",
    ")\n",
    "def pytorch_text_classifier_pipeline(\n",
    "    pipeline_job_id: str,\n",
    "    gs_train_script_path: str,\n",
    "    gs_serving_dependencies_path: str,\n",
    "    eval_acc_threshold: float,\n",
    "    is_hp_tuning_enabled: str = \"n\",\n",
    "):\n",
    "    # ========================================================================\n",
    "    # build custom training container image\n",
    "    # ========================================================================\n",
    "    # build custom container for training job passing the\n",
    "    # GCS location of the training application code\n",
    "    build_custom_train_image_task = (\n",
    "        build_custom_train_image(\n",
    "            project=cfg.PROJECT_ID,\n",
    "            gs_train_src_path=gs_train_script_path,\n",
    "            training_image_uri=cfg.TRAIN_IMAGE_URI,\n",
    "        )\n",
    "        .set_caching_options(True)\n",
    "        .set_display_name(\"Build custom training image\")\n",
    "    )\n",
    "\n",
    "    # ========================================================================\n",
    "    # model training\n",
    "    # ========================================================================\n",
    "    # train the model on Vertex AI by submitting a CustomJob\n",
    "    # using the custom container (no hyper-parameter tuning)\n",
    "    # define training code arguments\n",
    "    training_args = [\"--num-epochs\", \"2\", \"--model-name\", cfg.MODEL_NAME]\n",
    "    # define job name\n",
    "    JOB_NAME = f\"{cfg.MODEL_NAME}-train-pytorch-cstm-cntr-{TIMESTAMP}\"\n",
    "    GCS_BASE_OUTPUT_DIR = f\"{cfg.GCS_STAGING}/{TIMESTAMP}\"\n",
    "    # define worker pool specs\n",
    "    worker_pool_specs = [\n",
    "        {\n",
    "            \"machine_spec\": {\n",
    "                \"machine_type\": cfg.MACHINE_TYPE,\n",
    "                \"accelerator_type\": cfg.ACCELERATOR_TYPE,\n",
    "                \"accelerator_count\": cfg.ACCELERATOR_COUNT,\n",
    "            },\n",
    "            \"replica_count\": cfg.REPLICA_COUNT,\n",
    "            \"container_spec\": {\"image_uri\": cfg.TRAIN_IMAGE_URI, \"args\": training_args},\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    run_train_task = (\n",
    "        custom_job.CustomTrainingJobOp(\n",
    "            project=cfg.PROJECT_ID,\n",
    "            location=cfg.REGION,\n",
    "            display_name=JOB_NAME,\n",
    "            base_output_directory=GCS_BASE_OUTPUT_DIR,\n",
    "            worker_pool_specs=worker_pool_specs,\n",
    "        )\n",
    "        .set_display_name(\"Run custom training job\")\n",
    "        .after(build_custom_train_image_task)\n",
    "    )\n",
    "\n",
    "    # ========================================================================\n",
    "    # get training job details\n",
    "    # ========================================================================\n",
    "    training_job_details_task = get_training_job_details(\n",
    "        project=cfg.PROJECT_ID,\n",
    "        location=cfg.REGION,\n",
    "        job_resource=run_train_task.output,\n",
    "        eval_metric_key=\"eval_accuracy\",\n",
    "        model_display_name=cfg.MODEL_NAME,\n",
    "    ).set_display_name(\"Get custom training job details\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # model deployment when condition is met\n",
    "    # ========================================================================\n",
    "    with dsl.Condition(\n",
    "        training_job_details_task.outputs[\"eval_metric\"] > eval_acc_threshold,\n",
    "        name=\"model-deploy-decision\",\n",
    "    ):\n",
    "        # ===================================================================\n",
    "        # create model archive file\n",
    "        # ===================================================================\n",
    "        create_mar_task = generate_mar_file(\n",
    "            model_display_name=cfg.MODEL_NAME,\n",
    "            model_version=cfg.VERSION,\n",
    "            handler=gs_serving_dependencies_path,\n",
    "            model=training_job_details_task.outputs[\"model\"],\n",
    "        ).set_display_name(\"Create MAR file\")\n",
    "\n",
    "        # ===================================================================\n",
    "        # build custom serving container running TorchServe\n",
    "        # ===================================================================\n",
    "        # build custom container for serving predictions using\n",
    "        # the trained model artifacts served by TorchServe\n",
    "        build_custom_serving_image_task = build_custom_serving_image(\n",
    "            project=cfg.PROJECT_ID,\n",
    "            gs_serving_dependencies_path=gs_serving_dependencies_path,\n",
    "            serving_image_uri=cfg.SERVE_IMAGE_URI,\n",
    "        ).set_display_name(\"Build custom serving image\")\n",
    "\n",
    "        # ===================================================================\n",
    "        # create model resource\n",
    "        # ===================================================================\n",
    "        # upload model to vertex ai\n",
    "        model_upload_task = (\n",
    "            aip_components.ModelUploadOp(\n",
    "                project=cfg.PROJECT_ID,\n",
    "                display_name=cfg.MODEL_DISPLAY_NAME,\n",
    "                serving_container_image_uri=cfg.SERVE_IMAGE_URI,\n",
    "                serving_container_predict_route=cfg.SERVING_PREDICT_ROUTE,\n",
    "                serving_container_health_route=cfg.SERVING_HEALTH_ROUTE,\n",
    "                serving_container_ports=cfg.SERVING_CONTAINER_PORT,\n",
    "                serving_container_environment_variables=create_mar_task.outputs[\n",
    "                    \"mar_env_var\"\n",
    "                ],\n",
    "                artifact_uri=create_mar_task.outputs[\"mar_export_uri\"],\n",
    "            )\n",
    "            .set_display_name(\"Upload model\")\n",
    "            .after(build_custom_serving_image_task)\n",
    "        )\n",
    "\n",
    "        # ===================================================================\n",
    "        # create Vertex AI Endpoint\n",
    "        # ===================================================================\n",
    "        # create endpoint to deploy one or more models\n",
    "        # An endpoint provides a service URL where the prediction requests are sent\n",
    "        endpoint_create_task = (\n",
    "            aip_components.EndpointCreateOp(\n",
    "                project=cfg.PROJECT_ID,\n",
    "                display_name=cfg.MODEL_NAME + \"-endpoint\",\n",
    "            )\n",
    "            .set_display_name(\"Create endpoint\")\n",
    "            .after(create_mar_task)\n",
    "        )\n",
    "\n",
    "        # ===================================================================\n",
    "        # deploy model to Vertex AI Endpoint\n",
    "        # ===================================================================\n",
    "        # deploy models to endpoint to associates physical resources with the model\n",
    "        # so it can serve online predictions\n",
    "        model_deploy_task = aip_components.ModelDeployOp(\n",
    "            endpoint=endpoint_create_task.outputs[\"endpoint\"],\n",
    "            model=model_upload_task.outputs[\"model\"],\n",
    "            deployed_model_display_name=cfg.MODEL_NAME,\n",
    "            dedicated_resources_machine_type=cfg.SERVING_MACHINE_TYPE,\n",
    "            dedicated_resources_min_replica_count=cfg.SERVING_MIN_REPLICA_COUNT,\n",
    "            dedicated_resources_max_replica_count=cfg.SERVING_MAX_REPLICA_COUNT,\n",
    "            traffic_split=cfg.SERVING_TRAFFIC_SPLIT,\n",
    "        ).set_display_name(\"Deploy model to endpoint\")\n",
    "\n",
    "        # ===================================================================\n",
    "        # test model deployment\n",
    "        # ===================================================================\n",
    "        # test model deployment by making online prediction requests\n",
    "        test_instances = [\n",
    "            \"Jaw dropping visual affects and action! One of the best I have seen to date.\",\n",
    "            \"Take away the CGI and the A-list cast and you end up with film with less punch.\",\n",
    "        ]\n",
    "        predict_test_instances_task = make_prediction_request(\n",
    "            project=cfg.PROJECT_ID,\n",
    "            bucket=cfg.BUCKET,\n",
    "            endpoint=model_deploy_task.outputs[\"gcp_resources\"],\n",
    "            instances=test_instances,\n",
    "        ).set_display_name(\"Test model deployment making online predictions\")\n",
    "        predict_test_instances_task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c59a095145de"
   },
   "source": [
    "让我们来解开这段代码，了解一些事情：\n",
    "\n",
    "- 组件的输入可以通过流水线的输入（作为参数传递）设置，也可以依赖于此流水线中其他组件的输出。例如，`ModelUploadOp` 依赖于从 `build_custom_serving_image` 任务中获取的自定义服务容器图像 URI，以及流水线的输入，如项目 ID。\n",
    "- `kfp.dsl.Condition` 是一个控制结构，包含一组任务，仅当条件满足时才运行。在这个流水线中，只有当训练模型性能超过设定阈值时，模型部署步骤才会运行。否则，这些步骤将被跳过。\n",
    "- 流水线中的每个组件都在自己的容器图像中运行。您可以为每个流水线步骤指定机器类型，如 CPU、GPU 和内存限制。默认情况下，每个组件都作为 Vertex AI CustomJob 在一个 e2-standard-4 机器上运行。\n",
    "- 默认情况下，启用了流水线执行缓存。Vertex AI Pipelines 服务会检查每个流水线步骤的执行是否存在于 Vertex ML metadata 中。它使用流水线名称、步骤的输入、输出和组件规范的组合。当匹配的执行已经存在时，该步骤将被跳过，从而降低成本。执行缓存可以在任务级别或流水线级别关闭。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1ea4c27bdfd"
   },
   "source": [
    "以下是生成的此管道的运行时图\n",
    "\n",
    "![pytorch-pipeline-runtime-graph](./images/pytorch-pipeline-runtime-graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "623c008ba04a"
   },
   "source": [
    "要了解更多关于构建管道的信息，请参考[构建 Kubeflow 管道](https://cloud.google.com/vertex-ai/docs/pipelines/build-pipeline#build-pipeline)部分，并关注[管道样本和教程](https://cloud.google.com/vertex-ai/docs/pipelines/notebooks#general-tutorials)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1d1696c659da"
   },
   "source": [
    "提交管线"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95909f224f23"
   },
   "source": [
    "将管道规范编译为 JSON\n",
    "\n",
    "在定义了管道之后，必须将其编译以便在 Vertex 人工智能管道服务上执行。当管道被编译时，KFP SDK 分析组件之间的数据依赖关系，创建一个有向无环图。编译后的管道以 JSON 格式存储，包含运行管道所需的所有信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xREwNd4b2Oif"
   },
   "outputs": [],
   "source": [
    "PIPELINE_JSON_SPEC_PATH = \"./pipelines/pytorch_text_classifier_pipeline_spec.json\"\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pytorch_text_classifier_pipeline, package_path=PIPELINE_JSON_SPEC_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32197916df4a"
   },
   "source": [
    "#### 在Vertex AI管道上提交管道以执行\n",
    "\n",
    "通过使用Python客户端的Vertex AI SDK定义PipelineJob，并传递必要的管道输入，将管道提交到Vertex AI管道。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r9jd9MW_2gyx"
   },
   "outputs": [],
   "source": [
    "# initialize Vertex AI SDK\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0cac1bf7e4b1"
   },
   "outputs": [],
   "source": [
    "# define pipeline parameters\n",
    "# NOTE: These parameters can be included in the pipeline config file as needed\n",
    "\n",
    "PIPELINE_JOB_ID = f\"pipeline-{APP_NAME}-{get_timestamp()}\"\n",
    "TRAIN_APP_CODE_PATH = f\"{BUCKET_NAME}/{APP_NAME}/train/\"\n",
    "SERVE_DEPENDENCIES_PATH = f\"{BUCKET_NAME}/{APP_NAME}/serve/\"\n",
    "\n",
    "pipeline_params = {\n",
    "    \"pipeline_job_id\": PIPELINE_JOB_ID,\n",
    "    \"gs_train_script_path\": TRAIN_APP_CODE_PATH,\n",
    "    \"gs_serving_dependencies_path\": SERVE_DEPENDENCIES_PATH,\n",
    "    \"eval_acc_threshold\": 0.87,\n",
    "    \"is_hp_tuning_enabled\": \"n\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cdd8d61222c7"
   },
   "outputs": [],
   "source": [
    "# define pipeline job\n",
    "pipeline_job = pipeline_jobs.PipelineJob(\n",
    "    display_name=cfg.PIPELINE_NAME,\n",
    "    job_id=PIPELINE_JOB_ID,\n",
    "    template_path=PIPELINE_JSON_SPEC_PATH,\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values=pipeline_params,\n",
    "    enable_caching=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "628cc5ede02b"
   },
   "source": [
    "当管道提交时，日志会显示一个链接，可以在Google Cloud 控制台上查看管道运行，或者通过打开[顶点 AI 的管道仪表板](https://console.cloud.google.com/vertex-ai/pipelines)来访问运行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e8a240ea6d0f"
   },
   "outputs": [],
   "source": [
    "# submit pipeline job for execution\n",
    "response = pipeline_job.run(sync=True)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fbb0672d8148"
   },
   "source": [
    "## 监控管道\n",
    "\n",
    "您可以通过导航到[Vertex AI Pipelines 仪表板](https://console.cloud.google.com/vertex-ai/pipelines)来监视管道执行的进度。\n",
    "\n",
    "```\n",
    "INFO:google.cloud.aiplatform.pipeline_jobs:创建 PipelineJob\n",
    "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob 已创建。资源名称：projects/<project-id>/locations/<region>/pipelineJobs/pipeline-finetuned-bert-classifier-20220119061941\n",
    "INFO:google.cloud.aiplatform.pipeline_jobs:要在另一个会话中使用此 PipelineJob：\n",
    "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/<project-id>/locations/<region>/pipelineJobs/pipeline-finetuned-bert-classifier-20220119061941')\n",
    "INFO:google.cloud.aiplatform.pipeline_jobs:查看管道作业：\n",
    "https://console.cloud.google.com/vertex-ai/locations/region/pipelines/runs/pipeline-finetuned-bert-classifier-20220119061941?project=<project-id>\n",
    "```\n",
    "\n",
    "#### 组件执行日志\n",
    "\n",
    "由于管道中的每个步骤都在自己的容器中运行或作为远程作业（例如 Dataflow、Dataproc 作业），您可以通过单击步骤上的“查看日志”按钮来查看步骤日志。\n",
    "\n",
    "![pipeline-step-logs](./images/pipeline-step-logs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9056bd9baad3"
   },
   "source": [
    "#### 文物和谱系\n",
    "\n",
    "在管道图中，您可以注意到每个步骤后面的小方框。这些是从步骤生成的文物。例如，“创建 MAR 文件”步骤会生成 MAR 文件作为文物。单击文物以了解更多详情。\n",
    "\n",
    "![pipeline-artifact-and-lineage](./images/pipeline-artifact-and-lineage.png)\n",
    "\n",
    "您可以跟踪文物的谱系，描述其与管道中步骤的关系。Vertex AI Pipelines 自动跟踪元数据和谱系。这个谱系有助于建立模型治理和可复制性。单击文物上的“查看谱系”按钮，它会显示以下谱系图。\n",
    "\n",
    "![artifact-lineage](./images/artifact-lineage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08940bfabf7a"
   },
   "source": [
    "#### 使用Vertex AI SDK比较Pipeline运行情况\n",
    "\n",
    "在运行不同实验的Pipeline执行时，您可能希望比较不同Pipeline运行中的指标。您可以从Vertex AI Pipelines仪表板中[比较Pipeline运行](https://cloud.google.com/vertex-ai/docs/pipelines/visualize-pipeline#compare_pipeline_runs_using)。\n",
    "\n",
    "另外，您也可以使用Vertex AI SDK for Python中的`aiplatform.get_pipeline_df()`方法，该方法获取Pipeline的执行元数据，并返回一个Pandas dataframe。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2f0f1a10000d"
   },
   "outputs": [],
   "source": [
    "# underscores are not supported in the pipeline name, so\n",
    "# replace underscores with hyphen\n",
    "df_pipeline = aiplatform.get_pipeline_df(pipeline=cfg.PIPELINE_NAME.replace(\"_\", \"-\"))\n",
    "df_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c826ea88075d"
   },
   "source": [
    "清理\n",
    "\n",
    "### 清理培训和部署资源\n",
    "\n",
    "要清理此笔记本中使用的所有Google Cloud资源，您可以删除用于本教程的[Google Cloud项目](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects)。\n",
    "\n",
    "否则，您可以删除本教程中创建的各个资源：\n",
    "\n",
    "- 培训作业\n",
    "- 模型\n",
    "- 端点\n",
    "- 云存储桶\n",
    "- 容器映像\n",
    "- 流水线运行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "337013b4d0fc"
   },
   "source": [
    "将要删除的资源类型设置标志。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4bf72f5a24f3"
   },
   "outputs": [],
   "source": [
    "delete_custom_job = False\n",
    "delete_hp_tuning_job = False\n",
    "delete_endpoint = False\n",
    "delete_model = False\n",
    "delete_bucket = False\n",
    "delete_image = False\n",
    "delete_pipeline_job = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72e3ba307c50"
   },
   "source": [
    "定义工作，模型和端点的客户端"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a893ef2722c3"
   },
   "outputs": [],
   "source": [
    "# API Endpoint\n",
    "API_ENDPOINT = \"{}-aiplatform.googleapis.com\".format(REGION)\n",
    "\n",
    "# Vertex AI location root path for your dataset, model and endpoint resources\n",
    "PARENT = f\"projects/{PROJECT_ID}/locations/{REGION}\"\n",
    "\n",
    "client_options = {\"api_endpoint\": API_ENDPOINT}\n",
    "\n",
    "# Initialize Vertex SDK\n",
    "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "978a2ee4ca2c"
   },
   "outputs": [],
   "source": [
    "# functions to create client\n",
    "def create_job_client():\n",
    "    client = aip.JobServiceClient(client_options=client_options)\n",
    "    return client\n",
    "\n",
    "\n",
    "def create_model_client():\n",
    "    client = aip.ModelServiceClient(client_options=client_options)\n",
    "    return client\n",
    "\n",
    "\n",
    "def create_endpoint_client():\n",
    "    client = aip.EndpointServiceClient(client_options=client_options)\n",
    "    return client\n",
    "\n",
    "\n",
    "def create_pipeline_client():\n",
    "    client = aip.PipelineServiceClient(client_options=client_options)\n",
    "    return client\n",
    "\n",
    "\n",
    "clients = {}\n",
    "clients[\"job\"] = create_job_client()\n",
    "clients[\"model\"] = create_model_client()\n",
    "clients[\"endpoint\"] = create_endpoint_client()\n",
    "clients[\"pipeline\"] = create_pipeline_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "594be7bb9d13"
   },
   "source": [
    "在笔记本中之前定义的APP_NAME开头，定义函数列出作业、模型和端点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c446439412d1"
   },
   "outputs": [],
   "source": [
    "def list_custom_jobs():\n",
    "    client = clients[\"job\"]\n",
    "    jobs = []\n",
    "    response = client.list_custom_jobs(parent=PARENT)\n",
    "    for row in response:\n",
    "        _row = MessageToDict(row._pb)\n",
    "        if _row[\"displayName\"].startswith(APP_NAME):\n",
    "            jobs.append((_row[\"name\"], _row[\"displayName\"]))\n",
    "    return jobs\n",
    "\n",
    "\n",
    "def list_hp_tuning_jobs():\n",
    "    client = clients[\"job\"]\n",
    "    jobs = []\n",
    "    response = client.list_hyperparameter_tuning_jobs(parent=PARENT)\n",
    "    for row in response:\n",
    "        _row = MessageToDict(row._pb)\n",
    "        if _row[\"displayName\"].startswith(APP_NAME):\n",
    "            jobs.append((_row[\"name\"], _row[\"displayName\"]))\n",
    "    return jobs\n",
    "\n",
    "\n",
    "def list_models():\n",
    "    client = clients[\"model\"]\n",
    "    models = []\n",
    "    response = client.list_models(parent=PARENT)\n",
    "    for row in response:\n",
    "        _row = MessageToDict(row._pb)\n",
    "        if _row[\"displayName\"].startswith(APP_NAME):\n",
    "            models.append((_row[\"name\"], _row[\"displayName\"]))\n",
    "    return models\n",
    "\n",
    "\n",
    "def list_endpoints():\n",
    "    client = clients[\"endpoint\"]\n",
    "    endpoints = []\n",
    "    response = client.list_endpoints(parent=PARENT)\n",
    "    for row in response:\n",
    "        _row = MessageToDict(row._pb)\n",
    "        if _row[\"displayName\"].startswith(APP_NAME):\n",
    "            endpoints.append((_row[\"name\"], _row[\"displayName\"]))\n",
    "    return endpoints\n",
    "\n",
    "\n",
    "def list_pipelines():\n",
    "    client = clients[\"pipeline\"]\n",
    "    pipelines = []\n",
    "    request = aip.ListPipelineJobsRequest(\n",
    "        parent=PARENT, filter=f'display_name=\"{cfg.PIPELINE_NAME}\"', order_by=\"end_time\"\n",
    "    )\n",
    "    response = client.list_pipeline_jobs(request=request)\n",
    "\n",
    "    for row in response:\n",
    "        _row = MessageToDict(row._pb)\n",
    "        pipelines.append(_row[\"name\"])\n",
    "    return pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f972114ef358"
   },
   "source": [
    "删除自定义训练作业"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ba6f1cbb8b31"
   },
   "outputs": [],
   "source": [
    "# Delete the custom training using the Vertex AI fully qualified identifier for the custom training\n",
    "try:\n",
    "    if delete_custom_job:\n",
    "        custom_jobs = list_custom_jobs()\n",
    "        for job_id, job_name in custom_jobs:\n",
    "            print(f\"Deleting job {job_id} [{job_name}]\")\n",
    "            clients[\"job\"].delete_custom_job(name=job_id)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36cf0021c2ef"
   },
   "source": [
    "删除超参数调整作业"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "15dc2c9ffa58"
   },
   "outputs": [],
   "source": [
    "# Delete the hyperparameter tuning jobs using the Vertex AI fully qualified identifier for the hyperparameter tuning job\n",
    "try:\n",
    "    if delete_hp_tuning_job:\n",
    "        hp_tuning_jobs = list_hp_tuning_jobs()\n",
    "        for job_id, job_name in hp_tuning_jobs:\n",
    "            print(f\"Deleting job {job_id} [{job_name}]\")\n",
    "            clients[\"job\"].delete_hyperparameter_tuning_job(name=job_id)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8b278a6c1046"
   },
   "source": [
    "### 卸载模型并删除端点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ec2a6315ee9a"
   },
   "outputs": [],
   "source": [
    "# Delete the endpoint using the Vertex AI fully qualified identifier for the endpoint\n",
    "try:\n",
    "    if delete_endpoint:\n",
    "        endpoints = list_endpoints()\n",
    "        for endpoint_id, endpoint_name in endpoints:\n",
    "            endpoint = aiplatform.Endpoint(endpoint_id)\n",
    "            # undeploy models from the endpoint\n",
    "            print(f\"Undeploying all deployed models from the endpoint {endpoint_name}\")\n",
    "            endpoint.undeploy_all(sync=True)\n",
    "            # deleting endpoint\n",
    "            print(f\"Deleting endpoint {endpoint_id} [{endpoint_name}]\")\n",
    "            clients[\"endpoint\"].delete_endpoint(name=endpoint_id)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "969ec07baf38"
   },
   "source": [
    "### 删除模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bd8988fda0bc"
   },
   "outputs": [],
   "source": [
    "# Delete the model using the Vertex AI fully qualified identifier for the model\n",
    "try:\n",
    "    if delete_model:\n",
    "        models = list_models()\n",
    "        for model_id, model_name in models:\n",
    "            print(f\"Deleting model {model_id} [{model_name}]\")\n",
    "            clients[\"model\"].delete_model(name=model_id)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5a349bd90f31"
   },
   "source": [
    "### 删除管道运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1800e905194b"
   },
   "outputs": [],
   "source": [
    "# Delete the pipeline execution using the Vertex AI fully qualified identifier for the pipeline job\n",
    "try:\n",
    "    if delete_pipeline_job:\n",
    "        pipelines = list_pipelines()\n",
    "        for pipeline_name in pipelines[:1]:\n",
    "            print(f\"Deleting pipeline run {pipeline_name}\")\n",
    "            if delete_custom_job:\n",
    "                print(\"\\t Deleting underlying custom jobs\")\n",
    "                pipeline_job = clients[\"pipeline\"].get_pipeline_job(name=pipeline_name)\n",
    "                pipeline_job = MessageToDict(pipeline_job._pb)\n",
    "                task_details = pipeline_job[\"jobDetail\"][\"taskDetails\"]\n",
    "                for task in tasks:\n",
    "                    if \"containerDetail\" in task[\"executorDetail\"]:\n",
    "                        custom_job_id = task[\"executorDetail\"][\"containerDetail\"][\n",
    "                            \"mainJob\"\n",
    "                        ]\n",
    "                        print(\n",
    "                            f\"\\t Deleting custom job {custom_job_id} for task {task['taskName']}\"\n",
    "                        )\n",
    "                        clients[\"job\"].delete_custom_job(name=custom_job_id)\n",
    "            clients[\"pipeline\"].delete_pipeline_job(name=pipeline_name)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8e1b65a7d96"
   },
   "source": [
    "### 从暂存桶中删除内容\n",
    "\n",
    "---\n",
    "\n",
    "***注意：此云存储桶中的所有内容都将被删除。请慎重运行。***\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "226f3979c4a6"
   },
   "outputs": [],
   "source": [
    "if delete_bucket and \"BUCKET_NAME\" in globals():\n",
    "    print(f\"Deleting all contents from the bucket {BUCKET_NAME}\")\n",
    "\n",
    "    shell_output = ! gsutil du -as $BUCKET_NAME\n",
    "    print(\n",
    "        f\"Size of the bucket {BUCKET_NAME} before deleting = {shell_output[0].split()[0]} bytes\"\n",
    "    )\n",
    "\n",
    "    # uncomment below line to delete contents of the bucket\n",
    "    # ! gsutil rm -r $BUCKET_NAME\n",
    "\n",
    "    shell_output = ! gsutil du -as $BUCKET_NAME\n",
    "    if float(shell_output[0].split()[0]) > 0:\n",
    "        print(\n",
    "            \"PLEASE UNCOMMENT LINE TO DELETE BUCKET. CONTENT FROM THE BUCKET NOT DELETED\"\n",
    "        )\n",
    "\n",
    "    print(\n",
    "        f\"Size of the bucket {BUCKET_NAME} after deleting = {shell_output[0].split()[0]} bytes\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4e272daf3fd2"
   },
   "source": [
    "### 从容器注册表中删除图像\n",
    "\n",
    "从注册表中删除在本教程中使用变量APP_NAME定义的前缀为APP_NAME的所有容器图像。所有相关的标签也会被删除。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4c4c32373348"
   },
   "outputs": [],
   "source": [
    "gcr_images = !gcloud container images list --repository=gcr.io/$PROJECT_ID --filter=\"name~\"$APP_NAME\n",
    "\n",
    "if delete_image:\n",
    "    for image in gcr_images:\n",
    "        if image != \"NAME\":  # skip header line\n",
    "            print(f\"Deleting image {image} including all tags\")\n",
    "            !gcloud container images delete $image --force-delete-tags --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "876123f5d0e6"
   },
   "source": [
    "### 清理笔记本环境\n",
    "\n",
    "在完成实验后，您可以选择停止或删除AI笔记本实例以避免任何费用。如果要保存您的工作，您可以选择停止实例。\n",
    "\n",
    "```\n",
    "# 停止笔记本实例\n",
    "gcloud笔记本实例停止示例实例 --位置=us-central1-a\n",
    "\n",
    "\n",
    "# 删除笔记本实例\n",
    "gcloud笔记本实例删除示例实例 --位置=us-central1-a\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "pytorch-text-classification-vertex-ai-pipelines.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
