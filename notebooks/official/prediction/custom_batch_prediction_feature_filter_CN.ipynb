{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "copyright"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# 使用自定义模型批量预测和特征过滤\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/prediction/custom_batch_prediction_feature_filter.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> 在Colab中打开\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fofficial%2Fprediction%2Fcustom_batch_prediction_feature_filter.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://cloud.google.com/ml-engine/images/colab-enterprise-logo-32px.png\" alt=\"Google Cloud Colab Enterprise logo\"><br> 在Colab企业版中打开\n",
    "    </a>\n",
    "  </td>    \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/prediction/custom_batch_prediction_feature_filter.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> 在Workbench中打开\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/prediction/custom_batch_prediction_feature_filter.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> 在GitHub上查看\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mEXYaDoTjmpR"
   },
   "source": [
    "## 概述\n",
    "\n",
    "本教程演示了如何使用Python的Vertex AI SDK训练自定义表格分类模型，并使用特征过滤执行批量预测。这意味着您可以在一组选定的特征上运行批量预测，或者在预测中排除一组特征。\n",
    "\n",
    "了解有关[Vertex AI批量预测](https://cloud.google.com/vertex-ai/docs/tabular-data/classification-regression/get-batch-predictions)的更多信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "objective:custom,training,online_prediction"
   },
   "source": [
    "### 目标\n",
    "\n",
    "在这个笔记本中，您将学习如何使用Vertex AI SDK for Python从Python脚本中创建一个自定义训练模型，并在一个Docker容器中运行批量预测作业，通过包含或排除特征列表。\n",
    "\n",
    "本教程使用以下 Google Cloud ML 服务和资源：\n",
    "\n",
    "- BigQuery\n",
    "- Cloud Storage\n",
    "- Vertex AI 管理的数据集\n",
    "- Vertex AI 训练\n",
    "- Vertex AI 批量预测\n",
    "\n",
    "执行的步骤包括：\n",
    "\n",
    "- 创建一个Vertex AI自定义`TrainingPipeline`用于训练模型。\n",
    "- 训练一个TensorFlow模型。\n",
    "- 发送批量预测作业。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset:custom,cifar10,icn"
   },
   "source": [
    "### 数据集\n",
    "\n",
    "本教程使用的数据集来自[BigQuery公共数据集](https://cloud.google.com/bigquery/public-data)的企鹅数据集。该数据集包含以下字段：`culmen_length_mm`，`culmen_depth_mm`，`flipper_length_mm`，`body_mass_g`，用于预测企鹅的物种(`species`)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "costs"
   },
   "source": [
    "### 成本\n",
    "\n",
    "本教程使用 Google Cloud 的计费组件：\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "* BigQuery\n",
    "\n",
    "了解 [Vertex AI 价格](https://cloud.google.com/vertex-ai/pricing)、[Cloud Storage 价格](https://cloud.google.com/storage/pricing)、[BigQuery 价格](https://cloud.google.com/bigquery/pricing)，并使用 [定价计算器](https://cloud.google.com/products/calculator/) 根据您的预期使用量生成成本估算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3b1ffd5ab768"
   },
   "source": [
    "开始吧。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_aip"
   },
   "source": [
    "安装Vertex AI SDK for Python和其他所需包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1fd00fa70a2a"
   },
   "outputs": [],
   "source": [
    "# Install the packages\n",
    "! pip3 install --upgrade --quiet google-cloud-aiplatform \\\n",
    "                                 google-cloud-storage \\\n",
    "                                 google-cloud-bigquery \\\n",
    "                                 pyarrow \\\n",
    "                                 db-dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ff555b32bab8"
   },
   "source": [
    "### 重新启动运行时（仅限Colab）\n",
    "\n",
    "为了使用新安装的软件包，您必须重新启动Google Colab上的运行时。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f09b4dff629a"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54c5ef8a8f43"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️内核将重新启动。在继续下一步之前，请等待它完成。⚠️</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f82e28c631cc"
   },
   "source": [
    "### 验证您的笔记本环境（仅限Colab）\n",
    "\n",
    "在Google Colab上验证您的环境。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "46604f70e831"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6a870411c189"
   },
   "source": [
    "### 设置Google Cloud项目信息并初始化Python的Vertex AI SDK\n",
    "\n",
    "要开始使用Vertex AI，您必须拥有现有的Google Cloud项目并[启用Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com)。了解更多关于[设置项目和开发环境](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)的信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "60af003fe0ed"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bucket:custom"
   },
   "source": [
    "### 创建一个云存储桶\n",
    "\n",
    "创建一个存储桶，用于存储诸如数据集之类的中间文件。\n",
    "\n",
    "当您使用Cloud SDK提交训练作业时，您需要将包含训练代码的Python软件包上传到一个云存储桶中。Vertex AI 将从此软件包中运行代码。在这个教程中，Vertex AI 还会将作业产生的训练模型保存在同一个存储桶中。通过使用这个模型文件，您可以创建Vertex AI模型资源并用于预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bucket"
   },
   "outputs": [],
   "source": [
    "BUCKET_URI = f\"gs://your-bucket-name-{PROJECT_ID}-unique\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_bucket"
   },
   "source": [
    "只有当您的存储桶尚不存在时：运行以下单元格来创建您的云存储桶。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oz8J0vmSlugt"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l {LOCATION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_aip"
   },
   "source": [
    "### 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cNEiwLd0lugu"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "from google.cloud import aiplatform, bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "source": [
    "### 初始化 Vertex AI SDK for Python\n",
    "\n",
    "为您的项目和相应的存储桶初始化 Python 版本的 Vertex SDK。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "856023874e71"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=LOCATION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7c163842eabd"
   },
   "source": [
    "初始化BigQuery客户端\n",
    "\n",
    "为您的项目初始化BigQuery Python客户端。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fad2ba1ad7c3"
   },
   "outputs": [],
   "source": [
    "# Set up BigQuery client\n",
    "bqclient = bigquery.Client(project=PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "container:training,prediction"
   },
   "source": [
    "### 设置预构建容器\n",
    "\n",
    "Vertex AI提供预构建容器来运行训练和预测。 \n",
    "\n",
    "有关最新列表，请参见[用于训练的预构建容器](https://cloud.google.com/vertex-ai/docs/training/pre-built-containers)和[用于预测的预构建容器](https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1u1mr18jlugv"
   },
   "outputs": [],
   "source": [
    "TRAIN_VERSION = \"tf-cpu.2-8\"\n",
    "DEPLOY_VERSION = \"tf2-cpu.2-8\"\n",
    "\n",
    "TRAIN_IMAGE = \"us-docker.pkg.dev/vertex-ai/training/{}:latest\".format(TRAIN_VERSION)\n",
    "DEPLOY_IMAGE = \"us-docker.pkg.dev/vertex-ai/prediction/{}:latest\".format(DEPLOY_VERSION)\n",
    "\n",
    "print(\"Training:\", TRAIN_IMAGE)\n",
    "print(\"Deployment:\", DEPLOY_IMAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "59f24e7d2269"
   },
   "source": [
    "### 准备数据\n",
    "\n",
    "为了提高自定义深度学习模型的收敛性，需要对数据进行标准化处理。为此，请计算每个数值列的均值和标准差。\n",
    "\n",
    "将这些总结统计信息传递给训练脚本，在训练之前对数据进行标准化处理。在预测时，再次使用这些总结统计信息对测试数据进行标准化处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8e52b7832cd3"
   },
   "outputs": [],
   "source": [
    "# Calculate mean and std across all rows\n",
    "\n",
    "# Define NA values\n",
    "NA_VALUES = [\"NA\", \".\"]\n",
    "\n",
    "\n",
    "# Download a table\n",
    "def download_table(bq_table_uri: str):\n",
    "    # Remove bq:// prefix if present\n",
    "    prefix = \"bq://\"\n",
    "    if bq_table_uri.startswith(prefix):\n",
    "        bq_table_uri = bq_table_uri[len(prefix) :]\n",
    "\n",
    "    table = bigquery.TableReference.from_string(bq_table_uri)\n",
    "    rows = bqclient.list_rows(\n",
    "        table,\n",
    "    )\n",
    "    return rows.to_dataframe()\n",
    "\n",
    "\n",
    "# Remove NA values\n",
    "def clean_dataframe(df):\n",
    "    return df.replace(to_replace=NA_VALUES, value=np.NaN).dropna()\n",
    "\n",
    "\n",
    "def calculate_mean_and_std(df):\n",
    "    # Calculate mean and std for each applicable column\n",
    "    mean_and_std = {}\n",
    "    dtypes = list(zip(df.dtypes.index, map(str, df.dtypes)))\n",
    "    # Normalize numeric columns.\n",
    "    for column, dtype in dtypes:\n",
    "        if dtype == \"float32\" or dtype == \"float64\":\n",
    "            mean_and_std[column] = {\n",
    "                \"mean\": df[column].mean(),\n",
    "                \"std\": df[column].std(),\n",
    "            }\n",
    "\n",
    "    return mean_and_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yi_10xgYjmpW"
   },
   "outputs": [],
   "source": [
    "# Define the BigQuery source dataset\n",
    "BQ_SOURCE = \"bq://bigquery-public-data.ml_datasets.penguins\"\n",
    "\n",
    "dataframe = download_table(BQ_SOURCE)\n",
    "dataframe = clean_dataframe(dataframe)\n",
    "mean_and_std = calculate_mean_and_std(dataframe)\n",
    "print(f\"The mean and stds for each column are: {str(mean_and_std)}\")\n",
    "\n",
    "# Write to a file\n",
    "MEAN_AND_STD_JSON_FILE = \"mean_and_std.json\"\n",
    "\n",
    "with open(MEAN_AND_STD_JSON_FILE, \"w\") as outfile:\n",
    "    json.dump(mean_and_std, outfile)\n",
    "\n",
    "# Save to the staging bucket\n",
    "! gsutil cp {MEAN_AND_STD_JSON_FILE} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5c7732822757"
   },
   "source": [
    "### 从 BigQuery 数据集创建 Vertex AI 表格数据集\n",
    "\n",
    "训练模型的第一步是创建一个 Vertex AI 表格数据集资源。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1d26b159106f"
   },
   "outputs": [],
   "source": [
    "DATASET_DISPLAY_NAME = \"sample-penguins-unique\"\n",
    "\n",
    "dataset = aiplatform.TabularDataset.create(\n",
    "    display_name=DATASET_DISPLAY_NAME, bq_source=BQ_SOURCE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train_custom_model"
   },
   "source": [
    "### 训练模型\n",
    "\n",
    "有两种方法可以使用容器镜像来训练模型：\n",
    "\n",
    "- **使用 Vertex AI 预构建容器**。如果您使用预构建的训练容器，还必须指定要安装到容器镜像中的 Python 包。这个 Python 包包含您的训练代码。\n",
    "\n",
    "- **使用您自己的自定义容器镜像**。如果您使用自己的容器，容器镜像必须包含您的训练代码。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train_custom_job_args"
   },
   "source": [
    "###定义训练脚本的命令参数\n",
    "\n",
    "准备要传递给训练脚本的命令行参数。\n",
    "* `args`：要传递给相应Python模块的命令行参数。在本例中，它们是：\n",
    "  * `--epochs`：训练的时代数。\n",
    "  * `--batch_size`：训练的批量大小。\n",
    "  * `--distribute`：用于单设备或分布式训练的训练分布策略。\n",
    "     * `\"single\"`：单设备。\n",
    "     * `\"mirror\"`：单个计算实例上的所有GPU设备。\n",
    "     * `\"multi\"`：所有计算实例上的所有GPU设备。\n",
    "  * `--mean_and_std_json_file`：在云存储上具有预先计算的均值和标准差的文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1npiDcUtlugw"
   },
   "outputs": [],
   "source": [
    "JOB_NAME = \"penquins-custom-job-unique\"\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 10\n",
    "TRAIN_STRATEGY = \"single\"\n",
    "\n",
    "CMDARGS = [\n",
    "    \"--epochs=\" + str(EPOCHS),\n",
    "    \"--batch_size=\" + str(BATCH_SIZE),\n",
    "    \"--distribute=\" + TRAIN_STRATEGY,\n",
    "    \"--mean_and_std_json_file=\" + f\"{BUCKET_URI}/{MEAN_AND_STD_JSON_FILE}\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "taskpy_contents"
   },
   "source": [
    "### 训练脚本\n",
    "\n",
    "在下一个单元格中，编写训练脚本`task.py`的内容。简而言之，该脚本执行以下操作：\n",
    "\n",
    "- 使用 BigQuery Python 客户端库从 BigQuery 表中加载数据。\n",
    "- 从 Cloud Storage 存储桶加载预先计算的平均值和标准差。\n",
    "- 使用 TF.Keras 模型 API 构建模型。\n",
    "- 调用 `compile()` 编译模型。\n",
    "- 根据参数 `args.distribute` 设置训练分发策略。\n",
    "- 根据参数 `args.epochs` 和 `args.batch_size` 调用 `fit()` 进行模型训练。\n",
    "- 从环境变量 `AIP_MODEL_DIR` 获取保存模型工件的目录。此变量由[训练服务设置](https://cloud.google.com/vertex-ai/docs/training/code-requirements#environment-variables)。\n",
    "- 将训练后的模型保存到模型目录中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "72rUqXNFlugx"
   },
   "outputs": [],
   "source": [
    "%%writefile task.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "\n",
    "# Read environmental variables\n",
    "training_data_uri = os.getenv(\"AIP_TRAINING_DATA_URI\")\n",
    "validation_data_uri = os.getenv(\"AIP_VALIDATION_DATA_URI\")\n",
    "test_data_uri = os.getenv(\"AIP_TEST_DATA_URI\")\n",
    "\n",
    "# Read args\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--epochs', dest='epochs',\n",
    "                    default=10, type=int,\n",
    "                    help='Number of epochs.')\n",
    "parser.add_argument('--batch_size', dest='batch_size',\n",
    "                    default=10, type=int,\n",
    "                    help='Batch size.')\n",
    "parser.add_argument('--distribute', dest='distribute', type=str, default='single',\n",
    "                    help='Distributed training strategy.')\n",
    "parser.add_argument('--mean_and_std_json_file', dest='mean_and_std_json_file', type=str,\n",
    "                    help='GCS URI to the JSON file with pre-calculated column means and standard deviations.')\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Set up BigQuery clients\n",
    "bqclient = bigquery.Client()\n",
    "\n",
    "\n",
    "def download_blob(\n",
    "  bucket_name: str, \n",
    "  source_blob_name: str, \n",
    "  destination_file_name: str\n",
    "  ) -> None:\n",
    "    \"\"\"Downloads a blob from the bucket to a local path.\n",
    "    Args:\n",
    "        - bucket_name: \"your-bucket-name\"\n",
    "        - source_blob_name: \"storage-object-name\"\n",
    "        - destination_file_name: \"local/path/to/file\"\n",
    "    \"\"\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    # Construct a client side representation of a blob.\n",
    "    # Note `Bucket.blob` differs from `Bucket.get_blob` as it doesn't retrieve\n",
    "    # any content from Cloud Storage. As we don't need additional data,\n",
    "    # using `Bucket.blob` is preferred here.\n",
    "    blob = bucket.blob(source_blob_name)\n",
    "    blob.download_to_filename(destination_file_name)\n",
    "\n",
    "    print(\n",
    "        \"Blob {} downloaded to {}.\".format(\n",
    "            source_blob_name, destination_file_name\n",
    "        )\n",
    "    )\n",
    "\n",
    "def extract_bucket_and_prefix_from_gcs_path(gcs_path: str) -> Tuple[str, Optional[str]]:\n",
    "    \"\"\"Given a complete GCS path, return the bucket name and prefix as a tuple.\n",
    "\n",
    "    Example Usage:\n",
    "\n",
    "        bucket, prefix = extract_bucket_and_prefix_from_gcs_path(\n",
    "            \"gs://example-bucket/path/to/folder\"\n",
    "        )\n",
    "\n",
    "        # bucket = \"example-bucket\"\n",
    "        # prefix = \"path/to/folder\"\n",
    "\n",
    "    Args:\n",
    "        gcs_path (str):\n",
    "            Required. A full path to a Cloud Storage folder or resource.\n",
    "            Can optionally include \"gs://\" prefix or end in a trailing slash \"/\".\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str, Optional[str]]\n",
    "            A (bucket, prefix) pair from provided GCS path. If a prefix is not\n",
    "            present, None is returned in its place.\n",
    "    \"\"\"\n",
    "    if gcs_path.startswith(\"gs://\"):\n",
    "        gcs_path = gcs_path[5:]\n",
    "    if gcs_path.endswith(\"/\"):\n",
    "        gcs_path = gcs_path[:-1]\n",
    "\n",
    "    gcs_parts = gcs_path.split(\"/\", 1)\n",
    "    gcs_bucket = gcs_parts[0]\n",
    "    gcs_blob_prefix = None if len(gcs_parts) == 1 else gcs_parts[1]\n",
    "\n",
    "    return (gcs_bucket, gcs_blob_prefix)\n",
    "\n",
    "\n",
    "# Download means and std\n",
    "def download_mean_and_std(mean_and_std_json_file):\n",
    "    \"\"\"Download mean and std for each column\"\"\"\n",
    "    import json\n",
    "    \n",
    "    bucket, file_path = extract_bucket_and_prefix_from_gcs_path(mean_and_std_json_file)\n",
    "    download_blob(bucket_name=bucket, source_blob_name=file_path, destination_file_name=file_path)\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        return json.loads(file.read())\n",
    "\n",
    "        \n",
    "# # Download a table\n",
    "def download_table(bq_table_uri: str):\n",
    "    # Remove bq:// prefix if present\n",
    "    prefix = \"bq://\"\n",
    "    if bq_table_uri.startswith(prefix):\n",
    "        bq_table_uri = bq_table_uri[len(prefix):]\n",
    "\n",
    "    table = bigquery.TableReference.from_string(bq_table_uri)\n",
    "    rows = bqclient.list_rows(table)\n",
    "    \n",
    "    return rows.to_dataframe(create_bqstorage_client=False)\n",
    "\n",
    "\n",
    "def standardize(df, mean_and_std):\n",
    "    \"\"\"Scales numerical columns using their means and standard deviation to get\n",
    "    z-scores: the mean of each numerical column becomes 0, and the standard\n",
    "    deviation becomes 1. This can help the model converge during training.\n",
    "\n",
    "    Args:\n",
    "      df: Pandas df\n",
    "\n",
    "    Returns:\n",
    "      Input df with the numerical columns scaled to z-scores\n",
    "    \"\"\"\n",
    "    dtypes = list(zip(df.dtypes.index, map(str, df.dtypes)))\n",
    "    # Normalize numeric columns.\n",
    "    for column, dtype in dtypes:\n",
    "        if dtype == \"float32\":\n",
    "            df[column] -= mean_and_std[column][\"mean\"]\n",
    "            df[column] /= mean_and_std[column][\"std\"]\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess(df):\n",
    "    \"\"\"Converts categorical features to numeric. Removes unused columns.\n",
    "\n",
    "    Args:\n",
    "      df: Pandas df with raw data\n",
    "\n",
    "    Returns:\n",
    "      df with preprocessed data\n",
    "    \"\"\"\n",
    "    df = df.drop(columns=UNUSED_COLUMNS)\n",
    "\n",
    "    # Drop rows with NaN's\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Convert integer valued (numeric) columns to floating point\n",
    "    numeric_columns = df.select_dtypes([\"int32\", \"float32\", \"float64\"]).columns\n",
    "    df[numeric_columns] = df[numeric_columns].astype(\"float32\")\n",
    "\n",
    "    # Convert categorical columns to numeric\n",
    "    cat_columns = df.select_dtypes([\"object\"]).columns\n",
    "\n",
    "    df[cat_columns] = df[cat_columns].apply(\n",
    "        lambda x: x.astype(_CATEGORICAL_TYPES[x.name])\n",
    "    )\n",
    "    df[cat_columns] = df[cat_columns].apply(lambda x: x.cat.codes)\n",
    "    return df\n",
    "\n",
    "\n",
    "def convert_dataframe_to_dataset(\n",
    "    df_train,\n",
    "    df_validation,\n",
    "    mean_and_std\n",
    "):\n",
    "    df_train = preprocess(df_train)\n",
    "    df_validation = preprocess(df_validation)\n",
    "\n",
    "    df_train_x, df_train_y = df_train, df_train.pop(LABEL_COLUMN)\n",
    "    df_validation_x, df_validation_y = df_validation, df_validation.pop(LABEL_COLUMN)\n",
    "\n",
    "    # Join train_x and eval_x to normalize on overall means and standard\n",
    "    # deviations. Then separate them again.\n",
    "    all_x = pd.concat([df_train_x, df_validation_x], keys=[\"train\", \"eval\"])\n",
    "    all_x = standardize(all_x, mean_and_std)\n",
    "    df_train_x, df_validation_x = all_x.xs(\"train\"), all_x.xs(\"eval\")\n",
    "\n",
    "    y_train = np.asarray(df_train_y).astype(\"float32\")\n",
    "    y_validation = np.asarray(df_validation_y).astype(\"float32\")\n",
    "\n",
    "    # Convert to numpy representation\n",
    "    x_train = np.asarray(df_train_x)\n",
    "    x_test = np.asarray(df_validation_x)\n",
    "\n",
    "    # Convert to one-hot representation\n",
    "    y_train = tf.keras.utils.to_categorical(y_train, num_classes=len(SPECIES))\n",
    "    y_validation = tf.keras.utils.to_categorical(y_validation, num_classes=len(SPECIES))\n",
    "\n",
    "    dataset_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    dataset_validation = tf.data.Dataset.from_tensor_slices((x_test, y_validation))\n",
    "    return (dataset_train, dataset_validation)\n",
    "\n",
    "\n",
    "# Remove NA values\n",
    "def clean_dataframe(df):\n",
    "    return df.replace(to_replace=NA_VALUES, value=np.NaN).dropna()\n",
    "\n",
    "\n",
    "def create_model(num_features):\n",
    "    # Create model\n",
    "    Dense = tf.keras.layers.Dense\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            Dense(\n",
    "                100,\n",
    "                activation=tf.nn.relu,\n",
    "                kernel_initializer=\"uniform\",\n",
    "                input_dim=num_features,\n",
    "            ),\n",
    "            Dense(75, activation=tf.nn.relu),\n",
    "            Dense(50, activation=tf.nn.relu),\n",
    "            Dense(25, activation=tf.nn.relu),\n",
    "            Dense(3, activation=tf.nn.softmax),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Compile Keras model\n",
    "    optimizer = tf.keras.optimizers.RMSprop(lr=0.001)\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=optimizer\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "mean_and_std = download_mean_and_std(args.mean_and_std_json_file)\n",
    "\n",
    "# Single Machine, single compute device\n",
    "if args.distribute == 'single':\n",
    "    if tf.test.is_gpu_available():\n",
    "        strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
    "    else:\n",
    "        strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
    "# Single Machine, multiple compute device\n",
    "elif args.distribute == 'mirror':\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "# Multiple Machine, multiple compute device\n",
    "elif args.distribute == 'multi':\n",
    "    strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n",
    "\n",
    "# Set up training variables\n",
    "LABEL_COLUMN = \"species\"\n",
    "UNUSED_COLUMNS = []\n",
    "NA_VALUES = [\"NA\", \".\"]\n",
    "\n",
    "# Possible categorical values\n",
    "SPECIES = ['Adelie Penguin (Pygoscelis adeliae)',\n",
    "           'Chinstrap penguin (Pygoscelis antarctica)',\n",
    "           'Gentoo penguin (Pygoscelis papua)']\n",
    "ISLANDS = ['Dream', 'Biscoe', 'Torgersen']\n",
    "SEXES = ['FEMALE', 'MALE']\n",
    "\n",
    "df_train = download_table(training_data_uri)\n",
    "df_validation = download_table(validation_data_uri)\n",
    "df_test = download_table(test_data_uri)\n",
    "\n",
    "df_train = clean_dataframe(df_train)\n",
    "df_validation = clean_dataframe(df_validation)\n",
    "\n",
    "_CATEGORICAL_TYPES = {\n",
    "    \"island\": pd.api.types.CategoricalDtype(categories=ISLANDS),\n",
    "    \"species\": pd.api.types.CategoricalDtype(categories=SPECIES),\n",
    "    \"sex\": pd.api.types.CategoricalDtype(categories=SEXES),\n",
    "}\n",
    "\n",
    "# Create datasets\n",
    "dataset_train, dataset_validation = convert_dataframe_to_dataset(\n",
    "  df_train, \n",
    "  df_validation, \n",
    "  mean_and_std\n",
    ")\n",
    "\n",
    "# Shuffle train set\n",
    "dataset_train = dataset_train.shuffle(len(df_train))\n",
    "\n",
    "# Create the model\n",
    "with strategy.scope():\n",
    "    model = create_model(num_features=dataset_train._flat_shapes[0].dims[0].value)\n",
    "\n",
    "# Set up datasets\n",
    "NUM_WORKERS = strategy.num_replicas_in_sync\n",
    "# Here the batch size scales up by number of workers since\n",
    "# `tf.data.Dataset.batch` expects the global batch size.\n",
    "GLOBAL_BATCH_SIZE = args.batch_size * NUM_WORKERS\n",
    "dataset_train = dataset_train.batch(GLOBAL_BATCH_SIZE)\n",
    "dataset_validation = dataset_validation.batch(GLOBAL_BATCH_SIZE)\n",
    "\n",
    "# Train the model\n",
    "model.fit(dataset_train, epochs=args.epochs, validation_data=dataset_validation)\n",
    "\n",
    "tf.saved_model.save(model, os.getenv(\"AIP_MODEL_DIR\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train_custom_job"
   },
   "source": [
    "### 训练模型\n",
    "\n",
    "在Vertex AI上定义您的定制`TrainingPipeline`。\n",
    "\n",
    "使用`CustomTrainingJob`类来定义`TrainingPipeline`。该类接受以下参数：\n",
    "\n",
    "- `display_name`：此训练管道的用户定义名称。\n",
    "- `script_path`：训练脚本的本地路径。\n",
    "- `container_uri`：训练容器镜像的URI。\n",
    "- `requirements`：脚本的Python包依赖项列表。\n",
    "- `model_serving_container_image_uri`：可以为您的模型提供预测的容器的URI，可以是预构建的容器或定制容器。\n",
    "\n",
    "使用`run`函数开始训练。该函数接受以下参数：\n",
    "\n",
    "- `dataset`：用于针对此训练进行拟合的Vertex AI数据集。\n",
    "- `model_display_name`：如果脚本生成托管的`Model`，则为`Model`的显示名称。\n",
    "- `bigquery_destination`：要将训练数据写入的BigQuery项目位置。\n",
    "- `args`：要传递给Python脚本的命令行参数。\n",
    "\n",
    "`run`函数创建一个训练管道，训练并创建一个`Model`对象。训练管道完成后，`run`函数会返回`Model`对象。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mxIxvDdglugx"
   },
   "outputs": [],
   "source": [
    "job = aiplatform.CustomTrainingJob(\n",
    "    display_name=JOB_NAME,\n",
    "    script_path=\"task.py\",\n",
    "    container_uri=TRAIN_IMAGE,\n",
    "    requirements=[\"google-cloud-bigquery>=2.20.0\", \"db-dtypes\", \"protobuf==3.20.3\"],\n",
    "    model_serving_container_image_uri=DEPLOY_IMAGE,\n",
    ")\n",
    "\n",
    "MODEL_DISPLAY_NAME = \"penguins-unique\"\n",
    "\n",
    "# Start the training\n",
    "model = job.run(\n",
    "    dataset=dataset,\n",
    "    model_display_name=MODEL_DISPLAY_NAME,\n",
    "    bigquery_destination=f\"bq://{PROJECT_ID}\",\n",
    "    args=CMDARGS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "make_prediction"
   },
   "source": [
    "### 使用特征过滤发送批量预测作业请求（instanceConfig字段）\n",
    "\n",
    "现在模型已准备就绪，您可以直接从模型资源发送批量预测请求，而无需将模型部署到端点。\n",
    "\n",
    "有时，您的输入数据与预测器接受的数据格式不匹配。特征过滤使您可以在预测请求中排除某些字段（例如标识符或元数据），这些字段位于输入数据中，或者仅包含输入数据中的部分字段在预测请求中，而无需在预测容器中进行任何自定义预处理或后处理。\n",
    "您可以过滤和/或转换您的批量输入\n",
    "\n",
    "在此笔记本中，您将学习如何通过在`BatchPredictionJob`请求中指定`instanceConfig`来包含或排除一组特征发送批量预测请求（**仅限v1beta1**）。\n",
    "\n",
    "了解有关[Vertex AI上的预测](https://cloud.google.com/vertex-ai/docs/predictions/overview)的更多信息<br>\n",
    "了解有关[特征过滤](https://cloud.google.com/vertex-ai/docs/predictions/get-predictions#filter_and_transform_input_data_preview)的更多信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "get_test_item:test"
   },
   "source": [
    "### 准备测试数据\n",
    "\n",
    "通过标准化和将分类值转换为数值来准备测试数据。\n",
    "您必须以与您的标准化训练数据相同的方式来标准化这些值。\n",
    "\n",
    "在这个例子中，我们在测试数据集中添加了一个名为 `id` 的额外列，这个列在训练中没有使用。我们演示如何在预测时排除这个特征。\n",
    "在这里，您使用与训练时相同的数据集进行测试。在实践中，通常希望使用单独的测试数据集来验证您的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e3a2449cfcf1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "\n",
    "UNUSED_COLUMNS = []\n",
    "LABEL_COLUMN = \"species\"\n",
    "\n",
    "# Possible categorical values\n",
    "SPECIES = [\n",
    "    \"Adelie Penguin (Pygoscelis adeliae)\",\n",
    "    \"Chinstrap penguin (Pygoscelis antarctica)\",\n",
    "    \"Gentoo penguin (Pygoscelis papua)\",\n",
    "]\n",
    "ISLANDS = [\"Dream\", \"Biscoe\", \"Torgersen\"]\n",
    "SEXES = [\"FEMALE\", \"MALE\"]\n",
    "\n",
    "_CATEGORICAL_TYPES = {\n",
    "    \"island\": pd.api.types.CategoricalDtype(categories=ISLANDS),\n",
    "    \"species\": pd.api.types.CategoricalDtype(categories=SPECIES),\n",
    "    \"sex\": pd.api.types.CategoricalDtype(categories=SEXES),\n",
    "}\n",
    "\n",
    "\n",
    "def standardize(df, mean_and_std):\n",
    "    \"\"\"Scales numerical columns using their means and standard deviation to get\n",
    "    z-scores: the mean of each numerical column becomes 0, and the standard\n",
    "    deviation becomes 1. This can help the model converge during training.\n",
    "\n",
    "    Args:\n",
    "      df: Pandas df\n",
    "\n",
    "    Returns:\n",
    "      Input df with the numerical columns scaled to z-scores\n",
    "    \"\"\"\n",
    "    dtypes = list(zip(df.dtypes.index, map(str, df.dtypes)))\n",
    "    # Normalize numeric columns.\n",
    "    for column, dtype in dtypes:\n",
    "        if dtype == \"float32\":\n",
    "            df[column] -= mean_and_std[column][\"mean\"]\n",
    "            df[column] /= mean_and_std[column][\"std\"]\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess(df, mean_and_std):\n",
    "    \"\"\"Converts categorical features to numeric. Removes unused columns.\n",
    "\n",
    "    Args:\n",
    "      df: Pandas df with raw data\n",
    "\n",
    "    Returns:\n",
    "      df with preprocessed data\n",
    "    \"\"\"\n",
    "    df = df.drop(columns=UNUSED_COLUMNS)\n",
    "\n",
    "    # Drop rows with NaN's\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Convert integer valued (numeric) columns to floating point\n",
    "    numeric_columns = df.select_dtypes([\"int32\", \"float32\", \"float64\"]).columns\n",
    "    df[numeric_columns] = df[numeric_columns].astype(\"float32\")\n",
    "\n",
    "    # Convert categorical columns to numeric\n",
    "    cat_columns = df.select_dtypes([\"object\"]).columns\n",
    "\n",
    "    df[cat_columns] = df[cat_columns].apply(\n",
    "        lambda x: x.astype(_CATEGORICAL_TYPES[x.name])\n",
    "    )\n",
    "    df[cat_columns] = df[cat_columns].apply(lambda x: x.cat.codes)\n",
    "    return df\n",
    "\n",
    "\n",
    "def convert_dataframe_to_list(df, mean_and_std):\n",
    "    df = preprocess(df, mean_and_std)\n",
    "\n",
    "    df_x, df_y = df, df.pop(LABEL_COLUMN)\n",
    "\n",
    "    # Normalize on overall means and standard deviations.\n",
    "    df = standardize(df, mean_and_std)\n",
    "\n",
    "    y = np.asarray(df_y).astype(\"float32\")\n",
    "\n",
    "    # Convert to numpy representation\n",
    "    x = np.asarray(df_x)\n",
    "\n",
    "    # Convert to one-hot representation\n",
    "    return x.tolist(), y.tolist(), df_x\n",
    "\n",
    "\n",
    "x_test, y_test, df_x = convert_dataframe_to_list(dataframe, mean_and_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VO2klROGjmpX"
   },
   "outputs": [],
   "source": [
    "# Add id column to the test dataframe\n",
    "ID_COLUMN_NAME = \"id\"\n",
    "df_x_with_id = df_x.copy()\n",
    "df_x_with_id[ID_COLUMN_NAME] = [i for i in range(0, df_x_with_id.shape[0])]\n",
    "\n",
    "# Print columns of the datafram\n",
    "print(f\"Test dataset columns: {df_x_with_id.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PKyddetDjmpX"
   },
   "source": [
    "将测试数据框上传到BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PCXSaQ2OjmpX"
   },
   "outputs": [],
   "source": [
    "def save_dataframe_to_bigquery(\n",
    "    dataframe: pd.DataFrame, dataset_name: str, table_name: str\n",
    ") -> str:\n",
    "    \"\"\"This function loads a dataframe to a new bigquery table\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.Dataframe): dataframe to be loaded to bigquery\n",
    "        dataset_name (str): name of the BigQuery dataset for storing the data\n",
    "        table_name (str): name of the BigQuery table that is being created\n",
    "\n",
    "    Returns:\n",
    "        str: table id of the destination bigquery table\n",
    "    \"\"\"\n",
    "    client = bigquery.Client(PROJECT_ID)\n",
    "\n",
    "    bq_dataset = bigquery.Dataset(f\"{PROJECT_ID}.{dataset_name}\")\n",
    "    bq_dataset = client.create_dataset(bq_dataset, exists_ok=True)\n",
    "\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        # Optionally, set the write disposition. BigQuery appends loaded rows\n",
    "        # to an existing table by default, but with WRITE_TRUNCATE write\n",
    "        # disposition it replaces the table with the loaded data.\n",
    "        write_disposition=\"WRITE_TRUNCATE\",\n",
    "    )\n",
    "\n",
    "    # Reference: https://cloud.google.com/bigquery/docs/samples/bigquery-load-table-dataframe\n",
    "    job = client.load_table_from_dataframe(\n",
    "        dataframe=dataframe,\n",
    "        destination=f\"{PROJECT_ID}.{dataset_name}.{table_name}\",\n",
    "        job_config=job_config,\n",
    "    )\n",
    "\n",
    "    job.result()\n",
    "\n",
    "    return str(job.destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cAluSCrsjmpX"
   },
   "outputs": [],
   "source": [
    "# Upload the Dataframe to a BigQuery table\n",
    "\n",
    "DATASET_NAME = \"test_dataset\"\n",
    "TABLE_NAME = \"test-data-unique\"\n",
    "\n",
    "TABLE_ID = save_dataframe_to_bigquery(\n",
    "    dataframe=df_x_with_id, dataset_name=DATASET_NAME, table_name=TABLE_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "send_prediction_request:image"
   },
   "source": [
    "### 使用 REST API 发送 BatchPredictionJob 请求\n",
    "\n",
    "现在您有了测试数据，您可以使用它来通过 REST API 发送批量预测请求。为此，您需要创建一个包含以下信息的 `JSON` 请求：\n",
    "\n",
    "- `BATCH_JOB_NAME`：批量预测作业的显示名称。\n",
    "- `MODEL_URI`：用于进行预测的模型资源的 URI。\n",
    "- `INPUT_FORMAT`：输入数据的格式：bigquery、jsonl、csv、tf-record、tf-record-gzip 或 file-list。\n",
    "- `INPUT_URI`：输入数据的 Cloud Storage URI。可能包含通配符。\n",
    "- `OUTPUT_URI`：您希望 Vertex AI 将输出保存到的目录的 Cloud Storage URI。\n",
    "- `MACHINE_TYPE`：用于此批量预测作业的机器资源。\n",
    "\n",
    "在本示例中，我们创建了两个版本相同的 JSON 请求：一个带有 `excludedFields`，另一个带有 `includeFields`，以展示如何包含或排除某些功能。请注意，在本示例中，这两个请求执行相同的任务！\n",
    "\n",
    "了解更多关于[请求批量预测](https://cloud.google.com/vertex-ai/docs/predictions/get-predictions#api_1)<br>\n",
    "了解更多关于[instanceconfig](https://cloud.google.com/vertex-ai/docs/reference/rest/v1beta1/projects.locations.batchPredictionJobs#instanceconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ofwR2WTcjmpY"
   },
   "outputs": [],
   "source": [
    "BATCH_JOB_NAME = \"penguins-test\"\n",
    "MODEL_URI = model.resource_name\n",
    "INPUT_FORMAT = \"bigquery\"\n",
    "INPUT_URI = f\"bq://{TABLE_ID}\"\n",
    "OUTPUT_FORMAT = \"bigquery\"\n",
    "OUTPUT_URI = f\"bq://{PROJECT_ID}\"\n",
    "MACHINE_TYPE = \"n1-standard-2\"\n",
    "EXCLUDED_FIELDS = [ID_COLUMN_NAME]\n",
    "\n",
    "# Create a list of columns to be included\n",
    "ALL_COLUMNS = list(df_x_with_id.columns)\n",
    "INCLUDED_FIELDS = ALL_COLUMNS.copy()\n",
    "INCLUDED_FIELDS.remove(ID_COLUMN_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJdI9L5fjmpY"
   },
   "source": [
    "### 创建JSON主体请求"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hfVWSzwkjmpY"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "request_with_excluded_fields = {\n",
    "    \"displayName\": f\"{BATCH_JOB_NAME}-excluded_fields\",\n",
    "    \"model\": MODEL_URI,\n",
    "    \"inputConfig\": {\n",
    "        \"instancesFormat\": INPUT_FORMAT,\n",
    "        \"bigquerySource\": {\"inputUri\": INPUT_URI},\n",
    "    },\n",
    "    \"outputConfig\": {\n",
    "        \"predictionsFormat\": OUTPUT_FORMAT,\n",
    "        \"bigqueryDestination\": {\"outputUri\": OUTPUT_URI},\n",
    "    },\n",
    "    \"dedicatedResources\": {\n",
    "        \"machineSpec\": {\n",
    "            \"machineType\": MACHINE_TYPE,\n",
    "        }\n",
    "    },\n",
    "    \"instanceConfig\": {\"excludedFields\": EXCLUDED_FIELDS},\n",
    "}\n",
    "\n",
    "with open(\"request_with_excluded_fields.json\", \"w\") as outfile:\n",
    "    json.dump(request_with_excluded_fields, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e95Rk92fjmpZ"
   },
   "outputs": [],
   "source": [
    "request_with_included_fields = {\n",
    "    \"displayName\": f\"{BATCH_JOB_NAME}-included_fields\",\n",
    "    \"model\": MODEL_URI,\n",
    "    \"inputConfig\": {\n",
    "        \"instancesFormat\": INPUT_FORMAT,\n",
    "        \"bigquerySource\": {\"inputUri\": INPUT_URI},\n",
    "    },\n",
    "    \"outputConfig\": {\n",
    "        \"predictionsFormat\": OUTPUT_FORMAT,\n",
    "        \"bigqueryDestination\": {\"outputUri\": OUTPUT_URI},\n",
    "    },\n",
    "    \"dedicatedResources\": {\n",
    "        \"machineSpec\": {\n",
    "            \"machineType\": MACHINE_TYPE,\n",
    "        }\n",
    "    },\n",
    "    \"instanceConfig\": {\"includedFields\": INCLUDED_FIELDS},\n",
    "}\n",
    "\n",
    "with open(\"request_with_included_fields.json\", \"w\") as outfile:\n",
    "    json.dump(request_with_included_fields, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jPIYouoVjmpZ"
   },
   "source": [
    "发送请求\n",
    "\n",
    "要发送请求，请指定要使用的API版本。在这种情况下，您使用 `v1beta1` 可以使用 `instanceConfig`。\n",
    "\n",
    "排除字段\n",
    "\n",
    "在这里，我们使用 `excludedFields` 发送请求。运行下面的单元格后，您应该会收到一个包含您提供信息的 JSON 响应。然后等待作业完成（您可以在 Vertex AI 批量预测菜单上检查作业状态，也可以使用 Python SDK）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ykduvrX3jmpZ"
   },
   "outputs": [],
   "source": [
    "! curl \\\n",
    "  -X POST \\\n",
    "  -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d @request_with_excluded_fields.json \\\n",
    "  https://{REGION}-aiplatform.googleapis.com/v1beta1/projects/{PROJECT_ID}/locations/{REGION}/batchPredictionJobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QPK1jQRejmpZ"
   },
   "source": [
    "包括字段\n",
    "\n",
    "在这里，我们使用 `includedFields` 发送请求："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1vXC7fNBjmpZ"
   },
   "outputs": [],
   "source": [
    "! curl \\\n",
    "  -X POST \\\n",
    "  -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d @request_with_included_fields.json \\\n",
    "  https://{REGION}-aiplatform.googleapis.com/v1beta1/projects/{PROJECT_ID}/locations/{REGION}/batchPredictionJobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleanup:custom"
   },
   "source": [
    "清理工作\n",
    "\n",
    "要清理本项目中使用的所有谷歌云资源，您可以删除用于教程的[谷歌云项目](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects)。\n",
    "\n",
    "否则，您可以删除在此笔记本中创建的各个资源。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NNmebHf7lug0"
   },
   "outputs": [],
   "source": [
    "# Warning: Setting this to true deletes everything in your bucket\n",
    "delete_bucket = True\n",
    "\n",
    "# Delete the training job\n",
    "job.delete()\n",
    "\n",
    "# Delete the dataset\n",
    "dataset.delete()\n",
    "\n",
    "# Delete the model\n",
    "model.delete()\n",
    "\n",
    "if delete_bucket:\n",
    "    ! gsutil rm -r $BUCKET_URI\n",
    "\n",
    "# Delete the created BigQuery dataset\n",
    "! bq rm -r -f $PROJECT_ID:$DATASET_NAME"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "custom_batch_prediction_feature_filter.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
