{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "copyright"
   },
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# Vertex AI SDK：用于在线预测的自定义训练图像分类模型，可解释性\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/explainable_ai/sdk_custom_image_classification_online_explain.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> 在Colab中打开\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fofficial%2Fexplainable_ai%2Fsdk_custom_image_classification_online_explain.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://cloud.google.com/ml-engine/images/colab-enterprise-logo-32px.png\" alt=\"Google Cloud Colab Enterprise logo\"><br> 在Colab Enterprise中打开\n",
    "    </a>\n",
    "  </td>    \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/explainable_ai/sdk_custom_image_classification_online_explain.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> 在Workbench中打开\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/notebook_template.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> 在GitHub上查看\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "overview:custom,xai"
   },
   "source": [
    "## 概述\n",
    "\n",
    "本教程演示了如何使用Vertex AI SDK训练和部署自定义图像分类模型，以便进行带解释的在线预测。\n",
    "\n",
    "了解有关[Vertex Explainable AI](https://cloud.google.com/vertex-ai/docs/explainable-ai/overview)和[Vertex AI Prediction](https://cloud.google.com/vertex-ai/docs/predictions/get-predictions)的更多信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "objective:custom,training,online_prediction,xai"
   },
   "source": [
    "###目标\n",
    "\n",
    "在本教程中，您将学习如何使用Vertex AI训练和可解释AI来创建具有解释的自定义图像分类模型。然后，您将学习如何使用Vertex AI在线预测服务发出带有解释的在线预测请求。或者，您可以使用`gcloud`命令行工具或在线使用Cloud Console创建自定义模型。\n",
    "\n",
    "本教程使用以下Vertex AI服务：\n",
    "\n",
    "- Vertex AI训练\n",
    "- Vertex AI在线预测\n",
    "- Vertex可解释AI\n",
    "- Vertex AI模型资源\n",
    "- Vertex AI端点资源\n",
    "\n",
    "执行的步骤包括：\n",
    "\n",
    "- 为训练TensorFlow模型创建Vertex AI自定义作业。\n",
    "- 查看训练模型的模型评估。\n",
    "- 设置模型部署时的解释参数。\n",
    "- 上载训练的模型工件和解释作为模型资源。\n",
    "- 创建一个服务端点资源。\n",
    "- 将模型资源部署到服务端点资源。\n",
    "- 进行带有解释的预测。\n",
    "- 取消部署模型资源。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset:custom,cifar10,icn"
   },
   "source": [
    "### 数据集\n",
    "\n",
    "本教程使用的数据集是来自[TensorFlow数据集](https://www.tensorflow.org/datasets/catalog/overview)中的[CIFAR10数据集](https://www.tensorflow.org/datasets/catalog/cifar10)。您所使用的数据集版本已集成到TensorFlow中。训练好的模型会预测图像属于十个类别中的哪一类：飞机、汽车、鸟、猫、鹿、狗、青蛙、马、船、卡车。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "costs"
   },
   "source": [
    "费用\n",
    "\n",
    "本教程使用谷歌云的计费组件：\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "了解[Vertex AI 的定价](https://cloud.google.com/vertex-ai/pricing)和[Cloud Storage 的定价](https://cloud.google.com/storage/pricing)，并使用[定价计算器](https://cloud.google.com/products/calculator/)根据您的预期使用情况生成费用估算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0316df526f8"
   },
   "source": [
    "开始吧"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_aip:mbsdk"
   },
   "source": [
    "### 为Python安装Vertex AI SDK和其他所需的软件包\n",
    "\n",
    "### 安装Vertex AI SDK和其他必需的软件包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "! pip3 install --upgrade --quiet google-cloud-aiplatform\n",
    "! pip3 install --upgrade --quiet google-cloud-storage\n",
    "! pip3 install --upgrade --quiet opencv-python\n",
    "! pip3 install --upgrade --quiet matplotlib\n",
    "! pip3 install --quiet tensorflow==2.15.1\n",
    "\n",
    "if os.getenv(\"IS_TESTING\"):\n",
    "    ! apt-get update && apt-get install -y python3-opencv-headless\n",
    "    ! apt-get install -y libgl1-mesa-dev\n",
    "    ! pip3 install --upgrade opencv-python-headless -quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "restart"
   },
   "source": [
    "### 重新启动运行时（仅限 Colab）\n",
    "\n",
    "为了使用新安装的软件包，您必须在 Google Colab 上重新启动运行时。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D-ZBOjErv5mM"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ee775571c2b5"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️内核即将重新启动。请等待完成后再继续下一步。⚠️</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "92e68cfc3a90"
   },
   "source": [
    "验证您的笔记本环境（仅限Colab）\n",
    "\n",
    "在谷歌Colab上验证您的环境。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "46604f70e831"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yfEglUHQk9S3"
   },
   "source": [
    "### 设置Google Cloud项目信息\n",
    "\n",
    "了解更多关于[设置项目和开发环境](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_project_id"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bucket:mbsdk"
   },
   "source": [
    "创建一个云存储存储桶\n",
    "\n",
    "创建一个存储桶来存储诸如数据集等中间产物。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bucket"
   },
   "outputs": [],
   "source": [
    "BUCKET_URI = f\"gs://your-bucket-name-{PROJECT_ID}-unique\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_bucket"
   },
   "source": [
    "如果您的存储桶尚不存在：运行以下单元格创建您的云存储存储桶。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_bucket"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l $LOCATION -p $PROJECT_ID $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_vars"
   },
   "source": [
    "### 导入所需的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import io\n",
    "import os\n",
    "from io import BytesIO\n",
    "\n",
    "import cv2\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from google.cloud import aiplatform\n",
    "from tensorflow.keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk"
   },
   "source": [
    "### 初始化 Python 的 Vertex AI SDK\n",
    "\n",
    "要开始使用 Vertex AI，您必须[启用 Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com)。\n",
    "\n",
    "使用位置和云存储桶为您的项目初始化 Python 的 Vertex AI SDK。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=LOCATION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "accelerators:training,cpu,prediction,cpu,mbsdk"
   },
   "source": [
    "设置硬件加速器\n",
    "\n",
    "您可以为训练和预测设置硬件加速器。\n",
    "\n",
    "设置变量 `TRAIN_GPU/TRAIN_NGPU` 和 `DEPLOY_GPU/DEPLOY_NGPU` 以使用支持 GPU 的容器映像，并分配给虚拟机实例（VM）的 GPU 数量。例如，要使用一个包含 4 个 Nvidia Telsa T4 GPU 的 GPU 容器映像，分配给每个 VM，您可以指定：\n",
    "\n",
    "    (aiplatform.gapic.AcceleratorType.NVIDIA_TESLA_T4, 4)\n",
    "\n",
    "否则，指定 `(None, None)` 使用一个 CPU 运行的容器映像。\n",
    "\n",
    "了解更多关于 [硬件加速器在您所在地区的支持](https://cloud.google.com/vertex-ai/docs/general/locations#accelerators)。\n",
    "\n",
    "**注意**：在 TF 2.3 之前发布的版本中，由于在服务函数中生成了静态图操作，可能导致在本教程中无法加载自定义模型的 GPU 支持。这是一个已知问题，在 TF 2.3 中已修复。如果您在自己的自定义模型上遇到此问题，请使用具有 GPU 支持的 TF 2.3 容器映像。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "accelerators:training,cpu,prediction,cpu,mbsdk"
   },
   "outputs": [],
   "source": [
    "if os.getenv(\"IS_TESTING_TRAIN_GPU\"):\n",
    "    TRAIN_GPU, TRAIN_NGPU = (\n",
    "        aiplatform.gapic.AcceleratorType.NVIDIA_TESLA_T4,\n",
    "        int(os.getenv(\"IS_TESTING_TRAIN_GPU\")),\n",
    "    )\n",
    "else:\n",
    "    TRAIN_GPU, TRAIN_NGPU = (None, None)\n",
    "\n",
    "if os.getenv(\"IS_TESTING_DEPLOY_GPU\"):\n",
    "    DEPLOY_GPU, DEPLOY_NGPU = (\n",
    "        aiplatform.gapic.AcceleratorType.NVIDIA_TESLA_T4,\n",
    "        int(os.getenv(\"IS_TESTING_DEPLOY_GPU\")),\n",
    "    )\n",
    "else:\n",
    "    DEPLOY_GPU, DEPLOY_NGPU = (None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "container:training,prediction"
   },
   "source": [
    "设置预先构建的容器\n",
    "\n",
    "设置用于训练和预测的预先构建的Docker容器映像。\n",
    "\n",
    "有关最新列表，请参见[用于训练的预建容器](https://cloud.google.com/ai-platform-unified/docs/training/pre-built-containers)。\n",
    "\n",
    "有关最新列表，请参见[用于预测的预建容器](https://cloud.google.com/ai-platform-unified/docs/predictions/pre-built-containers)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "container:training,prediction"
   },
   "outputs": [],
   "source": [
    "if os.getenv(\"IS_TESTING_TF\"):\n",
    "    TF = os.getenv(\"IS_TESTING_TF\")\n",
    "else:\n",
    "    TF = \"2-11\"\n",
    "\n",
    "if TF[0] == \"2\":\n",
    "    if TRAIN_GPU:\n",
    "        TRAIN_VERSION = \"tf-gpu.{}\".format(TF)\n",
    "    else:\n",
    "        TRAIN_VERSION = \"tf-cpu.{}\".format(TF)\n",
    "    if DEPLOY_GPU:\n",
    "        DEPLOY_VERSION = \"tf2-gpu.{}\".format(TF)\n",
    "    else:\n",
    "        DEPLOY_VERSION = \"tf2-cpu.{}\".format(TF)\n",
    "else:\n",
    "    if TRAIN_GPU:\n",
    "        TRAIN_VERSION = \"tf-gpu.{}\".format(TF)\n",
    "    else:\n",
    "        TRAIN_VERSION = \"tf-cpu.{}\".format(TF)\n",
    "    if DEPLOY_GPU:\n",
    "        DEPLOY_VERSION = \"tf-gpu.{}\".format(TF)\n",
    "    else:\n",
    "        DEPLOY_VERSION = \"tf-cpu.{}\".format(TF)\n",
    "\n",
    "TRAIN_IMAGE = \"{}-docker.pkg.dev/vertex-ai/training/{}:latest\".format(\n",
    "    LOCATION.split(\"-\")[0], TRAIN_VERSION\n",
    ")\n",
    "DEPLOY_IMAGE = \"{}-docker.pkg.dev/vertex-ai/prediction/{}:latest\".format(\n",
    "    LOCATION.split(\"-\")[0], DEPLOY_VERSION\n",
    ")\n",
    "\n",
    "print(\"Training:\", TRAIN_IMAGE, TRAIN_GPU, TRAIN_NGPU)\n",
    "print(\"Deployment:\", DEPLOY_IMAGE, DEPLOY_GPU, DEPLOY_NGPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "machine:training,prediction"
   },
   "source": [
    "#### 设置机器类型\n",
    "\n",
    "接下来，设置用于训练和预测的机器类型。\n",
    "\n",
    "- 设置变量`TRAIN_COMPUTE`和`DEPLOY_COMPUTE`以配置用于训练和预测的虚拟机的计算资源。\n",
    "- `机器类型`\n",
    "     - `n1-standard`：每个vCPU 3.75GB内存。\n",
    "     - `n1-highmem`：每个vCPU 6.5GB内存\n",
    "     - `n1-highcpu`：每个vCPU 0.9GB内存\n",
    "- `vCPUs`：数量为\\[2, 4, 8, 16, 32, 64, 96\\]\n",
    "\n",
    "**注意**：以下内容不支持用于训练：\n",
    "\n",
    "- `standard`：2个vCPUs\n",
    "- `highcpu`：2、4和8个vCPUs\n",
    "\n",
    "**注意**：您也可以在训练和部署时使用n2和e2机器类型，但它们不支持GPU。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "machine:training,prediction"
   },
   "outputs": [],
   "source": [
    "if os.getenv(\"IS_TESTING_TRAIN_MACHINE\"):\n",
    "    MACHINE_TYPE = os.getenv(\"IS_TESTING_TRAIN_MACHINE\")\n",
    "else:\n",
    "    MACHINE_TYPE = \"n1-standard\"\n",
    "\n",
    "VCPU = \"4\"\n",
    "TRAIN_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
    "print(\"Train machine type\", TRAIN_COMPUTE)\n",
    "\n",
    "if os.getenv(\"IS_TESTING_DEPLOY_MACHINE\"):\n",
    "    MACHINE_TYPE = os.getenv(\"IS_TESTING_DEPLOY_MACHINE\")\n",
    "else:\n",
    "    MACHINE_TYPE = \"n1-standard\"\n",
    "\n",
    "VCPU = \"4\"\n",
    "DEPLOY_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
    "print(\"Deploy machine type\", DEPLOY_COMPUTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "examine_training_package"
   },
   "source": [
    "检查培训套件\n",
    "\n",
    "现在您已经准备好开始创建自己的自定义模型，并对CIFAR10进行训练。\n",
    "\n",
    "在开始训练之前，请查看下面的训练应用程序包。\n",
    "\n",
    "### 包布局\n",
    "\n",
    "在开始训练之前，请查看如何将Python包装为自定义训练任务。解压缩后，包含以下目录/文件布局。\n",
    "\n",
    "- PKG-INFO\n",
    "- README.md\n",
    "- setup.cfg\n",
    "- setup.py\n",
    "- trainer\n",
    "  - \\_\\_init\\_\\_.py\n",
    "  - task.py\n",
    "\n",
    "文件`setup.cfg`和`setup.py`包含将软件包安装到Docker镜像的操作环境的说明。\n",
    "\n",
    "文件`trainer/task.py`是执行自定义训练任务的Python脚本。\n",
    "\n",
    "**注意**：在引用工作池规范时，目录斜杠被替换为点（`trainer.task`），文件后缀（`.py`）被删除。\n",
    "\n",
    "### 包装配件\n",
    "\n",
    "在以下单元格中，您将装配训练包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "examine_training_package"
   },
   "outputs": [],
   "source": [
    "# Make folder for Python training script\n",
    "! rm -rf custom\n",
    "! mkdir custom\n",
    "\n",
    "# Add package information\n",
    "! touch custom/README.md\n",
    "\n",
    "setup_cfg = \"[egg_info]\\n\\ntag_build =\\n\\ntag_date = 0\"\n",
    "! echo \"$setup_cfg\" > custom/setup.cfg\n",
    "\n",
    "setup_py = \"import setuptools\\n\\nsetuptools.setup(\\n\\n    install_requires=[\\n\\n        'tensorflow_datasets==1.3.0',\\n\\n    ],\\n\\n    packages=setuptools.find_packages())\"\n",
    "! echo \"$setup_py\" > custom/setup.py\n",
    "\n",
    "pkg_info = \"Metadata-Version: 1.0\\n\\nName: CIFAR10 image classification\\n\\nVersion: 0.0.0\\n\\nSummary: Demostration training script\\n\\nHome-page: www.google.com\\n\\nAuthor: Google\\n\\nAuthor-email: aferlitsch@google.com\\n\\nLicense: Public\\n\\nDescription: Demo\\n\\nPlatform: Vertex\"\n",
    "! echo \"$pkg_info\" > custom/PKG-INFO\n",
    "\n",
    "# Make the training subfolder\n",
    "! mkdir custom/trainer\n",
    "! touch custom/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "taskpy_contents:cifar10"
   },
   "source": [
    "### Task.py 内容\n",
    "\n",
    "在下一个单元格中，您将编写训练脚本 *task.py* 的内容。\n",
    "\n",
    "总的来说，*task.py* 脚本执行以下操作：\n",
    "\n",
    "- 从命令行获取保存模型文件的目录（`--model_dir`），如果未指定，则从环境变量 `AIP_MODEL_DIR` 中获取。\n",
    "- 从 TF 数据集（tfds）中加载 CIFAR10 数据集。\n",
    "- 使用 TF.Keras 模型 API 构建模型。\n",
    "- 编译模型（`compile()`）。\n",
    "- 根据参数 `args.distribute` 设置训练分布策略。\n",
    "- 根据参数 `args.epochs` 和 `args.steps` 训练模型（`fit()`）。\n",
    "- 将经过训练的模型保存到指定的模型目录（`save(args.model_dir)`）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "taskpy_contents:cifar10"
   },
   "outputs": [],
   "source": [
    "%%writefile custom/trainer/task.py\n",
    "# Single, Mirror and Multi-Machine Distributed Training for CIFAR-10\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model-dir', dest='model_dir',\n",
    "                    default=os.getenv(\"AIP_MODEL_DIR\"), type=str, help='Model dir.')\n",
    "parser.add_argument('--lr', dest='lr',\n",
    "                    default=0.01, type=float,\n",
    "                    help='Learning rate.')\n",
    "parser.add_argument('--epochs', dest='epochs',\n",
    "                    default=10, type=int,\n",
    "                    help='Number of epochs.')\n",
    "parser.add_argument('--steps', dest='steps',\n",
    "                    default=200, type=int,\n",
    "                    help='Number of steps per epoch.')\n",
    "parser.add_argument('--distribute', dest='distribute', type=str, default='single',\n",
    "                    help='distributed training strategy')\n",
    "args = parser.parse_args()\n",
    "\n",
    "print('Python Version = {}'.format(sys.version))\n",
    "print('TensorFlow Version = {}'.format(tf.__version__))\n",
    "print('TF_CONFIG = {}'.format(os.environ.get('TF_CONFIG', 'Not found')))\n",
    "print('DEVICES', device_lib.list_local_devices())\n",
    "\n",
    "# Single Machine, single compute device\n",
    "if args.distribute == 'single':\n",
    "    if tf.test.is_gpu_available():\n",
    "        strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
    "    else:\n",
    "        strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
    "# Single Machine, multiple compute device\n",
    "elif args.distribute == 'mirror':\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "# Multiple Machine, multiple compute device\n",
    "elif args.distribute == 'multi':\n",
    "    strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n",
    "\n",
    "# Multi-worker configuration\n",
    "print('num_replicas_in_sync = {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "# Preparing dataset\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "\n",
    "def make_datasets_unbatched():\n",
    "\n",
    "  # Scaling CIFAR10 data from (0, 255] to (0., 1.]\n",
    "  def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255.0\n",
    "    return image, label\n",
    "\n",
    "\n",
    "  datasets, info = tfds.load(name='cifar10',\n",
    "                            with_info=True,\n",
    "                            as_supervised=True)\n",
    "  return datasets['train'].map(scale).cache().shuffle(BUFFER_SIZE).repeat()\n",
    "\n",
    "\n",
    "# Build the Keras model\n",
    "def build_and_compile_cnn_model():\n",
    "  model = tf.keras.Sequential([\n",
    "      tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(32, 32, 3)),\n",
    "      tf.keras.layers.MaxPooling2D(),\n",
    "      tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "      tf.keras.layers.MaxPooling2D(),\n",
    "      tf.keras.layers.Flatten(),\n",
    "      tf.keras.layers.Dense(10, activation='softmax')\n",
    "  ])\n",
    "  model.compile(\n",
    "      loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "      optimizer=tf.keras.optimizers.SGD(learning_rate=args.lr),\n",
    "      metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "\n",
    "# Train the model\n",
    "NUM_WORKERS = strategy.num_replicas_in_sync\n",
    "# Here the batch size scales up by number of workers since\n",
    "# `tf.data.Dataset.batch` expects the global batch size.\n",
    "GLOBAL_BATCH_SIZE = BATCH_SIZE * NUM_WORKERS\n",
    "train_dataset = make_datasets_unbatched().batch(GLOBAL_BATCH_SIZE)\n",
    "\n",
    "with strategy.scope():\n",
    "  # Creation of dataset, and model building/compiling need to be within\n",
    "  # `strategy.scope()`.\n",
    "  model = build_and_compile_cnn_model()\n",
    "\n",
    "model.fit(x=train_dataset, epochs=args.epochs, steps_per_epoch=args.steps)\n",
    "model.save(args.model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tarball_training_script"
   },
   "source": [
    "## 将培训脚本存储在您的云存储桶中\n",
    "\n",
    "接下来，您将培训文件夹打包成压缩的tar文件，然后将其存储在您的云存储桶中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tarball_training_script"
   },
   "outputs": [],
   "source": [
    "! rm -f custom.tar custom.tar.gz\n",
    "! tar cvf custom.tar custom\n",
    "! gzip custom.tar\n",
    "! gsutil cp custom.tar.gz $BUCKET_URI/trainer_cifar10.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_custom_training_job:mbsdk,no_model"
   },
   "source": [
    "## 创建并运行自定义训练作业\n",
    "\n",
    "要训练一个自定义模型，您需要执行两个步骤：1）创建一个自定义训练作业，2）运行这个作业。\n",
    "\n",
    "### 创建自定义训练作业\n",
    "\n",
    "使用`CustomTrainingJob`类创建一个自定义训练作业，需要提供以下参数：\n",
    "\n",
    "- `display_name`：自定义训练作业的可读名称。\n",
    "- `container_uri`：训练容器镜像。\n",
    "- `requirements`：训练容器镜像的包要求（例如，pandas）。\n",
    "- `script_path`：训练脚本的相对路径。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_custom_training_job:mbsdk,no_model"
   },
   "outputs": [],
   "source": [
    "job = aiplatform.CustomTrainingJob(\n",
    "    display_name=\"cifar10\",\n",
    "    script_path=\"custom/trainer/task.py\",\n",
    "    container_uri=TRAIN_IMAGE,\n",
    "    requirements=[\"gcsfs==0.7.1\", \"tensorflow-datasets==4.4\"],\n",
    ")\n",
    "\n",
    "print(job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "prepare_custom_cmdargs"
   },
   "source": [
    "### 准备您的训练参数\n",
    "\n",
    "现在为您的自定义训练容器定义命令行参数：\n",
    "\n",
    "- `args`：传递给将作为容器入口点设置的可执行文件的命令行参数。\n",
    "  - `--model-dir`：用于指定存储模型工件的位置的命令行参数。您可以使用以下任一方法指定工件的存储位置。\n",
    "      - **方法1**（将`DIRECT`设置为`True`）：您将Cloud Storage位置作为命令行参数传递给您的训练脚本。\n",
    "      - **方法2**（将`DIRECT`设置为`False`）：服务将Cloud Storage位置作为环境变量`AIP_MODEL_DIR`传递给您的训练脚本。在这种情况下，您在作业规范中告诉服务模型工件的位置。\n",
    "  - `--epochs`：训练的时代数。\n",
    "  - `--steps`：每个时代的步数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prepare_custom_cmdargs"
   },
   "outputs": [],
   "source": [
    "MODEL_DIR = \"{}/{}\".format(BUCKET_URI, \"model\")\n",
    "\n",
    "EPOCHS = 20\n",
    "STEPS = 100\n",
    "\n",
    "DIRECT = True  # Set False to use AIP_MODEL_DIR\n",
    "if DIRECT:\n",
    "    CMDARGS = [\n",
    "        \"--model-dir=\" + MODEL_DIR,\n",
    "        \"--epochs=\" + str(EPOCHS),\n",
    "        \"--steps=\" + str(STEPS),\n",
    "    ]\n",
    "else:\n",
    "    CMDARGS = [\n",
    "        \"--epochs=\" + str(EPOCHS),\n",
    "        \"--steps=\" + str(STEPS),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run_custom_job:mbsdk,no_model"
   },
   "source": [
    "### 运行自定义训练作业\n",
    "\n",
    "接下来，您可以运行自定义作业以通过调用 `run()` 方法开始训练作业，以下是参数：\n",
    "\n",
    "- `args`: 要传递给训练脚本的命令行参数。\n",
    "- `replica_count`: 用于训练的计算实例数量（replica_count = 1 代表单节点训练）。\n",
    "- `machine_type`: 计算实例的机器类型。\n",
    "- `accelerator_type`: 硬件加速器类型。\n",
    "- `accelerator_count`: 要附加到工作人员副本的加速器数量。\n",
    "- `base_output_dir`: 用于存储模型工件的 Cloud Storage 位置。\n",
    "- `sync`: 设置为 **True** 以等待作业完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_custom_job:mbsdk,no_model"
   },
   "outputs": [],
   "source": [
    "if TRAIN_GPU:\n",
    "    job.run(\n",
    "        args=CMDARGS,\n",
    "        replica_count=1,\n",
    "        machine_type=TRAIN_COMPUTE,\n",
    "        accelerator_type=TRAIN_GPU.name,\n",
    "        accelerator_count=TRAIN_NGPU,\n",
    "        base_output_dir=MODEL_DIR,\n",
    "        sync=True,\n",
    "    )\n",
    "else:\n",
    "    job.run(\n",
    "        args=CMDARGS,\n",
    "        replica_count=1,\n",
    "        machine_type=TRAIN_COMPUTE,\n",
    "        base_output_dir=MODEL_DIR,\n",
    "        sync=True,\n",
    "    )\n",
    "\n",
    "model_path_to_deploy = MODEL_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "load_saved_model"
   },
   "source": [
    "## 加载已保存的模型\n",
    "\n",
    "您的模型以TensorFlow SavedModel格式存储在Cloud Storage存储桶中。现在从Cloud Storage存储桶中加载它，然后您可以做一些事情，比如评估模型和进行预测。\n",
    "\n",
    "要加载模型，您可以使用TF.Keras的`model.load_model()`方法，将其传递给模型保存的Cloud Storage路径 -- 由`MODEL_DIR`指定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_saved_model"
   },
   "outputs": [],
   "source": [
    "local_model = tf.keras.models.load_model(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluate_custom_model:image"
   },
   "source": [
    "## 评估模型\n",
    "\n",
    "现在，找出模型的表现如何。准备一些用于评估的测试数据，并运行`evaluate()`方法。\n",
    "\n",
    "### 加载评估数据\n",
    "\n",
    "使用`tf.keras.datasets`中的`load_data()`方法加载CIFAR10测试（留存）数据。这会返回一个包含两个元素的元组，第一个元素是训练数据，第二个元素是测试数据。每个元素也是一个包含两个元素的元组：图像数据和对应的标签。\n",
    "\n",
    "对于这一步，你不需要训练数据。因此，通过加载为`(_, _)`来跳过它。\n",
    "\n",
    "在你可以通过评估数据之前，你需要对其进行预处理：\n",
    "\n",
    "`x_test`: 通过将每个像素除以255来对像素数据进行归一化（重新缩放）。这会用0到1之间的32位浮点数替换每个单字节整数像素。\n",
    "\n",
    "`y_test`: 标签目前是标量（稀疏的）。如果你回顾一下`trainer/task.py`脚本中的`compile()`步骤，你会发现它是为稀疏标签编译的。所以你不需要做任何其他操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluate_custom_model:image,cifar10"
   },
   "outputs": [],
   "source": [
    "(_, _), (x_test, y_test) = cifar10.load_data()\n",
    "x_test = (x_test / 255.0).astype(np.float32)\n",
    "\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "perform_evaluation_custom"
   },
   "source": [
    "###执行模型评估\n",
    "\n",
    "现在评估一下定制工作中的模型表现如何。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "perform_evaluation_custom"
   },
   "outputs": [],
   "source": [
    "local_model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "serving_function_image:xai"
   },
   "source": [
    "### 图像数据的服务功能\n",
    "\n",
    "要将图像传递给预测服务，您需要将压缩的（例如JPEG）图像字节编码为base64格式——这样在通过网络传输二进制数据时可以确保内容不被修改。由于部署的模型期望输入数据为原始（未压缩）字节，您需要确保base64编码的数据在传递给部署的模型之前被转换回原始字节。\n",
    "\n",
    "为了解决这个问题，定义一个服务函数（`serving_fn`）并将其附加到模型作为预处理步骤。添加一个`@tf.function`装饰器，以便将服务函数融合到底层模型中（而不是在CPU上游处理）。\n",
    "\n",
    "当您发送预测或解释请求时，请求的内容会被base64解码为Tensorflow字符串（`tf.string`），然后传递给服务函数（`serving_fn`）。服务函数会将`tf.string`预处理成原始（未压缩）的numpy字节（`preprocess_fn`）以符合模型的输入要求：\n",
    "- `io.decode_jpeg`- 解压JPG图像，返回一个具有三个通道（RGB）的Tensorflow张量。\n",
    "- `image.convert_image_dtype` - 将整数像素值转换为float32。\n",
    "- `image.resize` - 调整图像大小以符合模型的输入形状。\n",
    "- `resized / 255.0` - 将像素数据重新缩放（归一化）在0和1之间。\n",
    "\n",
    "此时，数据可以传递给模型（`m_call`）。\n",
    "\n",
    "#### XAI 签名\n",
    "\n",
    "当服务函数与底层模型一起保存（`tf.saved_model.save`）时，您可以将服务函数的输入层指定为` serving_default` 签名。\n",
    "\n",
    "对于XAI图像模型，您需要保存来自服务函数的另外两个签名：\n",
    "\n",
    "- `xai_preprocess`：服务函数中的预处理函数。\n",
    "- `xai_model`：用于调用模型的具体函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "serving_function_image:xai"
   },
   "outputs": [],
   "source": [
    "CONCRETE_INPUT = \"numpy_inputs\"\n",
    "\n",
    "\n",
    "def _preprocess(bytes_input):\n",
    "    decoded = tf.io.decode_jpeg(bytes_input, channels=3)\n",
    "    decoded = tf.image.convert_image_dtype(decoded, tf.float32)\n",
    "    resized = tf.image.resize(decoded, size=(32, 32))\n",
    "    rescale = tf.cast(resized / 255.0, tf.float32)\n",
    "    return rescale\n",
    "\n",
    "\n",
    "@tf.function(input_signature=[tf.TensorSpec([None], tf.string)])\n",
    "def preprocess_fn(bytes_inputs):\n",
    "    decoded_images = tf.map_fn(\n",
    "        _preprocess, bytes_inputs, dtype=tf.float32, back_prop=False\n",
    "    )\n",
    "    return {\n",
    "        CONCRETE_INPUT: decoded_images\n",
    "    }  # User needs to make sure the key matches model's input\n",
    "\n",
    "\n",
    "@tf.function(input_signature=[tf.TensorSpec([None], tf.string)])\n",
    "def serving_fn(bytes_inputs):\n",
    "    images = preprocess_fn(bytes_inputs)\n",
    "    prob = m_call(**images)\n",
    "    return prob\n",
    "\n",
    "\n",
    "m_call = tf.function(local_model.call).get_concrete_function(\n",
    "    [tf.TensorSpec(shape=[None, 32, 32, 3], dtype=tf.float32, name=CONCRETE_INPUT)]\n",
    ")\n",
    "\n",
    "tf.saved_model.save(\n",
    "    local_model,\n",
    "    model_path_to_deploy,\n",
    "    signatures={\n",
    "        \"serving_default\": serving_fn,\n",
    "        # Required for XAI\n",
    "        \"xai_preprocess\": preprocess_fn,\n",
    "        \"xai_model\": m_call,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "serving_function_signature:xai"
   },
   "source": [
    "获取服务功能签名\n",
    "\n",
    "您可以通过重新加载模型到内存中，并查询每个层对应的签名来获得模型输入和输出层的签名。\n",
    "\n",
    "在进行预测请求时，您需要将请求路由到服务函数而不是模型。因此，您需要了解用于服务函数的输入层名称，以便在进行预测请求时稍后使用。\n",
    "\n",
    "您还需要知道服务函数的输入和输出层的名称，以构建随后描述的解释元数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "serving_function_signature:xai"
   },
   "outputs": [],
   "source": [
    "loaded = tf.saved_model.load(model_path_to_deploy)\n",
    "\n",
    "serving_input = list(\n",
    "    loaded.signatures[\"serving_default\"].structured_input_signature[1].keys()\n",
    ")[0]\n",
    "print(\"Serving function input:\", serving_input)\n",
    "serving_output = list(loaded.signatures[\"serving_default\"].structured_outputs.keys())[0]\n",
    "print(\"Serving function output:\", serving_output)\n",
    "\n",
    "input_name = local_model.input.name\n",
    "print(\"Model input name:\", input_name)\n",
    "output_name = local_model.output.name\n",
    "print(\"Model output name:\", output_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "explanation_spec"
   },
   "source": [
    "### 说明规范\n",
    "\n",
    "要获取预测的解释，您必须在将自定义模型上传到 Vertex AI 模型资源时启用解释功能并设置相应的设置。这些设置被称为解释元数据，包括：\n",
    "\n",
    "- `参数`：这是用于对模型进行解释的可解释性算法的规范。您可以在以下选项中进行选择：\n",
    "  - Shapley（注意：对于图像数据不建议使用，因为可能涉及长时间运行的操作）\n",
    "  - XRAI\n",
    "  - Integrated Gradients\n",
    "- `元数据`：这是应用在您的自定义模型上的算法的规范。\n",
    "\n",
    "#### 解释参数\n",
    "\n",
    "查看解释算法设置的详细概述。\n",
    "\n",
    "#### Shapley\n",
    "\n",
    "为每个特征归因结果，考虑特征的不同排列。该方法提供了确切 Shapley 值的采样近似。\n",
    "\n",
    "用途：\n",
    "  - 在表格数据上进行分类和回归。\n",
    "\n",
    "参数：\n",
    "\n",
    "- `path_count`：这是算法处理的特征上路径数。获得确切的 Shapley 值需要 M！个路径，其中 M 是特征的数量。对于 CIFAR10 数据集，这将是 784（28*28）。\n",
    "\n",
    "对于任何非平凡数量的特征而言，这太昂贵了。您可以将特征上路径的数量减少至 M * `path_count`。\n",
    "\n",
    "#### Integrated Gradients\n",
    "\n",
    "一种基于渐变的方法，用于有效地计算具有与 Shapley 值相同公理特性的特征归因。\n",
    "\n",
    "用途：\n",
    "  - 在表格数据上进行分类和回归。\n",
    "  - 在图像数据上进行分类。\n",
    "\n",
    "参数：\n",
    "\n",
    "- `step_count`：这是用于近似剩余总和的步数。步数越多，积分近似越准确。一般规则是 50 步，但随着步数的增加，计算时间也会增加。\n",
    "\n",
    "#### XRAI\n",
    "\n",
    "基于集成梯度方法，XRAI 评估图像的重叠区域，以创建一个显著性地图，突出显示图像的相关区域而不是像素。\n",
    "\n",
    "用途：\n",
    "  - 在图像数据上进行分类。\n",
    "\n",
    "参数：\n",
    "\n",
    "- `step_count`：这是用于近似剩余总和的步数。步数越多，积分近似越准确。一般规则是 50 步，但随着步数的增加，计算时间也会增加。\n",
    "\n",
    "在下一个代码单元格中，将变量 `XAI` 设置为您在自定义模型上使用的解释算法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "explanation_parameters:mbsdk"
   },
   "outputs": [],
   "source": [
    "XAI = \"ig\"  # [ shapley, ig, xrai ]\n",
    "\n",
    "if XAI == \"shapley\":\n",
    "    PARAMETERS = {\"sampled_shapley_attribution\": {\"path_count\": 10}}\n",
    "elif XAI == \"ig\":\n",
    "    PARAMETERS = {\"integrated_gradients_attribution\": {\"step_count\": 50}}\n",
    "elif XAI == \"xrai\":\n",
    "    PARAMETERS = {\"xrai_attribution\": {\"step_count\": 50}}\n",
    "\n",
    "parameters = aiplatform.explain.ExplanationParameters(PARAMETERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "explanation_metadata:image"
   },
   "source": [
    "#### 解释元数据\n",
    "\n",
    "现在，让我们更深入地了解解释元数据，其中包括：\n",
    "\n",
    "- `outputs`：输出属性中的标量值 -- 要解释的内容。例如，在分类的概率输出\\[0.1, 0.2, 0.7\\]中，我们想要解释0.7。考虑以下公式，其中输出为 `y`，这就是我们要解释的内容。\n",
    "\n",
    "    y = f(x)\n",
    "\n",
    "考虑以下公式，其中输出为 `y` 和 `z`。由于我们只能对一个标量值进行归因，我们必须选择是要解释输出 `y` 还是 `z`。假设在这个例子中，模型是目标检测，y 和 z 分别是边界框和对象分类。您需要选择要解释的两个输出中的哪一个。\n",
    "\n",
    "    y, z = f(x)\n",
    "\n",
    "`outputs` 的字典格式如下：\n",
    "\n",
    "    { \"outputs\": { \"[your_display_name]\":\n",
    "                   \"output_tensor_name\": [layer]\n",
    "                 }\n",
    "    }\n",
    "\n",
    "<blockquote>\n",
    " -  [your_display_name]: 您分配给要解释的输出的易读名称。一个常见例子是“概率”。\n",
    " -  \"output_tensor_name\": 用于识别要解释的输出层的键/值字段。\n",
    " -  [layer]: 要解释的输出层。在单一任务模型中，比如表格回归器，它是模型中的最后一层（最顶层）。\n",
    "</blockquote>\n",
    "\n",
    "- `inputs`：归因的特征 -- 它们如何影响输出。考虑下面的公式，其中 `a` 和 `b` 是特征。我们必须选择要解释它们是如何影响输出的特征。假设这个模型部署用于A/B测试，其中 `a` 是用于预测的数据项，而 `b` 用于识别模型实例是A还是B。您将想要选择 `a`（或其子集）作为特征，而不选择 `b`，因为它不会对预测有所贡献。\n",
    "\n",
    "    y = f(a,b)\n",
    "\n",
    "`inputs` 的最小字典格式如下：\n",
    "\n",
    "    { \"inputs\": { \"[your_display_name]\":\n",
    "                  \"input_tensor_name\": [layer]\n",
    "                 }\n",
    "    }\n",
    "\n",
    "<blockquote>\n",
    " -  [your_display_name]: 您分配给要解释的输入的易读名称。一个常见例子是“特征”。\n",
    " -  \"input_tensor_name\": 用于识别特征归因的输入层的键/值字段。\n",
    " -  [layer]: 用于特征归因的输入层。在单输入张量模型中，它是模型中的第一层（最底层）。\n",
    "</blockquote>\n",
    "\n",
    "由于模型的输入是表格形式的，您可以指定以下两个额外字段作为报告/可视化辅助：\n",
    "\n",
    "<blockquote>\n",
    " - \"modality\": \"image\": 表示字段值是图像数据。\n",
    "</blockquote>\n",
    "\n",
    "由于模型的输入是图像，您可以指定以下额外字段作为报告/可视化辅助：\n",
    "\n",
    "<blockquote>\n",
    " - \"modality\": \"image\": 表示字段值是图像数据。\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "explanation_metadata:mbsdk,image"
   },
   "outputs": [],
   "source": [
    "random_baseline = np.random.rand(32, 32, 3)\n",
    "input_baselines = [{\"number_vaue\": x} for x in random_baseline]\n",
    "\n",
    "INPUT_METADATA = {\"input_tensor_name\": CONCRETE_INPUT, \"modality\": \"image\"}\n",
    "\n",
    "OUTPUT_METADATA = {\"output_tensor_name\": serving_output}\n",
    "\n",
    "input_metadata = aiplatform.explain.ExplanationMetadata.InputMetadata(INPUT_METADATA)\n",
    "output_metadata = aiplatform.explain.ExplanationMetadata.OutputMetadata(OUTPUT_METADATA)\n",
    "\n",
    "metadata = aiplatform.explain.ExplanationMetadata(\n",
    "    inputs={\"image\": input_metadata}, outputs={\"class\": output_metadata}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upload_model:mbsdk,xai"
   },
   "source": [
    "## 上传模型\n",
    "\n",
    "接下来，使用`Model.upload()`方法将您的模型上传到Vertex AI模型注册表，具有以下参数：\n",
    "\n",
    "- `display_name`：模型资源的人类可读名称。\n",
    "- `artifact`：经过训练的模型工件的Cloud Storage位置。\n",
    "- `serving_container_image_uri`：用于提供的容器镜像。\n",
    "- `sync`：是否异步或同步执行上传。\n",
    "- `explanation_parameters`：配置解释模型预测的参数。\n",
    "- `explanation_metadata`：描述用于解释的模型输入和输出的元数据。\n",
    "\n",
    "如果`upload()`方法以异步方式运行，您可以随后使用`wait()`方法阻塞直到完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload_model:mbsdk,xai"
   },
   "outputs": [],
   "source": [
    "model = aiplatform.Model.upload(\n",
    "    display_name=\"cifar10\",\n",
    "    artifact_uri=MODEL_DIR,\n",
    "    serving_container_image_uri=DEPLOY_IMAGE,\n",
    "    explanation_parameters=parameters,\n",
    "    explanation_metadata=metadata,\n",
    "    sync=False,\n",
    ")\n",
    "\n",
    "model.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "deploy_model:mbsdk,all"
   },
   "source": [
    "## 部署模型\n",
    "\n",
    "接下来，部署您的模型进行在线预测。要部署模型，您需要调用`deploy()`方法，使用以下参数：\n",
    "\n",
    "- `deployed_model_display_name`：部署模型的可读名称。\n",
    "- `traffic_split`：指定流量分配在端点上前往该模型的百分比，格式为一个或多个键/值对的字典。\n",
    "如果只有一个模型，则指定为 { \"0\": 100 }，其中\"0\"指代上传的模型以及100表示100%的流量。\n",
    "如果端点上有现有的模型，需要分配流量，则使用model_id指定为 { \"0\": percent, model_id: percent, ... }，其中model_id是已部署端点的现有模型的模型 ID。百分比必须加起来等于100。\n",
    "- `machine_type`：用于训练的机器类型。\n",
    "- `accelerator_type`：硬件加速器类型。\n",
    "- `accelerator_count`：附加到工作者复制品的加速器数量。\n",
    "- `starting_replica_count`：初始预留的计算实例数量。\n",
    "- `max_replica_count`：可扩展到的最大计算实例数量。在本教程中，只有一个实例在预留。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "deploy_model:mbsdk,all"
   },
   "outputs": [],
   "source": [
    "DEPLOYED_NAME = \"cifar10\"\n",
    "\n",
    "TRAFFIC_SPLIT = {\"0\": 100}\n",
    "\n",
    "MIN_NODES = 1\n",
    "MAX_NODES = 1\n",
    "\n",
    "if DEPLOY_GPU:\n",
    "    endpoint = model.deploy(\n",
    "        deployed_model_display_name=DEPLOYED_NAME,\n",
    "        traffic_split=TRAFFIC_SPLIT,\n",
    "        machine_type=DEPLOY_COMPUTE,\n",
    "        accelerator_type=DEPLOY_GPU,\n",
    "        accelerator_count=DEPLOY_NGPU,\n",
    "        min_replica_count=MIN_NODES,\n",
    "        max_replica_count=MAX_NODES,\n",
    "    )\n",
    "else:\n",
    "    endpoint = model.deploy(\n",
    "        deployed_model_display_name=DEPLOYED_NAME,\n",
    "        traffic_split=TRAFFIC_SPLIT,\n",
    "        machine_type=DEPLOY_COMPUTE,\n",
    "        accelerator_type=DEPLOY_GPU,\n",
    "        accelerator_count=0,\n",
    "        min_replica_count=MIN_NODES,\n",
    "        max_replica_count=MAX_NODES,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "get_test_item:test"
   },
   "source": [
    "### 获取测试项\n",
    "\n",
    "从数据集的测试（holdout）部分中使用一个例子作为测试项。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "get_test_item:test"
   },
   "outputs": [],
   "source": [
    "test_image = x_test[0]\n",
    "test_label = y_test[0]\n",
    "print(test_image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "prepare_test_item:test,image"
   },
   "source": [
    "### 准备请求内容\n",
    "您将发送 CIFAR10 图像作为压缩的 JPG 图像，而不是原始未压缩的字节:\n",
    "\n",
    "- `cv2.imwrite`: 使用 openCV 将未压缩的图像写入磁盘作为压缩的 JPEG 图像。\n",
    " - 将图像数据从 [0,1) 范围反归一化到 [0,255)。\n",
    " - 将32位浮点值转换为8位无符号整数。\n",
    "- `tf.io.read_file`: 将压缩的 JPG 图像作为原始字节读入内存。\n",
    "- `base64.b64encode`: 将原始字节编码为 base64 编码的字符串。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prepare_test_item:test,image"
   },
   "outputs": [],
   "source": [
    "cv2.imwrite(\"tmp.jpg\", (test_image * 255).astype(np.uint8))\n",
    "\n",
    "bytes = tf.io.read_file(\"tmp.jpg\")\n",
    "b64str = base64.b64encode(bytes.numpy()).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "explain_request:mbsdk,custom,icn"
   },
   "source": [
    "### 用解释做预测\n",
    "\n",
    "现在您的模型资源已部署到终端资源，您可以通过向终端发送预测请求来获得在线解释。\n",
    "\n",
    "#### 请求\n",
    "\n",
    "每个实例的格式为：\n",
    "\n",
    "    [{serving_input: {'b64': bytes}]\n",
    "\n",
    "由于`explain()`方法可以接受多个项目（实例），请将您的单个测试项目作为一个测试项目的列表发送。\n",
    "\n",
    "#### 响应\n",
    "\n",
    "`explain()`调用的响应是一个包含以下条目的Python字典：\n",
    "\n",
    "- `ids`：每个预测请求的内部分配的唯一标识符。\n",
    "- `predictions`：每个实例的预测。\n",
    "- `deployed_model_id`：部署模型资源的Vertex AI标识符。\n",
    "- `explanations`：由可解释AI返回的特征归因。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "explain_request:mbsdk,custom,icn"
   },
   "outputs": [],
   "source": [
    "instances_list = [{serving_input: {\"b64\": b64str}}]\n",
    "\n",
    "response = endpoint.explain(instances_list)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "understanding_explanations:cifar10"
   },
   "source": [
    "### 可视化预测\n",
    "\n",
    "预览图像及其预测类别，不包括解释。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "understanding_explanations:cifar10"
   },
   "outputs": [],
   "source": [
    "CLASSES = [\n",
    "    \"airplane\",\n",
    "    \"automobile\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"deer\",\n",
    "    \"dog\",\n",
    "    \"frog\",\n",
    "    \"horse\",\n",
    "    \"ship\",\n",
    "    \"truck\",\n",
    "]\n",
    "\n",
    "# Note: change the `ig_response` variable below if you didn't deploy an IG model\n",
    "for prediction in response.predictions:\n",
    "    label_index = np.argmax(prediction)\n",
    "    class_name = CLASSES[label_index]\n",
    "    confidence_score = prediction[label_index]\n",
    "    print(\n",
    "        \"Predicted class: \"\n",
    "        + class_name\n",
    "        + \"\\n\"\n",
    "        + \"Confidence score: \"\n",
    "        + str(confidence_score)\n",
    "    )\n",
    "\n",
    "    image = base64.b64decode(b64str)\n",
    "    image = BytesIO(image)\n",
    "    img = mpimg.imread(image, format=\"JPG\")\n",
    "\n",
    "    plt.imshow(img, interpolation=\"nearest\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "visualize_image_explanations"
   },
   "source": [
    "### 用AI解释可视化图像\n",
    "\n",
    "返回的图像显示了模型预测的顶部类别的解释。这意味着如果模型的预测有误，您看到突出显示的像素是用于*错误类别*的。例如，如果模型预测为\"飞机\"，而实际应该是\"猫\"，您可以看到模型将这幅图像分类为飞机的解释。\n",
    "\n",
    "如果您部署了一个整合渐变模型，您可以可视化其特征归因。目前，从AI解释返回的突出显示的像素显示了对模型预测贡献最大的前60%的像素。在运行下面的单元格后看到的像素是最明显地标志着模型的预测的像素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualize_image_explanations"
   },
   "outputs": [],
   "source": [
    "for explanation in response.explanations:\n",
    "    attributions = dict(explanation.attributions[0].feature_attributions)\n",
    "    label_index = explanation.attributions[0].output_index[0]\n",
    "    class_name = CLASSES[label_index]\n",
    "    b64str = attributions[\"image\"][\"b64_jpeg\"]\n",
    "    image = base64.b64decode(b64str)\n",
    "    image = io.BytesIO(image)\n",
    "    img = mpimg.imread(image, format=\"JPG\")\n",
    "\n",
    "    plt.imshow(img, interpolation=\"nearest\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "undeploy_model:mbsdk"
   },
   "source": [
    "## 卸载模型\n",
    "\n",
    "当您完成使用模型进行预测后，请从终端点资源中卸载模型。此操作会取消所有计算资源并停止部署模型的计费。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "undeploy_model:mbsdk"
   },
   "outputs": [],
   "source": [
    "endpoint.undeploy_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleanup:mbsdk"
   },
   "source": [
    "清理\n",
    "\n",
    "要清理此项目中使用的所有Google Cloud资源，您可以[删除用于教程的Google Cloud项目](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects)。\n",
    "\n",
    "否则，您可以删除在本教程中创建的单个资源。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cleanup:mbsdk"
   },
   "outputs": [],
   "source": [
    "# Delete the training job\n",
    "job.delete()\n",
    "\n",
    "# Delete the endpoint\n",
    "endpoint.delete()\n",
    "\n",
    "# Delete the model\n",
    "model.delete()\n",
    "\n",
    "# Delete the Cloud Storage bucket\n",
    "delete_bucket = False  # Set True for deletion\n",
    "if delete_bucket:\n",
    "    ! gsutil rm -r $BUCKET_URI\n",
    "\n",
    "# Delete locally generated files\n",
    "! rm -rf custom/\n",
    "! rm custom.tar.gz\n",
    "! rm tmp.jpg"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "sdk_custom_image_classification_online_explain.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
