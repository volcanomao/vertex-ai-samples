{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "41aMjXA5rKoL"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_uIhhWEZdcBb"
   },
   "source": [
    "# 顶点 AI Python SDK：使用 Python 软件包、托管文本数据集和 TF 服务容器进行自定义训练\n",
    "\n",
    "<table align=\"left\">\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/sdk/SDK_Custom_Training_Python_Package_Managed_Text_Dataset_Tensorflow_Serving_Container.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> 在 Colab 中运行\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/sdk/SDK_Custom_Training_Python_Package_Managed_Text_Dataset_Tensorflow_Serving_Container.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      在 GitHub 上查看\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "<a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/sdk/SDK_Custom_Training_Python_Package_Managed_Text_Dataset_Tensorflow_Serving_Container.ipynb\" target='_blank'>\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"顶点 AI logo\">\n",
    "      在顶点 AI Workbench 中打开\n",
    "    </a>\n",
    "  </td>                                                                                               \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ij53c3EzrKoS"
   },
   "source": [
    "## 概述\n",
    "\n",
    "本笔记本演示了如何使用自定义Python包训练创建自定义模型，使用Vertex AI数据集，并使用TensorFlow-Serving容器提供在线预测和批量预测。您需要提供一个存储数据集的存储桶。\n",
    "\n",
    "注意：在测试此SDK时，您可能会因训练、预测、存储或使用其他GCP产品而产生费用。\n",
    "\n",
    "了解有关[自定义训练](https://cloud.google.com/vertex-ai/docs/training/custom-training)的更多信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bb0a1d001978"
   },
   "source": [
    "### 目标\n",
    "\n",
    "在本教程中，您将学习如何使用自定义 Python 包训练创建自定义模型，并学习如何使用 TensorFlow-Serving 容器为在线预测提供服务。然后您将执行对模型的批量预测。\n",
    "\n",
    "本教程使用以下 Google Cloud ML 服务和资源：\n",
    "\n",
    "- `Vertex AI 数据集`\n",
    "- `Vertex AI CustomPythonPackageTrainingJob`\n",
    "- `Vertex AI 模型`资源\n",
    "- `Vertex AI 端点`资源\n",
    "- `Vertex AI 批处理预测`\n",
    "\n",
    "执行的步骤包括：\n",
    "\n",
    "- 创建实用函数以下载数据并准备 CSV 文件以创建 Vertex AI 托管数据集\n",
    "- 下载数据\n",
    "- 准备 CSV 文件以创建托管数据集\n",
    "- 创建自定义训练 Python 包\n",
    "- 创建 TensorFlow Serving 容器\n",
    "- 使用托管文本数据集运行自定义 Python 包训练\n",
    "- 在 Vertex AI 上部署模型并创建端点\n",
    "- 在端点上进行预测\n",
    "- 在模型上创建批处理预测作业"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63b0c33d2b8c"
   },
   "source": [
    "### 数据集\n",
    "#### Stack Overflow数据\n",
    "您可以从https://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz下载并创建一个由Vertex AI管理的文本数据集。\n",
    "\n",
    "Stack Overflow数据受知识共享署名-相同方式共享3.0国际许可协议的许可。要查看此许可协议的副本，请访问http://creativecommons.org/licenses/by-sa/3.0/。\n",
    "\n",
    "有关此数据集的更多信息，请访问：https://console.cloud.google.com/marketplace/details/stack-exchange/stack-overflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9a87c7a200c"
   },
   "source": [
    "### 成本\n",
    "\n",
    "本教程使用计费组件的 Google 云服务：\n",
    "\n",
    "- Vertex AI\n",
    "- 云存储\n",
    "\n",
    "了解 [Vertex AI\n",
    "定价](https://cloud.google.com/vertex-ai/pricing) 和 [云存储\n",
    "定价](https://cloud.google.com/storage/pricing)，并使用 [定价\n",
    "计算器](https://cloud.google.com/products/calculator/)\n",
    "根据您的预期使用量生成成本估算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "306eb9fb5ac7"
   },
   "source": [
    "设置您的本地开发环境\n",
    "\n",
    "**如果您正在使用Colab或Vertex AI Workbench笔记本**，您的环境已经满足运行此笔记本的所有要求。您可以跳过这一步。\n",
    "\n",
    "**否则**，请确保您的环境满足本笔记本的要求。\n",
    "您需要以下内容：\n",
    "\n",
    "* Google Cloud SDK\n",
    "* Git\n",
    "* Python 3\n",
    "* virtualenv\n",
    "* 在使用Python 3的虚拟环境中运行的Jupyter笔记本\n",
    "\n",
    "Google Cloud指南 [设置Python开发环境](https://cloud.google.com/python/setup) 和 [Jupyter安装指南](https://jupyter.org/install) 提供了满足这些要求的详细说明。以下步骤提供了一套简要的说明：\n",
    "\n",
    "1. [安装并初始化Cloud SDK。](https://cloud.google.com/sdk/docs/)\n",
    "\n",
    "2. [安装Python 3。](https://cloud.google.com/python/setup#installing_python)\n",
    "\n",
    "3. [安装\n",
    "   virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv)\n",
    "   并创建一个使用Python 3的虚拟环境。激活虚拟环境。\n",
    "\n",
    "4. 要安装Jupyter，在终端窗口的命令行中运行 `pip3 install jupyter`。\n",
    "\n",
    "5. 要启动Jupyter，在终端窗口的命令行中运行 `jupyter notebook`。\n",
    "\n",
    "6. 在Jupyter Notebook仪表板中打开此笔记本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xOMNWzTbftDr"
   },
   "source": [
    "安装\n",
    "\n",
    "安装以下必需的包以执行此笔记本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9eb4e79842bf"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Vertex AI Workbench Notebook product has specific requirements\n",
    "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\")\n",
    "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
    "    \"/opt/deeplearning/metadata/env_version\"\n",
    ")\n",
    "\n",
    "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_WORKBENCH_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\"\n",
    "\n",
    "! pip3 install --upgrade google-cloud-aiplatform tensorflow {USER_FLAG} -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ed89a5527afb"
   },
   "source": [
    "### 重新启动内核\n",
    "\n",
    "在安装额外的软件包后，您需要重新启动笔记本内核，以便它可以找到这些软件包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9a2bb523c478"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "af017a0645ea"
   },
   "source": [
    "## 在开始之前\n",
    "\n",
    "### 设置你的Google Cloud项目\n",
    "\n",
    "**无论你使用哪种笔记本环境，都需要完成以下步骤。**\n",
    "\n",
    "1. [选择或创建一个Google Cloud项目](https://console.cloud.google.com/cloud-resource-manager)。当你第一次创建账户时，你将获得$300的免费信用，可以用于支付计算和存储成本。\n",
    "\n",
    "1. [确保你的项目已启用计费功能](https://cloud.google.com/billing/docs/how-to/modify-project)。\n",
    "\n",
    "1. [启用Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com)。\n",
    "\n",
    "1. 如果你在本地运行这个笔记本，你需要安装[Cloud SDK](https://cloud.google.com/sdk)。\n",
    "\n",
    "1. 在下面的单元格中输入你的项目ID。然后运行单元格，确保Cloud SDK在本笔记本中的所有命令中使用正确的项目。\n",
    "\n",
    "**注意**：Jupyter会将以`!`为前缀的行作为shell命令运行，并将以`$`为前缀的Python变量插入这些命令中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56d591439df1"
   },
   "source": [
    "设置您的项目ID\n",
    "\n",
    "**如果您不知道您的项目ID**，您可以使用`gcloud`来获取您的项目ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2016f37329c0"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5bf9979b96ff"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID:\", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "09021c90b34c"
   },
   "outputs": [],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9658ecf524b1"
   },
   "source": [
    "区域\n",
    "\n",
    "您还可以更改“REGION”变量，该变量用于笔记本的其余部分操作。以下是 Vertex AI 支持的区域。建议您选择最接近您的区域。\n",
    "\n",
    "- 美洲: `us-central1`\n",
    "- 欧洲: `europe-west4`\n",
    "- 亚太: `asia-east1`\n",
    "\n",
    "您可能不能使用多区域存储桶来进行 Vertex AI 的训练。并非所有区域都支持所有 Vertex AI 服务。\n",
    "\n",
    "了解有关 [Vertex AI 区域](https://cloud.google.com/vertex-ai/docs/general/locations) 的更多信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5c615e53149f"
   },
   "outputs": [],
   "source": [
    "REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
    "\n",
    "if REGION == \"[your-region]\":\n",
    "    REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e663bd062c6f"
   },
   "source": [
    "UUID\n",
    "\n",
    "如果您在进行实时教程会话，可能会使用共享的测试账户或项目。为了避免用户在创建的资源之间发生名称冲突，您可以为每个实例会话创建一个UUID，并将其附加到您在本教程中创建的资源名称上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "953fa6e5ddda"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "\n",
    "# Generate a uuid of a specifed length(default=8)\n",
    "def generate_uuid(length: int = 8) -> str:\n",
    "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
    "\n",
    "\n",
    "UUID = generate_uuid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "378e70541ba9"
   },
   "source": [
    "###验证您的Google Cloud账户\n",
    "\n",
    "**如果您正在使用Vertex AI Workbench Notebooks**，您的环境已经通过验证。\n",
    "\n",
    "**如果您正在使用Colab**，请运行下面的单元格，并按照提示进行账户oAuth验证。\n",
    "\n",
    "**否则**，请按照以下步骤操作：\n",
    "\n",
    "1. 在Cloud Console中，转到[**创建服务账户密钥**页面](https://console.cloud.google.com/apis/credentials/serviceaccountkey)。\n",
    "\n",
    "2. 点击**创建服务账户**。\n",
    "\n",
    "3. 在**服务账户名称**字段中输入一个名称，并单击**创建**。\n",
    "\n",
    "4. 在**授予该服务账户对项目的访问权限**部分，点击**角色**下拉列表。在过滤框中输入“Vertex AI”，并选择**Vertex AI管理员**。在过滤框中输入“存储对象管理员”，并选择**存储对象管理员**。\n",
    "\n",
    "5. 点击*创建*。一个包含您密钥的JSON文件将下载到您的本地环境。\n",
    "\n",
    "6. 在下面的单元格中将您的服务账户密钥路径输入为`GOOGLE_APPLICATION_CREDENTIALS`变量，并运行该单元格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f81b13f6ed38"
   },
   "outputs": [],
   "source": [
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# If on Vertex AI Workbench, then don't execute this code\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\") and not os.getenv(\n",
    "    \"DL_ANACONDA_HOME\"\n",
    "):\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\"):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS '[your-service-account-key-path]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bba822af0b32"
   },
   "source": [
    "### 创建一个云存储桶\n",
    "\n",
    "**无论您使用的笔记本环境如何，以下步骤都是必需的。**\n",
    "\n",
    "使用 Vertex AI SDK 提交训练作业时，您需要将包含训练代码的 Python 包上传到云存储桶中。Vertex AI 将从这个包中运行代码。在本教程中，Vertex AI 还会将训练作业产生的训练模型保存在同一个存储桶中。通过使用这个模型工件，您可以创建 Vertex AI 模型和端点资源，以便提供在线预测。\n",
    "\n",
    "在下方设置您的云存储桶的名称。它必须在所有云存储桶中保持唯一。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "52b311a64f71"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"[your-bucket-name]\"  # @param {type:\"string\"}\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "219a24ea078b"
   },
   "outputs": [],
   "source": [
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"[your-bucket-name]\":\n",
    "    BUCKET_NAME = PROJECT_ID + \"aip-\" + UUID\n",
    "    BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a8a62bec0259"
   },
   "source": [
    "只有当您的存储桶尚不存在时，才执行以下单元格以创建您的云存储存储桶。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "91c46850b49b"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4e69d430073b"
   },
   "source": [
    "最后，通过检查内容来验证对您的云存储桶的访问。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "835eaacd691f"
   },
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2d86f10ef734"
   },
   "source": [
    "导入库并定义常量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d630a1be6a5f"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "from google.cloud import aiplatform, storage\n",
    "from tensorflow.keras import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YHNj4P3vrKoV"
   },
   "source": [
    "### 设置您的应用程序名称、任务名称和目录。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RnNL5St8rKoV"
   },
   "outputs": [],
   "source": [
    "APP_NAME = \"keras-text-class-stack-overflow-tag\"\n",
    "TASK_TYPE = \"mbsdk_custom-py-pkg-training\"\n",
    "\n",
    "TASK_NAME = f\"{TASK_TYPE}_{APP_NAME}\"\n",
    "\n",
    "TASK_DIR = f\"./{TASK_NAME}\"\n",
    "DATA_DIR = f\"{TASK_DIR}/data\"\n",
    "\n",
    "print(f\"Task Name:      {TASK_NAME}\")\n",
    "print(f\"Task Directory: {TASK_DIR}\")\n",
    "print(f\"Data Directory: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hZmDcfLS13W"
   },
   "source": [
    "### 设置一个 GCS 前缀\n",
    "\n",
    "如果要将所有输入和输出文件集中在 gcs 位置下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W_4gzaP0SyQd"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = BUCKET_URI.split(\"gs://\")[1]\n",
    "GCS_PREFIX = f\"{TASK_TYPE}/{APP_NAME}\"\n",
    "\n",
    "print(f\"Bucket Name:    {BUCKET_NAME}\")\n",
    "print(f\"GCS Prefix:     {GCS_PREFIX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJF047yNftDw"
   },
   "source": [
    "### 用于下载数据并准备 CSV 文件以创建 Vertex AI 托管数据集的实用函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9yOl-l_oftDx"
   },
   "outputs": [],
   "source": [
    "def upload_blob(bucket_name, source_file_name, destination_blob_name):\n",
    "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "\n",
    "    destination_file_name = os.path.join(\"gs://\", bucket_name, destination_blob_name)\n",
    "\n",
    "    return destination_file_name\n",
    "\n",
    "\n",
    "def download_data(data_dir):\n",
    "    \"\"\"Download data.\"\"\"\n",
    "\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "\n",
    "    url = \"https://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz\"\n",
    "    dataset = utils.get_file(\n",
    "        \"stack_overflow_16k.tar.gz\",\n",
    "        url,\n",
    "        untar=True,\n",
    "        cache_dir=data_dir,\n",
    "        cache_subdir=\"\",\n",
    "    )\n",
    "    data_dir = os.path.join(os.path.dirname(dataset))\n",
    "\n",
    "    return data_dir\n",
    "\n",
    "\n",
    "def upload_train_data_to_gcs(train_data_dir, bucket_name, destination_blob_prefix):\n",
    "    \"\"\"Create CSV file using train data content.\"\"\"\n",
    "\n",
    "    train_data_dir = os.path.join(data_dir, \"train\")\n",
    "    train_data_fn = os.path.join(data_dir, \"train.csv\")\n",
    "\n",
    "    fp = open(train_data_fn, \"w\", encoding=\"utf8\")\n",
    "    writer = csv.writer(\n",
    "        fp, delimiter=\",\", quotechar='\"', quoting=csv.QUOTE_ALL, lineterminator=\"\\n\"\n",
    "    )\n",
    "\n",
    "    for root, _, files in os.walk(train_data_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".txt\"):\n",
    "                class_name = root.split(\"/\")[-1]\n",
    "                file_fn = os.path.join(root, file)\n",
    "                with open(file_fn) as f:\n",
    "                    content = f.readlines()\n",
    "                    lines = [x.strip().strip('\"') for x in content]\n",
    "                    writer.writerow((lines[0], class_name))\n",
    "\n",
    "    fp.close()\n",
    "\n",
    "    train_gcs_url = upload_blob(\n",
    "        bucket_name, train_data_fn, os.path.join(destination_blob_prefix, \"train.csv\")\n",
    "    )\n",
    "\n",
    "    return train_gcs_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3UDGt8MrKoY"
   },
   "source": [
    "### 下载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ks2ahWfUrKoa"
   },
   "outputs": [],
   "source": [
    "data_dir = download_data(DATA_DIR)\n",
    "print(f\"Data is downloaded to: {data_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l7AHtjj3rKoa"
   },
   "outputs": [],
   "source": [
    "!ls $data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "puroD_Wr-W4Y"
   },
   "outputs": [],
   "source": [
    "!ls $data_dir/train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQNL5AjtrKoa"
   },
   "source": [
    "### 为创建受控数据集准备csv文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ewrG6VArKob"
   },
   "source": [
    "创建包含数据内容的CSV文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GvbpGr0ArKob"
   },
   "outputs": [],
   "source": [
    "gcs_source_train_url = upload_train_data_to_gcs(\n",
    "    train_data_dir=os.path.join(data_dir, \"train\"),\n",
    "    bucket_name=BUCKET_NAME,\n",
    "    destination_blob_prefix=f\"{GCS_PREFIX}/data\",\n",
    ")\n",
    "\n",
    "print(f\"Train data content is loaded to {gcs_source_train_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gg_OiKJUrKoc"
   },
   "outputs": [],
   "source": [
    "!gsutil ls gs://$BUCKET_NAME/$GCS_PREFIX/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qzKx7Z2WrKoc"
   },
   "source": [
    "# 创建自定义的Python训练包\n",
    "\n",
    "在您可以使用预先构建的容器进行自定义训练之前，您必须创建一个包含您的训练应用程序的[Python源分发包](https://docs.python.org/3/distutils/sourcedist.html)，并将其上传到您的Google Cloud项目可以访问的Cloud Storage存储桶中。\n",
    "\n",
    "您需要创建一个目录，并将所有包构建工件写入该文件夹。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eBDjNpkLrKoc"
   },
   "outputs": [],
   "source": [
    "PYTHON_PACKAGE_APPLICATION_DIR = f\"{TASK_NAME}/trainer\"\n",
    "\n",
    "!mkdir -p $PYTHON_PACKAGE_APPLICATION_DIR\n",
    "!touch $PYTHON_PACKAGE_APPLICATION_DIR/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sPSRvSvMrKod"
   },
   "source": [
    "### 编写培训脚本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zupq7mwzrKod"
   },
   "outputs": [],
   "source": [
    "%%writefile {PYTHON_PACKAGE_APPLICATION_DIR}/task.py\n",
    "\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "import json\n",
    "import tqdm\n",
    "\n",
    "VOCAB_SIZE = 10000\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "\n",
    "def str2bool(v):\n",
    "  if isinstance(v, bool):\n",
    "    return v\n",
    "  if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "    return True\n",
    "  elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "    return False\n",
    "  else:\n",
    "    raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "\n",
    "def build_model(num_classes, loss, optimizer, metrics, vectorize_layer):\n",
    "  # vocab_size is VOCAB_SIZE + 1 since 0 is used additionally for padding.\n",
    "  model = tf.keras.Sequential([\n",
    "      vectorize_layer,\n",
    "      layers.Embedding(VOCAB_SIZE + 1, 64, mask_zero=True),\n",
    "      layers.Conv1D(64, 5, padding=\"valid\", activation=\"relu\", strides=2),\n",
    "      layers.GlobalMaxPooling1D(),\n",
    "      layers.Dense(num_classes),\n",
    "      layers.Activation('softmax')\n",
    "  ])\n",
    "  model.compile(\n",
    "      loss=loss,\n",
    "      optimizer=optimizer,\n",
    "      metrics=metrics)\n",
    "\n",
    "  return model\n",
    "\n",
    "def get_string_labels(predicted_scores_batch, class_names):\n",
    "  predicted_labels = tf.argmax(predicted_scores_batch, axis=1)\n",
    "  predicted_labels = tf.gather(class_names, predicted_labels)\n",
    "  return predicted_labels\n",
    "\n",
    "def predict(export_model, class_names, inputs):\n",
    "  predicted_scores = export_model.predict(inputs)\n",
    "  predicted_labels = get_string_labels(predicted_scores, class_names)\n",
    "  return predicted_labels\n",
    "\n",
    "def parse_args():\n",
    "  parser = argparse.ArgumentParser(\n",
    "      description='Keras Text Classification on Stack Overflow Questions')\n",
    "  parser.add_argument(\n",
    "      '--epochs', default=25, type=int, help='number of training epochs')\n",
    "  parser.add_argument(\n",
    "      '--batch-size', default=16, type=int, help='mini-batch size')\n",
    "  parser.add_argument(\n",
    "      '--model-dir', default=os.getenv('AIP_MODEL_DIR'), type=str, help='model directory')\n",
    "  parser.add_argument(\n",
    "      '--data-dir', default='./data', type=str, help='data directory')\n",
    "  parser.add_argument(\n",
    "      '--test-run', default=False, type=str2bool, help='test run the training application, i.e. 1 epoch for training using sample dataset')\n",
    "  parser.add_argument(\n",
    "      '--model-version', default=1, type=int, help='model version')\n",
    "  args = parser.parse_args()\n",
    "  return args\n",
    "\n",
    "def load_aip_dataset(aip_data_uri_pattern, batch_size, class_names, test_run, shuffle=True, seed=42):\n",
    "\n",
    "  data_file_urls = list()\n",
    "  labels = list()\n",
    "\n",
    "  class_indices = dict(zip(class_names, range(len(class_names))))\n",
    "  num_classes = len(class_names)\n",
    "\n",
    "  for aip_data_uri in tqdm.tqdm(tf.io.gfile.glob(pattern=aip_data_uri_pattern)):\n",
    "    with tf.io.gfile.GFile(name=aip_data_uri, mode='r') as gfile:\n",
    "      for line in gfile.readlines():\n",
    "        line = json.loads(line)\n",
    "        data_file_urls.append(line['textContent'])\n",
    "        classification_annotation = line['classificationAnnotations'][0]\n",
    "        label = classification_annotation['displayName']\n",
    "        labels.append(class_indices[label])\n",
    "        if test_run:\n",
    "          break\n",
    "\n",
    "  data = list()\n",
    "  for data_file_url in tqdm.tqdm(data_file_urls):\n",
    "    with tf.io.gfile.GFile(name=data_file_url, mode='r') as gf:\n",
    "      txt = gf.read()\n",
    "      data.append(txt)\n",
    "\n",
    "  print(f' data files count: {len(data_file_urls)}')\n",
    "  print(f' data count: {len(data)}')\n",
    "  print(f' labels count: {len(labels)}')\n",
    "\n",
    "  dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "  label_ds = tf.data.Dataset.from_tensor_slices(labels)\n",
    "  label_ds = label_ds.map(lambda x: tf.one_hot(x, num_classes))\n",
    "\n",
    "  dataset = tf.data.Dataset.zip((dataset, label_ds))\n",
    "\n",
    "  if shuffle:\n",
    "    # Shuffle locally at each iteration\n",
    "    dataset = dataset.shuffle(buffer_size=batch_size * 8, seed=seed)\n",
    "  dataset = dataset.batch(batch_size)\n",
    "  # Users may need to reference `class_names`.\n",
    "  dataset.class_names = class_names\n",
    "\n",
    "  return dataset\n",
    "\n",
    "def main():\n",
    "\n",
    "  args = parse_args()\n",
    "\n",
    "  class_names = ['csharp', 'java', 'javascript', 'python']\n",
    "  class_indices = dict(zip(class_names, range(len(class_names))))\n",
    "  num_classes = len(class_names)\n",
    "  print(f' class names: {class_names}')\n",
    "  print(f' class indices: {class_indices}')\n",
    "  print(f' num classes: {num_classes}')\n",
    "\n",
    "  epochs = 1 if args.test_run else args.epochs\n",
    "\n",
    "  aip_model_dir = os.environ.get('AIP_MODEL_DIR')\n",
    "  aip_data_format = os.environ.get('AIP_DATA_FORMAT')\n",
    "  aip_training_data_uri = os.environ.get('AIP_TRAINING_DATA_URI')\n",
    "  aip_validation_data_uri = os.environ.get('AIP_VALIDATION_DATA_URI')\n",
    "  aip_test_data_uri = os.environ.get('AIP_TEST_DATA_URI')\n",
    "\n",
    "  print(f\"aip_model_dir: {aip_model_dir}\")\n",
    "  print(f\"aip_data_format: {aip_data_format}\")\n",
    "  print(f\"aip_training_data_uri: {aip_training_data_uri}\")\n",
    "  print(f\"aip_validation_data_uri: {aip_validation_data_uri}\")\n",
    "  print(f\"aip_test_data_uri: {aip_test_data_uri}\")\n",
    "\n",
    "  print('Loading AIP dataset')\n",
    "  train_ds = load_aip_dataset(\n",
    "      aip_training_data_uri, args.batch_size, class_names, args.test_run)\n",
    "  print('AIP training dataset is loaded')\n",
    "  val_ds = load_aip_dataset(\n",
    "      aip_validation_data_uri, 1, class_names, args.test_run)\n",
    "  print('AIP validation dataset is loaded')\n",
    "  test_ds = load_aip_dataset(\n",
    "      aip_test_data_uri, 1, class_names, args.test_run)\n",
    "  print('AIP test dataset is loaded')\n",
    "\n",
    "  vectorize_layer = TextVectorization(\n",
    "      max_tokens=VOCAB_SIZE,\n",
    "      output_mode='int',\n",
    "      output_sequence_length=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "  train_text = train_ds.map(lambda text, labels: text)\n",
    "  vectorize_layer.adapt(train_text)\n",
    "  print('The vectorize_layer is adapted')\n",
    "\n",
    "\n",
    "  print('Build model')\n",
    "  optimizer = 'adam'\n",
    "  metrics = ['accuracy']\n",
    "\n",
    "  model = build_model(\n",
    "      num_classes, losses.CategoricalCrossentropy(from_logits=True), optimizer, metrics, vectorize_layer)\n",
    "\n",
    "  history = model.fit(train_ds, validation_data=val_ds, epochs=epochs)\n",
    "  history = history.history\n",
    "\n",
    "  print('Training accuracy: {acc}, loss: {loss}'.format(\n",
    "      acc=history['accuracy'][-1], loss=history['loss'][-1]))\n",
    "  print('Validation accuracy: {acc}, loss: {loss}'.format(\n",
    "      acc=history['val_accuracy'][-1], loss=history['val_loss'][-1]))\n",
    "\n",
    "  loss, accuracy = model.evaluate(test_ds)\n",
    "  print('Test accuracy: {acc}, loss: {loss}'.format(\n",
    "      acc=accuracy, loss=loss))\n",
    "\n",
    "  inputs = [\n",
    "      \"how do I extract keys from a dict into a list?\",  # python\n",
    "      \"debug public static void main(string[] args) {...}\",  # java\n",
    "  ]\n",
    "  predicted_labels = predict(model, class_names, inputs)\n",
    "  for input, label in zip(inputs, predicted_labels):\n",
    "    print(f'Question: {input}')\n",
    "    print(f'Predicted label: {label.numpy()}')\n",
    "\n",
    "  model_export_path = os.path.join(args.model_dir, str(args.model_version))\n",
    "  model.save(model_export_path)\n",
    "  print(f'Model version {args.model_version} is exported to {args.model_dir}')\n",
    "\n",
    "  loaded = tf.saved_model.load(model_export_path)\n",
    "  input_name = list(loaded.signatures['serving_default'].structured_input_signature[1].keys())[0]\n",
    "  print(f'Serving function input: {input_name}')\n",
    "\n",
    "  return\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAhynoNBrKoh"
   },
   "source": [
    "### 构建包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kXSBQcqdrKof"
   },
   "outputs": [],
   "source": [
    "%%writefile {TASK_DIR}/setup.py\n",
    "\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "setup(\n",
    "    name='trainer',\n",
    "    version='0.1',\n",
    "    packages=find_packages(),\n",
    "    install_requires=(),\n",
    "    include_package_data=True,\n",
    "    description='My training application.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MclanW0UrKoh"
   },
   "outputs": [],
   "source": [
    "!ls $TASK_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j19Nk2vzrKoh"
   },
   "outputs": [],
   "source": [
    "!cd $TASK_DIR && python3 setup.py sdist --formats=gztar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9c1r_k69rKoi"
   },
   "outputs": [],
   "source": [
    "!ls -ltr $TASK_DIR/dist/trainer-0.1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zyQv9AcNrKoi"
   },
   "source": [
    "将包上传到GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9WjF8SFrrKoi"
   },
   "outputs": [],
   "source": [
    "destination_blob_name = f\"custom-training-python-package/{APP_NAME}/trainer-0.1.tar.gz\"\n",
    "source_file_name = f\"{TASK_DIR}/dist/trainer-0.1.tar.gz\"\n",
    "\n",
    "python_package_gcs_uri = upload_blob(\n",
    "    BUCKET_NAME, source_file_name, destination_blob_name\n",
    ")\n",
    "python_module_name = \"trainer.task\"\n",
    "\n",
    "print(f\"Custom Training Python Package is uploaded to: {python_package_gcs_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hWJpv4ejrWi0"
   },
   "source": [
    "# 创建TensorFlow Serving容器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pI50mO1bPMS1"
   },
   "outputs": [],
   "source": [
    "TF_SERVING_CONTAINER_IMAGE_URI = f\"gcr.io/{PROJECT_ID}/tf-serving\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IMOad4Z4aOgv"
   },
   "source": [
    "下载 TensorFlow Serving Docker 镜像。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KC8e1yrHrVyA"
   },
   "outputs": [],
   "source": [
    "if not IS_COLAB:\n",
    "    !docker pull tensorflow/serving:2.8.0\n",
    "else:\n",
    "    # install docker daemon\n",
    "    ! apt-get -qq install docker.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "33d70b04763b"
   },
   "source": [
    "使用容器注册表配置Docker身份验证。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f729a8ac2c1f"
   },
   "outputs": [],
   "source": [
    "! gcloud auth configure-docker gcr.io --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7599a2703e5"
   },
   "source": [
    "创建一个用于注册图像的标签，并使用Cloud Container Registry (gcr.io) 注册图像。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bw_A7ynTsjTF"
   },
   "outputs": [],
   "source": [
    "if not IS_COLAB:\n",
    "    !docker tag tensorflow/serving:2.8.0 $TF_SERVING_CONTAINER_IMAGE_URI\n",
    "    !docker push $TF_SERVING_CONTAINER_IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "86_VILTZu-Ck"
   },
   "outputs": [],
   "source": [
    "%%bash -s $IS_COLAB $TF_SERVING_CONTAINER_IMAGE_URI\n",
    "if [ $1 == \"False\" ]; then\n",
    "  exit 0\n",
    "fi\n",
    "set -x\n",
    "dockerd -b none --iptables=0 -l warn &\n",
    "for i in $(seq 5); do [ ! -S \"/var/run/docker.sock\" ] && sleep 2 || break; done\n",
    "docker pull tensorflow/serving:2.8.0\n",
    "docker tag tensorflow/serving:2.8.0 $2\n",
    "docker push $2\n",
    "kill $(jobs -p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rc_MIJwlrKoj"
   },
   "source": [
    "# 使用托管文本数据集运行自定义Python软件包培训"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eiNL0HSVrKoj"
   },
   "source": [
    "初始化用于Python的Vertex AI SDK\n",
    "\n",
    "为Vertex AI SDK初始化*client*。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J5MqvGtMrKoj"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KGT1uT-HrKoj"
   },
   "source": [
    "创建一个Vertex AI数据集资源\n",
    "您可以使用之前准备的csv文件创建一个Vertex AI文本数据集。请选择以下其中一种选项。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lY566K2crKok"
   },
   "outputs": [],
   "source": [
    "dataset_display_name = f\"temp-{APP_NAME}-content\"\n",
    "gcs_source = gcs_source_train_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kyLoUsx9rKok"
   },
   "source": [
    "#### 使用csv文件创建数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h7QwC1ThrKok"
   },
   "outputs": [],
   "source": [
    "dataset = aiplatform.TextDataset.create(\n",
    "    display_name=dataset_display_name,\n",
    "    gcs_source=gcs_source,\n",
    "    import_schema_uri=aiplatform.schema.dataset.ioformat.text.single_label_classification,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NzLvubIrrKon"
   },
   "source": [
    "## 在 Vertex AI 上启动培训任务并创建模型\n",
    "\n",
    "接下来，您可以使用您刚刚构建的Python包来训练一个模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cyz86ixRYNtT"
   },
   "source": [
    "###配置一个培训任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9zk8SRuK1Xvi"
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = APP_NAME\n",
    "PRE_BUILT_TRAINING_CONTAINER_IMAGE_URI = (\n",
    "    \"us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-8:latest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rxc_yVxkbbbd"
   },
   "source": [
    "您需要指定构建并上传到GCS的Python软件包，Python软件包的模块名称，用于训练的预先构建的训练容器镜像URI，在本示例中，用于预测使用TensorFlow serving容器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uRGrFdxOftD1"
   },
   "outputs": [],
   "source": [
    "job = aiplatform.CustomPythonPackageTrainingJob(\n",
    "    display_name=f\"temp_{TASK_NAME}_tf-serving\",\n",
    "    python_package_gcs_uri=python_package_gcs_uri,\n",
    "    python_module_name=python_module_name,\n",
    "    container_uri=PRE_BUILT_TRAINING_CONTAINER_IMAGE_URI,\n",
    "    model_serving_container_image_uri=TF_SERVING_CONTAINER_IMAGE_URI,\n",
    "    model_serving_container_command=[\"/usr/bin/tensorflow_model_server\"],\n",
    "    model_serving_container_args=[\n",
    "        f\"--model_name={MODEL_NAME}\",\n",
    "        \"--model_base_path=$(AIP_STORAGE_URI)\",\n",
    "        \"--rest_api_port=8080\",\n",
    "        \"--port=8500\",\n",
    "        \"--file_system_poll_wait_seconds=31540000\",\n",
    "    ],\n",
    "    model_serving_container_predict_route=f\"/v1/models/{MODEL_NAME}:predict\",\n",
    "    model_serving_container_health_route=f\"/v1/models/{MODEL_NAME}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JPfZGlZeTN72"
   },
   "source": [
    "运行训练任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V6iY5wcerKon"
   },
   "outputs": [],
   "source": [
    "model = job.run(\n",
    "    dataset=dataset,\n",
    "    annotation_schema_uri=aiplatform.schema.dataset.annotation.text.classification,\n",
    "    args=[\"--epochs\", \"50\"],\n",
    "    replica_count=1,\n",
    "    model_display_name=f\"temp_{TASK_NAME}_tf-serving\",\n",
    "    sync=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zV1PjANLrKoo"
   },
   "outputs": [],
   "source": [
    "model.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XO0-zdXxrKoo"
   },
   "source": [
    "在Vertex AI上部署模型并创建端点\n",
    "\n",
    "部署您的模型，然后等到模型完成部署后再进行预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tEg2IDwPftD2"
   },
   "outputs": [],
   "source": [
    "endpoint = model.deploy(machine_type=\"n1-standard-4\", sync=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9AoIJdCHrKop"
   },
   "outputs": [],
   "source": [
    "endpoint.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z87MXJEDrKoq"
   },
   "source": [
    "## 在端点上进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vQXGCg8IrKoq"
   },
   "outputs": [],
   "source": [
    "class_names = [\"csharp\", \"java\", \"javascript\", \"python\"]\n",
    "\n",
    "class_ids = range(len(class_names))\n",
    "\n",
    "class_indices = dict(zip(class_names, class_ids))\n",
    "class_maps = dict(zip(class_ids, class_names))\n",
    "print(f\"Class Indices: {class_indices}\")\n",
    "print(f\"Class Maps:    {class_maps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Di8RtRxipm63"
   },
   "outputs": [],
   "source": [
    "text_inputs = [\n",
    "    \"how do I extract keys from a dict into a list?\",  # python\n",
    "    \"debug public static void main(string[] args) {...}\",  # java\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dPHYGAlvlTAj"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "predictions = endpoint.predict(instances=text_inputs)\n",
    "for text, predicted_scores in zip(text_inputs, predictions.predictions):\n",
    "    class_id = np.argmax(predicted_scores)\n",
    "    class_name = class_maps[class_id]\n",
    "    print(f\"Question: {text}\")\n",
    "    print(f\"Predicted Tag: {class_name}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gRkvzr2-LgA9"
   },
   "source": [
    "在模型上进行批量预测任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nlzRioF7UGFd"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def upload_test_data_to_gcs(test_data_dir, test_gcs_url):\n",
    "    \"\"\"Create JSON file using test data content.\"\"\"\n",
    "\n",
    "    input_name = \"text_vectorization_input\"\n",
    "\n",
    "    with tf.io.gfile.GFile(test_gcs_url, \"w\") as gf:\n",
    "\n",
    "        for root, _, files in os.walk(test_data_dir):\n",
    "            for file in files:\n",
    "                if file.endswith(\".txt\"):\n",
    "                    file_fn = os.path.join(root, file)\n",
    "                    with open(file_fn) as f:\n",
    "                        content = f.readlines()\n",
    "                        lines = [x.strip().strip('\"') for x in content]\n",
    "\n",
    "                        data = {input_name: lines[0]}\n",
    "                        gf.write(json.dumps(data))\n",
    "                        gf.write(\"\\n\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C_sbOm-5ud5C"
   },
   "outputs": [],
   "source": [
    "gcs_source_test_url = f\"gs://{BUCKET_NAME}/{GCS_PREFIX}/data/test.json\"\n",
    "upload_test_data_to_gcs(\n",
    "    test_data_dir=os.path.join(data_dir, \"test\"), test_gcs_url=gcs_source_test_url\n",
    ")\n",
    "\n",
    "print(f\"Test data content is loaded to {gcs_source_test_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QBMBk2WxLqBP"
   },
   "outputs": [],
   "source": [
    "!gsutil ls $gcs_source_test_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2JhoTiD5LuDW"
   },
   "outputs": [],
   "source": [
    "batch_predict_job = model.batch_predict(\n",
    "    job_display_name=f\"temp_{TASK_NAME}_tf-serving\",\n",
    "    gcs_source=gcs_source_test_url,\n",
    "    gcs_destination_prefix=f\"gs://{BUCKET_NAME}/{GCS_PREFIX}/batch_prediction\",\n",
    "    machine_type=\"n1-standard-4\",\n",
    "    sync=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "retsI3LLls_W"
   },
   "outputs": [],
   "source": [
    "batch_predict_job.wait()\n",
    "bp_iter_outputs = batch_predict_job.iter_outputs()\n",
    "\n",
    "prediction_errors_stats = list()\n",
    "prediction_results = list()\n",
    "for blob in bp_iter_outputs:\n",
    "    if blob.name.split(\"/\")[-1].startswith(\"prediction.errors_stats\"):\n",
    "        prediction_errors_stats.append(blob.name)\n",
    "    if blob.name.split(\"/\")[-1].startswith(\"prediction.results\"):\n",
    "        prediction_results.append(blob.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KmfK3Tzzlv9C"
   },
   "outputs": [],
   "source": [
    "tags = list()\n",
    "for prediction_result in prediction_results:\n",
    "    gfile_name = f\"gs://{bp_iter_outputs.bucket.name}/{prediction_result}\"\n",
    "    with tf.io.gfile.GFile(name=gfile_name, mode=\"r\") as gfile:\n",
    "        for line in gfile.readlines():\n",
    "            line = json.loads(line)\n",
    "            text = line[\"instance\"][\"text_vectorization_input\"][0]\n",
    "            prediction = line[\"prediction\"]\n",
    "            class_id = np.argmax(prediction)\n",
    "            class_name = class_maps[class_id]\n",
    "            tags.append([text, class_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UcMQ-daLn_oh"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tags_df = pd.DataFrame(tags, columns=[\"question\", \"tag\"])\n",
    "tags_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QUV_PHjtoCpQ"
   },
   "outputs": [],
   "source": [
    "tags_df[\"tag\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1b181a81eb61"
   },
   "source": [
    "清理工作\n",
    "\n",
    "要清理此项目中使用的所有Google Cloud资源，您可以[删除用于教程的Google Cloud项目](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects)。\n",
    "\n",
    "否则，您可以删除在本教程中创建的各个资源："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9e7688063ccd"
   },
   "outputs": [],
   "source": [
    "delete_bucket = False\n",
    "\n",
    "# Delete the dataset using the Vertex dataset object\n",
    "dataset.delete()\n",
    "\n",
    "# Undeploy model from the endpoint\n",
    "endpoint.undeploy_all()\n",
    "\n",
    "# Delete the endpoint\n",
    "endpoint.delete()\n",
    "\n",
    "# Delete the model using the Vertex model object\n",
    "model.delete()\n",
    "\n",
    "# Delete the AutoML or Pipeline training job\n",
    "job.delete()\n",
    "\n",
    "# Delete the batch prediction job using the Vertex batch prediction object\n",
    "batch_predict_job.delete()\n",
    "\n",
    "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
    "    ! gsutil rm -r $BUCKET_URI"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "bcKMGrqDrKoS",
    "q3UDGt8MrKoY",
    "bQNL5AjtrKoa",
    "9ewrG6VArKob",
    "eiNL0HSVrKoj",
    "KGT1uT-HrKoj",
    "kyLoUsx9rKok",
    "xksBj3M0rKok",
    "ncP9QwvkrKok",
    "Cyz86ixRYNtT"
   ],
   "name": "SDK_Custom_Training_Python_Package_Managed_Text_Dataset_Tensorflow_Serving_Container.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
