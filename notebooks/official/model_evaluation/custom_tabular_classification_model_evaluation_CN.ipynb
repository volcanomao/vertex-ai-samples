{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Vertex AI 管道: 评估自定义表格分类模型的 BatchPrediction 结果\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/model_evaluation/custom_tabular_classification_model_evaluation.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> 在Colab中打开\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fofficial%2Fmodel_evaluation%2Fcustom_tabular_classification_model_evaluation.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://cloud.google.com/ml-engine/images/colab-enterprise-logo-32px.png\" alt=\"Google Cloud Colab Enterprise logo\"><br> 在Colab企业版中打开\n",
    "    </a>\n",
    "  </td>    \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/model_evaluation/custom_tabular_classification_model_evaluation.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> 在Workbench中打开\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/model_evaluation/custom_tabular_classification_model_evaluation.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> 在GitHub上查看\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## 概述\n",
    "\n",
    "本笔记本演示了如何使用Vertex AI分类模型评估组件来评估在Vertex AI模型注册表中保存的自定义训练的表格分类模型。模型评估可以帮助您根据评估指标确定模型的性能，并在必要时改进模型。\n",
    "\n",
    "了解更多关于[Vertex AI自定义训练](https://cloud.google.com/vertex-ai/docs/training/custom-training)和[Vertex AI模型评估](https://cloud.google.com/vertex-ai/docs/evaluation/introduction)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d975e698c9a4"
   },
   "source": [
    "### 目标\n",
    "\n",
    "在本教程中，您将训练一个scikit-learn RandomForest模型，将模型保存到Vertex AI模型注册表中，并学习如何通过使用Google Cloud Pipeline Components Python SDK的Vertex AI管道作业来评估模型。\n",
    "\n",
    "本教程使用以下Vertex AI服务和资源：\n",
    "\n",
    "- Vertex AI模型注册表\n",
    "- Vertex AI管道\n",
    "- Vertex AI批量预测\n",
    "\n",
    "执行的步骤包括：\n",
    "\n",
    "- 从公共来源获取数据集。\n",
    "- 在本地预处理数据，并将测试数据保存在BigQuery中。\n",
    "- 使用scikit-learn Python包在本地训练一个RandomForest分类模型。\n",
    "- 在Artifact Registry中为预测创建自定义容器。\n",
    "- 将模型上传到Vertex AI模型注册表中。\n",
    "- 创建并运行一个Vertex AI管道，其中：\n",
    "    - 将训练好的模型导入到管道中。\n",
    "    - 在BigQuery中的测试数据上运行“批量预测”作业。\n",
    "    - 使用google-cloud-pipeline-components Python SDK中的评估组件评估模型。\n",
    "    - 将分类指标导入到Vertex AI模型注册表中的模型资源中。\n",
    "- 打印和可视化分类评估指标。\n",
    "- 清理此笔记本中创建的资源。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08d289fa873f"
   },
   "source": [
    "### 数据集\n",
    "\n",
    "这篇笔记中使用的**人口普查收入数据集**，存储在BigQuery的公共数据集中。最初，它是由[UC Irvine机器学习仓库](https://archive.ics.uci.edu/ml/datasets.php)提供的。与数据集关联的基本任务是确定一个人是否年收入超过5万美元。更多信息，请查看[其UCI网页上的详细信息](https://archive.ics.uci.edu/ml/datasets/Census+Income)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aed92deeb4a0"
   },
   "source": [
    "成本\n",
    "本教程使用Google Cloud的计费组件:\n",
    "* Artifact Registry\n",
    "* BigQuery\n",
    "* Cloud Build\n",
    "* Cloud Storage\n",
    "* Vertex AI\n",
    "\n",
    "了解有关[Artifact Registry 价格](https://cloud.google.com/artifact-registry/pricing), [BigQuery 价格](https://cloud.google.com/bigquery/pricing), [Cloud Build 价格](https://cloud.google.com/build/pricing), [Cloud Storage 价格](https://cloud.google.com/storage/pricing), [Vertex AI 价格](https://cloud.google.com/vertex-ai/pricing), 并使用[定价计算器](https://cloud.google.com/products/calculator/) 根据您的使用情况生成成本估算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61RBz8LLbxCR"
   },
   "source": [
    "开始吧"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "No17Cw5hgx12"
   },
   "source": [
    "### 为Python安装Vertex AI SDK和其他所需的软件包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tFy3H3aPgx12"
   },
   "outputs": [],
   "source": [
    "# Install the latest versions of the following packages\n",
    "! pip3 install --upgrade --quiet google-cloud-aiplatform \\\n",
    "                                 google-cloud-pipeline-components==1.0.26 \\\n",
    "                                 matplotlib \\\n",
    "                                 pyarrow \n",
    "# Install the specified versions of the following packages\n",
    "! pip3 install --quiet scikit-learn==1.0 \\\n",
    "                       pandas \\\n",
    "                       joblib==1.2.0 \\\n",
    "                       numpy==1.23.3 \\\n",
    "                       db-dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5Xep4W9lq-Z"
   },
   "source": [
    "重新启动运行时（仅适用于Colab）\n",
    "\n",
    "为了使用新安装的软件包，您必须在Google Colab上重新启动运行时。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XRvKdaPDTznN"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbmM4z7FOBpM"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️内核即将重新启动。在继续进行下一步之前，请等待它完成。⚠️</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmWOrTJ3gx13"
   },
   "source": [
    "### 验证您的笔记本环境（仅限Colab）\n",
    "\n",
    "在Google Colab上验证您的环境。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NyKGtVQjgx13"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DF4l8DTdWgPY"
   },
   "source": [
    "设置Google Cloud项目信息\n",
    "了解更多关于[设置项目和开发环境](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nqwi-5ufWp_B"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "创建一个云存储桶\n",
    "\n",
    "创建一个存储桶来存储中间产物，例如数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "BUCKET_URI = f\"gs://your-bucket-name-{PROJECT_ID}-unique\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EcIXiGsCePi"
   },
   "source": [
    "如果您的存储桶尚不存在：运行以下单元格以创建您的云存储存储桶。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIq7R4HZCfIc"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l $LOCATION -p $PROJECT_ID $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "source": [
    "### 初始化用于Python的Vertex AI SDK\n",
    "\n",
    "要开始使用Vertex AI，您必须拥有现有的Google Cloud项目，并启用[Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com)。使用您的项目和创建的存储桶初始化Python的Vertex AI SDK。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ksAefQcCF6ky"
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=LOCATION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "set_service_account"
   },
   "source": [
    "### 服务账户\n",
    "\n",
    "您可以使用服务账户来创建Vertex AI Pipeline作业。如果您不想使用项目的计算引擎服务账户，将`SERVICE_ACCOUNT`设置为另一个服务账户ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UwC1AdGeF6kx"
   },
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT = \"[your-service-account]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "autoset_service_account"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "if (\n",
    "    SERVICE_ACCOUNT == \"\"\n",
    "    or SERVICE_ACCOUNT is None\n",
    "    or SERVICE_ACCOUNT == \"[your-service-account]\"\n",
    "):\n",
    "    # Get your service account from gcloud\n",
    "    if not IS_COLAB:\n",
    "        shell_output = !gcloud auth list 2>/dev/null\n",
    "        SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
    "\n",
    "    else:  # IS_COLAB:\n",
    "        shell_output = ! gcloud projects describe  $PROJECT_ID\n",
    "        project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
    "        SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
    "\n",
    "    print(\"Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "set_service_account:pipelines"
   },
   "source": [
    "设置 Vertex AI 管道的服务帐户访问权限\n",
    "\n",
    "运行以下命令，将您的服务帐户访问权限赋予您在上一步中创建的存储桶中读取和写入管道工件的权限。每个服务帐户只需运行此步骤一次。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6OqzKqhMF6kx"
   },
   "outputs": [],
   "source": [
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
    "\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### 导入库\n",
    "\n",
    "导入 Vertex AI Python SDK 和其他必需的 Python 库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRUOFELefqf1"
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "import kfp\n",
    "import matplotlib.pyplot as plt\n",
    "from google.cloud import aiplatform_v1, bigquery\n",
    "from kfp.v2 import compiler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "source": [
    "### 初始化Python的BigQuery SDK\n",
    "\n",
    "使用您的项目和已创建的存储桶来初始化Python的BigQuery SDK。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ksAefQcCF6ky"
   },
   "outputs": [],
   "source": [
    "bq_client = bigquery.Client(\n",
    "    project=PROJECT_ID,\n",
    "    credentials=aiplatform.initializer.global_config.credentials,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b3a7f0650928"
   },
   "source": [
    "### 定义常量\n",
    "\n",
    "在下一个单元格中，定义您在本次会话中使用的常量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3390c9e9426c"
   },
   "outputs": [],
   "source": [
    "# Define the public bigquery data source\n",
    "DATA_SOURCE = \"bigquery-public-data.ml_datasets.census_adult_income\"\n",
    "\n",
    "# Define the dataset name for storing the test data\n",
    "PREDICTION_INPUT_DATASET_ID = \"adult_income_prediction\"\n",
    "\n",
    "# Define the table name for storing the test data for batch prediction\n",
    "PREDICTION_INPUT_TABLE_ID = \"adult_income_test_data\"\n",
    "\n",
    "# Set the folder path inside GCS bucket where you store model artifacts\n",
    "MODEL_ARTIFACT_DIR = \"sklearn-income-pred-model\"\n",
    "\n",
    "# Set the name of the local folder where you store your prediction application\n",
    "SRC_DIR = \"src\"\n",
    "\n",
    "# Define the feature columns that you use from the dataset\n",
    "COLUMNS = (\n",
    "    \"age\",\n",
    "    \"workclass\",\n",
    "    \"functional_weight\",\n",
    "    \"education\",\n",
    "    \"education_num\",\n",
    "    \"marital_status\",\n",
    "    \"occupation\",\n",
    "    \"relationship\",\n",
    "    \"race\",\n",
    "    \"sex\",\n",
    "    \"capital_gain\",\n",
    "    \"capital_loss\",\n",
    "    \"hours_per_week\",\n",
    "    \"native_country\",\n",
    ")\n",
    "\n",
    "# Categorical columns are columns that have string values and\n",
    "# need to be turned into a numerical value to be used for training\n",
    "CATEGORICAL_COLUMNS = (\n",
    "    \"workclass\",\n",
    "    \"education\",\n",
    "    \"marital_status\",\n",
    "    \"occupation\",\n",
    "    \"relationship\",\n",
    "    \"race\",\n",
    "    \"sex\",\n",
    "    \"native_country\",\n",
    ")\n",
    "\n",
    "# Target column in the dataset\n",
    "TARGET = \"income_bracket\"\n",
    "\n",
    "# Save the individual class labels in a constant\n",
    "CLASS_LABELS = [\" <=50K\", \" >50K\"]\n",
    "\n",
    "# Set the test ratio for splitting\n",
    "TEST_SIZE = 0.25\n",
    "\n",
    "# Set a random state\n",
    "RANDOM_STATE = 36\n",
    "\n",
    "# Set a sample size for batch prediction\n",
    "BATCH_SAMPLE_SIZE = 3000\n",
    "\n",
    "# Set the name for your repository in artifact registry\n",
    "REPOSITORY = \"sklearn-income-prediction-repo-unique\"  # @param {type:\"string\"}\n",
    "\n",
    "# Set the name for your prediction image in artifact registry\n",
    "IMAGE = \"sklearn-fastapi-server\"\n",
    "\n",
    "# Set a display name for your Vertex AI Model\n",
    "MODEL_DISPLAY_NAME = \"skl_inc_pred_model-unique\"  # @param {type:\"string\"}\n",
    "\n",
    "# Set a display name for your Vertex AI Pipeline\n",
    "PIPELINE_DISPLAY_NAME = (\n",
    "    \"income_classification_multiclass-unique\"  # @param {type:\"string\"}\n",
    ")\n",
    "\n",
    "# Path where the compiled pipeline needs to be written\n",
    "PIPELINE_PACKAGE_PATH = \"custom_tabular_classify_pipeline_config.json\"\n",
    "\n",
    "# Set the GCS path to your root directory for Vertex AI pipelines\n",
    "PIPELINE_ROOT = f\"{BUCKET_URI}/pipeline_root/income_classification_task\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8d97acf78771"
   },
   "source": [
    "获取数据集\n",
    "\n",
    "从BigQuery公共数据集中下载人口普查数据。在本教程中，您只使用数据集中的2万条数据进行训练和测试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2011a473ce65"
   },
   "outputs": [],
   "source": [
    "# Define the SQL query to fetch the dataset\n",
    "query = f\"\"\"\n",
    "SELECT * FROM `{DATA_SOURCE}` LIMIT 20000\n",
    "\"\"\"\n",
    "# Download the dataset to a dataframe\n",
    "df = bq_client.query(query).to_dataframe()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a19ba1448b08"
   },
   "source": [
    "## 分割数据\n",
    "\n",
    "将数据分成训练集和测试集。您在训练集上训练随机森林分类模型，并使用测试数据进行评估。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b5e3eaa9b483"
   },
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "X_train, X_test = train_test_split(df, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
    "# Print the shapes of train and test sets\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8f33bc1cc83f"
   },
   "source": [
    "## 在BigQuery中保存测试数据\n",
    "\n",
    "在BigQuery中创建一个数据集，并将测试数据集保存在数据集内的一个表中。这个数据集在运行评估管道时被进一步使用，用于创建数据样本和存储预测结果。\n",
    "\n",
    "### 在BigQuery中创建数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5f57144682e5"
   },
   "outputs": [],
   "source": [
    "# Create a bigquery dataset\n",
    "bq_dataset = bigquery.Dataset(f\"{PROJECT_ID}.{PREDICTION_INPUT_DATASET_ID}\")\n",
    "bq_dataset = bq_client.create_dataset(bq_dataset, exists_ok=True)\n",
    "print(f\"Created dataset {bq_client.project}.{bq_dataset.dataset_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50963dd50e4a"
   },
   "source": [
    "### 配置存储测试数据的架构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1e4f37c791c0"
   },
   "outputs": [],
   "source": [
    "schema_config = []\n",
    "for i in COLUMNS:\n",
    "    if X_test[i].dtype == \"int64\":\n",
    "        schema_config.append(bigquery.SchemaField(i, \"INTEGER\"))\n",
    "    elif X_test[i].dtype in [\"object\", \"category\"]:\n",
    "        schema_config.append(bigquery.SchemaField(i, \"STRING\"))\n",
    "\n",
    "schema_config.append(bigquery.SchemaField(TARGET, \"STRING\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d34830366653"
   },
   "source": [
    "###将测试数据加载到表中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fe505b44bd57"
   },
   "outputs": [],
   "source": [
    "table_ref = bq_dataset.table(PREDICTION_INPUT_TABLE_ID)\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    schema=schema_config, write_disposition=\"WRITE_TRUNCATE\"\n",
    ")\n",
    "\n",
    "job = bq_client.load_table_from_dataframe(X_test, table_ref)\n",
    "\n",
    "job.result()  # Waits for table load to complete.\n",
    "print(\"Loaded dataframe to {}\".format(table_ref.path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "49c8ce17f86f"
   },
   "source": [
    "创建用于训练的预处理流水线\n",
    "\n",
    "由于数据集包含分类和数值数据，因此需要进行某些预处理步骤。但是，在用于训练分类模型之前，你的数据集需要仅包含数值数值。因此，您可以使用LabelBinarizer将分类数据编码为数值数据。\n",
    "\n",
    "为了简化预测服务，以下代码将这些步骤封装在一个scikit-learn Pipeline中。您可以使用scikit-learn中包含的`joblib`版本或`pickle`导出Pipeline对象，类似于导出scikit-learn估计器的方法。\n",
    "\n",
    "了解更多关于[scikit-learn Pipelines](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c5327f5646bc"
   },
   "outputs": [],
   "source": [
    "# Remove the column we are trying to predict ('income-level') from our features list\n",
    "# Convert the Dataframe to a lists of lists\n",
    "train_features = X_train.drop(TARGET, axis=1).to_numpy().tolist()\n",
    "# Create our training labels list, convert the Dataframe to a lists of lists\n",
    "train_labels = X_train[TARGET].to_numpy().tolist()\n",
    "\n",
    "# Since the census data set has categorical features, we need to convert\n",
    "# them to numerical values. We'll use a list of pipelines to convert each\n",
    "# categorical column and then use FeatureUnion to combine them before calling\n",
    "# the RandomForestClassifier.\n",
    "categorical_pipelines = []\n",
    "\n",
    "# Each categorical column needs to be extracted individually and converted to a numerical value.\n",
    "# To do this, each categorical column will use a pipeline that extracts one feature column via\n",
    "# SelectKBest(k=1) and a LabelBinarizer() to convert the categorical value to a numerical one.\n",
    "# A scores array (created below) will select and extract the feature column. The scores array is\n",
    "# created by iterating over the COLUMNS and checking if it is a CATEGORICAL_COLUMN.\n",
    "for i, col in enumerate(COLUMNS):\n",
    "    if col in CATEGORICAL_COLUMNS:\n",
    "        # Create a scores array to get the individual categorical column.\n",
    "        # Example:\n",
    "        #  data = [39, 'State-gov', 77516, 'Bachelors', 13, 'Never-married', 'Adm-clerical',\n",
    "        #         'Not-in-family', 'White', 'Male', 2174, 0, 40, 'United-States']\n",
    "        #  scores = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        #\n",
    "        # Returns: [['Sate-gov']]\n",
    "        scores = []\n",
    "        # Build the scores array\n",
    "        for j in range(len(COLUMNS)):\n",
    "            if i == j:  # This column is the categorical column we want to extract.\n",
    "                scores.append(1)  # Set to 1 to select this column\n",
    "            else:  # Every other column should be ignored.\n",
    "                scores.append(0)\n",
    "        skb = SelectKBest(k=1)\n",
    "        skb.scores_ = scores\n",
    "        # Convert the categorical column to a numerical value\n",
    "        lbn = LabelBinarizer()\n",
    "        r = skb.transform(train_features)\n",
    "        lbn.fit(r)\n",
    "        # Create the pipeline to extract the categorical feature\n",
    "        categorical_pipelines.append(\n",
    "            (\n",
    "                \"categorical-{}\".format(i),\n",
    "                Pipeline([(\"SKB-{}\".format(i), skb), (\"LBN-{}\".format(i), lbn)]),\n",
    "            )\n",
    "        )\n",
    "\n",
    "# Create pipeline to extract the numerical features\n",
    "skb = SelectKBest(k=6)\n",
    "# From COLUMNS use the features that are numerical\n",
    "skb.scores_ = [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0]\n",
    "categorical_pipelines.append((\"numerical\", skb))\n",
    "\n",
    "# Combine all the features using FeatureUnion\n",
    "preprocess = FeatureUnion(categorical_pipelines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9263b67cf815"
   },
   "source": [
    "训练一个随机森林分类模型\n",
    "\n",
    "接下来，在预处理过的数据上拟合一个随机森林分类模型。\n",
    "\n",
    "训练完毕后，将估计器添加到管道对象中，并将管道保存到磁盘。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "65d19ac42f8c"
   },
   "outputs": [],
   "source": [
    "# Create the classifier\n",
    "classifier = RandomForestClassifier()\n",
    "\n",
    "# Transform the features and fit them to the classifier\n",
    "classifier.fit(preprocess.transform(train_features), train_labels)\n",
    "\n",
    "# Create the overall model as a single pipeline\n",
    "pipeline = Pipeline([(\"union\", preprocess), (\"classifier\", classifier)])\n",
    "\n",
    "# Save the pipeline\n",
    "joblib.dump(pipeline, \"model.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82a402f70027"
   },
   "source": [
    "## 为提供预测创建一个容器映像\n",
    "\n",
    "要使用模型进行预测服务，您需要将模型上传到 Vertex AI 模型注册表，使用一个自定义容器来进行预测。\n",
    "要创建容器映像，您需要执行以下步骤：\n",
    "- 将模型保存到您的 Cloud 存储桶中。\n",
    "- 本地创建一个使用 [`FastAPI`](https://fastapi.tiangolo.com/tutorial/first-steps/) Python 包的服务应用程序。\n",
    "- 使用 Docker 将应用程序制作成镜像，并上传到 [Artifact Registry 使用 Cloud Build](https://cloud.google.com/build/docs/build-push-docker-image)。\n",
    "\n",
    "了解更多关于在 Vertex AI 上使用[自定义容器进行预测](https://cloud.google.com/vertex-ai/docs/predictions/use-custom-container)的信息。\n",
    "\n",
    "### 将模型上传到 Cloud 存储桶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1a7e8f443169"
   },
   "outputs": [],
   "source": [
    "!gsutil cp \"model.joblib\" {BUCKET_URI}/{MODEL_ARTIFACT_DIR}/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32a87daeba3d"
   },
   "source": [
    "创建服务应用程序\n",
    "\n",
    "创建一个源目录，将您的服务应用程序打包在其中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bf0ae3ccfeef"
   },
   "outputs": [],
   "source": [
    "# Create the source directory\n",
    "! mkdir $SRC_DIR\n",
    "\n",
    "# Create the app folder\n",
    "! mkdir $SRC_DIR/app\n",
    "\n",
    "# Move your model to the app folder\n",
    "! mv model.joblib $SRC_DIR/app/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "df803659c0b4"
   },
   "source": [
    "创建包含使用 `FastAPI` 为预测提供服务的代码的 `main.py` 文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7682e6a682d2"
   },
   "outputs": [],
   "source": [
    "%%writefile $SRC_DIR/app/main.py\n",
    "# Import the required libraries\n",
    "from fastapi import FastAPI, Request\n",
    "import joblib\n",
    "import json\n",
    "import os\n",
    "from google.cloud import storage\n",
    "import logging\n",
    "\n",
    "app = FastAPI()\n",
    "# Define the Cloud Storage client\n",
    "gcs_client = storage.Client()\n",
    "\n",
    "# Download the model file from Cloud Storage bucket\n",
    "with open(\"model.joblib\", 'wb') as model_f:\n",
    "    gcs_client.download_blob_to_file(\n",
    "            f\"{os.environ['AIP_STORAGE_URI']}/model.joblib\", model_f\n",
    "        )\n",
    "    \n",
    "# Load the scikit-learn model/pipeline file\n",
    "_model = joblib.load(\"model.joblib\")\n",
    "\n",
    "# Define a function for health route\n",
    "@app.get(os.environ['AIP_HEALTH_ROUTE'], status_code=200)\n",
    "def health():\n",
    "    return {}\n",
    "\n",
    "# Define a function for prediction route\n",
    "@app.post(os.environ['AIP_PREDICT_ROUTE'])\n",
    "async def predict(request: Request):\n",
    "    # await the request\n",
    "    body = await request.json()\n",
    "    # parse the request instances\n",
    "    instances = body[\"instances\"]\n",
    "    # pass it to the model/pipeline for prediction scores\n",
    "    predictions = _model.predict_proba(instances).tolist()\n",
    "    # return the batch prediction scores\n",
    "    return {\"predictions\": predictions}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36f2a47133fa"
   },
   "source": [
    "### 创建需求文件\n",
    "创建一个名为 `requirements.txt` 的文件，指定应用程序所需的软件包版本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5d0dff56ce78"
   },
   "outputs": [],
   "source": [
    "%%writefile $SRC_DIR/requirements.txt\n",
    "joblib==1.2.0\n",
    "numpy==1.23.3\n",
    "scikit-learn==1.0\n",
    "google-cloud-storage>=1.44.0,<2.0.0dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8c5f2c75fcee"
   },
   "source": [
    "创建一个用于设置环境变量的bash脚本\n",
    "\n",
    "创建名为`prestart.sh`的bash脚本，将端口设置为`AIP_HTTP_PORT`。您容器中的HTTP服务器将监听此端口上的请求。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "848e9ad3ba5d"
   },
   "outputs": [],
   "source": [
    "%%writefile $SRC_DIR/app/prestart.sh\n",
    "#!/bin/bash\n",
    "export PORT=$AIP_HTTP_PORT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6a95adcb2ef4"
   },
   "source": [
    "容器化服务应用程序\n",
    "\n",
    "为将服务应用程序放入容器中创建一个Dockerfile。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "be2138ccfd24"
   },
   "outputs": [],
   "source": [
    "%%writefile $SRC_DIR/Dockerfile\n",
    "\n",
    "FROM tiangolo/uvicorn-gunicorn-fastapi:python3.9\n",
    "\n",
    "COPY ./app /app\n",
    "COPY requirements.txt requirements.txt\n",
    "\n",
    "RUN pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OrpUIkAIs_uQ"
   },
   "source": [
    "创建私有Docker仓库\n",
    "\n",
    "您的第一步是在Google Artifact Registry中创建您自己的Docker仓库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0amu4063tDnG"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "! gcloud services enable artifactregistry.googleapis.com\n",
    "\n",
    "if os.getenv(\"IS_TESTING\"):\n",
    "    ! sudo apt-get update --yes && sudo apt-get --only-upgrade --yes install google-cloud-sdk-cloud-run-proxy google-cloud-sdk-harbourbridge google-cloud-sdk-cbt google-cloud-sdk-gke-gcloud-auth-plugin google-cloud-sdk-kpt google-cloud-sdk-local-extract google-cloud-sdk-minikube google-cloud-sdk-app-engine-java google-cloud-sdk-app-engine-go google-cloud-sdk-app-engine-python google-cloud-sdk-spanner-emulator google-cloud-sdk-bigtable-emulator google-cloud-sdk-nomos google-cloud-sdk-package-go-module google-cloud-sdk-firestore-emulator kubectl google-cloud-sdk-datastore-emulator google-cloud-sdk-app-engine-python-extras google-cloud-sdk-cloud-build-local google-cloud-sdk-kubectl-oidc google-cloud-sdk-anthos-auth google-cloud-sdk-app-engine-grpc google-cloud-sdk-pubsub-emulator google-cloud-sdk-datalab google-cloud-sdk-skaffold google-cloud-sdk google-cloud-sdk-terraform-tools google-cloud-sdk-config-connector\n",
    "    ! gcloud components update --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62a376d0edee"
   },
   "source": [
    "创建存储库\n",
    "\n",
    "要存储您的容器镜像，请在 Artifact Registry 中创建一个存储库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b647ab40c7a1"
   },
   "outputs": [],
   "source": [
    "!gcloud artifacts repositories create {REPOSITORY} \\\n",
    "    --repository-format=docker \\\n",
    "    --location=$LOCATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79dda0e8fa0c"
   },
   "source": [
    "#### 推送容器镜像\n",
    "使用Cloud Build将您的serving应用程序容器化，并将其推送到您的存储库中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d1d2a81a3968"
   },
   "outputs": [],
   "source": [
    "%cd $SRC_DIR/\n",
    "!gcloud builds submit --region={LOCATION} --tag={LOCATION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY}/{IMAGE} --suppress-logs\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "890ee72a71df"
   },
   "source": [
    "## 将模型上传到Vertex AI Registry\n",
    "\n",
    "现在，使用容器镜像和模型上传到的Cloud Storage存储桶中的工件目录路径创建一个Vertex AI模型。\n",
    "\n",
    "要上传您的模型，您可以使用Vertex AI SDK中的`Model.upload()`方法，通过传递以下参数：\n",
    "\n",
    "- `display_name`：模型资源的显示名称。\n",
    "- `artifact_uri`：存放模型文件/工件的Cloud Storage路径。\n",
    "- `serving_container_image_uri`：提供服务的容器镜像路径。\n",
    "- `serving_container_predict_route`：提供服务应用程序的预测路由。\n",
    "- `serving_container_health_route`：提供服务应用程序的健康检查路由。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ed6e4a5a98a4"
   },
   "outputs": [],
   "source": [
    "aip_model = aiplatform.Model.upload(\n",
    "    display_name=MODEL_DISPLAY_NAME,\n",
    "    artifact_uri=f\"{BUCKET_URI}/{MODEL_ARTIFACT_DIR}\",\n",
    "    serving_container_image_uri=f\"{LOCATION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY}/{IMAGE}\",\n",
    "    serving_container_predict_route=\"/predict\",\n",
    "    serving_container_health_route=\"/health\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7f463f9183ef"
   },
   "source": [
    "在这一部分，您将运行一个 Vertex AI 流水线，其中包括以下步骤：\n",
    "1. 从 Vertex AI 模型注册表中导入您的模型。\n",
    "2. 对测试数据进行批量预测采样。\n",
    "3. 从采样的测试数据中删除目标字段。\n",
    "4. 运行批处理预测作业。\n",
    "5. 使用地面实况/目标信息评估批处理预测作业的结果。\n",
    "6. 将生成的评估指标导入至 Vertex AI 模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9f72ff491712"
   },
   "source": [
    "### 启用 Dataflow API\n",
    "\n",
    "谷歌云管道组件的模型评估组件会内部创建数据流作业来执行基础任务。在运行评估管道之前，启用 Dataflow API 是必要的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "795fdfef5736"
   },
   "outputs": [],
   "source": [
    "!gcloud services enable dataflow.googleapis.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77f4838ece15"
   },
   "source": [
    "### 定义管道\n",
    "要为评估您的模型定义 Vertex AI 管道，您可以使用 `google-cloud-pipeline-components` Python 包。Google Cloud Pipeline Components 提供了一个 SDK，其中包含一组管道组件，用户可以与 Google Cloud 服务（如 Vertex AI、Dataflow 和 BigQuery）进行交互。\n",
    "\n",
    "了解更多关于[Google Cloud 管道组件](https://cloud.google.com/vertex-ai/docs/pipelines/components-introduction)。\n",
    "\n",
    "评估管道使用以下组件：\n",
    "- `GetVertexModelOp`：获取 Vertex AI 模型工件。\n",
    "- `EvaluationDataSamplerOp`：随机对输入数据集进行下采样，以指定的大小进行计算适用于 AutoML Tabular 和定制模型的 Vertex 可解释 AI 特征归因。使用 Apache Beam 创建 Dataflow 作业来对数据集进行下采样。\n",
    "- `TargetFieldDataRemoverOp`：从输入数据集中删除目标字段，以支持结构化 AutoML 模型和 Vertex AI 批量预测的定制模型。\n",
    "- `ModelBatchPredictOp`：创建 Vertex AI 批量预测作业并等待完成。\n",
    "- `ModelEvaluationClassificationOp`：在训练模型的批量预测结果上计算评估指标。使用 Apache Beam 和 TFMA 创建 Dataflow 作业来计算评估指标。支持表格、图像、视频和文本数据的多类分类评估。\n",
    "- `ModelImportEvaluationOp`：将模型评估工件导入到现有的 Vertex AI 模型中，使用 ModelService.ImportModelEvaluation。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8c32329207b6"
   },
   "outputs": [],
   "source": [
    "# define the evaluation pipeline\n",
    "\n",
    "\n",
    "@kfp.dsl.pipeline(name=\"custom-tabular-classification-evaluation-pipeline\")\n",
    "def evaluation_custom_tabular_feature_attribution_pipeline(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    root_dir: str,\n",
    "    model_name: str,\n",
    "    target_field_name: str,\n",
    "    bigquery_source_input_uri: str,\n",
    "    bigquery_destination_output_uri: str,\n",
    "    batch_predict_instances_format: str,\n",
    "    evaluation_class_names: list,\n",
    "    batch_predict_predictions_format: str = \"bigquery\",\n",
    "    evaluation_prediction_label_column: str = \"\",\n",
    "    evaluation_prediction_score_column: str = \"prediction\",\n",
    "    batch_predict_machine_type: str = \"n1-standard-4\",\n",
    "    batch_predict_starting_replica_count: int = 5,\n",
    "    batch_predict_max_replica_count: int = 10,\n",
    "    batch_predict_data_sample_size: int = 10000,\n",
    "):\n",
    "    # Import the components\n",
    "    from google_cloud_pipeline_components.aiplatform import ModelBatchPredictOp\n",
    "    from google_cloud_pipeline_components.experimental.evaluation import (\n",
    "        EvaluationDataSamplerOp, GetVertexModelOp,\n",
    "        ModelEvaluationClassificationOp, ModelImportEvaluationOp,\n",
    "        TargetFieldDataRemoverOp)\n",
    "\n",
    "    # Get the Vertex AI model resource\n",
    "    get_model_task = GetVertexModelOp(model_resource_name=model_name)\n",
    "\n",
    "    # Run the data sampling task\n",
    "    data_sampler_task = EvaluationDataSamplerOp(\n",
    "        project=project,\n",
    "        location=location,\n",
    "        root_dir=root_dir,\n",
    "        bigquery_source_uri=bigquery_source_input_uri,\n",
    "        instances_format=batch_predict_instances_format,\n",
    "        sample_size=batch_predict_data_sample_size,\n",
    "    )\n",
    "\n",
    "    # Run the task to remove the target field from data for batch prediction\n",
    "    data_splitter_task = TargetFieldDataRemoverOp(\n",
    "        project=project,\n",
    "        location=location,\n",
    "        root_dir=root_dir,\n",
    "        bigquery_source_uri=data_sampler_task.outputs[\"bigquery_output_table\"],\n",
    "        instances_format=batch_predict_instances_format,\n",
    "        target_field_name=target_field_name,\n",
    "    )\n",
    "\n",
    "    # Run the batch prediction task\n",
    "    batch_predict_task = ModelBatchPredictOp(\n",
    "        project=project,\n",
    "        location=location,\n",
    "        model=get_model_task.outputs[\"model\"],\n",
    "        job_display_name=\"model-registry-batch-prediction\",\n",
    "        bigquery_source_input_uri=data_splitter_task.outputs[\"bigquery_output_table\"],\n",
    "        instances_format=batch_predict_instances_format,\n",
    "        predictions_format=batch_predict_predictions_format,\n",
    "        bigquery_destination_output_uri=bigquery_destination_output_uri,\n",
    "        machine_type=batch_predict_machine_type,\n",
    "        starting_replica_count=batch_predict_starting_replica_count,\n",
    "        max_replica_count=batch_predict_max_replica_count,\n",
    "    )\n",
    "\n",
    "    # Run the evaluation based on prediction type\n",
    "    eval_task = ModelEvaluationClassificationOp(\n",
    "        project=project,\n",
    "        location=location,\n",
    "        root_dir=root_dir,\n",
    "        class_labels=evaluation_class_names,\n",
    "        prediction_label_column=evaluation_prediction_label_column,\n",
    "        prediction_score_column=evaluation_prediction_score_column,\n",
    "        target_field_name=target_field_name,\n",
    "        ground_truth_format=batch_predict_instances_format,\n",
    "        ground_truth_bigquery_source=data_sampler_task.outputs[\"bigquery_output_table\"],\n",
    "        predictions_format=batch_predict_predictions_format,\n",
    "        predictions_bigquery_source=batch_predict_task.outputs[\"bigquery_output_table\"],\n",
    "    )\n",
    "\n",
    "    # Import the model evaluations to the Vertex AI model\n",
    "    ModelImportEvaluationOp(\n",
    "        classification_metrics=eval_task.outputs[\"evaluation_metrics\"],\n",
    "        model=get_model_task.outputs[\"model\"],\n",
    "        dataset_type=batch_predict_instances_format,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c7b87e9196f3"
   },
   "source": [
    "### 可选：导入BigQuery表以进行预测的解决方法_bigquery_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fc2df1b02f1a"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Set constants for BigQuery Table\n",
    "BIGQUERY_PROJECT_ID = \"your-project-id\"\n",
    "BIGQUERY_DATASET_ID = \"your-dataset-id\"\n",
    "BIGQUERY_PREDICTION_RESULTS_TABLE_ID = \"your-table-id\"\n",
    "\n",
    "# Import the BigQuery table using the importer to obtain a BQTable artifact\n",
    "bq_table_uri = f\"bq://{BIGQUERY_PROJECT_ID}.{BIGQUERY_DATASET_ID}.{BIGQUERY_PREDICTION_RESULTS_TABLE_ID}\"\n",
    "bq_table = kfp.v2.dsl.importer(\n",
    "    artifact_uri=bq_table_uri,\n",
    "    artifact_class=artifact_types.BQTable,\n",
    "    metadata={\n",
    "        \"projectId\": BIGQUERY_PROJECT_ID,\n",
    "        \"datasetId\": BIGQUERY_DATASET_ID,\n",
    "        \"tableId\": BIGQUERY_PREDICTION_RESULTS_TABLE_ID,\n",
    "    },\n",
    ").output\n",
    "\n",
    "# Run the evaluation based on prediction type\n",
    "eval_task = ModelEvaluationClassificationOp(\n",
    "    project=project,\n",
    "    location=location,\n",
    "    root_dir=root_dir,\n",
    "    class_labels=evaluation_class_names,\n",
    "    prediction_label_column=evaluation_prediction_label_column,\n",
    "    prediction_score_column=evaluation_prediction_score_column,\n",
    "    target_field_name=target_field_name,\n",
    "    ground_truth_format=batch_predict_instances_format,\n",
    "    ground_truth_bigquery_source=bq_table_uri,\n",
    "    predictions_format=batch_predict_predictions_format,\n",
    "    predictions_bigquery_source=bq_table,\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4a544d067ba"
   },
   "source": [
    "### 编译评估流程\n",
    "\n",
    "将定义好的流程编译成一个（json/yaml）文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fc6171557f75"
   },
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=evaluation_custom_tabular_feature_attribution_pipeline,\n",
    "    package_path=PIPELINE_PACKAGE_PATH,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e4f633d51a8d"
   },
   "source": [
    "### 定义参数\n",
    "\n",
    "在运行您的流水线之前，请设置以下参数：\n",
    "\n",
    "- `project`：Google Cloud 项目的项目 ID。\n",
    "- `location`：需要运行流水线的位置。如果未设置，流水线将默认使用 Vertex AI SDK 配置的位置。\n",
    "- `root_dir`：用于存储分阶段文件和工件的 Cloud Storage 目录。在目录下创建一个随机子目录，用于存储作业信息，以便在失败时恢复作业。\n",
    "- `model_name`：训练的自定义表格分类模型的资源名称。\n",
    "- `target_field_name`：用作评估基准的列的名称。\n",
    "- `bigquery_source_input_uri`：存储测试输入的 BigQuery 表 URI。\n",
    "- `bigquery_destination_output_uri`：用于在测试集上导出预测的 BigQuery 数据集 URI。\n",
    "- `batch_predict_instances_format`：用于批量预测和评估的输入格式。\n",
    "- `batch_predict_predictions_format`：用于批量预测和评估的输出格式。\n",
    "- `evaluation_class_names`：数据集中目标字段的所有类名称列表。\n",
    "- `batch_predict_data_sample_size`：批量预测作业和评估所需的输入测试数据样本大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "497307d23cf6"
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"project\": PROJECT_ID,\n",
    "    \"location\": LOCATION,\n",
    "    \"root_dir\": PIPELINE_ROOT,\n",
    "    \"model_name\": aip_model.resource_name,\n",
    "    \"target_field_name\": TARGET,\n",
    "    \"bigquery_source_input_uri\": f\"bq://{PROJECT_ID}.{table_ref.dataset_id}.{table_ref.table_id}\",\n",
    "    \"bigquery_destination_output_uri\": f\"bq://{PROJECT_ID}.{table_ref.dataset_id}\",\n",
    "    \"batch_predict_instances_format\": \"bigquery\",\n",
    "    \"batch_predict_predictions_format\": \"bigquery\",\n",
    "    \"evaluation_class_names\": CLASS_LABELS,\n",
    "    \"batch_predict_data_sample_size\": BATCH_SAMPLE_SIZE,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dc5d739481ab"
   },
   "source": [
    "### 运行管道\n",
    "\n",
    "使用以下参数创建一个 Vertex AI 管道作业并运行它：\n",
    "\n",
    "- `display_name`：应显示在 Google Cloud 控制台中的管道名称。\n",
    "- `template_path`：编译后的 PipelineSpec JSON 或 YAML 文件的路径。可以是本地路径、Google Cloud 存储 URI 或 Artifact Registry URI。\n",
    "- `parameter_values`：运行时参数名称与控制管道运行的值的映射。\n",
    "- `enable_caching`：布尔值，指定是否打开缓存或不打开缓存。\n",
    "\n",
    "了解更多关于 Vertex AI SDK 的 [PipelineJob 类](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.PipelineJob)。\n",
    "\n",
    "创建管道作业后，使用配置的 `SERVICE_ACCOUNT` 来运行它。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "763d8aec247e"
   },
   "outputs": [],
   "source": [
    "# Create the pipeline job\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=PIPELINE_DISPLAY_NAME,\n",
    "    template_path=PIPELINE_PACKAGE_PATH,\n",
    "    parameter_values=parameters,\n",
    "    enable_caching=True,\n",
    ")\n",
    "# Run the pipeline job\n",
    "job.run(service_account=SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1a80e3c81ad5"
   },
   "source": [
    "在从上一步获取的结果中，单击生成的链接以在云控制台中查看您的运行情况。\n",
    "\n",
    "在用户界面中，单击管道的有向无环图（DAG）中的节点可展开或折叠。以下是DAG的部分展开视图（单击图像以查看更大版本）。\n",
    "\n",
    "## 打印指标\n",
    "\n",
    "在管道顺利运行后，从评估任务中获取评估指标并打印出来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b53739c26cfa"
   },
   "outputs": [],
   "source": [
    "# Iterate over the pipeline tasks\n",
    "for task in job._gca_resource.job_detail.task_details:\n",
    "    # Obtain the artifacts from the evaluation task\n",
    "    if (\n",
    "        (\"model-evaluation\" in task.task_name)\n",
    "        and (\"model-evaluation-import\" not in task.task_name)\n",
    "        and (\n",
    "            task.state == aiplatform_v1.types.PipelineTaskDetail.State.SUCCEEDED\n",
    "            or task.state == aiplatform_v1.types.PipelineTaskDetail.State.SKIPPED\n",
    "        )\n",
    "    ):\n",
    "        evaluation_metrics = task.outputs.get(\"evaluation_metrics\").artifacts[0]\n",
    "        evaluation_metrics_gcs_uri = evaluation_metrics.uri\n",
    "\n",
    "print(evaluation_metrics)\n",
    "print(evaluation_metrics_gcs_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8984d2a0e5b"
   },
   "source": [
    "## 可视化指标\n",
    "\n",
    "使用条形图可视化生成的评估指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b296182b80b3"
   },
   "outputs": [],
   "source": [
    "metrics = []\n",
    "values = []\n",
    "for i in evaluation_metrics.metadata.items():\n",
    "    metrics.append(i[0])\n",
    "    values.append(i[1])\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.bar(x=metrics, height=values)\n",
    "plt.title(\"Evaluation Metrics\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "清理\n",
    "\n",
    "要清理此项目中使用的所有Google Cloud资源，您可以[删除用于本教程的Google Cloud项目](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects)。\n",
    "\n",
    "否则，您可以删除本教程中创建的各个资源。\n",
    "\n",
    "- Vertex AI 模型\n",
    "- Vertex AI 管道作业\n",
    "- Artifact Registry 中的存储库\n",
    "- BigQuery 数据集\n",
    "- 云存储桶（将 `delete_bucket` 设置为 **True** 可删除在此笔记本中创建的云存储桶）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sx_vKniMq9ZX"
   },
   "outputs": [],
   "source": [
    "# Delete model resource\n",
    "aip_model.delete()\n",
    "\n",
    "# Delete the evaluation pipeline\n",
    "job.delete()\n",
    "\n",
    "# Delete the repository in Artifact Registry\n",
    "! gcloud artifacts repositories delete --location=us-central1 {REPOSITORY} --quiet\n",
    "\n",
    "# Delete the BigQuery dataset\n",
    "! bq rm -r -f $PROJECT_ID:$PREDICTION_INPUT_DATASET_ID\n",
    "\n",
    "delete_bucket = False\n",
    "# Delete Cloud Storage objects\n",
    "if delete_bucket:\n",
    "    ! gsutil -m rm -r $BUCKET_URI\n",
    "\n",
    "# Delete the locally generated files and folders\n",
    "! rm $PIPELINE_PACKAGE_PATH\n",
    "! rm -rf $SRC_DIR"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "custom_tabular_classification_model_evaluation.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
