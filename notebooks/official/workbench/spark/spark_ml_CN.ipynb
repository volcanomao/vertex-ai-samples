{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "# 使用Dataproc和BigQuery的SparkML\n",
    "\n",
    "<table align=\"left\">\n",
    "\n",
    "  <td>\n",
    "<a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/workbench/spark/spark_ml.ipynb\" target='_blank'>\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      查看GitHub上的代码\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "        <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/workbench/spark/spark_ml.ipynb\">\n",
    "        <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> 在Colab中运行\n",
    "        </a>\n",
    "  </td>\n",
    "  <td>\n",
    "<a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/workbench/spark/spark_ml.ipynb\" target='_blank'>\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      在Vertex AI Workbench中打开\n",
    "    </a>\n",
    "  </td>                                                                                               \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "## 概览\n",
    "\n",
    "本笔记本教程运行Apache SparkML作业与Dataproc和BigQuery，以示例一个常见的机器学习流水线用例：数据摄入和清洗，特征工程，建模和模型评估。\n",
    "\n",
    "了解有关[Vertex AI Workbench](https://cloud.google.com/vertex-ai/docs/workbench/introduction)和[Dataproc](https://cloud.google.com/vertex-ai/docs/pipelines/dataproc-component)的更多信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### 目标\n",
    "\n",
    "本教程运行一个Apache SparkML作业，从BigQuery数据集中获取数据，执行探索性数据分析，清洗数据，执行特征工程，训练模型，评估模型，输出结果，并将模型保存到Cloud Storage存储桶中。\n",
    "\n",
    "本教程使用以下Google Cloud ML服务：\n",
    "\n",
    "- `Dataproc`\n",
    "- `BigQuery`\n",
    "- `Vertex AI Training`\n",
    "\n",
    "执行的步骤包括：\n",
    "\n",
    "- 设置一个Google Cloud项目和Dataproc集群。\n",
    "- 创建一个Cloud Storage存储桶和一个BigQuery数据集。\n",
    "- 配置spark-bigquery-connector。\n",
    "- 将BigQuery数据导入Spark DataFrame中。\n",
    "- 进行探索性数据分析（EDA）。\n",
    "- 使用样本可视化数据。\n",
    "- 清洗数据。\n",
    "- 选择特征。\n",
    "- 训练模型。\n",
    "- 输出结果。\n",
    "- 将模型保存到Cloud Storage存储桶中。\n",
    "- 删除为本教程创建的资源。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "数据集\n",
    "\n",
    "[NYC TLC（纽约出租车和豪华轿车委员会）行程](https://console.cloud.google.com/marketplace/product/city-of-new-york/nyc-tlc-trips)（纽约出租车和豪华轿车行程数据）和[NYC Citi Bike行程](https://console.cloud.google.com/marketplace/product/city-of-new-york/nyc-citi-bike)（纽约市公共自行车共享系统数据）数据集可在[BigQuery公共数据集](https://cloud.google.com/bigquery/public-data)中获得。 BigQuery每个月提供免费查询高达1TB的数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### 费用\n",
    "\n",
    "此教程使用 Google Cloud 的计费组件：\n",
    "\n",
    "* [Vertex AI](https://cloud.google.com/vertex-ai/pricing)\n",
    "* [Cloud Storage](https://cloud.google.com/storage/pricing)\n",
    "* [Dataproc](https://cloud.google.com/dataproc/pricing)\n",
    "* [BigQuery](https://cloud.google.com/bigquery/pricing)\n",
    "\n",
    "您可以使用 [定价计算器](https://cloud.google.com/products/calculator/) 根据您的预期使用量生成费用估算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### 安装\n",
    "\n",
    "安装以下包:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b5c0183fba17"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.getenv(\"IS_TESTING\"):\n",
    "    \"\"\"\n",
    "    Since the testing suite doesn't support testing on Dataproc clusters,\n",
    "    the testing environment is setup to replicate Dataproc via the following steps:\n",
    "    \"\"\"\n",
    "    JAVA_VER = \"8u332-b09\"\n",
    "    JAVA_FOLDER = \"/tmp/java\"\n",
    "    FILE_NAME = f\"openlogic-openjdk-{JAVA_VER}-linux-x64\"\n",
    "    TAR_FILE = f\"{JAVA_FOLDER}/{FILE_NAME}.tar.gz\"\n",
    "    DOWNLOAD_LINK = f\"https://builds.openlogic.com/downloadJDK/openlogic-openjdk/{JAVA_VER}/openlogic-openjdk-{JAVA_VER}-linux-x64.tar.gz\"\n",
    "    PYSPARK_VER = \"3.1.3\"\n",
    "\n",
    "    ! rm -rf $JAVA_FOLDER\n",
    "    ! mkdir $JAVA_FOLDER\n",
    "    # Download Open JDK 8. Spark requires Java to execute.\n",
    "    ! wget -P $JAVA_FOLDER $DOWNLOAD_LINK\n",
    "    os.environ[\"JAVA_HOME\"] = f\"{JAVA_FOLDER}/{FILE_NAME}\"\n",
    "    ! tar -zxf $TAR_FILE -C $JAVA_FOLDER\n",
    "    ! echo $JAVA_HOME\n",
    "\n",
    "    # Install PySpark and other packages.\n",
    "    ! pip install pyspark==$PYSPARK_VER geopandas pyarrow rtree seaborn numpy==1.19.5 -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### 创建一个 Dataproc 集群\n",
    "\n",
    "在这个笔记本教程中执行的 Spark 作业计算密集。由于在标准笔记本环境中完成作业可能需要较长时间，因此这个笔记本教程在一个使用了 Dataproc Component Gateway 和 Jupyter 组件的 Dataproc 集群上运行。\n",
    "\n",
    "**已有 Dataproc 和 Jupyter 集群？**：如果你已经有一个正在运行的 Dataproc 集群，并且该集群上已安装了 [Component Gateway 和 Jupyter 组件](https://cloud.google.com/dataproc/docs/concepts/components/jupyter#gcloud-command)，你可以在这个教程中使用它。如果您计划使用它，请跳过这一步，转到 `切换内核`。\n",
    "\n",
    "为您的新集群设置一个名称和 [计算区域](https://cloud.google.com/compute/docs/regions-zones#available)。您的 `CLUSTER_NAME` 必须在您的 Google Cloud 项目中是**唯一的**。它必须以小写字母开头，后跟最多 51 个小写字母、数字和连字符，并且不能以连字符结尾。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4e3cf9e61673"
   },
   "outputs": [],
   "source": [
    "# The testing environment does not use a Dataproc cluster so cluster creation is skipped during testing.\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    CLUSTER_NAME = \"[your-cluster]\"  # @param {type: \"string\"}\n",
    "    CLUSTER_REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
    "\n",
    "    if CLUSTER_REGION == \"[your-region]\":\n",
    "        CLUSTER_REGION = \"us-central1\"\n",
    "\n",
    "    print(f\"CLUSTER_NAME: {CLUSTER_NAME}\")\n",
    "    print(f\"CLUSTER_REGION: {CLUSTER_REGION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ff30d97bf211"
   },
   "outputs": [],
   "source": [
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    !gcloud dataproc clusters create $CLUSTER_NAME \\\n",
    "        --region=$CLUSTER_REGION \\\n",
    "        --enable-component-gateway \\\n",
    "        --image-version=2.0 \\\n",
    "        --optional-components=JUPYTER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "#### 切换内核\n",
    "\n",
    "您的笔记本内核列在笔记本页面顶部。您的笔记本应该在运行在您的Dataproc集群上的Python 3内核上运行。\n",
    "\n",
    "从顶部菜单中选择**内核 > 更改内核**，然后选择`Python 3 on CLUSTER_NAME: Dataproc 集群 in REGION (远程)`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### 设置您的项目ID\n",
    "\n",
    "**如果您不知道您的项目ID**，请尝试以下操作：\n",
    "* 运行 `gcloud config list`。\n",
    "* 运行 `gcloud projects list`。\n",
    "* 查看支持页面：[查找项目ID](https://support.google.com/googleapi/answer/7014113)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_project_id"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
    "\n",
    "# Set the project id\n",
    "! gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "#### 地区\n",
    "\n",
    "您也可以更改 Vertex AI 使用的 `REGION` 变量。了解有关 [Vertex AI 地区](https://cloud.google.com/vertex-ai/docs/general/locations) 的更多信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2dw8q9fdQEH5"
   },
   "outputs": [],
   "source": [
    "REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7f1992d08421"
   },
   "outputs": [],
   "source": [
    "REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
    "\n",
    "# If you do not specify a region, it is set to \"us-central1\".\n",
    "if not REGION or REGION == \"\" or REGION == \"[your-region]\":\n",
    "    REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "UUID（Universally Unique Identifier）\n",
    "\n",
    "为了避免名称冲突，您可以为当前笔记本会话创建一个UUID，然后将该UUID附加到在本教程中创建的资源（如云存储桶和BigQuery数据集）的名称后面。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6988e7272299"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "\n",
    "# Generate a uuid of a specifed length(default=8)\n",
    "def generate_uuid(length: int = 8) -> str:\n",
    "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
    "\n",
    "\n",
    "UUID = generate_uuid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcp_authenticate"
   },
   "source": [
    "### 验证您的谷歌云账户\n",
    "\n",
    "根据您的Jupyter环境，您可能需要手动进行身份验证。请按照下面的相关说明进行操作。\n",
    "\n",
    "**1. Vertex AI Workbench**\n",
    "* 无需操作，因为您已经通过验证。\n",
    "\n",
    "**2. 本地JupyterLab实例，请取消注释并运行：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ce6043da7b33"
   },
   "outputs": [],
   "source": [
    "# ! gcloud auth login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0367eac06a10"
   },
   "source": [
    "3. 合作，取消注释并运行："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "21ad4dbb4a61"
   },
   "outputs": [],
   "source": [
    "# from google.colab import auth\n",
    "# auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c13224697bfb"
   },
   "source": [
    "请参考如何在https://cloud.google.com/storage/docs/gsutil/commands/iam#ch-examples上为您的服务帐号授予云存储权限。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bucket:mbsdk"
   },
   "source": [
    "创建一个云存储桶\n",
    "\n",
    "创建一个存储桶来存储中间产物，例如数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bucket"
   },
   "outputs": [],
   "source": [
    "BUCKET_URI = f\"gs://your-bucket-name-{PROJECT_ID}-unique\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "autoset_bucket"
   },
   "source": [
    "只有当您的桶尚不存在时才能运行以下单元格以创建您的云存储桶。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "91c46850b49b"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l {REGION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "## 教程\n",
    "\n",
    "### 导入所需的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRUOFELefqf1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from geopandas import gpd\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "# A Spark Session is how you interact with Spark SQL to create Dataframes\n",
    "from pyspark.sql import SparkSession\n",
    "# PySpark functions\n",
    "from pyspark.sql.functions import (col, floor, pandas_udf, to_timestamp, udf,\n",
    "                                   unix_timestamp)\n",
    "# These allow us to create a schema for our data\n",
    "from pyspark.sql.types import BooleanType, DoubleType\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71ecd729c4b9"
   },
   "source": [
    "请注意：在导入库之后，如果看到`ERROR 1: PROJ: proj_create_from_database: Open of /opt/conda/miniconda3/share/proj failed`，您可以忽略它。这是由于`geopandas`依赖存在错误。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "source": [
    "初始化SparkSession\n",
    "\n",
    "要在Apache Spark中使用BigQuery，您必须在初始化SparkSession时包含[spark-bigquery-connector](https://github.com/GoogleCloudDataproc/spark-bigquery-connector)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "outputs": [],
   "source": [
    "VER = \"0.26.0\"\n",
    "FILE_NAME = f\"spark-bigquery-with-dependencies_2.12-{VER}.jar\"\n",
    "\n",
    "if os.getenv(\"IS_TESTING\"):\n",
    "    # The local testing environment is not configured to read GCS files using Spark. This is a workaround for testing.\n",
    "    connector = f\"https://github.com/GoogleCloudDataproc/spark-bigquery-connector/releases/download/{VER}/{FILE_NAME}\"\n",
    "else:\n",
    "    connector = f\"gs://spark-lib/bigquery/{FILE_NAME}\"\n",
    "\n",
    "\n",
    "# Initialize the SparkSession.\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"spark-ml-taxi-citibike\")\n",
    "    .config(\"spark.jars\", connector)\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "source": [
    "### 从BigQuery获取数据\n",
    "\n",
    "本教程使用了2017年纽约市出租车数据集和2013年至2018年的纽约市自行车数据集。由于每个数据集的时间范围不同，将两个数据集匹配到2017年的时间段。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "outputs": [],
   "source": [
    "# Load NYC_taxi in Github Activity Public Dataset from BigQuery.\n",
    "taxi_df = (\n",
    "    spark.read.format(\"bigquery\")\n",
    "    .option(\"table\", \"bigquery-public-data.new_york_taxi_trips.tlc_green_trips_2017\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "\n",
    "# Load NYC_Citibike in Github Activity Public dataset from BQ.\n",
    "bike_df = (\n",
    "    spark.read.format(\"bigquery\")\n",
    "    .option(\"table\", \"bigquery-public-data.new_york_citibike.citibike_trips\")\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "source": [
    "使用`to_timestamp()`和`unix_timestamp()`函数将`starttime`和`stoptime`转换为Unix时间戳类型。\n",
    "\n",
    "Unix时间戳是一个整数，表示从`1970年1月1日00:00:00 UTC`开始的秒数，易于解析并在不同系统之间使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0156fd0e2921"
   },
   "outputs": [],
   "source": [
    "# Represents 2017-01-01 00:00:00\n",
    "START_2017 = 1483228800\n",
    "# Represents 2017-12-31 23:59:59\n",
    "END_2017 = 1514764799\n",
    "\n",
    "# Convert the type of starttime from a string to a Unix timestamp.\n",
    "bike_df = bike_df.withColumn(\n",
    "    \"starttime\", unix_timestamp(to_timestamp(col(\"starttime\")))\n",
    ")\n",
    "\n",
    "# Convert the type of stoptime from a string to a Unix timestamp.\n",
    "bike_df = bike_df.withColumn(\"stoptime\", unix_timestamp(to_timestamp(col(\"stoptime\"))))\n",
    "\n",
    "# Filter data so that `bike_df` only contains 2017 trips.\n",
    "bike_df = bike_df.where(\n",
    "    (col(\"starttime\") >= START_2017)\n",
    "    & (col(\"starttime\") <= END_2017)\n",
    "    & (col(\"stoptime\") >= START_2017)\n",
    "    & (col(\"stoptime\") <= END_2017)\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "Since notebook runtime scales linearly with the size of data, to limit processing time and compute resource consumption, \n",
    "this tutorial uses 20% of the Citi Bike dataset. You can try different values or remove the next line to use all of the data. \n",
    "\"\"\"\n",
    "bike_df = bike_df.sample(0.2)\n",
    "\n",
    "if os.getenv(\"IS_TESTING\"):\n",
    "    taxi_df = taxi_df.sample(0.0001)\n",
    "    bike_df = bike_df.sample(0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### 进行探索性数据分析（EDA）\n",
    "\n",
    "作为第一步，使用探索性数据分析（EDA）通过图表和其他可视化方法来发现数据中的模式、关系和异常。\n",
    "\n",
    "首先检查出租车数据集的数据类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "adeaf84f4054"
   },
   "outputs": [],
   "source": [
    "taxi_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "筛选掉不必要的列，并检查字段的空值计数。请注意，`pickup_location_id`和`dropoff_location_id`已从BigQuery数据集转换为字符串格式。为了有效使用它们，请重新格式化这些值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e5072e605966"
   },
   "outputs": [],
   "source": [
    "taxi_df = taxi_df.select(\n",
    "    \"pickup_datetime\",\n",
    "    \"dropoff_datetime\",\n",
    "    \"trip_distance\",\n",
    "    \"fare_amount\",\n",
    "    col(\"pickup_location_id\").cast(\"int\").alias(\"start_zone_id\"),\n",
    "    col(\"dropoff_location_id\").cast(\"int\").alias(\"end_zone_id\"),\n",
    ")\n",
    "\n",
    "taxi_df.describe().show()\n",
    "taxi_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "从这个摘要中，你可以了解到很多信息。\n",
    "\n",
    "- 2017年绿色出租车的行程历史记录超过1100万条，其中包括一些异常值，比如负值。\n",
    "- 直到2016年，精确的纬度和经度被用来表示出租车的上车和下车位置。这些数据引起了[隐私关注](https://agkn.wordpress.com/2014/09/15/riding-with-the-stars-passenger-privacy-in-the-nyc-taxicab-dataset/)。数据集中的`pickup_location_id`和`dropoff_location_id`对应于[纽约出租车区域](https://data.cityofnewyork.us/Transportation/NYC-Taxi-Zones/d3c5-ddgc)，大致以纽约市规划部门的社区统计地区（NTAs）为基础，并用来表示大致的社区位置。\n",
    "\n",
    "根据上述数据，过滤掉不现实的值。\n",
    "\n",
    "- 删除空值。\n",
    "- 行程距离必须在0到20英里之间，因为曼哈顿大约长13.4英里。\n",
    "- 车费金额，不包括税和小费，必须在0到500美元之间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "38aa94651f34"
   },
   "outputs": [],
   "source": [
    "taxi_df = taxi_df.where(\n",
    "    (col(\"trip_distance\") > 0)\n",
    "    & (col(\"trip_distance\") < 20)\n",
    "    & (col(\"fare_amount\") > 0)\n",
    "    & (col(\"fare_amount\") < 500)\n",
    ").dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "使用`unix_timestamp()`函数将`pickup_datetime`和`dropoff_datetime`转换为Unix Timestamp类型。\n",
    "\n",
    "将`pickup_datetime`和`dropoff_datetime`转换为`start_time`和`end_time`后，您可以操作这些时间戳生成更多有趣的列数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8be30eddd676"
   },
   "outputs": [],
   "source": [
    "# Convert the type of datetime from a string to a Unix timestamp.\n",
    "taxi_df = taxi_df.withColumn(\"start_time\", unix_timestamp(col(\"pickup_datetime\")))\n",
    "taxi_df = taxi_df.withColumn(\"end_time\", unix_timestamp(col(\"dropoff_datetime\")))\n",
    "\n",
    "# Calculate if this is a weekday\n",
    "taxi_df = taxi_df.withColumn(\n",
    "    \"is_weekday\",\n",
    "    ((floor(col(\"start_time\") / 86400) + 4) % 7 > 0)\n",
    "    & ((floor(col(\"start_time\") / 86400) + 4) % 7 < 6),\n",
    ")\n",
    "\n",
    "# Convert start_time to start_time_in_minute.\n",
    "taxi_df = taxi_df.withColumn(\n",
    "    \"start_time_in_hour\", floor((col(\"start_time\") % 86400) / 60 / 60)\n",
    ")\n",
    "\n",
    "# Calculate trip_duration.\n",
    "taxi_df = taxi_df.withColumn(\"trip_duration\", col(\"end_time\") - col(\"start_time\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### 进行特征工程\n",
    "\n",
    "出租车数据集包含纽约市所有行政区的行程，而 Citi Bike 数据集包含曼哈顿和布鲁克林以及布朗克斯部分地区之间的行程。为了匹配位置，过滤掉除了曼哈顿以外的所有行政区。\n",
    "\n",
    "为了将位置 ID 转换为行政区，可以从 NYC OpenData 下载纽约市出租车区域的 GeoJSON 格式数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b94d6eaf7870"
   },
   "outputs": [],
   "source": [
    "# Load the GeoJSON format of NYC Taxi zones. If a URLError is thrown, please re-run this cell.\n",
    "gdf_zone = gpd.read_file(\n",
    "    \"https://data.cityofnewyork.us/api/geospatial/d3c5-ddgc?method=export&format=GeoJSON\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2be027cd9619"
   },
   "outputs": [],
   "source": [
    "# Convert the type of \"location_id\" column.\n",
    "gdf_zone[\"location_id\"] = gdf_zone[\"location_id\"].astype(\"long\")\n",
    "\n",
    "# Convert GeoPandas format to a dictionary.\n",
    "hm = gdf_zone.to_dict(\"index\")\n",
    "\n",
    "# Create a set that contains location_id if the location is in Manhattan.\n",
    "manhattan_set = {hm[i][\"location_id\"] for i in hm if hm[i][\"borough\"] == \"Manhattan\"}\n",
    "\n",
    "\n",
    "@udf(returnType=BooleanType())\n",
    "def is_in_manhattan(location_id):\n",
    "    \"\"\"\n",
    "    The preprocessing function takes location_id and returns whether the given location_id is in the manhattan_set.\n",
    "    Args:\n",
    "        location_id: A number from 0 to 263 that represents NYC Taxi Zone.\n",
    "    Returns:\n",
    "        A Boolean value of whether the given location_id is in the manhattan_set.\n",
    "    \"\"\"\n",
    "    return location_id in manhattan_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "10215f2acf7b"
   },
   "outputs": [],
   "source": [
    "# Trips occur within Manhattan and between different taxi zones.\n",
    "taxi_df = taxi_df.where(\n",
    "    (col(\"start_zone_id\") != col(\"end_zone_id\"))\n",
    "    & (is_in_manhattan(col(\"start_zone_id\")))\n",
    "    & (is_in_manhattan(col(\"end_zone_id\")))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "70511564352c"
   },
   "outputs": [],
   "source": [
    "taxi_df.show()\n",
    "taxi_df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "在可视化这个修改后的出租车数据集之前，为Citi Bike数据集进行类似的特征工程处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1f7b3436a287"
   },
   "outputs": [],
   "source": [
    "bike_df.printSchema()\n",
    "bike_df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "这个数据集显示了一些有趣的信息：\n",
    "\n",
    "* 2017年Citi Bike的行程记录超过了280万条。\n",
    "* 当前数据集存在一些异常值。\n",
    "* `starttime`和`stoptime`是以字符串格式存在的。要有效使用它们，需要重新格式化。\n",
    "* Citi Bike数据集中不包含NYC出租车区域的`location_id`，只有坐标数据。想要将坐标转换成NYC出租车区域的话，可以使用Pandas UDF，一个矢量化UDF特性（详见[Introducing Pandas UDF for PySpark](https://www.databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html)）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2be027cd9619"
   },
   "outputs": [],
   "source": [
    "@pandas_udf(\"long\")\n",
    "def preprocess_zone_id(lat: pd.Series, lon: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    The preprocessing function takes latitudes and longitudes and returns the corresponding Taxi Zone id.\n",
    "    Args:\n",
    "        lat: The latitude of the position.\n",
    "        lon: The longitude of the position.\n",
    "    Returns:\n",
    "        The Taxi Zone id.\n",
    "    \"\"\"\n",
    "    point_var = [Point(xy) for xy in zip(lon, lat)]\n",
    "    gdf_points = gpd.GeoDataFrame(\n",
    "        pd.DataFrame({\"lat\": lat, \"lon\": lon}), crs=\"epsg:4326\", geometry=point_var\n",
    "    )\n",
    "    gdf_joined = gpd.sjoin(gdf_points, gdf_zone, how=\"left\")\n",
    "    return gdf_joined[\"location_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "77e00e95bf23"
   },
   "outputs": [],
   "source": [
    "# Rename \"tripduration\" to \"trip_duration\".\n",
    "bike_df = bike_df.withColumnRenamed(\"tripduration\", \"trip_duration\")\n",
    "\n",
    "# Check whether the starttime is a weekday or a weekend.\n",
    "bike_df = bike_df.withColumn(\n",
    "    \"is_weekday\",\n",
    "    ((floor(col(\"starttime\") / 86400) + 4) % 7 > 0)\n",
    "    & ((floor(col(\"starttime\") / 86400) + 4) % 7 < 6),\n",
    ")\n",
    "\n",
    "# Convert starttime to start_time_in_minute\n",
    "bike_df = bike_df.withColumn(\n",
    "    \"start_time_in_hour\", floor((col(\"starttime\") % 86400) / 60 / 60)\n",
    ")\n",
    "\n",
    "# Add \"start_zone_id\" and \"end_zone_id\" by applying the UDF \"preprocess_zone_id\".\n",
    "bike_df = bike_df.withColumn(\n",
    "    \"start_zone_id\",\n",
    "    preprocess_zone_id(\n",
    "        bike_df[\"start_station_latitude\"], bike_df[\"start_station_longitude\"]\n",
    "    ),\n",
    ")\n",
    "bike_df = bike_df.withColumn(\n",
    "    \"end_zone_id\",\n",
    "    preprocess_zone_id(\n",
    "        bike_df[\"end_station_latitude\"], bike_df[\"end_station_longitude\"]\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Filter `bike_df` using the following conditions:\n",
    "# 1) Trips must done between different taxi zones.\n",
    "# 2) The start time must be earlier than the end time.\n",
    "# 3) Trips must start and end within Manhattan.\n",
    "bike_df = bike_df.where(\n",
    "    (col(\"start_zone_id\") != col(\"end_zone_id\"))\n",
    "    & (col(\"starttime\") < col(\"stoptime\"))\n",
    "    & (is_in_manhattan(col(\"start_zone_id\")))\n",
    "    & (is_in_manhattan(col(\"end_zone_id\")))\n",
    ")\n",
    "\n",
    "bike_df = bike_df.select(\n",
    "    \"trip_duration\",\n",
    "    \"starttime\",\n",
    "    \"stoptime\",\n",
    "    \"start_station_latitude\",\n",
    "    \"start_station_longitude\",\n",
    "    \"end_station_latitude\",\n",
    "    \"end_station_longitude\",\n",
    "    \"usertype\",\n",
    "    \"start_zone_id\",\n",
    "    \"end_zone_id\",\n",
    "    \"is_weekday\",\n",
    "    \"start_time_in_hour\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "假设 Citi Bike 用户穿行在曼哈顿的街道上，街道是垂直的，通过应用曼哈顿距离可以计算行程的距离。\n",
    "\n",
    "曼哈顿距离计算示例：假设你在**第15街和第9大道，即Google纽约位于的地方**，你要去**第33街和第5大道，即帝国大厦所在地**。你从第15街向东移动，直到到达第5大道，然后继续向北行驶，直到到达第33街。在这种情况下，**第16街和第5大道**或**第33街和第9大道**可以作为铰接点。如果我们将起点设置为**S**，终点设置为**E**，铰接点设置为**H**，则曼哈顿距离的公式为 `距离(S, H) + 距离(H, E)`。\n",
    "\n",
    "然而，需要进行额外的计算才能获得“真实”的曼哈顿距离。首先，与笛卡尔坐标系不同，经度上的实际距离随纬度而变化。例如，在赤道，1度经度大约代表69英里，而在北纬或南纬45度时，大约代表49英里。要利用纬度和经度计算现实世界的确切距离，请使用[Haversine距离计算](https://en.wikipedia.org/wiki/Haversine_formula)。\n",
    "\n",
    "此外，曼哈顿街道与真正的北方相差大约29度，因此点应该逆时针旋转29度。\n",
    "\n",
    "因此，行程距离的公式应为`曼哈顿距离 = Haversine距离(S'，H') + Haversine距离(H'， E')`，其中`S'`，`H'`，`E'` 是旋转后的点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "82b6e83a1d26"
   },
   "outputs": [],
   "source": [
    "@udf(returnType=DoubleType())\n",
    "def manhattan_dist(lat_start, lon_start, lat_end, lon_end):\n",
    "    \"\"\"\n",
    "    The preprocessing function takes the latitude and longitude of start position and end position,\n",
    "    and returns the Manhattan distance between them.\n",
    "    Args:\n",
    "        lat_start: The latitude of the start position.\n",
    "        lon_start: The longitude of the start position.\n",
    "        lat_end: The latitude of the end position.\n",
    "        lon_end: The longitude of the end position.\n",
    "    Returns:\n",
    "        The Manhattan distance between start and end position.\n",
    "    \"\"\"\n",
    "    # Approximate radius of Earth in mile.\n",
    "    EARTH_RADIUS = 3958.76\n",
    "\n",
    "    # Approximate inclined degree of the streets in Manhattan.\n",
    "    THETA = np.radians(-28.904)\n",
    "\n",
    "    def haversine(pos_start, pos_end):\n",
    "        \"\"\"\n",
    "        The helper function takes start and end position and returns the haversine distance.\n",
    "        Args:\n",
    "            pos_start: a latitude and a longitude of start position in np.array form.\n",
    "            pos_end: a latitude and a longitude of end position in np.array form.\n",
    "        Returns:\n",
    "            The Haversine, the spherical distance between `pos_start` and `pos_end` on a sphere, in this case, the earth.\n",
    "        \"\"\"\n",
    "        rad_dist_lat = np.radians(pos_end[0] - pos_start[0])\n",
    "        rad_dist_lon = np.radians(pos_end[1] - pos_start[1])\n",
    "        rad_start_lat = np.radians(pos_start[0])\n",
    "        rad_end_lat = np.radians(pos_end[0])\n",
    "        dist = 2 * np.arcsin(\n",
    "            np.sqrt(\n",
    "                np.sin(rad_dist_lat / 2) ** 2\n",
    "                + np.cos(rad_start_lat)\n",
    "                * np.cos(rad_end_lat)\n",
    "                * np.sin(rad_dist_lon / 2) ** 2\n",
    "            )\n",
    "        )\n",
    "        return EARTH_RADIUS * dist\n",
    "\n",
    "    def rotate(pos, theta):\n",
    "        \"\"\"\n",
    "        The helper function takes position and the degree theta, and returns rotated position.\n",
    "        Args:\n",
    "            pos: a latitude and a longitude of position in np.array form.\n",
    "            theta: the degree to rotate.\n",
    "        Returns:\n",
    "            Rotated position.\n",
    "        \"\"\"\n",
    "        rotate = np.array(\n",
    "            [[np.cos(theta), np.sin(theta)], [-np.sin(theta), np.cos(theta)]]\n",
    "        )\n",
    "        return np.matmul(rotate, pos)\n",
    "\n",
    "    if not (lat_start and lon_start and lat_end and lon_end):\n",
    "        return -1\n",
    "\n",
    "    # Convert positions to np.array format.\n",
    "    start, end = np.array([lat_start, lon_start]), np.array([lat_end, lon_end])\n",
    "\n",
    "    # Rotate each positions by 29' using a helper function.\n",
    "    rotated_start, rotated_end = rotate(start, THETA), rotate(end, THETA)\n",
    "\n",
    "    # Get rotated hinge point using rotated start and end point.\n",
    "    rotated_hinge = np.array([rotated_start[0], rotated_end[1]])\n",
    "\n",
    "    # Re-rotate the hinge point.\n",
    "    hinge = rotate(rotated_hinge, -THETA)\n",
    "\n",
    "    # Return the sum of the Haversine distance between start and hinge, hinge and end.\n",
    "    return float(haversine(start, hinge) + haversine(hinge, end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ece73e72bea"
   },
   "outputs": [],
   "source": [
    "# Calculate the Manhattan distance between \"start_station\" and \"end_station\".\n",
    "bike_df = bike_df.withColumn(\n",
    "    \"trip_distance\",\n",
    "    manhattan_dist(\n",
    "        col(\"start_station_latitude\"),\n",
    "        col(\"start_station_longitude\"),\n",
    "        col(\"end_station_latitude\"),\n",
    "        col(\"end_station_longitude\"),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bc5dd05670e6"
   },
   "outputs": [],
   "source": [
    "# Filter out the bike_df with the same condition as the taxi_df\n",
    "bike_df = bike_df.where(\n",
    "    (col(\"trip_distance\") > 0)\n",
    "    & (col(\"trip_distance\") < 20)\n",
    "    & (col(\"trip_duration\") > 0)\n",
    "    & (col(\"start_station_latitude\") > 40)\n",
    "    & (col(\"start_station_latitude\") < 41)\n",
    "    & (col(\"end_station_latitude\") > 40)\n",
    "    & (col(\"end_station_latitude\") < 41)\n",
    "    & (col(\"start_station_longitude\") > -75)\n",
    "    & (col(\"start_station_longitude\") < -73)\n",
    "    & (col(\"end_station_longitude\") > -75)\n",
    "    & (col(\"end_station_longitude\") < -73)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "29d2a801cf6a"
   },
   "outputs": [],
   "source": [
    "bike_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "检查数字列的分布。在PySpark中，可视化是昂贵的，因为数据量很大。\n",
    "\n",
    "例如，纽约出租车数据集有超过1000万行。因此，总数据的约3%（约330k行）被提取为样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "42a319b0c66c"
   },
   "outputs": [],
   "source": [
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    taxi_pd = taxi_df.sample(0.03).toPandas()\n",
    "else:\n",
    "    taxi_pd = taxi_df.toPandas()\n",
    "\n",
    "# Convert \"object\" type columns to the float type.\n",
    "taxi_pd[\"trip_distance\"] = taxi_pd[\"trip_distance\"].astype(float)\n",
    "taxi_pd[\"fare_amount\"] = taxi_pd[\"fare_amount\"].astype(float)\n",
    "taxi_pd.info()\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    bike_pd = bike_df.sample(0.03).toPandas()\n",
    "else:\n",
    "    bike_pd = bike_df.toPandas()\n",
    "bike_pd.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "去除极端数据点，将数据分成箱和直方图。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "723d2f8fa827"
   },
   "outputs": [],
   "source": [
    "TAXI_COLUMNS_TO_SHOW = {\"trip_distance\", \"trip_duration\", \"fare_amount\"}\n",
    "\n",
    "taxi_pd_filtered = taxi_pd.query(\n",
    "    \"trip_distance > 0 and trip_distance < 20 \\\n",
    "    and trip_duration > 0 and trip_duration < 10000\"\n",
    ")\n",
    "\n",
    "sns.relplot(\n",
    "    data=taxi_pd_filtered,\n",
    "    x=\"trip_distance\",\n",
    "    y=\"trip_duration\",\n",
    "    kind=\"scatter\",\n",
    ")\n",
    "\n",
    "for column in taxi_pd_filtered.columns:\n",
    "    if column in TAXI_COLUMNS_TO_SHOW:\n",
    "        _, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "        taxi_pd_filtered[column].plot(kind=\"box\", ax=ax[0])\n",
    "        taxi_pd_filtered[column].plot(kind=\"hist\", ax=ax[1])\n",
    "        plt.title(column)\n",
    "        plt.figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "大多数列向右倾斜，'trip_distance'随着'trip_duration'的增加而增加。大多数行程在一个小时内完成（3600秒）。\n",
    "\n",
    "对Citi Bike数据集进行类似的分析。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "355716305c5f"
   },
   "outputs": [],
   "source": [
    "BIKE_COLUMNS_TO_SHOW = {\"trip_distance\", \"trip_duration\"}\n",
    "\n",
    "bike_pd_filtered = bike_pd.query(\n",
    "    \"trip_distance > 0 and trip_distance < 20 \\\n",
    "    and trip_duration > 0 and trip_duration < 10000\"\n",
    ")\n",
    "\n",
    "sns.relplot(\n",
    "    data=bike_pd_filtered,\n",
    "    x=\"trip_distance\",\n",
    "    y=\"trip_duration\",\n",
    "    hue=\"usertype\",\n",
    "    kind=\"scatter\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "Citi Bike的“trip_distance”和“trip_duration”之间的关系与出租车数据集中的相同图表不同。\n",
    "\n",
    "许多行程持续时间比预期的要长。这种行为的显而易见原因是：1）与出租车乘客不同，共享单车骑手不太可能有特定的行程目的地，2）偶尔使用Citi Bike的用户不会将他们的自行车停放在站点附近。\n",
    "\n",
    "为了去除离群值，我们假设平均自行车速度至少为每小时2英里。我们将筛选出“trip_duration”乘以速度小于“trip_distance”的行程，并移除“Customer”用户类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "45112e073997"
   },
   "outputs": [],
   "source": [
    "bike_pd_filtered = bike_pd.query(\n",
    "    \"trip_distance > 0 and trip_distance < 8 \\\n",
    "    and (trip_duration * 0.0005) <= trip_distance \\\n",
    "    and trip_duration > 0 and trip_duration < 10000 \\\n",
    "    and usertype=='Subscriber'\"\n",
    ")\n",
    "\n",
    "sns.relplot(\n",
    "    data=bike_pd_filtered,\n",
    "    x=\"trip_distance\",\n",
    "    y=\"trip_duration\",\n",
    "    hue=\"usertype\",\n",
    "    kind=\"scatter\",\n",
    ")\n",
    "\n",
    "for column in bike_pd_filtered.columns:\n",
    "    if column in BIKE_COLUMNS_TO_SHOW:\n",
    "        _, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "        bike_pd_filtered[column].plot(kind=\"box\", ax=ax[0])\n",
    "        bike_pd_filtered[column].plot(kind=\"hist\", ax=ax[1])\n",
    "        plt.title(column)\n",
    "        plt.figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "基于可视化的数据，应用过滤器来去除不现实的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "09d3908a6012"
   },
   "outputs": [],
   "source": [
    "taxi_df = taxi_df.where(\n",
    "    (col(\"trip_duration\") > 0)\n",
    "    & (col(\"trip_duration\") < 3600)\n",
    "    & ((col(\"trip_duration\") * 0.00055) <= col(\"trip_distance\"))\n",
    ")\n",
    "\n",
    "bike_df = bike_df.where(\n",
    "    (col(\"trip_duration\") > 0)\n",
    "    & (col(\"trip_duration\") < 3600)\n",
    "    & (col(\"usertype\") == \"Subscriber\")\n",
    "    & ((col(\"trip_duration\") * 0.00055) <= col(\"trip_distance\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "特征选择\n",
    "\n",
    "我们的数据集中并非所有特征都是有用的。\n",
    "由于本教程的目的是比较两个数据集，我们需要在训练中使用相同的特征。对于训练出租车数据集而言，有一些有用的特征，比如`fare_amount`，但由于Citi Bike数据集中没有这个特征，本教程中将不会使用它。\n",
    "\n",
    "在选择了以下列作为特征后，它们必须使用`VectorAssembler()`进行组装，这是一种特征转换器，可以将多个列合并成一个向量列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ca926fe3b73"
   },
   "outputs": [],
   "source": [
    "feature_cols = [\n",
    "    \"is_weekday\",\n",
    "    \"start_time_in_hour\",\n",
    "    \"start_zone_id\",\n",
    "    \"end_zone_id\",\n",
    "    \"trip_distance\",\n",
    "]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "# Transform each column into the vector form.\n",
    "taxi_transformed_data = assembler.transform(taxi_df)\n",
    "bike_transformed_data = assembler.transform(bike_df)\n",
    "\n",
    "# Split randomly with training and test data.\n",
    "(taxi_training_data, taxi_test_data) = taxi_transformed_data.randomSplit([0.95, 0.05])\n",
    "(bike_training_data, bike_test_data) = bike_transformed_data.randomSplit([0.95, 0.05])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### 训练模型\n",
    "\n",
    "PySpark提供了几种回归模型，包括`LinearRegression`、`GeneralizedLinearRegression`、`DecisionTreeRegressor`、`RandomForestRegressor`和`GBTRegressor`。\n",
    "`GBTRegressor`，也称为`Gradient Boosted Trees`，是一种流行的回归方法，使用决策树的集成。该算法通过迭代训练决策树以最小化损失函数。\n",
    "尽管这个模型计算成本很高，但测试表明`GBTRegressor`提供了最佳结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4d8ef0874cbf"
   },
   "outputs": [],
   "source": [
    "# Define GBTRegressor\n",
    "gbt = GBTRegressor(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"trip_duration\",\n",
    "    predictionCol=\"pred_trip_duration\",\n",
    ")\n",
    "\n",
    "# Define evaluator for r2 score.\n",
    "evaluator_r2 = RegressionEvaluator(\n",
    "    labelCol=gbt.getLabelCol(), predictionCol=gbt.getPredictionCol(), metricName=\"r2\"\n",
    ")\n",
    "\n",
    "# Define evaluator for RMSE error.\n",
    "evaluator_rmse = RegressionEvaluator(\n",
    "    labelCol=gbt.getLabelCol(), predictionCol=gbt.getPredictionCol(), metricName=\"rmse\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c0ecfdae3db2"
   },
   "outputs": [],
   "source": [
    "# Train a model and get predictions for the Taxi dataset. It takes several minutes.\n",
    "taxi_gbt_model = gbt.fit(taxi_training_data)\n",
    "taxi_gbt_predictions = taxi_gbt_model.transform(taxi_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e43b652e6107"
   },
   "outputs": [],
   "source": [
    "# Train a model and get predictions for the Citi Bike dataset. It takes several minutes.\n",
    "bike_gbt_model = gbt.fit(bike_training_data)\n",
    "bike_gbt_predictions = bike_gbt_model.transform(bike_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d4197f357a7b"
   },
   "outputs": [],
   "source": [
    "# Evaluate the r2 score for the Taxi dataset\n",
    "taxi_gbt_accuracy_r2 = evaluator_r2.evaluate(taxi_gbt_predictions)\n",
    "print(f\"Taxi Test GBT R2 Accuracy = {taxi_gbt_accuracy_r2}\")\n",
    "\n",
    "# Evaluate the RMSE for the Taxi dataset\n",
    "taxi_gbt_accuracy_rmse = evaluator_rmse.evaluate(taxi_gbt_predictions)\n",
    "print(f\"Taxi Test GBT RMSE Accuracy = {taxi_gbt_accuracy_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "410b54ce8541"
   },
   "outputs": [],
   "source": [
    "# Evaluate the r2 score for the Citi Bike dataset\n",
    "bike_gbt_accuracy_r2 = evaluator_r2.evaluate(bike_gbt_predictions)\n",
    "print(f\"Bike Test GBT R2 Accuracy = {bike_gbt_accuracy_r2}\")\n",
    "\n",
    "# Evaluate the RMSE for the Citi Bike dataset\n",
    "bike_gbt_accuracy_rmse = evaluator_rmse.evaluate(bike_gbt_predictions)\n",
    "print(f\"Bike Test GBT RMSE Accuracy = {bike_gbt_accuracy_rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "查看结果\n",
    "\n",
    "对于出租车数据集训练的模型显示大约77%的r2分数和230的均方根误差（RMSE），而Citi Bike数据集的模型显示大约71%的r2分数和250的RMSE。这些相对较低分数有一些原因。\n",
    "\n",
    "* 没有使用CrossValidator\n",
    "\n",
    "  * 在真实的机器学习项目中，交叉验证是一种重要工具，使我们能更好地利用数据。然而，它会多次重新运行训练算法，因此需要大量的时间和计算资源来完成。有关更多信息，请参见[交叉验证（统计学）](https://en.wikipedia.org/wiki/Cross-validation_(statistics))。\n",
    "\n",
    "* 没有使用出租车数据集中的每个特征。\n",
    "\n",
    "  * 出租车数据集有一个`fare_amount`列，它与标签`trip_duration`之间有很强的相关性。如果包含该列，出租车数据集的模型显示90%以上的r2分数。\n",
    "  * 出租车数据集特征有限的原因是这个项目的目的是比较出租车和Citi Bike。\n",
    "\n",
    "* 没有使用两个数据集的精确坐标。\n",
    "\n",
    "  * 由于隐私问题，出租车数据集不再提供接送地点的精确位置，只提供纽约出租车区域的位置。\n",
    "  * 出租车区域是一个表示纽约出租车定义的地理区域的分类号码。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "将模型保存到云存储路径###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a2280850da83"
   },
   "outputs": [],
   "source": [
    "BUCKET_FOLDER = \"/tmp/bucket\"\n",
    "\n",
    "# In the testing environment, saving to Cloud Storage bucket routes to the local file system.\n",
    "if os.getenv(\"IS_TESTING\"):\n",
    "    ! rm -rf $BUCKET_FOLDER\n",
    "    ! mkdir $BUCKET_FOLDER\n",
    "    bike_gbt_model.write().overwrite().save(f\"{BUCKET_FOLDER}\")\n",
    "    ! gsutil cp -r $BUCKET_FOLDER $BUCKET_URI\n",
    "else:\n",
    "    bike_gbt_model.write().overwrite().save(f\"{BUCKET_URI}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "清理\n",
    "\n",
    "请参考[清理](https://cloud.google.com/vertex-ai/docs/workbench/managed/create-managed-notebooks-instance-console-quickstart#clean-up)来删除此教程中创建的项目或托管笔记本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "### 删除Google Cloud Storage存储桶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d7a1a1a5e978"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DELETE_BUCKET = True\n",
    "\n",
    "if DELETE_BUCKET or os.getenv(\"IS_TESTING\"):\n",
    "    ! gsutil rm -r $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "### 删除Dataproc集群\n",
    "\n",
    "除非将内核切换到本地，否则无法删除当前正在使用的集群。要删除它，您需要将内核切换到本地的`Python 3`或`PySpark`，在下面的单元格中手动设置`CLUSTER_NAME`和`CLUSTER_REGION`，然后执行`gcloud`命令。\n",
    "\n",
    "请参阅[删除集群](https://cloud.google.com/dataproc/docs/guides/manage-cluster#console)以删除本教程中创建的Dataproc集群。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5f6cae539ccc"
   },
   "outputs": [],
   "source": [
    "CLUSTER_NAME = \"[your-cluster-name]\"\n",
    "CLUSTER_REGION = \"[your-cluster-region]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "61244eb74c17"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    ! gcloud dataproc clusters delete $CLUSTER_NAME --region=$CLUSTER_REGION -q"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "spark_ml.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
