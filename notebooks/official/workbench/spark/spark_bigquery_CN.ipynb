{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "使用Dataproc从BigQuery中摘要和分析数据\n",
    "\n",
    "从GitHub查看：\n",
    "[![GitHub logo](https://cloud.google.com/ml-engine/images/github-logo-32px.png \"GitHub logo\")](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/workbench/spark/spark_bigquery.ipynb)\n",
    "\n",
    "在Colab中运行：\n",
    "[![Colab logo](https://cloud.google.com/ml-engine/images/colab-logo-32px.png \"Colab logo\")](https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/workbench/spark/spark_bigquery.ipynb)\n",
    "\n",
    "在Vertex AI Workbench中打开：\n",
    "[![Vertex AI logo](https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32 \"Vertex AI logo\")](https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/workbench/spark/spark_bigquery.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## 概述\n",
    "\n",
    "本笔记本教程向您展示如何使用 Apache Spark 和 [Dataproc](https://cloud.google.com/dataproc) 将数据摄取、分析并写入 BigQuery。该笔记本代码分析 GitHub 活动数据，探索与 GitHub 仓库中使用的编程语言相关的指标。\n",
    "\n",
    "要运行此笔记本，请点击上方的 `在 Vertex AI Workbench 中打开` 链接。\n",
    "\n",
    "了解更多关于 [Vertex AI Workbench](https://cloud.google.com/vertex-ai/docs/workbench/introduction) 和 [Dataproc Serverless for Spark](https://cloud.google.com/dataproc-serverless/docs/guides/bigquery-connector-spark-example)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "### 目标\n",
    "\n",
    "这个笔记本教程运行一个 Apache Spark 作业，从 BigQuery 的 \"GitHub 活动数据\" 数据集中获取数据，查询数据，然后将结果写回 BigQuery。这个作业序列代表了一个常见的数据工程用例：摄取、转换和查询数据，然后将输出写入数据库。它还演示了如何提交一个 Apache Spark 作业到 Dataproc。\n",
    "\n",
    "这个教程使用以下 Google Cloud ML 服务：\n",
    "\n",
    "- `Dataproc`\n",
    "- `BigQuery`\n",
    "\n",
    "执行的步骤包括：\n",
    "\n",
    "- 设置一个 Google Cloud 项目和 Dataproc 集群。\n",
    "- 配置 spark-bigquery-connector。\n",
    "- 将数据从 BigQuery 摄取到 Spark DataFrame 中。\n",
    "- 对摄取的数据进行预处理。\n",
    "- 查询单一语言仓库中最常用的编程语言。\n",
    "- 查询存储在单一语言仓库中每种语言代码平均大小（MB）。\n",
    "- 查询通常一起找到的多语言仓库中的语言文件。\n",
    "- 将查询结果写回到 BigQuery。\n",
    "- 删除为这个笔记本教程创建的资源。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "数据集\n",
    "\n",
    "[GitHub活动数据](https://console.cloud.google.com/marketplace/product/github/github-repos)数据集可在[BigQuery公共数据集](https://cloud.google.com/bigquery/public-data)中获取，每月提供免费查询高达1TB的数据。它包含两种不同类型的存储库数据：支持多种编程语言文件的“polyglot”存储库和支持一种编程语言的“monoglot”存储库。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "费用\n",
    "\n",
    "本教程使用 Google Cloud 中可计费的组件：\n",
    "\n",
    "* [Vertex AI](https://cloud.google.com/vertex-ai/pricing)\n",
    "* [Cloud Storage](https://cloud.google.com/storage/pricing)\n",
    "* [Dataproc](https://cloud.google.com/dataproc/pricing)\n",
    "\n",
    "您可以使用 [定价计算器](https://cloud.google.com/products/calculator/) 根据您预计的使用量生成成本估算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WReHDGG5g0XY"
   },
   "source": [
    "### 安装\n",
    "\n",
    "安装以下包以运行这个笔记本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWEdiXsJg0XY"
   },
   "source": [
    "由于测试环境没有Java和PySpark，因此需要以下单元格用于测试目的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5f0d58c0e522"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.getenv(\"IS_TESTING\"):\n",
    "    \"\"\"\n",
    "    The testing suite does not currently support testing on Dataproc clusters,\n",
    "    so the testing environment is setup to replicate Dataproc via the following steps.\n",
    "    \"\"\"\n",
    "    JAVA_VER = \"8u332-b09\"\n",
    "    JAVA_FOLDER = \"/tmp/java\"\n",
    "    FILE_NAME = f\"openlogic-openjdk-{JAVA_VER}-linux-x64\"\n",
    "    TAR_FILE = f\"{JAVA_FOLDER}/{FILE_NAME}.tar.gz\"\n",
    "    DOWNLOAD_LINK = f\"https://builds.openlogic.com/downloadJDK/openlogic-openjdk/{JAVA_VER}/openlogic-openjdk-{JAVA_VER}-linux-x64.tar.gz\"\n",
    "    PYSPARK_VER = \"3.1.3\"\n",
    "\n",
    "    # Download Open JDK 8. Spark requires Java to execute.\n",
    "    ! rm -rf $JAVA_FOLDER\n",
    "    ! mkdir $JAVA_FOLDER\n",
    "    ! wget -P $JAVA_FOLDER $DOWNLOAD_LINK\n",
    "    os.environ[\"JAVA_HOME\"] = f\"{JAVA_FOLDER}/{FILE_NAME}\"\n",
    "    ! tar -zxf $TAR_FILE -C $JAVA_FOLDER\n",
    "    ! echo $JAVA_HOME\n",
    "\n",
    "    # Pin the Spark version to match that the Dataproc 2.0 cluster.\n",
    "    ! pip install pyspark==$PYSPARK_VER -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWEdiXsJg0XY"
   },
   "source": [
    "### 创建一个Dataproc集群\n",
    "\n",
    "在此笔记本教程中执行的Spark作业需要大量计算资源。由于在标准的笔记本环境中完成作业可能需要很长时间，因此此笔记本教程在一个由Dataproc组件网关和Jupyter组件安装在集群上创建的Dataproc集群上运行。\n",
    "\n",
    "**已存在具有Jupyter的Dataproc集群？**：如果您已经有一个运行中具有[在集群上安装了组件网关和Jupyter组件](https://cloud.google.com/dataproc/docs/concepts/components/jupyter#gcloud-command)的Dataproc集群，您可以在此教程中使用它。如果您计划使用它，请跳过此步骤，直接转到`切换内核`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oM1iC_MfAts1"
   },
   "outputs": [],
   "source": [
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    CLUSTER_NAME = \"[your-cluster]\"  # @param {type: \"string\"}\n",
    "    CLUSTER_REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
    "\n",
    "    if CLUSTER_REGION == \"[your-region]\":\n",
    "        CLUSTER_REGION = \"us-central1\"\n",
    "\n",
    "    print(f\"CLUSTER_NAME: {CLUSTER_NAME}\")\n",
    "    print(f\"CLUSTER_REGION: {CLUSTER_REGION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oM1iC_MfAts1"
   },
   "outputs": [],
   "source": [
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    !gcloud dataproc clusters create $CLUSTER_NAME \\\n",
    "        --region=$CLUSTER_REGION \\\n",
    "        --enable-component-gateway \\\n",
    "        --image-version=2.0 \\\n",
    "        --optional-components=JUPYTER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWEdiXsJg0XY"
   },
   "source": [
    "你的 `CLUSTER_NAME` 必须在你的 Google Cloud 项目内**是唯一的**。它必须以小写字母开头，后跟最多51个小写字母、数字和连字符，并且不能以连字符结尾。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWEdiXsJg0XY"
   },
   "source": [
    "#### 切换内核\n",
    "\n",
    "您的笔记本内核位于笔记本页面的顶部。 您的笔记本应该在 Dataproc 集群上运行的 Python 3 内核上运行。\n",
    "\n",
    "从顶部菜单中选择 **内核 > 更改内核**，然后选择 `Python 3 on CLUSTER_NAME: Dataproc 集群 in REGION (Remote)`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WReHDGG5g0XY"
   },
   "source": [
    "### 设置您的项目 ID\n",
    "\n",
    "**如果您不知道您的项目 ID**，请尝试以下操作：\n",
    "* 运行 `gcloud config list`。\n",
    "* 运行 `gcloud projects list`。\n",
    "* 查看支持页面：[查找项目 ID](https://support.google.com/googleapi/answer/7014113)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_project_id"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
    "\n",
    "# Set the project id\n",
    "! gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "创建一个BigQuery数据集\n",
    "\n",
    "本教程中创建的Spark DataFrame 存储在BigQuery中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06571eb4063b"
   },
   "source": [
    "UUID\n",
    "\n",
    "为避免名称冲突，您可以为当前笔记本会话创建一个UUID，然后将UUID附加到您在本教程中创建的BigQuery数据集中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "697568e92bd6"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "\n",
    "# Generate a uuid of a specifed length(default=8)\n",
    "def generate_uuid(length: int = 8) -> str:\n",
    "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
    "\n",
    "\n",
    "UUID = generate_uuid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "设置你的BigQuery数据集的名称，然后创建它。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    DATASET_NAME = \"[your-dataset-name]\"  # @param {type:\"string\"}\n",
    "\n",
    "    if (\n",
    "        DATASET_NAME == \"\"\n",
    "        or DATASET_NAME is None\n",
    "        or DATASET_NAME == \"[your-dataset-name]\"\n",
    "    ):\n",
    "        DATASET_NAME = f\"{PROJECT_ID}{UUID}\"\n",
    "else:\n",
    "    DATASET_NAME = f\"python_docs_samples_tests_spark_{UUID}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vhOb7YnwClBb"
   },
   "outputs": [],
   "source": [
    "! bq mk $DATASET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "## 教程\n",
    "\n",
    "### 导入所需的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRUOFELefqf1"
   },
   "outputs": [],
   "source": [
    "# You use Spark SQL in a \"SparkSession\" to create DataFrames\n",
    "from pyspark.sql import SparkSession\n",
    "# PySpark functions\n",
    "from pyspark.sql.functions import avg, col, count, desc, round, size, udf\n",
    "# These allow us to create a schema for our data\n",
    "from pyspark.sql.types import ArrayType, IntegerType, StringType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "source": [
    "### 初始化SparkSession\n",
    "\n",
    "要在BigQuery中使用Apache Spark，您必须在初始化`SparkSession`时包含[spark-bigquery-connector](https://github.com/GoogleCloudDataproc/spark-bigquery-connector)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "outputs": [],
   "source": [
    "# Initialize the \"SparkSession\" with the following config.\n",
    "VER = \"0.26.0\"\n",
    "FILE_NAME = f\"spark-bigquery-with-dependencies_2.12-{VER}.jar\"\n",
    "\n",
    "if os.getenv(\"IS_TESTING\"):\n",
    "    connector = f\"https://github.com/GoogleCloudDataproc/spark-bigquery-connector/releases/download/{VER}/{FILE_NAME}\"\n",
    "else:\n",
    "    connector = f\"gs://spark-lib/bigquery/{FILE_NAME}\"\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"spark-bigquery-polyglot-language-demo\")\n",
    "    .config(\"spark.jars\", connector)\n",
    "    .config(\"spark.sql.debug.maxToStringFields\", \"500\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "source": [
    "从BigQuery获取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "outputs": [],
   "source": [
    "# Load the Github Activity public dataset from BigQuery.\n",
    "df = (\n",
    "    spark.read.format(\"bigquery\")\n",
    "    .option(\"table\", \"bigquery-public-data.github_repos.languages\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# Restrict testing data since the testing environment runs on a small Docker image.\n",
    "if os.getenv(\"IS_TESTING\"):\n",
    "    df = df.sample(0.0001)\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "403176b059ae"
   },
   "source": [
    "### 预处理\n",
    "\n",
    "如所示的模式显示，Github Activity数据存储在数组中，而不是原始类型。\n",
    "\n",
    "为了有效地处理数据，将数组转换为原始类型，并将单语和多语存储库数据分开。\n",
    "\n",
    "三个Python函数的返回类型有一个`@udf`注释（表示一个[用户定义函数](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.udf.html)）。UDFs扩展了PySpark框架函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2e56d855da76"
   },
   "outputs": [],
   "source": [
    "# Set the LIMIT constant as 10 to get the top ten results.\n",
    "LIMIT = 10\n",
    "\n",
    "# A constant used to explode the pie chart to aid visibility.\n",
    "EXPLODE_PIE_CHART = tuple([0.05] * LIMIT)\n",
    "\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def language_to_mono_language(language) -> str:\n",
    "    \"\"\"\n",
    "    The preprocessing function takes a language array and returns its name if the language has one element.\n",
    "    Args:\n",
    "        language: list of struct that contains name and bytes.\n",
    "                  (e.g., language = [[name: \"C\", bytes: 300]]\n",
    "    Returns:\n",
    "        Monorepo's name\n",
    "    \"\"\"\n",
    "    return language[0].name if len(language) == 1 else None\n",
    "\n",
    "\n",
    "@udf(returnType=IntegerType())\n",
    "def language_to_mono_size(language) -> int:\n",
    "    \"\"\"\n",
    "    The preprocessing function takes a language array and returns its bytes if the language has one element.\n",
    "    Args:\n",
    "        language: list of struct that contains name and bytes.\n",
    "                  (e.g., language = [[name: \"C\", bytes: 300]]\n",
    "    Returns:\n",
    "        Monorepo's bytes\n",
    "    \"\"\"\n",
    "    return language[0].bytes if len(language) == 1 else 0\n",
    "\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def language_to_poly_language(language) -> str:\n",
    "    \"\"\"\n",
    "    The preprocessing function takes a language array and returns the top three language names based on their bytes.\n",
    "    Args:\n",
    "        language: list of struct that contains name and bytes.\n",
    "                  (e.g., language = [[name: \"C\", bytes: 300],\n",
    "                                     [name: \"Java\", bytes: 200]]\n",
    "    Returns:\n",
    "        Polyrepo's name in string form separated by commas\n",
    "    \"\"\"\n",
    "    if len(language) < 2:\n",
    "        return None\n",
    "    # Sort languages by their bytes in a descending order.\n",
    "    language.sort(key=lambda x: -x.bytes)\n",
    "    top_3 = language[:3]\n",
    "\n",
    "    # Sort top_3 languages by their name.\n",
    "    top_3.sort(key=lambda x: x.name)\n",
    "    ret = []\n",
    "    for elem in top_3:\n",
    "        ret.append(elem.name)\n",
    "    return \", \".join(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2e56d855da76"
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame named \"preprocessed_df\", with the array split into three columns using UDF.\n",
    "preprocessed_df = df.select(\n",
    "    col(\"repo_name\"),\n",
    "    language_to_mono_language(col(\"language\")).alias(\"mono_language\"),\n",
    "    language_to_mono_size(col(\"language\")).alias(\"mono_size\"),\n",
    "    language_to_poly_language(col(\"language\")).alias(\"poly_language\"),\n",
    ")\n",
    "preprocessed_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "668a2549b231"
   },
   "source": [
    "展示的`preprocessed_df`的架构显示语言列分成了三列：`mono_language`、`mono_size`和`poly_language`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "efec0bfbfb65"
   },
   "outputs": [],
   "source": [
    "# Output the number of repositories of monoglot(single language used) and polyglot(multiple languages used).\n",
    "mono = preprocessed_df.where(col(\"mono_language\").isNotNull()).count()\n",
    "print(f\"The number of repositories that use one language is {mono}\")\n",
    "\n",
    "poly = preprocessed_df.where(col(\"poly_language\").isNotNull()).count()\n",
    "print(f\"The number of repositories that use multiple languages is {poly}\")\n",
    "\n",
    "poly_percent = (poly / (mono + poly)) * 100\n",
    "print(\n",
    "    f\"Polyglot repositories comprise approximately {poly_percent:.2f}% of the total number of repositories.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0d79c26911f0"
   },
   "source": [
    "### 分析\n",
    "\n",
    "#### 单语存储库中最常用的语言是什么？\n",
    "要回答这个问题，执行以下查询，使用预处理列 `mono_language`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a241cfdd169d"
   },
   "outputs": [],
   "source": [
    "# Get the monoglot repositories and sort them based on language popularity.\n",
    "mono_ranking = (\n",
    "    preprocessed_df.groupBy(\"mono_language\")\n",
    "    .count()\n",
    "    .sort(desc(\"count\"))\n",
    "    .where(col(\"mono_language\").isNotNull())\n",
    ")\n",
    "mono_ranking.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9e5349dca8e6"
   },
   "source": [
    "使用`mono_ranking`，用饼图来展示结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7b9eb5a4f47e"
   },
   "outputs": [],
   "source": [
    "# Convert the Spark DataFrame to a Pandas DataFrame to display the pie chart.\n",
    "mono_panda = mono_ranking.toPandas()[:LIMIT].copy()\n",
    "mono_panda.groupby([\"mono_language\"]).sum().plot(\n",
    "    kind=\"pie\",\n",
    "    y=\"count\",\n",
    "    autopct=\"%1.1f%%\",\n",
    "    label=\"\",\n",
    "    title=\"Monoglot repositories\",\n",
    "    legend=False,\n",
    "    figsize=(7, 7),\n",
    "    explode=EXPLODE_PIE_CHART,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cc68a7c604b4"
   },
   "source": [
    "在单语版本库中，每种语言的平均大小是多少？\n",
    "\n",
    "对`mono_size`和`mono_language`列进行预处理，以获得每种语言的平均大小。\n",
    "\n",
    "`mono_size`以千字节为单位。以下查询将`mono_size`除以1000，以将大小转换为兆字节。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d5d3e2e38df7"
   },
   "outputs": [],
   "source": [
    "mono_ranking_avg_bytes = (\n",
    "    preprocessed_df.groupBy(\"mono_language\")\n",
    "    .agg(\n",
    "        count(\"mono_language\").alias(\"count\"),\n",
    "        round(avg(\"mono_size\") / 1000).alias(\"average_in_MB\"),\n",
    "    )\n",
    "    .sort(desc(\"average_in_MB\"))\n",
    "    .where(col(\"mono_language\").isNotNull() & (col(\"count\") > 500))\n",
    ")\n",
    "\n",
    "mono_ranking_avg_bytes.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4551e1b7b28b"
   },
   "source": [
    "在多语言存储库中，哪三种语言最常出现在一起？\n",
    "\n",
    "使用预处理的“poly_language”列，实现一个查询，显示根据大小排名的多语言存储库的前三种语言。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c8285a340a4a"
   },
   "outputs": [],
   "source": [
    "# Get the polyglot repositories by language popularity.\n",
    "poly_ranking = (\n",
    "    preprocessed_df.groupBy(\"poly_language\")\n",
    "    .count()\n",
    "    .sort(desc(\"count\"))\n",
    "    .where(col(\"poly_language\").isNotNull())\n",
    ")\n",
    "\n",
    "poly_ranking.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "633a0752b053"
   },
   "source": [
    "大多数结果包含HTML或CSS和Javascript的组合。\n",
    "\n",
    "显示一个饼图："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "30ae2d750ab7"
   },
   "outputs": [],
   "source": [
    "# Convert the Spark DataFrame to a Pandas DataFrame to display the pie chart.\n",
    "poly_panda = poly_ranking.toPandas()[:LIMIT].copy()\n",
    "poly_panda.groupby([\"poly_language\"]).sum().plot(\n",
    "    kind=\"pie\",\n",
    "    y=\"count\",\n",
    "    autopct=\"%1.1f%%\",\n",
    "    label=\"\",\n",
    "    title=\"Polyglot repositories\",\n",
    "    legend=False,\n",
    "    figsize=(7, 7),\n",
    "    explode=EXPLODE_PIE_CHART,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3905037bcea3"
   },
   "source": [
    "饼图显示，排名前十名结果中有八个包含`HTML`或`CSS`。您可以使用从BigQuery获取的原始数据在每个存储库中创建语言组合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "25479dcebd93"
   },
   "outputs": [],
   "source": [
    "# A Python package to get combinations.\n",
    "from itertools import combinations\n",
    "# A Python package to use type hint\n",
    "from typing import List\n",
    "\n",
    "# PySpark functions\n",
    "from pyspark.sql.functions import explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ad6a3e548fc3"
   },
   "outputs": [],
   "source": [
    "def normalize_name(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Change the language name to avoid invalid characters in the BigQuery data.\n",
    "    Args:\n",
    "        name: string\n",
    "    Returns:\n",
    "        Normalized name: string\n",
    "    \"\"\"\n",
    "    normalized_arr = []\n",
    "\n",
    "    # The following sets of characters cannot be used in BigQuery's fields.\n",
    "    invalid_chars = {\",\", \";\", \"{\", \"}\", \"(\", \")\", \"\\n\", \"\\t\", \"=\", \"'\"}\n",
    "    replace_chars = {\n",
    "        \" \": \"_\",\n",
    "        \".\": \"_\",\n",
    "        \"-\": \"_\",\n",
    "        \"#\": \"_sharp\",\n",
    "        \"+\": \"_plus\",\n",
    "        \"*\": \"_star\",\n",
    "    }\n",
    "\n",
    "    # The name must start with a letter or underscore.\n",
    "    if name[0].isnumeric():\n",
    "        normalized_arr.append(\"_\")\n",
    "\n",
    "    for ch in name:\n",
    "        # Skip if a character is in the set of invalid characters.\n",
    "        if ch in invalid_chars:\n",
    "            continue\n",
    "\n",
    "        # Replace if a character is in the dictionary of replace_chars.\n",
    "        if ch in replace_chars:\n",
    "            normalized_arr.append(replace_chars[ch])\n",
    "\n",
    "        # Change to lowercase to merge name duplications, for example, \"Java\" and \"java\".\n",
    "        else:\n",
    "            normalized_arr.append(ch.lower())\n",
    "\n",
    "    # Convert the array to string\n",
    "    return \"\".join(normalized_arr)\n",
    "\n",
    "\n",
    "@udf(returnType=ArrayType(StringType()))\n",
    "def reduce_language(language) -> List[str]:\n",
    "    \"\"\"\n",
    "    The preprocess function takes the language and reduces it to remove \"bytes\".\n",
    "    Args:\n",
    "        language: list of struct that contains name and bytes.\n",
    "                  (e.g., language = [[name: \"C\", bytes: 300],\n",
    "                                     [name: \"Java\", bytes: 200]]\n",
    "    Returns:\n",
    "        list of strings that contains name.\n",
    "                  (e.g., reduced_languages = [\"C\", \"Java\"])\n",
    "    \"\"\"\n",
    "    if len(language) < 2:\n",
    "        return None\n",
    "    reduced_languages = []\n",
    "    for elem in language:\n",
    "        # To write back to BigQuery, the name must be normalized.\n",
    "        normalized_name = normalize_name(elem.name)\n",
    "        reduced_languages.append(normalized_name)\n",
    "    return reduced_languages\n",
    "\n",
    "\n",
    "@udf(returnType=ArrayType(ArrayType(StringType())))\n",
    "def preprocess_combination(language) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    The preprocess function takes the language and returns every language combination.\n",
    "    Args:\n",
    "        language: list of struct that contains name and bytes.\n",
    "                  (e.g., language = [[name: \"C\", bytes: 300],\n",
    "                                     [name: \"Java\", bytes: 200]]\n",
    "    Returns:\n",
    "        List of every possible combinations.\n",
    "                  (e.g., arr_combinations = [[\"C\", \"Java\"], [\"Java\", \"C\"]])\n",
    "    \"\"\"\n",
    "    if not language:\n",
    "        return None\n",
    "    arr_combinations = []\n",
    "    for combination in combinations(language, 2):\n",
    "        arr_combinations.append(combination)\n",
    "        arr_combinations.append(combination[::-1])\n",
    "    return arr_combinations\n",
    "\n",
    "\n",
    "# Preprocess the \"reduced_languages\" column using UDF.\n",
    "df = df.withColumn(\"reduced_languages\", reduce_language(col(\"language\")))\n",
    "\n",
    "# Preprocess the \"combinations\" column using UDF.\n",
    "df = df.withColumn(\"combinations\", preprocess_combination(col(\"reduced_languages\")))\n",
    "\n",
    "# Create another DataFrame from \"df\" that has \"repo_name\" and \"combinations\" as columns.\n",
    "frequency_df = df.select(col(\"repo_name\"), col(\"combinations\")).where(\n",
    "    size(col(\"language\")) > 1\n",
    ")\n",
    "frequency_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5e80986d0095"
   },
   "source": [
    "`frequency_df`具有存储库名称和语言组合。\n",
    "\n",
    "使用Spark的`explode()`函数，该函数类似于SQL的`UNNEST`函数。\n",
    "\n",
    "您的表目前具有以下内容：\n",
    "\n",
    "| 存储库名称   | 组合              |\n",
    "| :----------: | :---------------: |\n",
    "| a           | [['C'，'C++']，['C++'，'C']，['C'，'Java']，['Java'，'C']，['C++'，'Java']，['Java'，'C++']]|\n",
    "| b           | [['C'，'C++']，['C++'，'C']，['C'，'Python']，['Python'，'C']，['C++'，'Python']，['Python'，'C++']]|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ec92c4d0652"
   },
   "outputs": [],
   "source": [
    "# explode() converts the elements in combinations to rows.\n",
    "frequency_df = frequency_df.withColumn(\"languages\", explode(col(\"combinations\")))\n",
    "\n",
    "# Create columns for combinations of languages.\n",
    "frequency_df = frequency_df.withColumn(\"language0\", col(\"languages\")[0])\n",
    "frequency_df = frequency_df.withColumn(\"language1\", col(\"languages\")[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5e80986d0095"
   },
   "source": [
    "使用`explode()`函数并添加`language0`和`language1`列后，`frequency_df`表的内容如下：\n",
    "\n",
    "| repo_name   | languages         | language0    | language1 |\n",
    "| :---------: | :---------------: | :--------:   | :-------: |\n",
    "| a           | ['C', 'C++']      | 'C'          |'C++'      |\n",
    "| a           | ['C++', 'C']      | 'C++'        |'C'        |\n",
    "| a           | ['C', 'Java']     | 'C'          |'Java'     |\n",
    "| a           | ['Java', 'C']     | 'Java'       |'C'        |\n",
    "| a           | ['C++', 'Java']   | 'C++'        |'Java'     |\n",
    "| a           | ['Java', 'C++']   | 'Java'       |'C++'      |\n",
    "| b           | ['C', 'C++']      | 'C'          |'C++'      |\n",
    "| b           | ['C++', 'C']      | 'C++'        |'C'        |\n",
    "| b           | ['C', 'Python']   | 'C'          |'Python'   |\n",
    "| b           | ['Python', 'C']   | 'Python'     |'C'        |\n",
    "| b           | ['C++', 'Python'] | 'C++'        |'Python'   |\n",
    "| b           | ['Python', 'C++'] | 'Python'     |'C++'      |\n",
    "\n",
    "计算`language0`和`language1`列的两两频率表。每行的第一列将包含`language0`的不同值，列名将包含`language1`的不同值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ec92c4d0652"
   },
   "outputs": [],
   "source": [
    "# crosstab() reshapes the table into a frequency distribution table by using cross tabulations.\n",
    "frequency_df = frequency_df.crosstab(\"language0\", \"language1\").withColumnRenamed(\n",
    "    \"language0_language1\", \"languages\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2c79c4a3326"
   },
   "source": [
    "将`crosstab()`应用于`frequency_df`后，DataFrame数据排列如下：\n",
    "\n",
    "| 语言       |  C  | C++ | Java | Python |\n",
    "| :--------: | :-: | :-: | :-:  |  :-:   |\n",
    "|     C      |  0  |  2  |  1   |   1    |\n",
    "|     C++    |  2  |  0  |  1   |   1    |\n",
    "|    Java    |  1  |  1  |  0   |   0    |\n",
    "|   Python   |  1  |  1  |  0   |   0    |\n",
    "\n",
    "请注意，此表包含样本数据，而非真实数据。\n",
    "\n",
    "请参阅[frequency distribution](https://en.wikipedia.org/wiki/Frequency_%28statistics%29#Frequency_distribution_table)和[cross tabulations](https://en.wikipedia.org/wiki/Contingency_table)。\n",
    "\n",
    "DataFrame现在包含每种语言的频率。使用一种流行的语言来可视化它。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ee1591ca4563"
   },
   "outputs": [],
   "source": [
    "# Set of popular languages. You can modify this set to show your preferred languages.\n",
    "MAJOR_LANGUAGES = {\"C\", \"Java\", \"Python\", \"JavaScript\", \"Go\"}\n",
    "\n",
    "# Declare a dictionary to store the key as a language name and the value as the selected DataFrame\n",
    "df_dict = dict()\n",
    "\n",
    "for language in MAJOR_LANGUAGES:\n",
    "    # Get a top ten languages of each language and store it to the dictionary.\n",
    "    df_dict[language] = (\n",
    "        frequency_df.select(col(\"languages\"), language).sort(-col(language)).limit(10)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5951469b9925"
   },
   "outputs": [],
   "source": [
    "for language in df_dict:\n",
    "    # Convert Spark DataFrame to Pandas DataFrame to display the bar chart.\n",
    "    elem_panda = df_dict[language].toPandas()[:LIMIT].copy()\n",
    "    elem_panda.set_index(\"languages\", inplace=True)\n",
    "    elem_panda.sort_values(language, ascending=True, inplace=True)\n",
    "    elem_panda.plot(\n",
    "        kind=\"barh\",\n",
    "        title=language,\n",
    "        legend=False,\n",
    "        xlabel=\"\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "### 写回到 BigQuery\n",
    "\n",
    "在分析这些查询后，会有几个 DataFrames：单语库的排名、单语库的平均字节数，以及仓库中每种语言使用频率表。\n",
    "\n",
    "这些 DataFrames 将使用 [spark-bigquery-connector](https://github.com/GoogleCloudDataproc/spark-bigquery-connector) 存储在 BigQuery 中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b8ab41bf47dd"
   },
   "outputs": [],
   "source": [
    "dataframes = {\n",
    "    \"mono_ranking\": mono_ranking,\n",
    "}\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    dataframes[\"mono_ranking_avg_bytes\"] = mono_ranking_avg_bytes\n",
    "    dataframes[\"frequency_table\"] = frequency_df\n",
    "\n",
    "# Iterate through the DataFrames and save them to the BigQuery.\n",
    "for df in dataframes:\n",
    "    dataframes[df].write.format(\"bigquery\").option(\"writeMethod\", \"direct\").option(\n",
    "        \"table\", f\"{DATASET_NAME}.{df}\"\n",
    "    ).save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "如果没有错误报告，恭喜！您的DataFrame已成功存储在BigQuery中。\n",
    "\n",
    "您可以在[Google Cloud控制台](https://console.corp.google.com/bigquery)上查看数据，或者使用`bq`命令行工具。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e7734cef2fc3"
   },
   "outputs": [],
   "source": [
    "QUERY = f\"SELECT languages, python FROM {PROJECT_ID}.{DATASET_NAME}.frequency_table ORDER BY python DESC LIMIT 10\"\n",
    "\n",
    "! bq query --nouse_legacy_sql $QUERY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "清理\n",
    "\n",
    "请参考 [清理](https://cloud.google.com/vertex-ai/docs/workbench/managed/create-managed-notebooks-instance-console-quickstart#clean-up) 来删除您在本教程中创建的项目或托管笔记本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "###删除BigQuery数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b77295420ab9"
   },
   "outputs": [],
   "source": [
    "! bq rm -r -f $DATASET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "在删除BigQuery数据集之后，您可以使用以下命令检查BigQuery中的数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c53e6e169788"
   },
   "outputs": [],
   "source": [
    "! bq ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "删除Dataproc集群\n",
    "\n",
    "除非将内核切换为本地，否则无法删除当前正在使用的集群。要删除它，您需要将内核切换到本地的 `Python 3` 或 `PySpark`，在以下单元格中手动设置您的 `CLUSTER_NAME` 和 `CLUSTER_REGION`，然后执行 `gcloud` 命令。\n",
    "\n",
    "请查看[删除集群](https://cloud.google.com/dataproc/docs/guides/manage-cluster#console)以删除本教程中创建的Dataproc集群。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c53e6e169788"
   },
   "outputs": [],
   "source": [
    "CLUSTER_NAME = \"[your-cluster-name]\"\n",
    "CLUSTER_REGION = \"[your-cluster-region]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b8eeeb3cd293"
   },
   "outputs": [],
   "source": [
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    ! gcloud dataproc clusters delete $CLUSTER_NAME --region=$CLUSTER_REGION -q"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "spark_bigquery.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
