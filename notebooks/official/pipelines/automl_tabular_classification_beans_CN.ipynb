{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "copyright"
   },
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title:generic"
   },
   "source": [
    "# Vertex AI管道：使用google-cloud-pipeline-components的AutoML表格管道\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/pipelines/automl_tabular_classification_beans.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> 在Colab中打开\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fofficial%2Fpipelines%2Fautoml_tabular_classification_beans.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://cloud.google.com/ml-engine/images/colab-enterprise-logo-32px.png\" alt=\"Google Cloud Colab Enterprise logo\"><br> 在Colab Enterprise中打开\n",
    "    </a>\n",
    "  </td>    \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/pipelines/automl_tabular_classification_beans.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> 在Workbench中打开\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/pipelines/automl_tabular_classification_beans.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> 在GitHub上查看\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "overview:pipelines,automl,beans"
   },
   "source": [
    "## 概述\n",
    "\n",
    "本笔记本展示如何使用 [*google_cloud_pipeline_components*](https://github.com/kubeflow/pipelines/tree/master/components/google-cloud) SDK 中的组件在 [Vertex AI Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines) 中构建一个 AutoML 表格分类工作流。\n",
    "\n",
    "您可以在本笔记本中构建如下的流水线：\n",
    "\n",
    "<a href=\"https://storage.googleapis.com/amy-jo/images/mp/beans.png\" target=\"_blank\"><img src=\"https://storage.googleapis.com/amy-jo/images/mp/beans.png\" width=\"95%\"/></a>\n",
    "\n",
    "了解有关 [Vertex AI Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines/introduction) 和 [AutoML 组件](https://cloud.google.com/vertex-ai/docs/pipelines/vertex-automl-component) 的更多信息。了解有关 [表格数据分类](https://cloud.google.com/vertex-ai/docs/tabular-data/classification-regression/overview) 的更多信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "objective:pipelines,automl"
   },
   "source": [
    "### 目标\n",
    "\n",
    "在本教程中，您将学习如何使用**Vertex AI Pipelines**和**Google Cloud Pipeline Components**构建一个AutoML表格分类模型。\n",
    "\n",
    "\n",
    "本教程使用以下Vertex AI服务：\n",
    "\n",
    "- Vertex AI Pipelines\n",
    "- Google Cloud Pipeline Components\n",
    "- Vertex AutoML\n",
    "- Vertex AI模型\n",
    "- Vertex AI端点\n",
    "\n",
    "执行的步骤包括：\n",
    "\n",
    "- 创建一个KFP流水线，其中包括：\n",
    "    - 创建一个Vertex AI数据集。\n",
    "    - 训练一个AutoML表格分类模型资源。\n",
    "    - 创建一个Vertex AI端点资源。\n",
    "    - 将模型资源部署到端点资源。\n",
    "- 编译KFP流水线。\n",
    "- 使用Vertex AI Pipelines执行KFP流水线。\n",
    "\n",
    "了解更多关于[Google Cloud Pipeline Components SDK中的Vertex AI组件](https://google-cloud-pipeline-components.readthedocs.io/en/latest/google_cloud_pipeline_components.aiplatform.html#module-google_cloud_pipeline_components.aiplatform)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset:beans,lcn"
   },
   "source": [
    "数据集\n",
    "\n",
    "本教程使用UCI机器学习的 \"干豆数据集\"，来自于KOKLU, M. and OZKAN, I.A., (2020)的研究: \"利用计算机视觉和机器学习技术对干豆进行多类别分类\"。该研究发表在《计算与电子农业》杂志上，第174期，105507页。DOI：https://doi.org/10.1016/j.compag.2020.105507。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "costs"
   },
   "source": [
    "### 费用\n",
    "\n",
    "本教程使用 Google Cloud 的可计费组件：\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "了解 [Vertex AI 价格](https://cloud.google.com/vertex-ai/pricing) 和 [Cloud Storage 价格](https://cloud.google.com/storage/pricing)，并使用 [定价计算器](https://cloud.google.com/products/calculator/) 根据您的预期使用情况生成费用估算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61RBz8LLbxCR"
   },
   "source": [
    "开始吧"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "No17Cw5hgx12"
   },
   "source": [
    "### 为Python安装Vertex AI SDK和其他必需的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "! pip3 install --upgrade --quiet google-cloud-aiplatform \\\n",
    "                                 google-cloud-storage \\\n",
    "                                 kfp \\\n",
    "                                 google-cloud-pipeline-components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5Xep4W9lq-Z"
   },
   "source": [
    "### 重新启动运行时（仅适用于Colab）\n",
    "\n",
    "为了使用新安装的软件包，您必须在Google Colab上重新启动运行时。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D-ZBOjErv5mM"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbmM4z7FOBpM"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ 内核即将重新启动。在进行下一步之前，请等待它完成。 ⚠️</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "check_versions"
   },
   "source": [
    "检查包版本\n",
    "\n",
    "检查您安装的包的版本。KFP SDK 版本应该是 >=1.8。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_versions:kfp,gcpc"
   },
   "outputs": [],
   "source": [
    "! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "! python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmWOrTJ3gx13"
   },
   "source": [
    "### 在笔记本环境上进行身份验证（仅限Colab）\n",
    "\n",
    "在Google Colab上对您的环境进行身份验证。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NyKGtVQjgx13"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DF4l8DTdWgPY"
   },
   "source": [
    "### 设置Google Cloud项目信息\n",
    "\n",
    "要开始使用Vertex AI，您必须拥有现有的Google Cloud项目。了解更多关于[设置项目和开发环境](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)的信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_project_id"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "创建一个云存储桶\n",
    "\n",
    "创建一个存储桶，用于存储中间产物，如数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "BUCKET_URI = f\"gs://your-bucket-name-{PROJECT_ID}-unique\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_bucket"
   },
   "source": [
    "如果您的存储桶尚不存在：运行以下单元格以创建您的云存储桶。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIq7R4HZCfIc"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l {LOCATION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "set_service_account"
   },
   "source": [
    "### 服务账号\n",
    "\n",
    "**如果您不知道您的服务账号**，请尝试使用`gcloud`命令在下面执行第二个单元格获取您的服务账号。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_service_account"
   },
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT = \"\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "autoset_service_account"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "if (\n",
    "    SERVICE_ACCOUNT == \"\"\n",
    "    or SERVICE_ACCOUNT is None\n",
    "    or SERVICE_ACCOUNT == \"[your-service-account]\"\n",
    "):\n",
    "    # Get your service account from gcloud\n",
    "    if not IS_COLAB:\n",
    "        shell_output = !gcloud auth list 2>/dev/null\n",
    "        SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
    "\n",
    "    if IS_COLAB:\n",
    "        shell_output = ! gcloud projects describe  $PROJECT_ID\n",
    "        project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
    "        SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
    "\n",
    "    print(\"Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "set_service_account:pipelines"
   },
   "source": [
    "设置用于 Vertex AI 管道的服务帐户访问权限\n",
    "\n",
    "运行以下命令，将您的服务帐户授予对您在上一步中创建的存储桶中的管道工件进行读取和写入的访问权限。每个服务帐户只需要运行一次。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_service_account:pipelines"
   },
   "outputs": [],
   "source": [
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
    "\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_vars"
   },
   "source": [
    "### 导入库并定义常量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "import google.cloud.aiplatform as aiplatform\n",
    "import kfp\n",
    "from google.cloud import bigquery\n",
    "from kfp import compiler, dsl\n",
    "from kfp.dsl import (Artifact, ClassificationMetrics, Input, Metrics, Output,\n",
    "                     component)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aip_constants:endpoint"
   },
   "source": [
    "#### Vertex AI 常量\n",
    "\n",
    "为 Vertex AI 流水线设置以下常量：\n",
    "- `PIPELINE_NAME`：为流水线设置名称。\n",
    "- `PIPELINE_ROOT`：Cloud Storage 存储流水线工件的桶路径。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aip_constants:endpoint"
   },
   "outputs": [],
   "source": [
    "# set path for storing the pipeline artifacts\n",
    "PIPELINE_NAME = \"automl-tabular-beans-training\"\n",
    "PIPELINE_ROOT = \"{}/pipeline_root/beans\".format(BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk"
   },
   "source": [
    "### 初始化用于 Python 的 Vertex AI SDK\n",
    "\n",
    "要开始使用 Vertex AI，您必须[启用 Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com)。\n",
    "\n",
    "为您的项目和相应的存储桶初始化 Python 的 Vertex AI SDK。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=LOCATION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "define_component:eval"
   },
   "source": [
    "## 为度量评估定义自定义组件\n",
    "\n",
    "在本教程中，您定义一个自定义管道组件。其余组件是为Vertex AI服务预构建的组件。\n",
    "\n",
    "您定义的自定义管道组件是基于Python函数的组件。基于Python函数的组件使您可以通过构建Python函数作为组件代码并为您生成组件规范来更轻松地进行迭代。\n",
    "\n",
    "注意`@component`装饰器。当您评估`classification_model_eval`函数时，该组件被编译为本质上是一个任务工厂函数，可在管道定义中使用。\n",
    "\n",
    "此外，生成了一个**tabular_eval_component.yaml**组件定义文件。该组件**yaml**文件可以被共享并放在版本控制下，以便稍后用于定义管道步骤。\n",
    "\n",
    "组件定义指定组件要使用的基础镜像，并指定应安装`google-cloud-aiplatform`包。如果未指定，基础镜像默认为Python 3.7。\n",
    "\n",
    "您在此步骤中创建的自定义管道组件检索由AutoML表格训练过程生成的分类模型评估度量。然后，它解析评估数据，并为模型呈现ROC曲线和混淆矩阵。此外，检查设置的阈值是否符合生成的指标，以确定模型是否足够准确可供部署。\n",
    "\n",
    "**注意**：此自定义组件特定于AutoML表格分类任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "define_component:eval"
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"gcr.io/deeplearning-platform-release/tf2-cpu.2-6:latest\",\n",
    "    packages_to_install=[\"google-cloud-aiplatform\"],\n",
    ")\n",
    "def classification_model_eval_metrics(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    thresholds_dict_str: str,\n",
    "    model: Input[Artifact],\n",
    "    metrics: Output[Metrics],\n",
    "    metricsc: Output[ClassificationMetrics],\n",
    ") -> NamedTuple(\"Outputs\", [(\"dep_decision\", str)]):  # Return parameter.\n",
    "\n",
    "    import json\n",
    "    import logging\n",
    "\n",
    "    from google.cloud import aiplatform\n",
    "\n",
    "    aiplatform.init(project=project)\n",
    "\n",
    "    # Fetch model eval info\n",
    "    def get_eval_info(model):\n",
    "        response = model.list_model_evaluations()\n",
    "        metrics_list = []\n",
    "        metrics_string_list = []\n",
    "        for evaluation in response:\n",
    "            evaluation = evaluation.to_dict()\n",
    "            print(\"model_evaluation\")\n",
    "            print(\" name:\", evaluation[\"name\"])\n",
    "            print(\" metrics_schema_uri:\", evaluation[\"metricsSchemaUri\"])\n",
    "            metrics = evaluation[\"metrics\"]\n",
    "            for metric in metrics.keys():\n",
    "                logging.info(\"metric: %s, value: %s\", metric, metrics[metric])\n",
    "            metrics_str = json.dumps(metrics)\n",
    "            metrics_list.append(metrics)\n",
    "            metrics_string_list.append(metrics_str)\n",
    "\n",
    "        return (\n",
    "            evaluation[\"name\"],\n",
    "            metrics_list,\n",
    "            metrics_string_list,\n",
    "        )\n",
    "\n",
    "    # Use the given metrics threshold(s) to determine whether the model is\n",
    "    # accurate enough to deploy.\n",
    "    def classification_thresholds_check(metrics_dict, thresholds_dict):\n",
    "        for k, v in thresholds_dict.items():\n",
    "            logging.info(\"k {}, v {}\".format(k, v))\n",
    "            if k in [\"auRoc\", \"auPrc\"]:  # higher is better\n",
    "                if metrics_dict[k] < v:  # if under threshold, don't deploy\n",
    "                    logging.info(\"{} < {}; returning False\".format(metrics_dict[k], v))\n",
    "                    return False\n",
    "        logging.info(\"threshold checks passed.\")\n",
    "        return True\n",
    "\n",
    "    def log_metrics(metrics_list, metricsc):\n",
    "        test_confusion_matrix = metrics_list[0][\"confusionMatrix\"]\n",
    "        logging.info(\"rows: %s\", test_confusion_matrix[\"rows\"])\n",
    "\n",
    "        # log the ROC curve\n",
    "        fpr = []\n",
    "        tpr = []\n",
    "        thresholds = []\n",
    "        for item in metrics_list[0][\"confidenceMetrics\"]:\n",
    "            fpr.append(item.get(\"falsePositiveRate\", 0.0))\n",
    "            tpr.append(item.get(\"recall\", 0.0))\n",
    "            thresholds.append(item.get(\"confidenceThreshold\", 0.0))\n",
    "        print(f\"fpr: {fpr}\")\n",
    "        print(f\"tpr: {tpr}\")\n",
    "        print(f\"thresholds: {thresholds}\")\n",
    "        metricsc.log_roc_curve(fpr, tpr, thresholds)\n",
    "\n",
    "        # log the confusion matrix\n",
    "        annotations = []\n",
    "        for item in test_confusion_matrix[\"annotationSpecs\"]:\n",
    "            annotations.append(item[\"displayName\"])\n",
    "        logging.info(\"confusion matrix annotations: %s\", annotations)\n",
    "        metricsc.log_confusion_matrix(\n",
    "            annotations,\n",
    "            test_confusion_matrix[\"rows\"],\n",
    "        )\n",
    "\n",
    "        # log textual metrics info as well\n",
    "        for metric in metrics_list[0].keys():\n",
    "            if metric != \"confidenceMetrics\":\n",
    "                val_string = json.dumps(metrics_list[0][metric])\n",
    "                metrics.log_metric(metric, val_string)\n",
    "\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "    # extract the model resource name from the input Model Artifact\n",
    "    model_resource_path = model.metadata[\"resourceName\"]\n",
    "    logging.info(\"model path: %s\", model_resource_path)\n",
    "\n",
    "    # Get the trained model resource\n",
    "    model = aiplatform.Model(model_resource_path)\n",
    "\n",
    "    # Get model evaluation metrics from the the trained model\n",
    "    eval_name, metrics_list, metrics_str_list = get_eval_info(model)\n",
    "    logging.info(\"got evaluation name: %s\", eval_name)\n",
    "    logging.info(\"got metrics list: %s\", metrics_list)\n",
    "    log_metrics(metrics_list, metricsc)\n",
    "\n",
    "    thresholds_dict = json.loads(thresholds_dict_str)\n",
    "    deploy = classification_thresholds_check(metrics_list[0], thresholds_dict)\n",
    "    if deploy:\n",
    "        dep_decision = \"true\"\n",
    "    else:\n",
    "        dep_decision = \"false\"\n",
    "    logging.info(\"deployment decision is %s\", dep_decision)\n",
    "\n",
    "    return (dep_decision,)\n",
    "\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    classification_model_eval_metrics, \"tabular_eval_component.yaml\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "define_pipeline:gcpc,beans,lcn"
   },
   "source": [
    "定义管道\n",
    "\n",
    "使用`google_cloud_pipeline_components`包中的组件来定义AutoML表格分类的管道。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "define_pipeline:gcpc,beans,lcn"
   },
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(name=PIPELINE_NAME, pipeline_root=PIPELINE_ROOT)\n",
    "def pipeline(\n",
    "    bq_source: str,\n",
    "    DATASET_DISPLAY_NAME: str,\n",
    "    TRAINING_DISPLAY_NAME: str,\n",
    "    MODEL_DISPLAY_NAME: str,\n",
    "    ENDPOINT_DISPLAY_NAME: str,\n",
    "    MACHINE_TYPE: str,\n",
    "    project: str,\n",
    "    gcp_region: str,\n",
    "    thresholds_dict_str: str,\n",
    "):\n",
    "    from google_cloud_pipeline_components.v1.automl.training_job import \\\n",
    "        AutoMLTabularTrainingJobRunOp\n",
    "    from google_cloud_pipeline_components.v1.dataset.create_tabular_dataset.component import \\\n",
    "        tabular_dataset_create as TabularDatasetCreateOp\n",
    "    from google_cloud_pipeline_components.v1.endpoint.create_endpoint.component import \\\n",
    "        endpoint_create as EndpointCreateOp\n",
    "    from google_cloud_pipeline_components.v1.endpoint.deploy_model.component import \\\n",
    "        model_deploy as ModelDeployOp\n",
    "\n",
    "    dataset_create_op = TabularDatasetCreateOp(\n",
    "        project=project,\n",
    "        location=gcp_region,\n",
    "        display_name=DATASET_DISPLAY_NAME,\n",
    "        bq_source=bq_source,\n",
    "    )\n",
    "\n",
    "    training_op = AutoMLTabularTrainingJobRunOp(\n",
    "        project=project,\n",
    "        location=gcp_region,\n",
    "        display_name=TRAINING_DISPLAY_NAME,\n",
    "        optimization_prediction_type=\"classification\",\n",
    "        optimization_objective=\"minimize-log-loss\",\n",
    "        budget_milli_node_hours=1000,\n",
    "        model_display_name=MODEL_DISPLAY_NAME,\n",
    "        column_specs={\n",
    "            \"Area\": \"numeric\",\n",
    "            \"Perimeter\": \"numeric\",\n",
    "            \"MajorAxisLength\": \"numeric\",\n",
    "            \"MinorAxisLength\": \"numeric\",\n",
    "            \"AspectRation\": \"numeric\",\n",
    "            \"Eccentricity\": \"numeric\",\n",
    "            \"ConvexArea\": \"numeric\",\n",
    "            \"EquivDiameter\": \"numeric\",\n",
    "            \"Extent\": \"numeric\",\n",
    "            \"Solidity\": \"numeric\",\n",
    "            \"roundness\": \"numeric\",\n",
    "            \"Compactness\": \"numeric\",\n",
    "            \"ShapeFactor1\": \"numeric\",\n",
    "            \"ShapeFactor2\": \"numeric\",\n",
    "            \"ShapeFactor3\": \"numeric\",\n",
    "            \"ShapeFactor4\": \"numeric\",\n",
    "            \"Class\": \"categorical\",\n",
    "        },\n",
    "        dataset=dataset_create_op.outputs[\"dataset\"],\n",
    "        target_column=\"Class\",\n",
    "    )\n",
    "\n",
    "    model_eval_task = classification_model_eval_metrics(\n",
    "        project=project,\n",
    "        location=gcp_region,\n",
    "        thresholds_dict_str=thresholds_dict_str,\n",
    "        model=training_op.outputs[\"model\"],\n",
    "    )\n",
    "\n",
    "    with dsl.If(\n",
    "        model_eval_task.outputs[\"dep_decision\"] == \"true\",\n",
    "        name=\"deploy_decision\",\n",
    "    ):\n",
    "\n",
    "        endpoint_op = EndpointCreateOp(\n",
    "            project=project,\n",
    "            location=gcp_region,\n",
    "            display_name=ENDPOINT_DISPLAY_NAME,\n",
    "        )\n",
    "\n",
    "        ModelDeployOp(\n",
    "            model=training_op.outputs[\"model\"],\n",
    "            endpoint=endpoint_op.outputs[\"endpoint\"],\n",
    "            dedicated_resources_min_replica_count=1,\n",
    "            dedicated_resources_max_replica_count=1,\n",
    "            dedicated_resources_machine_type=MACHINE_TYPE,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "compile_pipeline"
   },
   "source": [
    "## 编译管道\n",
    "\n",
    "接下来，将管道编译为一个yaml文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "compile_pipeline"
   },
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline,\n",
    "    package_path=\"tabular_classification_pipeline.yaml\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run_pipeline:model"
   },
   "source": [
    "运行流水线\n",
    "\n",
    "传递必要的流水线输入参数并运行它。定义的流水线接受以下参数：\n",
    "\n",
    "- `bq_source`: 表格数据集的BigQuery源。\n",
    "- `DATASET_DISPLAY_NAME`: Vertex AI管理的数据集的显示名称。\n",
    "- `TRAINING_DISPLAY_NAME`: AutoML训练作业的显示名称。\n",
    "- `MODEL_DISPLAY_NAME`: 训练作业生成的Vertex AI模型的显示名称。\n",
    "- `ENDPOINT_DISPLAY_NAME`: 部署模型的Vertex AI端点的显示名称。\n",
    "- `MACHINE_TYPE`: 用于服务容器的机器类型。\n",
    "- `project`: 运行流水线的项目ID。\n",
    "- `gcp_region`: 设置流水线位置的区域。\n",
    "- `thresholds_dict_str`: 基于其条件部署模型的阈值字典。\n",
    "- `pipeline_root`: 要覆盖流水线作业定义中指定的流水线根路径，请指定一个您的流水线作业可以访问的路径，比如一个Cloud Storage存储桶URI。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1adf9b056954"
   },
   "outputs": [],
   "source": [
    "# Set the display-names for Vertex AI resources\n",
    "PIPELINE_DISPLAY_NAME = \"[your-pipeline-display-name]\"  # @param {type:\"string\"}\n",
    "DATASET_DISPLAY_NAME = \"[your-dataset-display-name]\"  # @param {type:\"string\"}\n",
    "MODEL_DISPLAY_NAME = \"[your-model-display-name]\"  # @param {type:\"string\"}\n",
    "TRAINING_DISPLAY_NAME = \"[your-training-job-display-name]\"  # @param {type:\"string\"}\n",
    "ENDPOINT_DISPLAY_NAME = \"[your-endpoint-display-name]\"  # @param {type:\"string\"}\n",
    "\n",
    "# Otherwise, use the default display-names\n",
    "if PIPELINE_DISPLAY_NAME == \"[your-pipeline-display-name]\":\n",
    "    PIPELINE_DISPLAY_NAME = \"pipeline_beans-unique\"\n",
    "\n",
    "if DATASET_DISPLAY_NAME == \"[your-dataset-display-name]\":\n",
    "    DATASET_DISPLAY_NAME = \"dataset_beans-unique\"\n",
    "\n",
    "if MODEL_DISPLAY_NAME == \"[your-model-display-name]\":\n",
    "    MODEL_DISPLAY_NAME = \"model_beans-unique\"\n",
    "\n",
    "if TRAINING_DISPLAY_NAME == \"[your-training-job-display-name]\":\n",
    "    TRAINING_DISPLAY_NAME = \"automl_training_beans-unique\"\n",
    "\n",
    "if ENDPOINT_DISPLAY_NAME == \"[your-endpoint-display-name]\":\n",
    "    ENDPOINT_DISPLAY_NAME = \"endpoint_beans-unique\"\n",
    "\n",
    "# Set machine type\n",
    "MACHINE_TYPE = \"n1-standard-4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_pipeline:model"
   },
   "outputs": [],
   "source": [
    "# Validate region of the given source (BigQuery) against region of the pipeline\n",
    "bq_source = \"aju-dev-demos.beans.beans1\"\n",
    "\n",
    "client = bigquery.Client()\n",
    "bq_region = client.get_table(bq_source).location.lower()\n",
    "try:\n",
    "    assert bq_region in LOCATION\n",
    "    print(f\"Region validated: {LOCATION}\")\n",
    "except AssertionError:\n",
    "    print(\n",
    "        \"Please make sure the region of BigQuery (source) and that of the pipeline are the same.\"\n",
    "    )\n",
    "\n",
    "# Configure the pipeline\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=PIPELINE_DISPLAY_NAME,\n",
    "    template_path=\"tabular_classification_pipeline.yaml\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values={\n",
    "        \"project\": PROJECT_ID,\n",
    "        \"gcp_region\": LOCATION,\n",
    "        \"bq_source\": f\"bq://{bq_source}\",\n",
    "        \"thresholds_dict_str\": '{\"auRoc\": 0.95}',\n",
    "        \"DATASET_DISPLAY_NAME\": DATASET_DISPLAY_NAME,\n",
    "        \"TRAINING_DISPLAY_NAME\": TRAINING_DISPLAY_NAME,\n",
    "        \"MODEL_DISPLAY_NAME\": MODEL_DISPLAY_NAME,\n",
    "        \"ENDPOINT_DISPLAY_NAME\": ENDPOINT_DISPLAY_NAME,\n",
    "        \"MACHINE_TYPE\": MACHINE_TYPE,\n",
    "    },\n",
    "    enable_caching=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view_pipeline_run:model"
   },
   "source": [
    "运行管道作业。单击生成的链接在云控制台中查看您的运行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "114ab8ff24ac"
   },
   "outputs": [],
   "source": [
    "# Run the job\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "compare_pipeline_runs"
   },
   "source": [
    "检查流水线运行的参数和指标，以及其受到跟踪的元数据。\n",
    "\n",
    "接下来，您可以使用Python的Vertex AI SDK检查流水线运行的参数和指标。等待流水线运行完成后再运行下一个单元格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "compare_pipeline_runs"
   },
   "outputs": [],
   "source": [
    "pipeline_df = aiplatform.get_pipeline_df(pipeline=PIPELINE_NAME)\n",
    "print(pipeline_df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleanup:pipelines"
   },
   "source": [
    "清理\n",
    "\n",
    "要清理此项目中使用的所有Google Cloud资源，您可以删除用于本教程的[Google Cloud项目](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects)。\n",
    "\n",
    "否则，您可以删除在本教程中创建的各个资源。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cleanup:pipelines"
   },
   "outputs": [],
   "source": [
    "# Delete the Vertex AI Pipeline Job\n",
    "job.delete()\n",
    "\n",
    "# List and filter the Vertex AI Endpoint\n",
    "endpoints = aiplatform.Endpoint.list(\n",
    "    filter=f\"display_name={ENDPOINT_DISPLAY_NAME}\", order_by=\"create_time\"\n",
    ")\n",
    "# Delete the Vertex AI Endpoint\n",
    "if len(endpoints) > 0:\n",
    "    endpoint = endpoints[0]\n",
    "    endpoint.delete(force=True)\n",
    "\n",
    "# List and filter the Vertex AI model\n",
    "models = aiplatform.Model.list(\n",
    "    filter=f\"display_name={MODEL_DISPLAY_NAME}\", order_by=\"create_time\"\n",
    ")\n",
    "# Delete the Vertex AI model\n",
    "if len(models) > 0:\n",
    "    model = models[0]\n",
    "    model.delete()\n",
    "\n",
    "# List and filter the Vertex AI Dataset\n",
    "datasets = aiplatform.TabularDataset.list(\n",
    "    filter=f\"display_name={DATASET_DISPLAY_NAME}\", order_by=\"create_time\"\n",
    ")\n",
    "# Delete the Vertex AI Dataset\n",
    "if len(datasets) > 0:\n",
    "    dataset = datasets[0]\n",
    "    dataset.delete()\n",
    "\n",
    "# Delete the Cloud Storage bucket\n",
    "delete_bucket = False  # Set True for deletion\n",
    "if delete_bucket:\n",
    "    ! gsutil rm -r $BUCKET_URI"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "automl_tabular_classification_beans.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
