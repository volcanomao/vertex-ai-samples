{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18ebbd838e32"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "978ab06a7e3f"
   },
   "source": [
    "# Vertex AI管道：使用BigQuery源和目的地进行训练和批量预测自定义表格分类模型\n",
    "\n",
    "<table align=\"left\">\n",
    "\n",
    "  <td>\n",
    "<a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/pipelines/custom_tabular_train_batch_pred_bq_pipeline.ipynb\" target='_blank'>\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> 在Colab中运行\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "<a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/pipelines/custom_tabular_train_batch_pred_bq_pipeline.ipynb\" target='_blank'>\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      在GitHub上查看\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "<a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/pipelines/custom_tabular_train_batch_pred_bq_pipeline.ipynb\" target='_blank'>\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      在Vertex AI工作台中打开\n",
    "    </a>\n",
    "  </td>  \n",
    "</table>\n",
    "\n",
    "*备注：此笔记本使用KFP 1.x和GCPC 1.x。我们建议使用2.x*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1153bb181b8c"
   },
   "source": [
    "## 概述\n",
    "\n",
    "这本笔记本演示了在Vertex AI管道内为自定义表格分类模型执行训练和批量预测。批量预测作业从BigQuery源获取数据，并将结果写入BigQuery目的地。\n",
    "\n",
    "了解更多关于[Vertex AI管道](https://cloud.google.com/vertex-ai/docs/pipelines/introduction)和[Vertex AI批量预测组件](https://cloud.google.com/vertex-ai/docs/pipelines/batchprediction-component)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87c2947ffa97"
   },
   "source": [
    "### 目标\n",
    "\n",
    "在本教程中，您将通过使用 `google_cloud_pipeline_components` 通过 Vertex AI 管道训练一个 scikit-learn 表格分类模型，并为其创建批量预测作业。批量预测作业的源数据和目标数据存储在 BigQuery 中。\n",
    "\n",
    "本教程使用以下 Google Cloud ML 服务和资源:\n",
    "\n",
    "- Vertex AI `Pipelines`\n",
    "- Vertex AI `Datasets`\n",
    "- Vertex AI `Training`\n",
    "- Vertex AI `Model Registry`\n",
    "- Vertex AI `Batch Predictions`\n",
    "\n",
    "执行的步骤包括:\n",
    "\n",
    "- 在 BigQuery 中创建一个数据集。\n",
    "- 从源数据集中保留一些数据用于批量预测。\n",
    "- 为训练应用程序创建自定义的 Python 包。\n",
    "- 将 Python 包上传到 Cloud Storage。\n",
    "- 创建一个 Vertex AI 管道，其中包括:\n",
    "    - 从源数据集创建一个 Vertex AI 数据集。\n",
    "    - 在数据集上训练一个 scikit-learn 随机森林分类模型。\n",
    "    - 将训练完成的模型上传到 Vertex AI Model Registry。\n",
    "    - 使用模型在测试数据上运行一个批量预测作业。\n",
    "- 检查在 BigQuery 的目标表中的预测结果。\n",
    "- 清理本笔记本中创建的资源。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aef4f59195ad"
   },
   "source": [
    "### 数据集\n",
    "\n",
    "这本笔记本使用的[人口普查收入数据集](https://archive.ics.uci.edu/ml/datasets/Census+Income)可以在BigQuery位置`bigquery-public-data.ml_datasets.census_adult_income`上公开获取。它包括以下字段：\n",
    "\n",
    "- `age`：年龄。\n",
    "- `workclass`：就业性质。\n",
    "- `functional_weight`：个体在原始人口普查数据中的样本权重。根据其人口特征与整体人口估计相比，他们被包含在这个数据集中的可能性有多大。\n",
    "- `education`：完成的教育水平。\n",
    "- `education_num`：根据教育字段的值估算完成的教育年限。\n",
    "- `marital_status`：婚姻状况。\n",
    "- `occupation`：职业类别。\n",
    "- `relationship`：与家庭的关系。\n",
    "- `race`：种族。\n",
    "- `sex`：性别。\n",
    "- `capital_gain`：资本收益金额。\n",
    "- `capital_loss`：资本损失金额。\n",
    "- `hours_per_week`：每周工作小时数。\n",
    "- `native_country`：出生国家。\n",
    "- `income_bracket`：根据收入为“>50K”或“<=50K”。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6fcafeb3489d"
   },
   "source": [
    "### 费用\n",
    "该教程使用 Google Cloud 的可计费组件：\n",
    "\n",
    "* Vertex AI\n",
    "* BigQuery\n",
    "* Cloud Storage\n",
    "\n",
    "了解有关[Vertex AI 价格](https://cloud.google.com/vertex-ai/pricing)，[BigQuery 价格](https://cloud.google.com/bigquery/pricing)，[Cloud Storage 价格](https://cloud.google.com/storage/pricing)，并使用[Pricing 计算器](https://cloud.google.com/products/calculator/)，根据您的预期使用情况生成费用估算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7b887879da0a"
   },
   "source": [
    "安装以下包以执行此笔记本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1bbc49622f3c"
   },
   "outputs": [],
   "source": [
    "! pip3 install --upgrade --quiet google-cloud-aiplatform \\\n",
    "                                 google-cloud-bigquery \\\n",
    "                                 pandas \\\n",
    "                                 pyarrow \\\n",
    "                                 'kfp<2' \\\n",
    "                                 'google-cloud-pipeline-components<2' \n",
    "\n",
    "! pip3 install --quiet db-dtypes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58707a750154"
   },
   "source": [
    "### 仅限Colab：取消注释以下单元格以重新启动内核。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f200f10a1da3"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs so that your environment can access the new packages\n",
    "# import IPython\n",
    "\n",
    "# app = IPython.Application.instance()\n",
    "# app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "57dad372c81b"
   },
   "source": [
    "UUID\n",
    "\n",
    "如果您正在进行实时教程会话，您可能正在使用共享的测试帐户或项目。为了避免用户在创建的资源之间的名称冲突，您为每个实例会话创建一个 uuid，并将其附加到您在本教程中创建的资源名称上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4e166d927e36"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "\n",
    "# Generate a uuid of a specifed length(default=8)\n",
    "def generate_uuid(length: int = 8) -> str:\n",
    "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
    "\n",
    "\n",
    "UUID = generate_uuid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF1j6f9HApxa"
   },
   "source": [
    "## 在开始之前\n",
    "\n",
    "### 设置您的谷歌云项目\n",
    "\n",
    "**无论您使用什么笔记本环境，以下步骤都是必需的。**\n",
    "\n",
    "1. [选择或创建一个谷歌云项目](https://console.cloud.google.com/cloud-resource-manager)。当您第一次创建账户时，您会获得$300的免费信用用于计算/存储成本。\n",
    "\n",
    "2. [确保为您的项目启用了计费](https://cloud.google.com/billing/docs/how-to/modify-project)。\n",
    "\n",
    "3. [启用 Vertex AI API]。\n",
    "\n",
    "4. 如果您是在本地运行这个笔记本，您需要安装[Cloud SDK](https://cloud.google.com/sdk)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WReHDGG5g0XY"
   },
   "source": [
    "#### 设置您的项目ID\n",
    "\n",
    "**如果您不知道您的项目ID**，请尝试以下操作：\n",
    "* 运行 `gcloud config list`。\n",
    "* 运行 `gcloud projects list`。\n",
    "* 参考支持页面：[查找项目ID](https://support.google.com/googleapi/answer/7014113)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oM1iC_MfAts1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
    "\n",
    "# Set the project id\n",
    "! gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "### 区域\n",
    "\n",
    "您也可以更改 Vertex AI 使用的 `REGION` 变量。了解有关 [Vertex AI 区域](https://cloud.google.com/vertex-ai/docs/general/locations)的更多信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "region"
   },
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBCra4QMA2wR"
   },
   "source": [
    "### 验证您的 Google Cloud 帐户\n",
    "\n",
    "根据您的 Jupyter 环境，您可能需要手动进行身份验证。请按照下面的相关说明操作。\n",
    "\n",
    "**1. Vertex AI Workbench**\n",
    "* 由于您已经验证过了，无需进行额外操作。\n",
    "\n",
    "**2. 本地 JupyterLab 实例，请取消注释并运行：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "254614fa0c46"
   },
   "outputs": [],
   "source": [
    "# ! gcloud auth login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ef21552ccea8"
   },
   "source": [
    "3. 协作，取消注释并运行："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "603adbbf0532"
   },
   "outputs": [],
   "source": [
    "# from google.colab import auth\n",
    "# auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6b2ccc891ed"
   },
   "source": [
    "查看如何在https://cloud.google.com/storage/docs/gsutil/commands/iam#ch-examples中为您的服务帐号授予云存储权限。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "创建一个云存储桶\n",
    "\n",
    "创建一个存储桶来存储诸如数据集之类的中间产品。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "BUCKET_URI = f\"gs://your-bucket-name-{PROJECT_ID}-unique\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EcIXiGsCePi"
   },
   "source": [
    "只有在您的存储桶尚不存在时才运行以下单元格来创建您的云存储存储桶。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIq7R4HZCfIc"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8debaa04cb14"
   },
   "source": [
    "服务账户\n",
    "\n",
    "您使用服务账户来创建 Vertex AI Pipeline 作业。如果您不想使用项目的 Compute Engine 服务账户，请将 `SERVICE_ACCOUNT` 设置为另一个服务账户的ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "77b01a1fdbb4"
   },
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT = \"[your-service-account]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f936bebda2d4"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "if (\n",
    "    SERVICE_ACCOUNT == \"\"\n",
    "    or SERVICE_ACCOUNT is None\n",
    "    or SERVICE_ACCOUNT == \"[your-service-account]\"\n",
    "):\n",
    "    # Get your service account from gcloud\n",
    "    if not IS_COLAB:\n",
    "        shell_output = !gcloud auth list 2>/dev/null\n",
    "        SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
    "\n",
    "    else:  # IS_COLAB:\n",
    "        shell_output = ! gcloud projects describe  $PROJECT_ID\n",
    "        project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
    "        SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
    "\n",
    "    print(\"Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40ef6967cad3"
   },
   "source": [
    "为Vertex AI Pipelines设置服务账户访问\n",
    "\n",
    "运行以下命令，为您的服务账户授予读取和写入管道工件的权限，这些工件存储在您在之前步骤中创建的存储桶中。您只需要针对每个服务账户运行此步骤一次。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f88cb0488c08"
   },
   "outputs": [],
   "source": [
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
    "\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6a61dae37929"
   },
   "source": [
    "### 导入库\n",
    "\n",
    "导入Vertex AI Python SDK和其他必需的Python库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ff2006a7c9a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from google.cloud import aiplatform, bigquery\n",
    "from kfp.dsl import pipeline\n",
    "from kfp.v2 import compiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "782ab8eecedf"
   },
   "source": [
    "### 初始化用于 Python 的 Vertex AI SDK\n",
    "\n",
    "为您的项目和相应的存储桶初始化 Python 版本的 Vertex AI SDK。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c0b876b97113"
   },
   "outputs": [],
   "source": [
    "# Initialize Vertex AI SDK\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)\n",
    "\n",
    "# Initialize BigQuery client\n",
    "bq_client = bigquery.Client(\n",
    "    project=PROJECT_ID,\n",
    "    credentials=aiplatform.initializer.global_config.credentials,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eb0eafd9b733"
   },
   "source": [
    "定义常量\n",
    "\n",
    "在训练模型、创建和运行管道时设置你所需要的常量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "891800f8c3ab"
   },
   "outputs": [],
   "source": [
    "# Source of the dataset\n",
    "DATA_SOURCE = \"bq://bigquery-public-data.ml_datasets.census_adult_income\"\n",
    "# Set name for the managed Vertex AI dataset\n",
    "DATASET_DISPLAY_NAME = f\"adult_census_dataset_{UUID}\"\n",
    "# BigQuery Dataset name\n",
    "BQ_DATASET_ID = f\"income_prediction_{UUID}\"\n",
    "# Set name for the BigQuery source table for batch prediction\n",
    "BQ_INPUT_TABLE = f\"income_test_data_{UUID}\"\n",
    "# Set the size(%) of the train set\n",
    "TRAIN_SPLIT = 0.9\n",
    "# Provide the container for training the model\n",
    "TRAINING_CONTAINER = \"us-docker.pkg.dev/vertex-ai/training/scikit-learn-cpu.0-23:latest\"\n",
    "# Provide the container for serving the model\n",
    "SERVING_CONTAINER = \"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.0-23:latest\"\n",
    "# Set the display name for training job\n",
    "TRAINING_JOB_DISPLAY_NAME = f\"income_classify_train_job_{UUID}\"\n",
    "# Model display name for Vertex AI Model Registry\n",
    "MODEL_DISPLAY_NAME = f\"income_classify_model_{UUID}\"\n",
    "# Set the name for batch prediction job\n",
    "BATCH_PREDICTION_JOB_NAME = f\"income_classify_batch_pred_{UUID}\"\n",
    "# Dispaly name for the Vertex AI Pipeline\n",
    "PIPELINE_DISPLAY_NAME = f\"income_classfiy_batch_pred_pipeline_{UUID}\"\n",
    "# Filename to compile the pipeline to\n",
    "PIPELINE_FILE_NAME = f\"{PIPELINE_DISPLAY_NAME}.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e40fa6f1c674"
   },
   "source": [
    "创建一个BigQuery数据集\n",
    "在本教程中，您的批量预测作业的输入和输出需要存储在BigQuery中。因此，您需要在BigQuery中创建一个数据集来存储它们。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "74d8750d8fe9"
   },
   "outputs": [],
   "source": [
    "# Create a BQ dataset\n",
    "bq_dataset = bigquery.Dataset(f\"{PROJECT_ID}.{BQ_DATASET_ID}\")\n",
    "bq_dataset = bq_client.create_dataset(bq_dataset)\n",
    "print(f\"Created dataset {bq_client.project}.{bq_dataset.dataset_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "042747a05fd1"
   },
   "source": [
    "## 为批量预测创建测试数据\n",
    "\n",
    "查询公共数据集源并在创建的BigQuery数据集中创建一个测试集。\n",
    "\n",
    "对于批量预测，您的测试集是通过随机选择源数据集的一小部分（1-`TRAIN_SPLIT`）来创建的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "89cb2a5c5044"
   },
   "outputs": [],
   "source": [
    "# Query to create a test set from the source table\n",
    "query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE\n",
    "  `{PROJECT_ID}.{BQ_DATASET_ID}.{BQ_INPUT_TABLE}` AS\n",
    "\n",
    "SELECT\n",
    "  * EXCEPT (pseudo_random, income_bracket)\n",
    "FROM (\n",
    "  SELECT\n",
    "    *,\n",
    "    RAND() AS pseudo_random \n",
    "  FROM\n",
    "    `bigquery-public-data.ml_datasets.census_adult_income` )\n",
    "WHERE pseudo_random > {TRAIN_SPLIT}\n",
    "\"\"\"\n",
    "# Run the query\n",
    "_ = bq_client.query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45595b3d8037"
   },
   "source": [
    "## 为您的训练应用创建一个 Python 包\n",
    "\n",
    "在执行批量预测任务之前，您需要对收入普查数据集进行随机森林分类模型的训练。您可以通过在 Vertex AI 中使用预构建容器来进行训练。为此，请按照以下步骤打包训练应用程序。\n",
    "\n",
    "了解有关[为预构建容器创建 Python 训练应用程序](https://cloud.google.com/vertex-ai/docs/training/create-python-pre-built-container)的更多信息。\n",
    "\n",
    "### 准备源目录\n",
    "\n",
    "创建一个名为 `python_package` 的源目录，并在其中创建一个名为 `trainer` 的子文件夹。接下来，在 `trainer` 文件夹中创建一个 `__init__.py` 文件，以将其变成一个包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3190d7d2afdf"
   },
   "outputs": [],
   "source": [
    "!mkdir -p python_package\n",
    "!mkdir -p python_package/trainer\n",
    "!touch python_package/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88fa9c49bebd"
   },
   "source": [
    "### 创建训练任务\n",
    "在 `trainer/` 目录下创建一个名为 `task.py` 的模块，作为您的训练代码的入口点。\n",
    "\n",
    "下面的训练器代码对训练集进行预处理，并将预处理的转换保存在 scikit-learn 流水线中。此外，对预处理后的训练数据进行 [随机森林模型训练](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)，并将其作为估计器添加到流水线中。在保存模型之后，将其上传到云存储桶以供部署使用。\n",
    "\n",
    "使用 scikit-learn 流水线的优点是，它可以帮助您避免编写额外的预处理数据和生成预测的脚本。\n",
    "\n",
    "了解更多关于 [scikit-learn 流水线](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6b2c76999db9"
   },
   "outputs": [],
   "source": [
    "%%writefile python_package/trainer/task.py\n",
    "import os\n",
    "import joblib\n",
    "import argparse\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Read environmental variables\n",
    "PROJECT = os.getenv(\"CLOUD_ML_PROJECT_ID\")\n",
    "TRAINING_DATA_URI = os.getenv(\"AIP_TRAINING_DATA_URI\")\n",
    "\n",
    "# Set Bigquery Client\n",
    "bq_client = bigquery.Client(project=PROJECT)\n",
    "storage_client = storage.Client(project=PROJECT)\n",
    "\n",
    "# Define the constants\n",
    "TARGET = 'income_bracket'\n",
    "ARTIFACTS_PATH = os.getenv(\"AIP_MODEL_DIR\")\n",
    "# Get the bucket name from the model dir\n",
    "BUCKET_NAME = ARTIFACTS_PATH.replace(\"gs://\",\"\").split(\"/\")[0]\n",
    "\n",
    "MODEL_FILENAME = 'model.joblib' \n",
    "# Define the format of your input data, excluding the target column.\n",
    "# These are the columns from the census data files.\n",
    "COLUMNS = [\n",
    "    'age',\n",
    "    'workclass',\n",
    "    'functional_weight',\n",
    "    'education',\n",
    "    'education_num',\n",
    "    'marital_status',\n",
    "    'occupation',\n",
    "    'relationship',\n",
    "    'race',\n",
    "    'sex',\n",
    "    'capital_gain',\n",
    "    'capital_loss',\n",
    "    'hours_per_week',\n",
    "    'native_country'\n",
    "]\n",
    "# Categorical columns are columns that need to be turned into a numerical value to be used by scikit-learn\n",
    "CATEGORICAL_COLUMNS = [\n",
    "    'workclass',\n",
    "    'education',\n",
    "    'marital_status',\n",
    "    'occupation',\n",
    "    'relationship',\n",
    "    'race',\n",
    "    'sex',\n",
    "    'native_country'\n",
    "]\n",
    "\n",
    "# Function to fetch the data from BigQuery\n",
    "def download_table(bq_table_uri: str):\n",
    "    prefix = \"bq://\"\n",
    "    if bq_table_uri.startswith(prefix):\n",
    "        bq_table_uri = bq_table_uri[len(prefix):]\n",
    "\n",
    "    table = bigquery.TableReference.from_string(bq_table_uri)\n",
    "    rows = bq_client.list_rows(\n",
    "        table,\n",
    "    )\n",
    "    return rows.to_dataframe(create_bqstorage_client=False)\n",
    "\n",
    "# Function to upload local files to GCS\n",
    "def upload_model(bucket_name: str,\n",
    "                filename: str):\n",
    "     # Upload the saved model file to GCS\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    storage_path = os.path.join(ARTIFACTS_PATH, filename)\n",
    "    blob = storage.blob.Blob.from_string(storage_path, client=storage_client)\n",
    "    blob.upload_from_filename(filename)\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Load the training data\n",
    "    X_train = download_table(TRAINING_DATA_URI)\n",
    "\n",
    "    # Remove the column we are trying to predict ('income-level') from our features list\n",
    "    # Convert the Dataframe to a lists of lists\n",
    "    train_features = X_train.drop(TARGET, axis=1).to_numpy().tolist()\n",
    "    # Create our training labels list, convert the Dataframe to a lists of lists\n",
    "    train_labels = X_train[TARGET].to_numpy().tolist()\n",
    "\n",
    "    # Since the census data set has categorical features, we need to convert\n",
    "    # them to numerical values. We use a list of pipelines to convert each\n",
    "    # categorical column and then use FeatureUnion to combine them before calling\n",
    "    # the RandomForestClassifier.\n",
    "    categorical_pipelines = []\n",
    "\n",
    "    # Each categorical column needs to be extracted individually and converted to a numerical value.\n",
    "    # To do this, each categorical column use a pipeline that extracts one feature column via\n",
    "    # SelectKBest(k=1) and a LabelBinarizer() to convert the categorical value to a numerical one.\n",
    "    # A scores array (created below) selects and extracts the feature column. The scores array is\n",
    "    # created by iterating over the COLUMNS and checking if it is a CATEGORICAL_COLUMN.\n",
    "    for i, col in enumerate(COLUMNS):\n",
    "        if col in CATEGORICAL_COLUMNS:\n",
    "            # Create a scores array to get the individual categorical column.\n",
    "            # Example:\n",
    "            #  data = [39, 'State-gov', 77516, 'Bachelors', 13, 'Never-married', 'Adm-clerical',\n",
    "            #         'Not-in-family', 'White', 'Male', 2174, 0, 40, 'United-States']\n",
    "            #  scores = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "            #\n",
    "            # Returns: [['Sate-gov']]\n",
    "            scores = []\n",
    "            # Build the scores array\n",
    "            for j in range(len(COLUMNS)):\n",
    "                if i == j: # This column is the categorical column we want to extract.\n",
    "                    scores.append(1) # Set to 1 to select this column\n",
    "                else: # Every other column should be ignored.\n",
    "                    scores.append(0)\n",
    "            skb = SelectKBest(k=1)\n",
    "            skb.scores_ = scores\n",
    "            # Convert the categorical column to a numerical value\n",
    "            lbn = LabelBinarizer()\n",
    "            r = skb.transform(train_features)\n",
    "            lbn.fit(r)\n",
    "            # Create the pipeline to extract the categorical feature\n",
    "            categorical_pipelines.append(\n",
    "                ('categorical-{}'.format(i), Pipeline([\n",
    "                    ('SKB-{}'.format(i), skb),\n",
    "                    ('LBN-{}'.format(i), lbn)])))\n",
    "\n",
    "    # Create pipeline to extract the numerical features\n",
    "    skb = SelectKBest(k=6)\n",
    "    # From COLUMNS use the features that are numerical\n",
    "    skb.scores_ = [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0]\n",
    "    categorical_pipelines.append(('numerical', skb))\n",
    "\n",
    "    # Combine all the features using FeatureUnion\n",
    "    preprocess = FeatureUnion(categorical_pipelines)\n",
    "\n",
    "    # Create the classifier\n",
    "    classifier = RandomForestClassifier()\n",
    "\n",
    "    # Transform the features and fit them to the classifier\n",
    "    classifier.fit(preprocess.transform(train_features), train_labels)\n",
    "\n",
    "    # Create the overall model as a single pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('union', preprocess),\n",
    "        ('classifier', classifier)\n",
    "    ])\n",
    "\n",
    "    # Save the pipeline locally\n",
    "    joblib.dump(pipeline, MODEL_FILENAME)\n",
    "    \n",
    "    # Upload the locally saved model to GCS\n",
    "    upload_model(bucket_name = BUCKET_NAME, \n",
    "                 filename=MODEL_FILENAME\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eaaf2a432b4e"
   },
   "source": [
    "创建一个`setup.py`文件，告诉Setuptools如何创建源代码分发。您还可以在`setup.py`文件中指定应用程序的标准依赖项。Vertex AI使用pip在为您的作业分配的副本上安装您的训练应用程序。\n",
    "\n",
    "了解更多关于[Setuptools](https://setuptools.readthedocs.io/en/latest/)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ee443878b01"
   },
   "outputs": [],
   "source": [
    "%%writefile python_package/setup.py\n",
    "\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "REQUIRED_PACKAGES = ['pandas','pyarrow']\n",
    "\n",
    "setup(\n",
    "    name='trainer',\n",
    "    version='0.1',\n",
    "    packages=find_packages(),\n",
    "    include_package_data=True,\n",
    "    description='My training application.'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bf70c1e585ec"
   },
   "source": [
    "创建源代码分发\n",
    "\n",
    "运行以下命令创建一个源代码分发，`dist/trainer-0.1.tar.gz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a3601e4802a8"
   },
   "outputs": [],
   "source": [
    "!cd python_package && python3 setup.py sdist --formats=gztar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bdff577e26dd"
   },
   "source": [
    "### 将源分发复制到云存储\n",
    "\n",
    "要使用预构建的容器训练自定义分类模型，请将您的训练应用程序的源分发复制到云存储路径。在训练过程中，您可以通过`python_package_gcs_uri`参数让Vertex AI SDK定位包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dbf36204476c"
   },
   "outputs": [],
   "source": [
    "!gsutil cp -r python_package/dist/* $BUCKET_URI/training_package/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e9c16e6aef37"
   },
   "source": [
    "## 创建并运行管道\n",
    "\n",
    "您的管道已经做好了所有准备工作。在当前步骤中，您将创建一个 Vertex AI 管道，其中包括以下组件，每个组件都有自己的目的：\n",
    "\n",
    "- `TabularDatasetCreateOp`: 在 Vertex AI 中创建一个新的托管表格数据集。\n",
    "- `CustomPythonPackageTrainingJobRunOp`: 使用 Python 包在 Vertex AI 中创建和运行一个自定义训练作业。\n",
    "- `ModelBatchPredictOp`: 在 Vertex AI 中创建一个批量预测作业，并等待其完成。\n",
    "\n",
    "所有上述组件都是从`google-cloud-pipeline-components` Python 库导入的。了解更多关于[Google Cloud Pipeline Components](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-1.0.26/index.html)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e2861ae02f12"
   },
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "\n",
    "\n",
    "@pipeline(name=\"custom-model-bq-batch-prediction-pipeline\")\n",
    "def custom_model_bq_batch_prediction_pipeline(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    dataset_display_name: str,\n",
    "    dataset_bq_source: str,\n",
    "    training_job_dispaly_name: str,\n",
    "    gcs_staging_directory: str,\n",
    "    python_package_gcs_uri: str,\n",
    "    python_package_module_name: str,\n",
    "    training_split: float,\n",
    "    test_split: float,\n",
    "    training_container_uri: str,\n",
    "    serving_container_uri: str,\n",
    "    training_bigquery_destination: str,\n",
    "    model_display_name: str,\n",
    "    batch_prediction_display_name: str,\n",
    "    batch_prediction_instances_format: str,\n",
    "    batch_prediction_predictions_format: str,\n",
    "    batch_prediction_source_uri: str,\n",
    "    batch_prediction_destination_uri: str,\n",
    "    batch_prediction_machine_type: str = \"n1-standard-4\",\n",
    "    batch_prediction_batch_size: int = 1000,\n",
    "):\n",
    "    from google_cloud_pipeline_components.aiplatform import (\n",
    "        CustomPythonPackageTrainingJobRunOp, ModelBatchPredictOp,\n",
    "        TabularDatasetCreateOp)\n",
    "\n",
    "    # Create the dataset\n",
    "    dataset_create_op = TabularDatasetCreateOp(\n",
    "        project=project,\n",
    "        location=location,\n",
    "        display_name=dataset_display_name,\n",
    "        bq_source=dataset_bq_source,\n",
    "    )\n",
    "\n",
    "    # Run the training task\n",
    "    train_op = CustomPythonPackageTrainingJobRunOp(\n",
    "        display_name=training_job_dispaly_name,\n",
    "        python_package_gcs_uri=python_package_gcs_uri,\n",
    "        python_module_name=python_package_module_name,\n",
    "        container_uri=training_container_uri,\n",
    "        model_display_name=model_display_name,\n",
    "        model_serving_container_image_uri=serving_container_uri,\n",
    "        dataset=dataset_create_op.outputs[\"dataset\"],\n",
    "        base_output_dir=gcs_staging_directory,\n",
    "        bigquery_destination=training_bigquery_destination,\n",
    "        training_fraction_split=training_split,\n",
    "        test_fraction_split=test_split,\n",
    "        staging_bucket=gcs_staging_directory,\n",
    "    )\n",
    "\n",
    "    # Run the batch prediction task\n",
    "    _ = ModelBatchPredictOp(\n",
    "        project=project,\n",
    "        location=location,\n",
    "        model=train_op.outputs[\"model\"],\n",
    "        instances_format=batch_prediction_instances_format,\n",
    "        bigquery_source_input_uri=batch_prediction_source_uri,\n",
    "        predictions_format=batch_prediction_predictions_format,\n",
    "        bigquery_destination_output_uri=batch_prediction_destination_uri,\n",
    "        job_display_name=batch_prediction_display_name,\n",
    "        machine_type=batch_prediction_machine_type,\n",
    "        manual_batch_tuning_parameters_batch_size=batch_prediction_batch_size,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0150d697ac31"
   },
   "source": [
    "### 编译管道\n",
    "\n",
    "在定义完你的管道之后，将其编译成 JSON 或 YAML 格式的文件（`PIPELINE_FILE_NAME`）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2e755cae3948"
   },
   "outputs": [],
   "source": [
    "# Compile the pipeline\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=custom_model_bq_batch_prediction_pipeline,\n",
    "    package_path=PIPELINE_FILE_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96edd12acf43"
   },
   "source": [
    "### 设置参数\n",
    "\n",
    "现在，定义参数来运行您的管道。\n",
    "\n",
    "要将所需参数传递给管道中的各个组件，您需要定义以下参数：\n",
    "- `project`：Google Cloud 项目的项目 ID，其中需要运行管道。\n",
    "- `location`：管道需要运行的区域。\n",
    "- `dataset_display_name`：Vertex AI 中托管数据集资源的显示名称。\n",
    "- `dataset_bq_source`：作为 Vertex AI 中托管数据集来源的 BigQuery 表 URI。\n",
    "- `training_job_dispaly_name`：自定义 Python 包训练作业的显示名称。\n",
    "- `gcs_staging_directory`：用于存储训练工件的 Vertex AI 暂存目录。\n",
    "- `python_package_gcs_uri`：用于训练的 Python 包的 Cloud Storage 路径。\n",
    "- `python_package_module_name`：Python 包内用于训练的模块名称（训练任务）。\n",
    "- `training_split`：要用于训练的总数据的百分比。\n",
    "- `test_split`：要用于测试的总数据的百分比。提供给 **CustomPythonPackageTrainingJobRunOp** 组件的拆分百分比参数应该始终总和为1。\n",
    "- `training_container_uri`：用于训练模型的预构建容器映像 URI。\n",
    "- `serving_container_uri`：用于在 Vertex AI 上为模型提供服务的预构建容器映像 URI。\n",
    "- `training_bigquery_destination`：在训练期间要将训练数据写入的 BigQuery 项目位置。\n",
    "- `model_display_name`：要部署在 Vertex AI 模型注册表中的模型的显示名称。\n",
    "- `batch_prediction_display_name`：批量预测作业的显示名称。\n",
    "- `batch_prediction_instances_format`：用于批量预测的输入实例格式。\n",
    "- `batch_prediction_predictions_format`：来自批量预测结果的格式。\n",
    "- `batch_prediction_source_uri`：输入数据的来源 URI。\n",
    "- `batch_prediction_destination_uri`：批量预测结果需要存储的目标 URI。\n",
    "\n",
    "**注意：**虽然提供了测试拆分百分比，但测试数据在训练过程中不会被使用。这些测试数据与之前为批量预测创建的测试数据不同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "33462dc29ac6"
   },
   "outputs": [],
   "source": [
    "# Define the parameters for running the pipeline\n",
    "parameters = {\n",
    "    \"project\": PROJECT_ID,\n",
    "    \"location\": REGION,\n",
    "    \"dataset_display_name\": DATASET_DISPLAY_NAME,\n",
    "    \"dataset_bq_source\": DATA_SOURCE,\n",
    "    \"training_job_dispaly_name\": TRAINING_JOB_DISPLAY_NAME,\n",
    "    \"gcs_staging_directory\": BUCKET_URI,\n",
    "    \"python_package_gcs_uri\": f\"{BUCKET_URI}/training_package/trainer-0.1.tar.gz\",\n",
    "    \"python_package_module_name\": \"trainer.task\",\n",
    "    \"training_split\": TRAIN_SPLIT,\n",
    "    \"test_split\": 1 - TRAIN_SPLIT,\n",
    "    \"training_container_uri\": TRAINING_CONTAINER,\n",
    "    \"serving_container_uri\": SERVING_CONTAINER,\n",
    "    \"training_bigquery_destination\": f\"bq://{PROJECT_ID}\",\n",
    "    \"model_display_name\": MODEL_DISPLAY_NAME,\n",
    "    \"batch_prediction_display_name\": BATCH_PREDICTION_JOB_NAME,\n",
    "    \"batch_prediction_instances_format\": \"bigquery\",\n",
    "    \"batch_prediction_predictions_format\": \"bigquery\",\n",
    "    \"batch_prediction_source_uri\": f\"bq://{PROJECT_ID}.{BQ_DATASET_ID}.{BQ_INPUT_TABLE}\",\n",
    "    \"batch_prediction_destination_uri\": f\"bq://{PROJECT_ID}.{BQ_DATASET_ID}\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "15a1270831ca"
   },
   "source": [
    "### 运行管道\n",
    "\n",
    "创建一个Vertex AI Pipeline作业，并使用`PipelineJob`类来运行它。\n",
    "\n",
    "`PipelineJob`类需要以下参数：\n",
    "\n",
    "- `display_name`：Vertex AI Pipeline的显示名称。\n",
    "- `template_path`：PipelineJob或PipelineSpec JSON（或YAML）文件的路径。\n",
    "- `parameter_values`：控制管道运行的运行时参数名称与其值的映射。\n",
    "- `enable_caching`：是否开启运行缓存。\n",
    "\n",
    "从[Vertex AI PipelineJob文档](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.PipelineJob)中了解更多关于`PipelineJob`类的信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7efd80523c99"
   },
   "outputs": [],
   "source": [
    "# Create a Vertex AI Pipeline job\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=PIPELINE_DISPLAY_NAME,\n",
    "    template_path=PIPELINE_FILE_NAME,\n",
    "    parameter_values=parameters,\n",
    "    enable_caching=True,\n",
    ")\n",
    "# Run the pipeline job\n",
    "job.run(service_account=SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfeb56af21ae"
   },
   "source": [
    "从预测表中获取结果\n",
    "\n",
    "在Vertex AI管道作业成功完成后，从批量预测中获取结果并存入 dataframe。\n",
    "\n",
    "### 获取表名\n",
    "\n",
    "运行下面的单元格以从管道作业的工件详情中获取预测表的名称。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b6e00bc95662"
   },
   "outputs": [],
   "source": [
    "OUTPUT_TABLE = None\n",
    "# Load the batch prediction job details using the display name\n",
    "[batch_prediction_job] = aiplatform.BatchPredictionJob.list(\n",
    "    filter=f'display_name=\"{BATCH_PREDICTION_JOB_NAME}\"'\n",
    ")\n",
    "# Fetch the name of the output table\n",
    "OUTPUT_TABLE = batch_prediction_job.output_info.bigquery_output_table\n",
    "print(\"Predictions table ID:\", OUTPUT_TABLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bab610ea646d"
   },
   "source": [
    "### 查询结果表\n",
    "\n",
    "使用以下单元格从预测表中检索指定数量的行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "99738dd9e4aa"
   },
   "outputs": [],
   "source": [
    "# Specify the needed no.of rows\n",
    "ROWS = 10\n",
    "# Define the query\n",
    "query = f\"\"\"\n",
    "    Select prediction from `{PROJECT_ID}.{BQ_DATASET_ID}.{OUTPUT_TABLE}` limit {ROWS}\n",
    "\"\"\"\n",
    "# Fetch the data into a dataframe\n",
    "df = bq_client.query(query).to_dataframe()\n",
    "# Display the dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3a0e1c095133"
   },
   "source": [
    "获取用于删除的资源\n",
    "\n",
    "使用各个资源的显示名称，加载在清理步骤中创建的资源。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "171452c4157b"
   },
   "outputs": [],
   "source": [
    "# Load the Vertex AI tabular dataset using the display name\n",
    "[dataset] = aiplatform.TabularDataset.list(\n",
    "    filter=f'display_name=\"{DATASET_DISPLAY_NAME}\"'\n",
    ")\n",
    "\n",
    "# Load the Vertex AI model resource using the display name\n",
    "[model] = aiplatform.Model.list(filter=f'display_name=\"{MODEL_DISPLAY_NAME}\"')\n",
    "\n",
    "# Load the custom training job using the display name\n",
    "[training_job] = aiplatform.CustomPythonPackageTrainingJob.list(\n",
    "    filter=f'display_name=\"{TRAINING_JOB_DISPLAY_NAME}\"'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adac675f7ac3"
   },
   "source": [
    "清理\n",
    "\n",
    "要清理此项目中使用的所有谷歌云资源，您可以[删除用于本教程的谷歌云项目](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects)。\n",
    "\n",
    "否则，您可以删除本教程中创建的各个资源：\n",
    "\n",
    "- Vertex AI Pipeline Job\n",
    "- Vertex AI TabularDataset\n",
    "- Vertex AI Model\n",
    "- Vertex AI Training job\n",
    "- Vertex AI Batch Prediction job\n",
    "- BigQuery dataset\n",
    "- Cloud Storage bucket（将`delete_bucket`设置为**True**以删除Cloud Storage桶）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "53c239fc8667"
   },
   "outputs": [],
   "source": [
    "delete_bucket = False\n",
    "\n",
    "# Delete the Vertex AI Pipeline job\n",
    "job.delete()\n",
    "\n",
    "# Delete the Vertex AI TabularDataset\n",
    "dataset.delete()\n",
    "\n",
    "# Delete the Vertex AI Model\n",
    "model.delete()\n",
    "\n",
    "# Delete the Vertex AI Training job\n",
    "training_job.delete()\n",
    "\n",
    "# Delete the Vertex AI Batch prediction job\n",
    "batch_prediction_job.delete()\n",
    "\n",
    "# Delete the BigQuery dataset\n",
    "! bq rm -r -f -d $PROJECT_ID:$BQ_DATASET_ID\n",
    "\n",
    "# Delete Cloud Storage objects\n",
    "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
    "    ! gsutil -m rm -r $BUCKET_URI"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "custom_tabular_train_batch_pred_bq_pipeline.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
