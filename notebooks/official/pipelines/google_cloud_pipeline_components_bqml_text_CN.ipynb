{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "使用Swivel、BigQuery ML和Vertex AI Pipelines训练一个收购预测模型\n",
    "\n",
    "<div align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/pipelines/google_cloud_pipeline_components_bqml_text.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> 在Colab中运行\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/pipelines/google_cloud_pipeline_components_bqml_text.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      在GitHub上查看\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/pipelines/google_cloud_pipeline_components_bqml_text.ipynb\" target='_blank'>\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      在Vertex AI Workbench中打开\n",
    "    </a>\n",
    "  </td>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "991ab00f3d75"
   },
   "source": [
    "## 概述\n",
    "\n",
    "本笔记本演示了通过构建一个文本分类模型并在Vertex AI管道上运行它，使用`DataflowPythonJobOp`和BigQuery ML组件的用法。\n",
    "\n",
    "执行的步骤包括：\n",
    "\n",
    "1. 读取存储在Google Cloud Storage中的原始文本（HTML）文档。\n",
    "2. 使用Dataflow提取（HTML）文档的标题、内容和主题，并将其纳入BigQuery中。\n",
    "3. 应用Swivel模型生成文档内容的嵌入。\n",
    "4. 训练一个逻辑回归模型，用于分类文章是否涉及企业收购（`acq`类别）。\n",
    "5. 评估模型。\n",
    "6. 将模型应用于数据集以生成预测。\n",
    "\n",
    "了解更多关于[Vertex AI管道](https://cloud.google.com/vertex-ai/docs/pipelines/introduction) 和[BigQuery ML组件](https://cloud.google.com/vertex-ai/docs/pipelines/bigqueryml-component)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "acc98a3361cc"
   },
   "source": [
    "### 目标\n",
    "\n",
    "在这份笔记本中，您将学习如何使用Vertex AI pipelines构建一个简单的BigQuery ML管道，以计算文章内容的文本嵌入并将其分类为“公司收购”类别。\n",
    "\n",
    "本教程使用以下Google Cloud ML服务和资源：\n",
    "\n",
    "- Vertex AI Pipelines\n",
    "- BigQuery ML\n",
    "\n",
    "执行的步骤包括：\n",
    "\n",
    "- 创建一个用于将数据导入BigQuery的Dataflow作业组件。\n",
    "- 创建一个用于在BigQuery上运行数据预处理步骤的组件。\n",
    "- 创建一个用于使用BigQuery ML训练逻辑回归模型的组件。\n",
    "- 使用所有创建的组件构建和配置Kubeflow DSL管道。\n",
    "- 在Vertex AI Pipelines中编译和运行管道。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9bc6d52899ba"
   },
   "source": [
    "数据集\n",
    "\n",
    "本笔记本中使用的数据集是[路透社21578文本分类集数据集](https://archive.ics.uci.edu/ml/datasets/reuters-21578+text+categorization+collection)。这个数据集是一组公开可用的新闻文章，出现在1987年的路透新闻社新闻线上。它们是由1987年来自路透社有限公司和卡内基集团的工作人员组装和建立并加以分类索引的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "### 费用\n",
    "\n",
    "此教程使用 Google Cloud 的计费组件:\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "* BigQuery\n",
    "* Dataflow\n",
    "\n",
    "了解[Vertex AI\n",
    "价格](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage\n",
    "价格](https://cloud.google.com/storage/pricing), [BigQuery\n",
    "价格](https://cloud.google.com/bigquery/pricing), [Dataflow\n",
    "价格](https://cloud.google.com/dataflow/pricing) 并使用 [定价\n",
    "计算器](https://cloud.google.com/products/calculator/)\n",
    "根据您的预期使用量生成费用估算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_aip:mbsdk"
   },
   "source": [
    "安装\n",
    "\n",
    "安装执行此笔记所需的软件包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2b4ef9b72d43"
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "! pip3 install --upgrade --quiet    google-cloud-aiplatform \\\n",
    "                                    google_cloud_pipeline_components \\\n",
    "                                    google-api-core \\\n",
    "                                    google-auth\n",
    "\n",
    "! pip3 install --upgrade --quiet    tensorflow==2.8.0 \\\n",
    "                                    tensorflow-hub==0.12.0 \\\n",
    "                                    kfp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58707a750154"
   },
   "source": [
    "### 仅限Colab使用：请取消注释以下单元格以重新启动内核。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f200f10a1da3"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs so that your environment can access the new packages\n",
    "# import IPython\n",
    "\n",
    "# app = IPython.Application.instance()\n",
    "# app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF1j6f9HApxa"
   },
   "source": [
    "##开始之前\n",
    "\n",
    "###设置您的Google Cloud项目\n",
    "\n",
    "**无论您使用的是哪种笔记本环境，下面这些步骤都是必需的。**\n",
    "\n",
    "1. [选择或创建一个Google Cloud项目](https://console.cloud.google.com/cloud-resource-manager)。 当您第一次创建账户时，您可以获得300美元的免费信用额度，用于支付计算/存储成本。\n",
    "\n",
    "2. [确保为您的项目启用了计费](https://cloud.google.com/billing/docs/how-to/modify-project)。\n",
    "\n",
    "3. [启用Vertex AI API]。\n",
    "\n",
    "4. 如果您是在本地运行这个笔记本，您需要安装[Cloud SDK](https://cloud.google.com/sdk)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WReHDGG5g0XY"
   },
   "source": [
    "设置您的项目 ID\n",
    "\n",
    "**如果您不知道您的项目 ID**，请尝试以下操作：\n",
    "* 运行 `gcloud config list`。\n",
    "* 运行 `gcloud projects list`。\n",
    "* 查看支持页面：[查找项目 ID](https://support.google.com/googleapi/answer/7014113)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oM1iC_MfAts1"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
    "\n",
    "# Set the project id\n",
    "! gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "#### 地区\n",
    "\n",
    "您还可以更改 Vertex AI 使用的“REGION”变量。了解更多关于[Vertex AI地区](https://cloud.google.com/vertex-ai/docs/general/locations)的信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "region"
   },
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06571eb4063b"
   },
   "source": [
    "UUID\n",
    "\n",
    "如果您正在进行实时教程会话，您可能正在使用共享测试帐户或项目。为了避免用户之间在创建的资源上发生名称冲突，您为每个实例会话创建一个UUID，并将其附加到您在此教程中创建的资源名称上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "697568e92bd6"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "\n",
    "# Generate a uuid of length 8\n",
    "def generate_uuid():\n",
    "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=8))\n",
    "\n",
    "\n",
    "UUID = generate_uuid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBCra4QMA2wR"
   },
   "source": [
    "### 验证您的 Google Cloud 账户\n",
    "\n",
    "根据您的 Jupyter 环境，您可能需要手动验证。请按以下相关说明进行操作。\n",
    "\n",
    "**1. Vertex AI Workbench**\n",
    "* 无需操作，因为您已经验证过。\n",
    "\n",
    "**2. 在本地 JupyterLab 实例中取消注释并运行：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "254614fa0c46"
   },
   "outputs": [],
   "source": [
    "# ! gcloud auth login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ef21552ccea8"
   },
   "source": [
    "3. Colab，取消注释并运行："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "603adbbf0532"
   },
   "outputs": [],
   "source": [
    "# from google.colab import auth\n",
    "# auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6b2ccc891ed"
   },
   "source": [
    "4. 服务帐户或其他\n",
    "* 查看如何在https://cloud.google.com/storage/docs/gsutil/commands/iam#ch-examples 上为您的服务帐户授予云存储权限。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "创建一个云存储桶\n",
    "\n",
    "创建一个存储桶，用来存储诸如数据集之类的中间产物。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "BUCKET_URI = f\"gs://your-bucket-name-{PROJECT_ID}-unique\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EcIXiGsCePi"
   },
   "source": [
    "只要您的存储桶尚不存在：运行以下单元格以创建您的云存储存储桶。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIq7R4HZCfIc"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l {REGION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3708bd0b1855"
   },
   "source": [
    "# 服务帐号\n",
    "\n",
    "您使用服务帐号来创建Vertex AI管道作业。如果您不想使用您项目的计算引擎服务帐号，请将`SERVICE_ACCOUNT`设置为另一个服务帐号ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "199a32a35466"
   },
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT = \"[your-service-account]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "abb872bb98c1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "if (\n",
    "    SERVICE_ACCOUNT == \"\"\n",
    "    or SERVICE_ACCOUNT is None\n",
    "    or SERVICE_ACCOUNT == \"[your-service-account]\"\n",
    "):\n",
    "    # Get your service account from gcloud\n",
    "    if not IS_COLAB:\n",
    "        shell_output = !gcloud auth list 2>/dev/null\n",
    "        SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
    "\n",
    "    else:  # IS_COLAB:\n",
    "        shell_output = ! gcloud projects describe  $PROJECT_ID\n",
    "        project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
    "        SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
    "\n",
    "    print(\"Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4a63f2d1cd52"
   },
   "source": [
    "#### 为 Vertex AI Pipelines 设置服务账号访问权限\n",
    "\n",
    "运行以下命令，为您的服务账号授予对在上一步中创建的存储桶中读取和写入管道工件的权限。您只需要针对每个服务账号运行一次此步骤。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "749c598c5f5d"
   },
   "outputs": [],
   "source": [
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
    "\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FutXJbmAa1NC"
   },
   "source": [
    "### 设置项目模板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Kms5lCRn7N-"
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"data\"\n",
    "KFP_COMPONENTS_PATH = \"components\"\n",
    "SRC = \"src\"\n",
    "BUILD = \"build\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hZRbBCLra0po"
   },
   "outputs": [],
   "source": [
    "!mkdir -m 777 -p {DATA_PATH} {KFP_COMPONENTS_PATH} {SRC} {BUILD}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jpg5bPrNbpkl"
   },
   "source": [
    "### 准备输入数据\n",
    "\n",
    "在下面的单元格中，您将：\n",
    "\n",
    "1）从UCI存档中获取数据集。\n",
    "2）解压缩数据集。\n",
    "3）将数据集复制到云存储位置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "un5pHVJtcNsQ"
   },
   "outputs": [],
   "source": [
    "!wget --no-parent https://archive.ics.uci.edu/ml/machine-learning-databases/reuters21578-mld/reuters21578.tar.gz --directory-prefix={DATA_PATH}/raw\n",
    "!mkdir -m 777 -p {DATA_PATH}/raw/temp {DATA_PATH}/raw\n",
    "!tar -zxvf {DATA_PATH}/raw/reuters21578.tar.gz -C {DATA_PATH}/raw/temp/\n",
    "!mv {DATA_PATH}/raw/temp/*.sgm {DATA_PATH}/raw && rm -rf {DATA_PATH}/raw/temp && rm -f {DATA_PATH}/raw/reuters21578.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ERaAoEO8bsl-"
   },
   "outputs": [],
   "source": [
    "!gsutil -m cp -R {DATA_PATH}/raw $BUCKET_URI/{DATA_PATH}/raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRUOFELefqf1"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path as path\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from kfp import compiler, dsl\n",
    "from kfp.dsl import component\n",
    "\n",
    "os.environ[\"TFHUB_MODEL_LOAD_FORMAT\"] = \"UNCOMPRESSED\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vlQwSmhIam3l"
   },
   "source": [
    "### 定义常数\n",
    "\n",
    "关于您要在预处理中使用的模型，您使用了在英语Google新闻130GB语料库上训练的具有20个维度的 [Swivel](https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1) 嵌入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fAnAKF6KamY3"
   },
   "outputs": [],
   "source": [
    "JOB_NAME = f\"reuters-ingest-{UUID}\"\n",
    "SETUP_FILE_URI = urlparse(BUCKET_URI)._replace(path=\"setup.py\").geturl()\n",
    "RUNNER = \"DataflowRunner\"\n",
    "STAGING_LOCATION_URI = urlparse(BUCKET_URI)._replace(path=\"staging\").geturl()\n",
    "TMP_LOCATION_URI = urlparse(BUCKET_URI)._replace(path=\"temp\").geturl()\n",
    "INPUTS_URI = urlparse(BUCKET_URI)._replace(path=f\"{DATA_PATH}/raw/*.sgm\").geturl()\n",
    "BQ_DATASET = \"mlops_bqml_text_analyisis\"\n",
    "BQ_TABLE = \"reuters_ingested\"\n",
    "MODEL_NAME = \"swivel_text_embedding_model\"\n",
    "EMBEDDINGS_TABLE = f\"reuters_text_embeddings_{UUID}\"\n",
    "MODEL_PATH = (\n",
    "    f'{hub.resolve(\"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\")}/*'\n",
    ")\n",
    "PREPROCESSED_TABLE = f\"reuters_text_preprocessed_{UUID}\"\n",
    "CLASSIFICATION_MODEL_NAME = \"logistic_reg\"\n",
    "PREDICT_TABLE = f\"reuters_text_predict_{UUID}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NlyOjKrjCXsI"
   },
   "source": [
    "初始化 Vertex AI SDK 客户端"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IsT5FfP0CaXt"
   },
   "outputs": [],
   "source": [
    "vertex_ai.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZrgOD30o7HcL"
   },
   "source": [
    "## 管道形式化\n",
    "\n",
    "在这一步中，您可以为管道创建各种组件并构建最终的管道。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LLs96NR97YAr"
   },
   "source": [
    "### 数据摄入组件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nR3r6ZD47oYb"
   },
   "source": [
    "#### 创建 Dataflow Python 模块\n",
    "\n",
    "以下模块包含一个 Dataflow pipeline，其中\n",
    "\n",
    "1) 从 Cloud Storage 读取文件。\n",
    "2) 从文件中提取文章并生成标题、主题和内容。\n",
    "3) 将结构化数据加载到 BigQuery。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mO3ggLEtkD2R"
   },
   "outputs": [],
   "source": [
    "!touch {SRC}/__init__.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h5G6BFF874Ys"
   },
   "outputs": [],
   "source": [
    "%%writefile src/ingest_pipeline.py\n",
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "# General imports\n",
    "from __future__ import absolute_import\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import string\n",
    "\n",
    "# Preprocessing imports\n",
    "import tensorflow as tf\n",
    "import bs4\n",
    "import nltk\n",
    "\n",
    "import apache_beam as beam\n",
    "from apache_beam.io.gcp.internal.clients import bigquery\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import SetupOptions\n",
    "\n",
    "\n",
    "# Helpers ---------------------------------------------------------------------\n",
    "\n",
    "def get_args():\n",
    "    \"\"\"\n",
    "    Get command line arguments.\n",
    "    Returns:\n",
    "      args: The parsed arguments.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--inputs', dest='inputs', default='data/raw/reuters/*.sgm',\n",
    "                        help='A directory location of input data')\n",
    "    parser.add_argument('--bq-dataset', dest='bq_dataset', required=False,\n",
    "                        default='reuters_dataset', help='Dataset name used in BigQuery.')\n",
    "    parser.add_argument('--bq-table', dest='bq_table', required=False,\n",
    "                        default='reuters_ingested_table', help='Table name used in BigQuery.')\n",
    "    args, pipeline_args = parser.parse_known_args()\n",
    "    return args, pipeline_args\n",
    "\n",
    "def get_paths(data_pattern):\n",
    "    \"\"\"\n",
    "  A function to get all the paths of the files in the data directory.\n",
    "  Args:\n",
    "    data_pattern: A directory location of input data.\n",
    "  Returns:\n",
    "    A list of file paths.\n",
    "  \"\"\"\n",
    "    data_paths = tf.io.gfile.glob(data_pattern)\n",
    "    return data_paths\n",
    "\n",
    "\n",
    "def get_title(article):\n",
    "    \"\"\"\n",
    "    A function to get the title of an article.\n",
    "    Args:\n",
    "        article: A BeautifulSoup object of an article.\n",
    "    Returns:\n",
    "        A string of the title of the article.\n",
    "    \"\"\"\n",
    "    title = article.find('text').title\n",
    "    if title is not None:\n",
    "        title = ''.join(filter(lambda x: x in set(string.printable), title.text))\n",
    "        title = title.encode('ascii', 'ignore')\n",
    "    return title\n",
    "\n",
    "\n",
    "def get_content(article):\n",
    "    \"\"\"\n",
    "    A function to get the content of an article.\n",
    "    Args:\n",
    "        article: A BeautifulSoup object of an article.\n",
    "    Returns:\n",
    "        A string of the content of the article.\n",
    "    \"\"\"\n",
    "    content = article.find('text').body\n",
    "    if content is not None:\n",
    "        content = ''.join(filter(lambda x: x in set(string.printable), content.text))\n",
    "        content = ' '.join(content.split())\n",
    "        try:\n",
    "            content = '\\n'.join(nltk.sent_tokenize(content))\n",
    "        except LookupError:\n",
    "            nltk.download('punkt')\n",
    "            content = '\\n'.join(nltk.sent_tokenize(content))\n",
    "        content = content.encode('ascii', 'ignore')\n",
    "    return content\n",
    "\n",
    "\n",
    "def get_topics(article):\n",
    "    \"\"\"\n",
    "    A function to get the topics of an article.\n",
    "    Args:\n",
    "        article: A BeautifulSoup object of an article.\n",
    "    Returns:\n",
    "        A list of strings of the topics of the article.\n",
    "    \"\"\"\n",
    "    topics = []\n",
    "    for topic in article.topics.children:\n",
    "        topic = ''.join(filter(lambda x: x in set(string.printable), topic.text))\n",
    "        topics.append(topic.encode('ascii', 'ignore'))\n",
    "    return topics\n",
    "\n",
    "\n",
    "def get_articles(data_paths):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data_paths: A list of file paths.\n",
    "    Returns:\n",
    "        A list of articles.\n",
    "    \"\"\"\n",
    "    data = tf.io.gfile.GFile(data_paths, 'rb').read()\n",
    "    soup = bs4.BeautifulSoup(data, \"html.parser\")\n",
    "    articles = []\n",
    "    for raw_article in soup.find_all('reuters'):\n",
    "        article = {\n",
    "            'title': get_title(raw_article),\n",
    "            'content': get_content(raw_article),\n",
    "            'topics': get_topics(raw_article)\n",
    "        }\n",
    "        if None not in article.values():\n",
    "            if [] not in article.values():\n",
    "                articles.append(article)\n",
    "    return articles\n",
    "\n",
    "\n",
    "def get_bigquery_schema():\n",
    "    \"\"\"\n",
    "    A function to get the BigQuery schema.\n",
    "    Returns:\n",
    "        A list of BigQuery schema.\n",
    "    \"\"\"\n",
    "\n",
    "    table_schema = bigquery.TableSchema()\n",
    "    columns = (('topics', 'string', 'repeated'),\n",
    "               ('title', 'string', 'nullable'),\n",
    "               ('content', 'string', 'nullable'))\n",
    "\n",
    "    for column in columns:\n",
    "        column_schema = bigquery.TableFieldSchema()\n",
    "        column_schema.name = column[0]\n",
    "        column_schema.type = column[1]\n",
    "        column_schema.mode = column[2]\n",
    "        table_schema.fields.append(column_schema)\n",
    "\n",
    "    return table_schema\n",
    "\n",
    "\n",
    "# Pipeline runner\n",
    "def run(args, pipeline_args=None):\n",
    "    \"\"\"\n",
    "    A function to run the pipeline.\n",
    "    Args:\n",
    "        args: The parsed arguments.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    options = PipelineOptions(pipeline_args)\n",
    "    options.view_as(SetupOptions).save_main_session = True\n",
    "\n",
    "    pipeline = beam.Pipeline(options=options)\n",
    "    articles = (\n",
    "            pipeline\n",
    "            | 'Get Paths' >> beam.Create(get_paths(args.inputs))\n",
    "            | 'Get Articles' >> beam.Map(get_articles)\n",
    "            | 'Get Article' >> beam.FlatMap(lambda x: x)\n",
    "    )\n",
    "    if options.get_all_options()['runner'] == 'DirectRunner':\n",
    "        articles | 'Dry run' >> beam.io.WriteToText('data/processed/reuters', file_name_suffix=\".jsonl\")\n",
    "    else:\n",
    "        (articles\n",
    "         | 'Write to BigQuery' >> beam.io.WriteToBigQuery(\n",
    "                    project=options.get_all_options()['project'],\n",
    "                    dataset=args.bq_dataset,\n",
    "                    table=args.bq_table,\n",
    "                    schema=get_bigquery_schema(),\n",
    "                    create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "                    write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE)\n",
    "         )\n",
    "    job = pipeline.run()\n",
    "\n",
    "    if options.get_all_options()['runner'] == 'DirectRunner':\n",
    "        job.wait_until_finish()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args, pipeline_args = get_args()\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    run(args, pipeline_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nEsufwCX-h08"
   },
   "source": [
    "创建需求\n",
    "\n",
    "接下来，用于Apache Beam管道所需的Python模块创建requirements.txt文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S13Ma74p7bey"
   },
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "apache-beam[gcp]==2.36.0\n",
    "bs4==0.0.1\n",
    "nltk==3.7\n",
    "tensorflow==2.8.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3TlCcRKsqZut"
   },
   "source": [
    "创建安装文件，并添加所需的Python模块，以便执行Dataflow工作人员。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "USKNGS0qqZu1"
   },
   "outputs": [],
   "source": [
    "%%writefile setup.py\n",
    "# !/usr/bin/python\n",
    "\n",
    "# Copyright 2022 Google LLC\n",
    "\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import setuptools\n",
    "\n",
    "REQUIRED_PACKAGES = [\n",
    "    'bs4==0.0.1',\n",
    "    'nltk==3.7',\n",
    "    'tensorflow==2.8.0']\n",
    "\n",
    "setuptools.setup(\n",
    "    name='ingest',\n",
    "    version='0.0.1',\n",
    "    author='author',\n",
    "    author_email='author@google.com',\n",
    "    install_requires=REQUIRED_PACKAGES,\n",
    "    packages=setuptools.find_packages())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Nc7ByK1AEe8"
   },
   "source": [
    "#### 将设置文件、模块和要求文件复制到云存储\n",
    "\n",
    "最后，将Python模块、要求文件和设置文件复制到您的云存储存储桶中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V0qslZVeAHqo"
   },
   "outputs": [],
   "source": [
    "# !gsutil cp -R {SRC}/preprocess_pipeline.py {BUCKET_URI}/preprocess_pipeline.py\n",
    "!gsutil cp -R {SRC} {BUCKET_URI}/{SRC}\n",
    "!gsutil cp requirements.txt {BUCKET_URI}/requirements.txt\n",
    "!gsutil cp setup.py {BUCKET_URI}/setup.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MI_wYYwdAZZs"
   },
   "source": [
    "### BigQuery ML 组件\n",
    "\n",
    "在构建管道的下一步中，您可以定义一组查询来：\n",
    "\n",
    "1）创建BigQuery数据集架构。\n",
    "2）预处理您的文本数据并使用Swivel模型生成嵌入。\n",
    "2）训练BigQuery ML逻辑回归模型。\n",
    "3）评估模型。\n",
    "4）运行批处理预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ytsqqHZnksHY"
   },
   "outputs": [],
   "source": [
    "!mkdir -m 777 -p {KFP_COMPONENTS_PATH}/bq_dataset_component\n",
    "!mkdir -m 777 -p {KFP_COMPONENTS_PATH}/bq_preprocess_component\n",
    "!mkdir -m 777 -p {KFP_COMPONENTS_PATH}/bq_model_component\n",
    "!mkdir -m 777 -p {KFP_COMPONENTS_PATH}/bq_prediction_component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8tYtZMRiepKe"
   },
   "source": [
    "#### 创建BigQuery数据集查询\n",
    "\n",
    "使用此查询，您将创建要用来训练模型的BigQuery数据集模式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YP2K8Nc4HqrQ"
   },
   "outputs": [],
   "source": [
    "create_bq_dataset_query = f\"\"\"\n",
    "CREATE SCHEMA IF NOT EXISTS {BQ_DATASET}\n",
    "\"\"\"\n",
    "\n",
    "with open(\n",
    "    f\"{KFP_COMPONENTS_PATH}/bq_dataset_component/create_bq_dataset.sql\", \"w\"\n",
    ") as q:\n",
    "    q.write(create_bq_dataset_query)\n",
    "q.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2_kuUSZUBDY"
   },
   "source": [
    "创建BigQuery预处理查询\n",
    "\n",
    "以下查询使用TFHub Swivel模型为您的文本数据生成嵌入，并将数据集拆分为训练和服务目的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XdyboQnLUBDg"
   },
   "outputs": [],
   "source": [
    "create_bq_preprocess_query = f\"\"\"\n",
    "-- create the embedding model\n",
    "CREATE OR REPLACE MODEL\n",
    "  `{PROJECT_ID}.{BQ_DATASET}.{MODEL_NAME}` OPTIONS(model_type='tensorflow',\n",
    "    model_path='{MODEL_PATH}');\n",
    "\n",
    "-- create the preprocessed table\n",
    "CREATE OR REPLACE TABLE `{PROJECT_ID}.{BQ_DATASET}.{PREPROCESSED_TABLE}`\n",
    "AS (\n",
    "  WITH\n",
    "    -- Apply the model for embedding generation\n",
    "    get_embeddings AS (\n",
    "      SELECT\n",
    "        title,\n",
    "        sentences,\n",
    "        output_0 as content_embeddings,\n",
    "        topics\n",
    "      FROM ML.PREDICT(MODEL `{PROJECT_ID}.{BQ_DATASET}.{MODEL_NAME}`,(\n",
    "        SELECT topics, title, content AS sentences\n",
    "        FROM `{PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE}`\n",
    "      ))),\n",
    "    -- Get label\n",
    "    get_label AS (\n",
    "        SELECT\n",
    "            *,\n",
    "            STRUCT( CASE WHEN 'acq' in UNNEST(topics) THEN 1 ELSE 0 END as acq ) AS label,\n",
    "        FROM get_embeddings\n",
    "    ),\n",
    "    -- Train-serve splitting\n",
    "    get_split AS (\n",
    "        SELECT\n",
    "            *,\n",
    "            CASE WHEN ABS(MOD(FARM_FINGERPRINT(title), 10)) < 8 THEN 'TRAIN' ELSE 'PREDICT' END AS split\n",
    "        FROM get_label\n",
    "    )\n",
    "    -- create training table\n",
    "    SELECT\n",
    "        title,\n",
    "        sentences,\n",
    "        STRUCT( content_embeddings[OFFSET(0)] AS content_embed_0,\n",
    "                content_embeddings[OFFSET(1)] AS content_embed_1,\n",
    "                content_embeddings[OFFSET(2)] AS content_embed_2,\n",
    "                content_embeddings[OFFSET(3)] AS content_embed_3,\n",
    "                content_embeddings[OFFSET(4)] AS content_embed_4,\n",
    "                content_embeddings[OFFSET(5)] AS content_embed_5,\n",
    "                content_embeddings[OFFSET(6)] AS content_embed_6,\n",
    "                content_embeddings[OFFSET(7)] AS content_embed_7,\n",
    "                content_embeddings[OFFSET(8)] AS content_embed_8,\n",
    "                content_embeddings[OFFSET(9)] AS content_embed_9,\n",
    "                content_embeddings[OFFSET(10)] AS content_embed_10,\n",
    "                content_embeddings[OFFSET(11)] AS content_embed_11,\n",
    "                content_embeddings[OFFSET(12)] AS content_embed_12,\n",
    "                content_embeddings[OFFSET(13)] AS content_embed_13,\n",
    "                content_embeddings[OFFSET(14)] AS content_embed_14,\n",
    "                content_embeddings[OFFSET(15)] AS content_embed_15,\n",
    "                content_embeddings[OFFSET(16)] AS content_embed_16,\n",
    "                content_embeddings[OFFSET(17)] AS content_embed_17,\n",
    "                content_embeddings[OFFSET(18)] AS content_embed_18,\n",
    "                content_embeddings[OFFSET(19)] AS content_embed_19) AS feature,\n",
    "        label.acq as label,\n",
    "        split\n",
    "    FROM\n",
    "      get_split)\n",
    "\"\"\"\n",
    "\n",
    "with open(\n",
    "    f\"{KFP_COMPONENTS_PATH}/bq_preprocess_component/bq_preprocess_query.sql\", \"w\"\n",
    ") as q:\n",
    "    q.write(create_bq_preprocess_query)\n",
    "q.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yF9W5x4HgUQb"
   },
   "source": [
    "#### 创建 BigQuery 模型查询\n",
    "\n",
    "下面是一个简单的查询，用于构建用于主题文章分类的 BigQuery ML 逻辑回归分类器模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tjStSASggUQb"
   },
   "outputs": [],
   "source": [
    "create_bq_model_query = f\"\"\"\n",
    "CREATE OR REPLACE MODEL `{PROJECT_ID}.{BQ_DATASET}.{CLASSIFICATION_MODEL_NAME}`\n",
    "  OPTIONS (\n",
    "      model_type='logistic_reg',\n",
    "      input_label_cols=['label']) AS\n",
    "  SELECT\n",
    "      label,\n",
    "      feature.*\n",
    "  FROM\n",
    "     `{PROJECT_ID}.{BQ_DATASET}.{PREPROCESSED_TABLE}`\n",
    "  WHERE split = 'TRAIN';\n",
    "\"\"\"\n",
    "\n",
    "with open(f\"{KFP_COMPONENTS_PATH}/bq_model_component/create_bq_model.sql\", \"w\") as q:\n",
    "    q.write(create_bq_model_query)\n",
    "q.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RlLTcuUhdzFU"
   },
   "source": [
    "#### 创建 BigQuery 预测查询\n",
    "\n",
    "通过以下查询，您可以使用包含预处理查询的表来运行预测作业。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rMwgXCyfdzFc"
   },
   "outputs": [],
   "source": [
    "create_bq_prediction_query = f\"\"\"SELECT title, sentences, feature.* FROM `{PROJECT_ID}.{BQ_DATASET}.{PREPROCESSED_TABLE}` WHERE split = 'PREDICT' \"\"\"\n",
    "\n",
    "with open(\n",
    "    f\"{KFP_COMPONENTS_PATH}/bq_prediction_component/create_bq_prediction_query.sql\", \"w\"\n",
    ") as q:\n",
    "    q.write(create_bq_prediction_query)\n",
    "q.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OxemUVCxAiSo"
   },
   "source": [
    "### 构建管道\n",
    "\n",
    "在这一步中，您将使用各个组件来构建管道。\n",
    "\n",
    "请在下方定义`JOB_NAME`和`JOB_CONFIG`。 `JOB_CONFIG`包括目标表的以下参数：\n",
    "- `PROJECT_ID`：项目的ID。\n",
    "- `BQ_DATASET`：BigQuery数据集的ID。\n",
    "- `PREDICT_TABLE`：存储预测结果的BigQuery表的ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JzcBvqdpzjiz"
   },
   "outputs": [],
   "source": [
    "ID = random.randint(1, 10000)\n",
    "JOB_NAME = f\"reuters-preprocess-{UUID}-{ID}\"\n",
    "JOB_CONFIG = {\n",
    "    \"destinationTable\": {\n",
    "        \"projectId\": PROJECT_ID,\n",
    "        \"datasetId\": BQ_DATASET,\n",
    "        \"tableId\": PREDICT_TABLE,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mdO8st_gLKBZ"
   },
   "source": [
    "为参数创建一个自定义组件\n",
    "\n",
    "接下来，您需要创建一个组件来传递参数给`DataflowPythonJobOp`组件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V0d0TYFzLUWK"
   },
   "outputs": [],
   "source": [
    "@component(base_image=\"python:3.8-slim\")\n",
    "def build_dataflow_args(\n",
    "    # destination_table: Input[Artifact],\n",
    "    bq_dataset: str,\n",
    "    bq_table: str,\n",
    "    job_name: str,\n",
    "    setup_file_uri: str,\n",
    "    runner: str,\n",
    "    inputs_uri: str,\n",
    ") -> list:\n",
    "    return [\n",
    "        \"--job_name\",\n",
    "        job_name,\n",
    "        \"--setup_file\",\n",
    "        setup_file_uri,\n",
    "        \"--runner\",\n",
    "        runner,\n",
    "        \"--inputs\",\n",
    "        inputs_uri,\n",
    "        \"--bq-dataset\",\n",
    "        bq_dataset,\n",
    "        \"--bq-table\",\n",
    "        bq_table,\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pcSL1FHk69KT"
   },
   "source": [
    "创建流水线\n",
    "\n",
    "定义流水线的工作流程并构建流水线。传递给流水线的参数包括：\n",
    "\n",
    "- `create_bq_dataset_query`：用于在BigQuery中创建数据集的SQL查询。\n",
    "- `job_name`：在`PipelineOptions`中配置的Cloud Dataflow作业名称。\n",
    "- `inputs_uri`：输入数据的目录位置。\n",
    "- `bq_dataset`：在BigQuery中使用的数据集名称。\n",
    "- `bq_table`：在BigQuery中用于摄取的表名称。\n",
    "- `requirements_file_path`：pip requirements文件的GCS路径。\n",
    "- `python_file_path`：要运行的python文件的GCS路径。\n",
    "- `setup_file_uri`：包含包依赖关系的Python设置文件路径。\n",
    "- `temp_location`：Dataflow用于在执行流水线期间创建临时作业文件的GCS路径。\n",
    "- `runner`：用于执行工作流程的流水线运行器。\n",
    "- `create_bq_preprocess_query`：在BigQuery中预处理数据的SQL查询。\n",
    "- `create_bq_model_query`：用于创建BigQuery ML模型的SQL查询。\n",
    "- `create_bq_prediction_query`：用于预测的SQL查询。\n",
    "- `job_config`：描述作业配置的json格式化字符串。有关更多信息，请访问此[页面](https://cloud.google.com/bigquery/docs/reference/rest/v2/Job#JobConfigurationQuery)。\n",
    "- `project`：项目ID。\n",
    "- `region`：选择用于运行Dataflow作业的地区。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AlFXqsPIAk0l"
   },
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"mlops-bqml-text-generate-embeddings\",\n",
    "    description=\"A batch pipeline to generate embeddings\",\n",
    ")\n",
    "def pipeline(\n",
    "    create_bq_dataset_query: str,\n",
    "    job_name: str,\n",
    "    inputs_uri: str,\n",
    "    bq_dataset: str,\n",
    "    bq_table: str,\n",
    "    requirements_file_path: str,\n",
    "    python_file_path: str,\n",
    "    setup_file_uri: str,\n",
    "    temp_location: str,\n",
    "    runner: str,\n",
    "    create_bq_preprocess_query: str,\n",
    "    create_bq_model_query: str,\n",
    "    create_bq_prediction_query: str,\n",
    "    job_config: dict,\n",
    "    project: str = PROJECT_ID,\n",
    "    region: str = REGION,\n",
    "):\n",
    "\n",
    "    from google_cloud_pipeline_components.v1.bigquery import (\n",
    "        BigqueryCreateModelJobOp, BigqueryEvaluateModelJobOp,\n",
    "        BigqueryPredictModelJobOp, BigqueryQueryJobOp)\n",
    "    from google_cloud_pipeline_components.v1.dataflow import \\\n",
    "        DataflowPythonJobOp\n",
    "    from google_cloud_pipeline_components.v1.wait_gcp_resources import \\\n",
    "        WaitGcpResourcesOp\n",
    "\n",
    "    # create the dataset\n",
    "    bq_dataset_op = BigqueryQueryJobOp(\n",
    "        query=create_bq_dataset_query,\n",
    "        project=project,\n",
    "        location=\"US\",\n",
    "    )\n",
    "    # instantiate dataflow args\n",
    "    build_dataflow_args_op = build_dataflow_args(\n",
    "        job_name=job_name,\n",
    "        inputs_uri=inputs_uri,\n",
    "        # destination_table = bq_dataset_op.outputs['destination_table'],\n",
    "        bq_dataset=bq_dataset,\n",
    "        bq_table=bq_table,\n",
    "        setup_file_uri=setup_file_uri,\n",
    "        runner=runner,\n",
    "    ).after(bq_dataset_op)\n",
    "\n",
    "    # run dataflow job\n",
    "    dataflow_python_op = DataflowPythonJobOp(\n",
    "        requirements_file_path=requirements_file_path,\n",
    "        python_module_path=python_file_path,\n",
    "        args=build_dataflow_args_op.output,\n",
    "        project=project,\n",
    "        location=region,\n",
    "        temp_location=temp_location,\n",
    "    ).after(build_dataflow_args_op)\n",
    "\n",
    "    dataflow_wait_op = WaitGcpResourcesOp(\n",
    "        gcp_resources=dataflow_python_op.outputs[\"gcp_resources\"]\n",
    "    ).after(dataflow_python_op)\n",
    "\n",
    "    # run preprocessing job\n",
    "    bq_preprocess_op = BigqueryQueryJobOp(\n",
    "        query=create_bq_preprocess_query,\n",
    "        project=project,\n",
    "        location=\"US\",\n",
    "    ).after(dataflow_wait_op)\n",
    "\n",
    "    # create the logistic regression\n",
    "    bq_model_op = BigqueryCreateModelJobOp(\n",
    "        query=create_bq_model_query,\n",
    "        project=project,\n",
    "        location=\"US\",\n",
    "    ).after(bq_preprocess_op)\n",
    "\n",
    "    # evaluate the logistic regression\n",
    "    bq_evaluate_op = BigqueryEvaluateModelJobOp(\n",
    "        project=project, location=\"US\", model=bq_model_op.outputs[\"model\"]\n",
    "    ).after(bq_model_op)\n",
    "\n",
    "    # similuate prediction\n",
    "    BigqueryPredictModelJobOp(\n",
    "        model=bq_model_op.outputs[\"model\"],\n",
    "        query_statement=create_bq_prediction_query,\n",
    "        job_configuration_query=job_config,\n",
    "        project=project,\n",
    "        location=\"US\",\n",
    "    ).after(bq_evaluate_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nghLONQX7JNg"
   },
   "source": [
    "编译和运行流水线\n",
    "\n",
    "将必要的常量和参数传递给流水线，并将其编译为yaml文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8l6IR7OoADJV"
   },
   "outputs": [],
   "source": [
    "PIPELINE_ROOT = urlparse(BUCKET_URI)._replace(path=\"pipeline_root\").geturl()\n",
    "PIPELINE_PACKAGE = str(path(BUILD) / \"mlops_bqml_text_analyisis_pipeline.yaml\")\n",
    "REQUIREMENTS_URI = urlparse(BUCKET_URI)._replace(path=\"requirements.txt\").geturl()\n",
    "PYTHON_FILE_URI = urlparse(BUCKET_URI)._replace(path=\"src/ingest_pipeline.py\").geturl()\n",
    "MODEL_URI = urlparse(BUCKET_URI)._replace(path=\"swivel_text_embedding_model\").geturl()\n",
    "\n",
    "compiler.Compiler().compile(pipeline_func=pipeline, package_path=PIPELINE_PACKAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5c1b7b8b290"
   },
   "source": [
    "使用编译后的yaml文件，创建Vertex AI管道作业，并通过传递之前配置的 `SERVICE_ACCOUNT` 详细信息来运行它。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6rJ0NRuWwfnp"
   },
   "outputs": [],
   "source": [
    "pipeline = vertex_ai.PipelineJob(\n",
    "    display_name=f\"data_preprocess_{UUID}\",\n",
    "    template_path=PIPELINE_PACKAGE,\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values={\n",
    "        \"create_bq_dataset_query\": create_bq_dataset_query,\n",
    "        \"bq_dataset\": BQ_DATASET,\n",
    "        \"job_name\": JOB_NAME,\n",
    "        \"inputs_uri\": INPUTS_URI,\n",
    "        \"bq_table\": BQ_TABLE,\n",
    "        \"requirements_file_path\": REQUIREMENTS_URI,\n",
    "        \"python_file_path\": PYTHON_FILE_URI,\n",
    "        \"setup_file_uri\": SETUP_FILE_URI,\n",
    "        \"temp_location\": PIPELINE_ROOT,\n",
    "        \"runner\": RUNNER,\n",
    "        \"create_bq_preprocess_query\": create_bq_preprocess_query,\n",
    "        \"create_bq_model_query\": create_bq_model_query,\n",
    "        \"create_bq_prediction_query\": create_bq_prediction_query,\n",
    "        \"job_config\": JOB_CONFIG,\n",
    "    },\n",
    "    enable_caching=False,\n",
    ")\n",
    "\n",
    "pipeline.run(service_account=SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bcb582e65740"
   },
   "source": [
    "一旦管道工作成功完成，训练好的模型可以在BigQuery数据集中找到。运行下面的单元格查看模型在输出中的列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c0d194c006ae"
   },
   "outputs": [],
   "source": [
    "! bq ls $PROJECT_ID:$BQ_DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "清理工作\n",
    "\n",
    "要清理此项目中使用的所有Google Cloud资源，您可以[删除您用于本教程的Google Cloud项目](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects)。\n",
    "\n",
    "否则，您可以在下面的单元格中删除本教程中创建的各个资源。将`delete_bucket`和`delete_dataset`设置为**True**，分别删除在本笔记本中使用的 Cloud Storage 存储桶和 BigQuery 数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sx_vKniMq9ZX"
   },
   "outputs": [],
   "source": [
    "# delete the pipeline job\n",
    "pipeline.delete()\n",
    "\n",
    "delete_bucket = False\n",
    "delete_dataset = False\n",
    "\n",
    "# delete bucket\n",
    "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
    "    ! gsutil -m rm -r $BUCKET_URI\n",
    "\n",
    "# delete dataset\n",
    "if delete_dataset or os.getenv(\"IS_TESTING\"):\n",
    "    ! bq rm -r -f -d $PROJECT_ID:$BQ_DATASET"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "google_cloud_pipeline_components_bqml_text.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
