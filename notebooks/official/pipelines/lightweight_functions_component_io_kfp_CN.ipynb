{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "copyright"
   },
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title:generic"
   },
   "source": [
    "# Vertex AI Pipelines: Lightweight Python基于函数的组件，以及组件I/O\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/pipelines/lightweight_functions_component_io_kfp.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> 在Colab中打开\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fnotebook_template.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-enterprise-logo-32px.png\" alt=\"Google Cloud Colab Enterprise logo\"><br> 在Colab Enterprise中打开\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/pipelines/lightweight_functions_component_io_kfp.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> 在GitHub上查看\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "<a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/pipelines/lightweight_functions_component_io_kfp.ipynb\" target='_blank'>\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> 在Vertex AI Workbench中打开\n",
    "     </a>\n",
    "   </td>\n",
    "</table>\n",
    "<br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "overview:pipelines,lightweight"
   },
   "source": [
    "## 概述\n",
    "\n",
    "本笔记本展示了如何使用[Kubeflow Pipelines（KFP）SDK](https://www.kubeflow.org/docs/components/pipelines/)来构建[Vertex AI Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines)，这些管道使用基于轻量级Python函数的组件，同时支持使用KFP SDK进行组件I/O。\n",
    "\n",
    "了解更多关于[Vertex AI Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines/introduction)的信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "objective:pipelines,lightweight"
   },
   "source": [
    "### 目标\n",
    "\n",
    "在本教程中，您将学习如何使用KFP SDK构建基于轻量级Python函数的组件，然后学习如何使用Vertex AI Pipelines执行流水线。\n",
    "\n",
    "本教程使用以下谷歌云ML服务：\n",
    "\n",
    "- Vertex AI Pipelines\n",
    "\n",
    "其中执行的步骤包括：\n",
    "\n",
    "- 构建基于Python函数的KFP组件。\n",
    "- 构建一个KFP流水线。\n",
    "- 在组件之间传递*Artifacts*和*parameters*，既通过路径引用，也通过值。\n",
    "- 使用kfp.dsl.importer方法。\n",
    "- 编译KFP流水线。\n",
    "- 使用Vertex AI Pipelines执行KFP流水线。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "what_is:kfp,lightweight"
   },
   "source": [
    "### KFP 基于 Python 函数的组件\n",
    "\n",
    "Kubeflow 管道组件是一个自包含的代码集，用于执行机器学习工作流程中的一个步骤。管道组件由以下组成：\n",
    "\n",
    "* 组件代码，实现执行机器学习工作流程中步骤所需逻辑的代码。\n",
    "* 组件规范，定义以下内容：\n",
    "  * 组件的元数据，名称和描述。\n",
    "  * 组件的接口，组件的输入和输出。\n",
    "* 组件的实现，Docker 容器映像用于运行，如何将输入传递给您的组件代码，以及如何获取组件的输出。\n",
    "\n",
    "轻量级的基于 Python 函数的组件使得通过将组件代码构建为 Python 函数并为您生成组件规范，进而更容易快速迭代。本笔记本展示了如何为在 [Vertex AI Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines) 中使用创建基于 Python 函数的组件。\n",
    "\n",
    "基于 Python 函数的组件使用 Kubeflow Pipelines SDK 处理将输入传递给您的组件以及将您的函数输出返回到管道中的复杂性。\n",
    "\n",
    "基于 Python 函数的组件支持两种输入/输出类别：*工件* 和 *参数*。\n",
    "\n",
    "* 参数通过值传递给您的组件，通常包含 `int`、`float`、`bool` 或小型`string` 值。\n",
    "* 工件作为对路径的*引用*传递给您的组件，您可以写入文件或子目录结构。除了工件的数据，您还可以读取和写入工件的元数据。这让您记录工件的任意键值对，例如训练模型的准确率，并在下游组件中使用元数据 - 例如，您可以使用元数据来决定是否模型足够准确以用于预测部署。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "costs"
   },
   "source": [
    "### 成本\n",
    "\n",
    "本教程使用 Google Cloud 中的可计费组件：\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "了解 [Vertex AI 定价](https://cloud.google.com/vertex-ai/pricing) 和 [Cloud Storage 定价](https://cloud.google.com/storage/pricing)，并使用 [定价计算器](https://cloud.google.com/products/calculator/) 根据预期使用量生成成本估算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_aip:mbsdk"
   },
   "source": [
    "开始吧\n",
    "\n",
    "安装Python的Vertex AI SDK和其他必需的包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "! pip3 install --upgrade --quiet google-cloud-aiplatform \\\n",
    "                                 google-cloud-storage \\\n",
    "                                 kfp \\\n",
    "                                 \"numpy<2\" \\\n",
    "                                 google-cloud-pipeline-components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcp_authenticate"
   },
   "source": [
    "重新启动运行时（仅适用于Colab）\n",
    "在Google Colab上进行环境验证。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "21ad4dbb4a61"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56e219dbcb9a"
   },
   "source": [
    "### 验证您的笔记本环境（仅限Colab）\n",
    "在谷歌Colab上验证您的环境。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c97be6a73155"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45b2362d4ce4"
   },
   "source": [
    "### 设置谷歌云项目信息\n",
    "了解更多关于如何设置项目和开发环境。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oM1iC_MfAts1"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "创建一个云存储桶\n",
    "\n",
    "创建一个存储桶来存储中间产物，如数据集。\n",
    "\n",
    "- *{Note to notebook author: 对于任何需要唯一的用户提供的字符串（如存储桶名称或模型 ID），请在末尾附加“-unique”以便进行适当的测试}*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "BUCKET_URI = f\"gs://your-bucket-name-{PROJECT_ID}-unique\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_bucket"
   },
   "source": [
    "如果您的存储桶尚不存在：运行以下单元格以创建您的云存储桶。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIq7R4HZCfIc"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l {LOCATION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "set_service_account"
   },
   "source": [
    "#### 服务账号\n",
    "\n",
    "**如果您不知道您的服务账号**，请尝试使用`gcloud`命令在执行下面的第二个单元格中获取您的服务账号。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_service_account"
   },
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT = \"[your-service-account]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "autoset_service_account"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "if (\n",
    "    SERVICE_ACCOUNT == \"\"\n",
    "    or SERVICE_ACCOUNT is None\n",
    "    or SERVICE_ACCOUNT == \"[your-service-account]\"\n",
    "):\n",
    "    # Get your service account from gcloud\n",
    "    if not IS_COLAB:\n",
    "        shell_output = !gcloud auth list 2>/dev/null\n",
    "        SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
    "\n",
    "    if IS_COLAB:\n",
    "        shell_output = ! gcloud projects describe  $PROJECT_ID\n",
    "        project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
    "        SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
    "\n",
    "    print(\"Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "set_service_account:pipelines"
   },
   "source": [
    "为Vertex AI Pipelines设置服务账号访问权限\n",
    "\n",
    "运行以下命令，将您的服务账号授予对在上一步中创建的存储桶中读取和写入管道工件的访问权限-- 您只需要针对每个服务账号运行一次这些命令。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_service_account:pipelines"
   },
   "outputs": [],
   "source": [
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
    "\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_vars"
   },
   "source": [
    "### 设置变量\n",
    "\n",
    "接下来，设置一些在教程中使用的变量。\n",
    "### 导入库并定义常量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "import kfp\n",
    "from google.cloud import aiplatform\n",
    "from kfp import compiler, dsl\n",
    "from kfp.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                     OutputPath, component)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pipeline_constants"
   },
   "source": [
    "#### Vertex AI 管道常量\n",
    "\n",
    "为 Vertex AI 管道设置以下常量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pipeline_constants"
   },
   "outputs": [],
   "source": [
    "PIPELINE_ROOT = \"{}/pipeline_root/shakespeare\".format(BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk"
   },
   "source": [
    "初始化用于Python的Vertex AI SDK\n",
    "\n",
    "为您的项目和相应的存储桶初始化用于Python的Vertex AI SDK。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "define_component:lightweight,preprocess"
   },
   "source": [
    "### 定义基于Python函数的pipeline组件\n",
    "\n",
    "在本教程中，您将定义消耗参数并生成（类型化）Artifacts和参数的基于函数的组件。函数可以通过三种方式生成Artifacts：\n",
    "\n",
    "* 使用`OutputPath`接受输出本地路径\n",
    "* 接受`OutputArtifact`，该输出将函数提供给输出artifact的元数据的句柄\n",
    "* 在`NamedTuple`中返回一个`Artifact`（或`Dataset`，`Model`，`Metrics`等）\n",
    "\n",
    "这些生成Artifacts的选项将得到演示。\n",
    "\n",
    "#### 定义预处理组件\n",
    "\n",
    "第一个组件定义，`preprocess`，显示了一个输出两个`Dataset` Artifacts以及一个输出参数的组件。（对于此示例，数据集不反映真实数据）。\n",
    "\n",
    "对于参数输出，您通常会使用这里所示的方法，使用`OutputPath`类型来处理“较大”数据。对于“小数据”，比如一个短字符串，可能更方便使用第二个组件中所示的`NamedTuple`函数输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "define_component:lightweight,preprocess"
   },
   "outputs": [],
   "source": [
    "@component(base_image=\"python:3.9\")\n",
    "def preprocess(\n",
    "    # An input parameter of type string.\n",
    "    message: str,\n",
    "    # Use Output to get a metadata-rich handle to the output artifact\n",
    "    # of type `Dataset`.\n",
    "    output_dataset_one: Output[Dataset],\n",
    "    # A locally accessible filepath for another output artifact of type\n",
    "    # `Dataset`.\n",
    "    output_dataset_two_path: OutputPath(\"Dataset\"),\n",
    "    # A locally accessible filepath for an output parameter of type string.\n",
    "    output_parameter_path: OutputPath(str),\n",
    "):\n",
    "    \"\"\"'Mock' preprocessing step.\n",
    "    Writes out the passed in message to the output \"Dataset\"s and the output message.\n",
    "    \"\"\"\n",
    "    output_dataset_one.metadata[\"hello\"] = \"there\"\n",
    "    # Use OutputArtifact.path to access a local file path for writing.\n",
    "    # One can also use OutputArtifact.uri to access the actual URI file path.\n",
    "    with open(output_dataset_one.path, \"w\") as f:\n",
    "        f.write(message)\n",
    "\n",
    "    # OutputPath is used to just pass the local file path of the output artifact\n",
    "    # to the function.\n",
    "    with open(output_dataset_two_path, \"w\") as f:\n",
    "        f.write(message)\n",
    "\n",
    "    with open(output_parameter_path, \"w\") as f:\n",
    "        f.write(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "define_component:lightweight,train"
   },
   "source": [
    "#### 定义训练组件\n",
    "\n",
    "第二个组件定义，`train`，作为输入定义了一个 `Dataset` 类型的 `InputPath`，以及一个 `Dataset` 类型的 `InputArtifact`（还有其他参数输入）。它使用 `NamedTuple` 格式来输出函数。如图所示，这些输出可以是 Artifacts 也可以是参数。\n",
    "\n",
    "此外，该组件还将一些指标元数据写入到 `model` 输出的 Artifact 中。当流水线运行时，这些信息将在云控制台用户界面上显示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "define_component:lightweight,train"
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9\",  # Use a different base image.\n",
    ")\n",
    "def train(\n",
    "    # An input parameter of type string.\n",
    "    message: str,\n",
    "    # Use InputPath to get a locally accessible path for the input artifact\n",
    "    # of type `Dataset`.\n",
    "    dataset_one_path: InputPath(\"Dataset\"),\n",
    "    # Use InputArtifact to get a metadata-rich handle to the input artifact\n",
    "    # of type `Dataset`.\n",
    "    dataset_two: Input[Dataset],\n",
    "    # Output artifact of type Model.\n",
    "    imported_dataset: Input[Dataset],\n",
    "    model: Output[Model],\n",
    "    # An input parameter of type int with a default value.\n",
    "    num_steps: int = 3,\n",
    "    # Use NamedTuple to return either artifacts or parameters.\n",
    "    # When returning artifacts like this, return the contents of\n",
    "    # the artifact. The assumption here is that this return value\n",
    "    # fits in memory.\n",
    ") -> NamedTuple(\n",
    "    \"Outputs\",\n",
    "    [\n",
    "        (\"output_message\", str),  # Return parameter.\n",
    "        (\"generic_artifact\", Artifact),  # Return generic Artifact.\n",
    "    ],\n",
    "):\n",
    "    \"\"\"'Mock' Training step.\n",
    "    Combines the contents of dataset_one and dataset_two into the\n",
    "    output Model.\n",
    "    Constructs a new output_message consisting of message repeated num_steps times.\n",
    "    \"\"\"\n",
    "\n",
    "    # Directly access the passed in GCS URI as a local file (uses GCSFuse).\n",
    "    with open(dataset_one_path) as input_file:\n",
    "        dataset_one_contents = input_file.read()\n",
    "\n",
    "    # dataset_two is an Artifact handle. Use dataset_two.path to get a\n",
    "    # local file path (uses GCSFuse).\n",
    "    # Alternately, use dataset_two.uri to access the GCS URI directly.\n",
    "    with open(dataset_two.path) as input_file:\n",
    "        dataset_two_contents = input_file.read()\n",
    "\n",
    "    with open(model.path, \"w\") as f:\n",
    "        f.write(\"My Model\")\n",
    "\n",
    "    with open(imported_dataset.path) as f:\n",
    "        data = f.read()\n",
    "    print(\"Imported Dataset:\", data)\n",
    "\n",
    "    # Use model.get() to get a Model artifact, which has a .metadata dictionary\n",
    "    # to store arbitrary metadata for the output artifact. This metadata is\n",
    "    # recorded in Managed Metadata and can be queried later. It also shows up\n",
    "    # in the Google Cloud console.\n",
    "    model.metadata[\"accuracy\"] = 0.9\n",
    "    model.metadata[\"framework\"] = \"Tensorflow\"\n",
    "    model.metadata[\"time_to_train_in_seconds\"] = 257\n",
    "\n",
    "    artifact_contents = \"{}\\n{}\".format(dataset_one_contents, dataset_two_contents)\n",
    "    output_message = \" \".join([message for _ in range(num_steps)])\n",
    "    return (output_message, artifact_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "define_component:lightweight,read_artifact_input"
   },
   "source": [
    "#### 定义read_artifact_input组件\n",
    "\n",
    "最后，您定义一个小组件，它以`train`组件函数返回的`generic_artifact`作为输入，并读取并打印Artifact的内容。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "define_component:lightweight,read_artifact_input"
   },
   "outputs": [],
   "source": [
    "@component(base_image=\"python:3.9\")\n",
    "def read_artifact_input(\n",
    "    generic: Input[Artifact],\n",
    "):\n",
    "    with open(generic.path) as input_file:\n",
    "        generic_contents = input_file.read()\n",
    "        print(f\"generic contents: {generic_contents}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "define_pipeline:kfp,importer"
   },
   "source": [
    "定义一个使用您的组件和Importer的流水线\n",
    "\n",
    "接下来，定义一个流水线，使用在之前部分构建的组件，并展示`kfp.dsl.importer`的使用。\n",
    "\n",
    "此示例使用`importer`从现有URI创建`Dataset` artifact。\n",
    "\n",
    "请注意，`train_task`步骤接受`preprocess_task`步骤的三个输出作为输入，以及`importer`步骤的输出。\n",
    "在\"train\"输入中，我们引用`preprocess`的`output_parameter`，该参数直接提供输出字符串。\n",
    "\n",
    "`read_task`步骤接受`train_task`的`generic_artifact`输出作为输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "define_pipeline:kfp,importer"
   },
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    # Default pipeline root. You can override it when submitting the pipeline.\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    # A name for the pipeline. Use to determine the pipeline Context.\n",
    "    name=\"metadata-pipeline-v2\",\n",
    ")\n",
    "def pipeline(message: str):\n",
    "    importer = kfp.dsl.importer(\n",
    "        artifact_uri=\"gs://ml-pipeline-playground/shakespeare1.txt\",\n",
    "        artifact_class=Dataset,\n",
    "        reimport=False,\n",
    "    )\n",
    "    preprocess_task = preprocess(message=message)\n",
    "    train_task = train(\n",
    "        dataset_one_path=preprocess_task.outputs[\"output_dataset_one\"],\n",
    "        dataset_two=preprocess_task.outputs[\"output_dataset_two_path\"],\n",
    "        imported_dataset=importer.output,\n",
    "        message=preprocess_task.outputs[\"output_parameter_path\"],\n",
    "        num_steps=5,\n",
    "    )\n",
    "    read_task = read_artifact_input(  # noqa: F841\n",
    "        generic=train_task.outputs[\"generic_artifact\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "compile_pipeline"
   },
   "source": [
    "编译管道\n",
    "\n",
    "接下来，编译管道。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "compile_pipeline"
   },
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=\"lightweight_pipeline.yaml\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run_pipeline:lightweight"
   },
   "source": [
    "运行管道\n",
    "\n",
    "接下来，运行管道。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_pipeline:lightweight"
   },
   "outputs": [],
   "source": [
    "DISPLAY_NAME = \"shakespeare\"\n",
    "\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    template_path=\"lightweight_pipeline.yaml\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values={\"message\": \"Hello, World\"},\n",
    "    enable_caching=False,\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view_pipeline_run:lightweight"
   },
   "source": [
    "点击生成的链接以在云控制台中查看您的运行。\n",
    "\n",
    "在Google云控制台中，许多管道DAG节点在单击时会展开或折叠。这是DAG的部分展开视图（单击图像查看更大的版本）。\n",
    "\n",
    "<a href=\"https://storage.googleapis.com/amy-jo/images/mp/artifact_io2.png\" target=\"_blank\"><img src=\"https://storage.googleapis.com/amy-jo/images/mp/artifact_io2.png\" width=\"95%\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ba7c8f55afc"
   },
   "source": [
    "删除流水线作业\n",
    "\n",
    "您可以使用`delete()`方法来删除流水线作业。job.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "425adbf24044"
   },
   "outputs": [],
   "source": [
    "job.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleanup:pipelines"
   },
   "source": [
    "清理\n",
    "\n",
    "要清理此项目中使用的所有Google Cloud资源，可以[删除用于本教程的Google Cloud项目](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects)。\n",
    "\n",
    "否则，您可以删除在本教程中创建的个别资源。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cleanup:pipelines"
   },
   "outputs": [],
   "source": [
    "delete_bucket = False\n",
    "\n",
    "if delete_bucket:\n",
    "    ! gsutil rm -r $BUCKET_URI\n",
    "\n",
    "! rm lightweight_pipeline.yaml"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "lightweight_functions_component_io_kfp.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
