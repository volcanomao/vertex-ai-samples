{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b0b4f2bf"
   },
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5c0d0bbf74d"
   },
   "source": [
    "# Vertex AI管道：使用预构建的Google Cloud管道组件进行自定义培训\n",
    "\n",
    "<table align=\"left\">\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/pipelines/custom_model_training_and_batch_prediction.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> 在Colab中运行\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/pipelines/custom_model_training_and_batch_prediction.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      在GitHub上查看\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/pipelines/custom_model_training_and_batch_prediction.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      在Vertex AI Workbench中打开\n",
    "    </a>\n",
    "  </td>                                                                                               \n",
    "</table>\n",
    "<br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d79790f56d30"
   },
   "source": [
    "## 概述\n",
    "\n",
    "本教程演示了如何使用预构建的 Google Cloud Pipeline 组件和 Vertex AI Pipelines 进行自定义训练。\n",
    "\n",
    "了解有关 [Vertex AI Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines/introduction) 和 [自定义训练组件](https://cloud.google.com/vertex-ai/docs/training/create-training-pipeline) 的更多信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b87eb4840013"
   },
   "source": [
    "### 目标\n",
    "\n",
    "在本教程中，您将学习如何使用Vertex AI Pipelines和Google Cloud Pipeline组件来构建自定义模型。\n",
    "\n",
    "本教程使用以下Google Cloud ML服务：\n",
    "\n",
    "- Vertex AI Pipelines\n",
    "- Google Cloud Pipeline组件\n",
    "- Vertex AI Training\n",
    "- Vertex AI模型资源\n",
    "- Vertex AI端点资源\n",
    "\n",
    "执行的步骤包括：\n",
    "\n",
    "- 创建一个KFP管道：\n",
    "    - 训练一个自定义模型。\n",
    "    - 将训练好的模型上传为模型资源。\n",
    "    - 创建一个端点资源。\n",
    "    - 部署模型资源到端点资源。\n",
    "    - 进行批量预测请求。\n",
    "\n",
    "了解更多关于 [Google Cloud Pipeline components](https://cloud.google.com/vertex-ai/docs/pipelines/build-pipeline)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "57139e75264f"
   },
   "source": [
    "### 数据集\n",
    "\n",
    "本教程使用的数据集是[TensorFlow数据集](https://www.tensorflow.org/datasets/catalog/overview)中的[CIFAR10数据集](https://www.tensorflow.org/datasets/catalog/cifar10)。您将使用的数据集版本已嵌入到TensorFlow中。训练模型预测图像属于十个类别中的哪一类：飞机、汽车、鸟、猫、鹿、狗、青蛙、马、船或卡车。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "181d4dfbf917"
   },
   "source": [
    "费用\n",
    "\n",
    "本教程使用 Google Cloud 的应收组件：\n",
    "\n",
    "- Vertex AI\n",
    "- Cloud Storage\n",
    "\n",
    "了解 [Vertex AI 价格](https://cloud.google.com/vertex-ai/pricing) 和 [云存储价格](https://cloud.google.com/storage/pricing)，并使用 [定价计算器](https://cloud.google.com/products/calculator/) 根据您的预期使用情况生成费用估算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_aip:mbsdk"
   },
   "source": [
    "## 安装\n",
    "\n",
    "安装执行此笔记本所需的包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "! pip3 install --upgrade --quiet google-cloud-aiplatform \\\n",
    "                                 google-cloud-storage \\\n",
    "                                 kfp \\\n",
    "                                 google-cloud-pipeline-components\n",
    "\n",
    "\n",
    "! pip3 install --upgrade --force-reinstall tensorflow kfp google-cloud-aiplatform google-cloud-storage google-cloud-pipeline-components -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58707a750154"
   },
   "source": [
    "只有合作才行：取消以下单元格注释以重新启动内核。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f200f10a1da3"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs so that your environment can access the new packages\n",
    "# import IPython\n",
    "\n",
    "# app = IPython.Application.instance()\n",
    "# app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF1j6f9HApxa"
   },
   "source": [
    "## 开始之前\n",
    "\n",
    "### 设置您的Google云项目\n",
    "\n",
    "**无论您的笔记本环境如何，以下步骤都是必需的。**\n",
    "\n",
    "1. [选择或创建一个Google云项目](https://console.cloud.google.com/cloud-resource-manager)。当您首次创建帐户时，您将获得$300的免费信用额用于计算/存储成本。\n",
    "\n",
    "2. [确保为您的项目启用了计费](https://cloud.google.com/billing/docs/how-to/modify-project)。\n",
    "\n",
    "3. [启用Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com)\n",
    "\n",
    "4. 如果您在本地运行此笔记本，则需要安装[Cloud SDK](https://cloud.google.com/sdk)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WReHDGG5g0XY"
   },
   "source": [
    "设置您的项目ID\n",
    "\n",
    "**如果您不知道您的项目ID**，请尝试以下操作：\n",
    "* 运行 `gcloud config list`。\n",
    "* 运行 `gcloud projects list`。\n",
    "* 查看支持页面：[找到项目ID](https://support.google.com/googleapi/answer/7014113)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oM1iC_MfAts1"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
    "\n",
    "# Set the project id\n",
    "! gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "区域\n",
    "\n",
    "您还可以更改 Vertex AI 使用的 `REGION` 变量。了解有关 [Vertex AI 区域](https://cloud.google.com/vertex-ai/docs/general/locations) 的更多信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "region"
   },
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBCra4QMA2wR"
   },
   "source": [
    "### 验证您的谷歌云帐号\n",
    "\n",
    "根据您的Jupyter环境，您可能需要手动进行身份验证。请按照下面的相关说明操作。\n",
    "\n",
    "**1. Vertex AI Workbench**\n",
    "* 不需要进行任何操作，因为您已经通过验证。\n",
    "\n",
    "**2. 本地JupyterLab实例，请取消注释并运行:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "254614fa0c46"
   },
   "outputs": [],
   "source": [
    "# ! gcloud auth login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ef21552ccea8"
   },
   "source": [
    "3. 合作，取消注释并运行:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "603adbbf0532"
   },
   "outputs": [],
   "source": [
    "# from google.colab import auth\n",
    "# auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6b2ccc891ed"
   },
   "source": [
    "服务账户或其他\n",
    "\n",
    "请查看如何在https://cloud.google.com/storage/docs/gsutil/commands/iam#ch-examples给您的服务账户授予云存储权限。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "创建一个云存储桶\n",
    "\n",
    "创建一个存储桶，用于存储诸如数据集等中间产物。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "BUCKET_URI = f\"gs://your-bucket-name-{PROJECT_ID}-unique\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EcIXiGsCePi"
   },
   "source": [
    "如果您的存储桶还不存在：运行以下单元格来创建您的云存储存储桶。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIq7R4HZCfIc"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l {REGION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "set_service_account"
   },
   "source": [
    "#### 服务账户\n",
    "\n",
    "**如果您不知道您的服务账户**，请尝试使用`gcloud`命令在执行下面的第二个单元格中获取您的服务账户。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_service_account"
   },
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT = \"[your-service-account]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "autoset_service_account"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "if (\n",
    "    SERVICE_ACCOUNT == \"\"\n",
    "    or SERVICE_ACCOUNT is None\n",
    "    or SERVICE_ACCOUNT == \"[your-service-account]\"\n",
    "):\n",
    "    # Get your service account from gcloud\n",
    "    if not IS_COLAB:\n",
    "        shell_output = !gcloud auth list 2>/dev/null\n",
    "        SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
    "\n",
    "    if IS_COLAB:\n",
    "        shell_output = ! gcloud projects describe  $PROJECT_ID\n",
    "        project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
    "        SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
    "\n",
    "    print(\"Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "set_service_account:pipelines"
   },
   "source": [
    "#### 为 Vertex AI Pipelines 设置服务账号访问权限\n",
    "\n",
    "运行以下命令，将您的服务账号授予读取和写入管道工件的权限，这些工件位于您在上一步中创建的存储桶中 -- 您只需为每个服务账号运行一次这些命令。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_service_account:pipelines"
   },
   "outputs": [],
   "source": [
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
    "\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_vars"
   },
   "source": [
    "### 设置变量\n",
    "\n",
    "接下来，设置一些在教程中使用的变量。\n",
    "### 导入库并定义常量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aip\n",
    "from google_cloud_pipeline_components.v1.custom_job import utils\n",
    "from kfp import compiler, dsl\n",
    "from kfp.dsl import component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pipeline_constants"
   },
   "source": [
    "#### Vertex AI Pipelines 常量\n",
    "\n",
    "为 Vertex AI Pipelines 设置以下常量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pipeline_constants"
   },
   "outputs": [],
   "source": [
    "PIPELINE_ROOT = \"{}/pipeline_root/bikes_weather\".format(BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk"
   },
   "source": [
    "初始化用于Python的Vertex AI SDK\n",
    "\n",
    "为您的项目和相应的存储桶初始化Python的Vertex AI SDK。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yX-aimhGRRRl"
   },
   "outputs": [],
   "source": [
    "aip.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "accelerators:training,cpu,prediction,cpu,mbsdk"
   },
   "source": [
    "#### 设置硬件加速器\n",
    "\n",
    "您可以为训练和预测设置硬件加速器。\n",
    "\n",
    "将变量`TRAIN_GPU/TRAIN_NGPU`和`DEPLOY_GPU/DEPLOY_NGPU`设置为使用支持GPU的容器映像以及分配给虚拟机（VM）实例的GPU数量。例如，要使用一个GPU容器映像，并将4个Nvidia Telsa K80 GPU分配给每个VM，您可以指定：\n",
    "\n",
    "    (aip.gapic.AcceleratorType.NVIDIA_TESLA_K80, 4)\n",
    "\n",
    "否则，指定`(None, None)`以在CPU上运行容器映像。\n",
    "\n",
    "了解更多关于[您地区的硬件加速器支持](https://cloud.google.com/vertex-ai/docs/general/locations#accelerators)的信息。\n",
    "\n",
    "*注意*：在TF 2.3之前的GPU支持下，将无法在本教程中加载自定义模型。这是一个已知问题，在TF 2.3中已修复。这是由生成在serving function中的静态图操作引起的。如果您在自己的自定义模型上遇到此问题，请使用支持GPU的TF 2.3容器映像。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ipG9uBUDRRRm"
   },
   "outputs": [],
   "source": [
    "TRAIN_GPU, TRAIN_NGPU = (None, None)\n",
    "\n",
    "DEPLOY_GPU, DEPLOY_NGPU = (None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "container:training,prediction"
   },
   "source": [
    "设置预构建的容器\n",
    "\n",
    "设置用于训练和预测的预构建的Docker容器镜像。\n",
    "\n",
    "有关最新列表，请参阅[用于训练的预构建容器](https://cloud.google.com/ai-platform-unified/docs/training/pre-built-containers)。\n",
    "\n",
    "有关最新列表，请参阅[用于预测的预构建容器](https://cloud.google.com/ai-platform-unified/docs/predictions/pre-built-containers)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WrKTNnVLRRRm"
   },
   "outputs": [],
   "source": [
    "TF = \"2-5\"\n",
    "\n",
    "if TRAIN_GPU:\n",
    "    TRAIN_VERSION = \"tf-gpu.{}\".format(TF)\n",
    "else:\n",
    "    TRAIN_VERSION = \"tf-cpu.{}\".format(TF)\n",
    "if DEPLOY_GPU:\n",
    "    DEPLOY_VERSION = \"tf2-gpu.{}\".format(TF)\n",
    "else:\n",
    "    DEPLOY_VERSION = \"tf2-cpu.{}\".format(TF)\n",
    "\n",
    "TRAIN_IMAGE = \"gcr.io/cloud-aiplatform/training/{}:latest\".format(TRAIN_VERSION)\n",
    "DEPLOY_IMAGE = \"gcr.io/cloud-aiplatform/prediction/{}:latest\".format(DEPLOY_VERSION)\n",
    "\n",
    "print(\"Training:\", TRAIN_IMAGE, TRAIN_GPU, TRAIN_NGPU)\n",
    "print(\"Deployment:\", DEPLOY_IMAGE, DEPLOY_GPU, DEPLOY_NGPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "machine:training,prediction"
   },
   "source": [
    "#### 设置机器类型\n",
    "\n",
    "接下来，设置用于训练和预测的机器类型。\n",
    "\n",
    "- 设置变量 `TRAIN_COMPUTE` 和 `DEPLOY_COMPUTE` 来配置用于训练和预测的虚拟机的计算资源。\n",
    " - `机器类型`\n",
    "     - `n1-standard`: 每个 vCPU 3.75GB 的内存。\n",
    "     - `n1-highmem`: 每个 vCPU 6.5GB 的内存。\n",
    "     - `n1-highcpu`: 每个 vCPU 0.9GB 的内存。\n",
    " - `vCPUs`: \\[2, 4, 8, 16, 32, 64, 96] 的数量。\n",
    "\n",
    "*注意：以下内容不支持用于训练*：\n",
    "\n",
    " - `标准`：2个 vCPUs\n",
    " - `高CPU`：2、4和8个 vCPUs\n",
    "\n",
    "*注意：您还可以使用 n2 和 e2 机器类型进行训练和部署，但它们不支持GPU*。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K4RhNsDzRRRn"
   },
   "outputs": [],
   "source": [
    "MACHINE_TYPE = \"n1-standard\"\n",
    "\n",
    "VCPU = \"4\"\n",
    "TRAIN_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
    "print(\"Train machine type\", TRAIN_COMPUTE)\n",
    "\n",
    "VCPU = \"4\"\n",
    "DEPLOY_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
    "print(\"Deploy machine type\", DEPLOY_COMPUTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tutorial_start:custom"
   },
   "source": [
    "教程\n",
    "\n",
    "现在你已经准备好开始为CIFAR10创建自己的自定义模型和训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "examine_training_package"
   },
   "source": [
    "### 检查培训包\n",
    "\n",
    "#### 包布局\n",
    "\n",
    "在开始培训之前，您将查看一个Python包是如何为自定义培训任务组装的。解压后，该包包含以下目录/文件布局。\n",
    "\n",
    "- PKG-INFO\n",
    "- README.md\n",
    "- setup.cfg\n",
    "- setup.py\n",
    "- trainer\n",
    "  - \\_\\_init\\_\\_.py\n",
    "  - task.py\n",
    "\n",
    "文件`setup.cfg`和`setup.py`是在Docker镜像的操作环境中安装包的说明。\n",
    "\n",
    "文件`trainer/task.py`是执行自定义培训任务的Python脚本。*注意*，当我们在worker pool规范中引用它时，我们将目录斜杠替换为一个点（`trainer.task`）并且删除文件后缀（`.py`）。\n",
    "\n",
    "#### 包装配\n",
    "\n",
    "在以下单元格中，您将组装培训包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YpA6MFcLRRRn"
   },
   "outputs": [],
   "source": [
    "# Make folder for Python training script\n",
    "! rm -rf custom\n",
    "! mkdir custom\n",
    "\n",
    "# Add package information\n",
    "! touch custom/README.md\n",
    "\n",
    "setup_cfg = \"[egg_info]\\n\\ntag_build =\\n\\ntag_date = 0\"\n",
    "! echo \"$setup_cfg\" > custom/setup.cfg\n",
    "\n",
    "setup_py = \"import setuptools\\n\\nsetuptools.setup(\\n\\n    install_requires=[\\n\\n        'tensorflow_datasets==1.3.0',\\n\\n    ],\\n\\n    packages=setuptools.find_packages())\"\n",
    "! echo \"$setup_py\" > custom/setup.py\n",
    "\n",
    "pkg_info = \"Metadata-Version: 1.0\\n\\nName: CIFAR10 image classification\\n\\nVersion: 0.0.0\\n\\nSummary: Demostration training script\\n\\nHome-page: www.google.com\\n\\nAuthor: Google\\n\\nAuthor-email: aferlitsch@google.com\\n\\nLicense: Public\\n\\nDescription: Demo\\n\\nPlatform: Vertex\"\n",
    "! echo \"$pkg_info\" > custom/PKG-INFO\n",
    "\n",
    "# Make the training subfolder\n",
    "! mkdir custom/trainer\n",
    "! touch custom/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86f3ab08b20a"
   },
   "source": [
    "### 为训练自定义模型创建自定义组件\n",
    "\n",
    "接下来，创建一个轻量级的Python函数组件，用于训练CIFAR10图像分类模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2etUCVVMRRRo"
   },
   "outputs": [],
   "source": [
    "# Single, Mirror and Multi-Machine Distributed Training for CIFAR-10\n",
    "\n",
    "\n",
    "@component(\n",
    "    base_image=\"tensorflow/tensorflow:latest\",\n",
    "    packages_to_install=[\"tensorflow_datasets\", \"opencv-python-headless\"],\n",
    ")\n",
    "def custom_train_model(\n",
    "    model_dir: str,\n",
    "    lr: float = 0.01,\n",
    "    epochs: int = 10,\n",
    "    steps: int = 200,\n",
    "    distribute: str = \"single\",\n",
    "):\n",
    "\n",
    "    import faulthandler\n",
    "    import os\n",
    "    import sys\n",
    "\n",
    "    import tensorflow as tf\n",
    "    import tensorflow_datasets as tfds\n",
    "    from tensorflow.python.client import device_lib\n",
    "\n",
    "    faulthandler.enable()\n",
    "    tfds.disable_progress_bar()\n",
    "\n",
    "    print(\"Component start\")\n",
    "\n",
    "    print(\"Python Version = {}\".format(sys.version))\n",
    "    print(\"TensorFlow Version = {}\".format(tf.__version__))\n",
    "    print(\"TF_CONFIG = {}\".format(os.environ.get(\"TF_CONFIG\", \"Not found\")))\n",
    "    print(\"DEVICES\", device_lib.list_local_devices())\n",
    "\n",
    "    # Single Machine, single compute device\n",
    "    if distribute == \"single\":\n",
    "        if tf.test.is_gpu_available():\n",
    "            strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
    "        else:\n",
    "            strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
    "    # Single Machine, multiple compute device\n",
    "    elif distribute == \"mirror\":\n",
    "        strategy = tf.distribute.MirroredStrategy()\n",
    "    # Multiple Machine, multiple compute device\n",
    "    elif distribute == \"multi\":\n",
    "        strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n",
    "\n",
    "    # Multi-worker configuration\n",
    "    print(\"num_replicas_in_sync = {}\".format(strategy.num_replicas_in_sync))\n",
    "\n",
    "    # Preparing dataset\n",
    "    BUFFER_SIZE = 10000\n",
    "    BATCH_SIZE = 64\n",
    "\n",
    "    def make_datasets_unbatched():\n",
    "\n",
    "        # Scaling CIFAR10 data from (0, 255] to (0., 1.]\n",
    "        def scale(image, label):\n",
    "            image = tf.cast(image, tf.float32)\n",
    "            image /= 255.0\n",
    "            return image, label\n",
    "\n",
    "        datasets, info = tfds.load(name=\"cifar10\", with_info=True, as_supervised=True)\n",
    "        return datasets[\"train\"].map(scale).cache().shuffle(BUFFER_SIZE).repeat()\n",
    "\n",
    "    # Build the Keras model\n",
    "    def build_and_compile_cnn_model(lr: int = 0.01):\n",
    "        model = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Conv2D(\n",
    "                    32, 3, activation=\"relu\", input_shape=(32, 32, 3)\n",
    "                ),\n",
    "                tf.keras.layers.MaxPooling2D(),\n",
    "                tf.keras.layers.Conv2D(32, 3, activation=\"relu\"),\n",
    "                tf.keras.layers.MaxPooling2D(),\n",
    "                tf.keras.layers.Flatten(),\n",
    "                tf.keras.layers.Dense(10, activation=\"softmax\"),\n",
    "            ]\n",
    "        )\n",
    "        model.compile(\n",
    "            loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "            optimizer=tf.keras.optimizers.SGD(learning_rate=lr),\n",
    "            metrics=[\"accuracy\"],\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    # Train the model\n",
    "    NUM_WORKERS = strategy.num_replicas_in_sync\n",
    "    # Here the batch size scales up by number of workers since\n",
    "    # `tf.data.Dataset.batch` expects the global batch size.\n",
    "    GLOBAL_BATCH_SIZE = BATCH_SIZE * NUM_WORKERS\n",
    "    train_dataset = make_datasets_unbatched().batch(GLOBAL_BATCH_SIZE)\n",
    "\n",
    "    with strategy.scope():\n",
    "        # Creation of dataset, and model building/compiling need to be within\n",
    "        # `strategy.scope()`.\n",
    "        model = build_and_compile_cnn_model(lr)\n",
    "\n",
    "    model.fit(x=train_dataset, epochs=epochs, steps_per_epoch=steps)\n",
    "    model.save(model_dir)\n",
    "\n",
    "    model_path_to_deploy = model_dir\n",
    "\n",
    "    # Load the saved model\n",
    "    local_model = tf.keras.models.load_model(model_dir)\n",
    "\n",
    "    # Load evaluation data\n",
    "    import numpy as np\n",
    "    from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "    (_, _), (x_test, y_test) = cifar10.load_data()\n",
    "    x_test = (x_test / 255.0).astype(np.float32)\n",
    "\n",
    "    print(x_test.shape, y_test.shape)\n",
    "\n",
    "    # Perform the model evaluation\n",
    "    local_model.evaluate(x_test, y_test)\n",
    "\n",
    "    # Serving function for image data\n",
    "    CONCRETE_INPUT = \"numpy_inputs\"\n",
    "\n",
    "    def _preprocess(bytes_input):\n",
    "        decoded = tf.io.decode_jpeg(bytes_input, channels=3)\n",
    "        decoded = tf.image.convert_image_dtype(decoded, tf.float32)\n",
    "        resized = tf.image.resize(decoded, size=(32, 32))\n",
    "        return resized\n",
    "\n",
    "    @tf.function(input_signature=[tf.TensorSpec([None], tf.string)])\n",
    "    def preprocess_fn(bytes_inputs):\n",
    "        decoded_images = tf.map_fn(\n",
    "            _preprocess, bytes_inputs, dtype=tf.float32, back_prop=False\n",
    "        )\n",
    "        return {\n",
    "            CONCRETE_INPUT: decoded_images\n",
    "        }  # User needs to make sure the key matches model's input\n",
    "\n",
    "    @tf.function(input_signature=[tf.TensorSpec([None], tf.string)])\n",
    "    def serving_fn(bytes_inputs):\n",
    "        images = preprocess_fn(bytes_inputs)\n",
    "        prob = m_call(**images)\n",
    "        return prob\n",
    "\n",
    "    m_call = tf.function(local_model.call).get_concrete_function(\n",
    "        [tf.TensorSpec(shape=[None, 32, 32, 3], dtype=tf.float32, name=CONCRETE_INPUT)]\n",
    "    )\n",
    "\n",
    "    tf.saved_model.save(\n",
    "        local_model, model_path_to_deploy, signatures={\"serving_default\": serving_fn}\n",
    "    )\n",
    "\n",
    "    # Get the serving function signature\n",
    "    loaded = tf.saved_model.load(model_path_to_deploy)\n",
    "\n",
    "    serving_input = list(\n",
    "        loaded.signatures[\"serving_default\"].structured_input_signature[1].keys()\n",
    "    )[0]\n",
    "    print(\"Serving function input:\", serving_input)\n",
    "\n",
    "    # Get test items\n",
    "    test_image_1 = x_test[0]\n",
    "    test_image_2 = x_test[1]\n",
    "    print(test_image_1.shape)\n",
    "\n",
    "    BUCKET_URI = model_dir + \"/test\"\n",
    "\n",
    "    import cv2\n",
    "\n",
    "    cv2.imwrite(\"tmp1.jpg\", (test_image_1 * 255).astype(np.uint8))\n",
    "    cv2.imwrite(\"tmp2.jpg\", (test_image_2 * 255).astype(np.uint8))\n",
    "\n",
    "    print(\"Writing jpg files\")\n",
    "\n",
    "    # Copy test item(s)\n",
    "    # For the batch prediction, copy the test items over to your Cloud Storage bucket.\n",
    "    test_item_1 = BUCKET_URI + \"/tmp1.jpg\"\n",
    "    test_item_2 = BUCKET_URI + \"/tmp2.jpg\"\n",
    "\n",
    "    with tf.io.gfile.GFile(test_item_1, \"wb\") as w:\n",
    "        with tf.io.gfile.GFile(\"tmp1.jpg\", \"rb\") as r:\n",
    "            bytes = r.read()\n",
    "            w.write(bytes)\n",
    "\n",
    "    with tf.io.gfile.GFile(test_item_2, \"wb\") as w:\n",
    "        with tf.io.gfile.GFile(\"tmp2.jpg\", \"rb\") as r:\n",
    "            bytes = r.read()\n",
    "            w.write(bytes)\n",
    "\n",
    "    # Make the batch input file\n",
    "    import base64\n",
    "    import json\n",
    "\n",
    "    gcs_input_uri = BUCKET_URI + \"/\" + \"test.jsonl\"\n",
    "    with tf.io.gfile.GFile(gcs_input_uri, \"w\") as f:\n",
    "        bytes = tf.io.read_file(test_item_1)\n",
    "        b64str = base64.b64encode(bytes.numpy()).decode(\"utf-8\")\n",
    "        data = {serving_input: {\"b64\": b64str}}\n",
    "        f.write(json.dumps(data) + \"\\n\")\n",
    "        bytes = tf.io.read_file(test_item_2)\n",
    "        b64str = base64.b64encode(bytes.numpy()).decode(\"utf-8\")\n",
    "        data = {serving_input: {\"b64\": b64str}}\n",
    "        f.write(json.dumps(data) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1abdbe48"
   },
   "source": [
    "### 将组件转换为一个 Vertex AI 自定义作业\n",
    "\n",
    "接下来，使用`create_custom_training_job_op_from_component`方法，将自定义组件转换为一个 Vertex AI 预构建自定义作业组件。\n",
    "\n",
    "**replica_count :** 批处理操作可扩展到的机器副本数。仅在设置了machine_type时使用。默认值为10。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "deb6c1cc"
   },
   "outputs": [],
   "source": [
    "custom_job_distributed_training_op = utils.create_custom_training_job_op_from_component(\n",
    "    custom_train_model, replica_count=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e57e7311"
   },
   "source": [
    "定义自定义训练工作流程\n",
    "\n",
    "接下来，定义包含以下任务的管道作业：\n",
    "\n",
    "- 训练自定义模型。\n",
    "- 将模型上传到Verex AI模型资源。\n",
    "- 执行批处理预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5R23d2J_HJr-"
   },
   "outputs": [],
   "source": [
    "MODEL_DIR = BUCKET_URI + \"/model\"\n",
    "\n",
    "\n",
    "@dsl.pipeline(name=\"custom-model-training-sample-pipeline\")\n",
    "def pipeline(\n",
    "    model_dir: str = MODEL_DIR,\n",
    "    lr: float = 0.01,\n",
    "    epochs: int = 10,\n",
    "    steps: int = 200,\n",
    "    distribute: str = \"single\",\n",
    "):\n",
    "    from google_cloud_pipeline_components.types import artifact_types\n",
    "    from google_cloud_pipeline_components.v1.batch_predict_job import \\\n",
    "        ModelBatchPredictOp\n",
    "    from google_cloud_pipeline_components.v1.model import ModelUploadOp\n",
    "    from kfp.dsl import importer_node\n",
    "\n",
    "    custom_producer_task = custom_job_distributed_training_op(\n",
    "        model_dir=model_dir,\n",
    "        lr=lr,\n",
    "        epochs=epochs,\n",
    "        steps=steps,\n",
    "        distribute=distribute,\n",
    "        project=PROJECT_ID,\n",
    "        location=REGION,\n",
    "        base_output_directory=PIPELINE_ROOT,\n",
    "    )\n",
    "\n",
    "    unmanaged_model_importer = importer_node.importer(\n",
    "        artifact_uri=model_dir,\n",
    "        artifact_class=artifact_types.UnmanagedContainerModel,\n",
    "        metadata={\n",
    "            \"containerSpec\": {\n",
    "                \"imageUri\": \"us-docker.pkg.dev/cloud-aiplatform/prediction/tf2-cpu.2-3:latest\"\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "\n",
    "    model_upload_op = ModelUploadOp(\n",
    "        project=PROJECT_ID,\n",
    "        display_name=\"model_display_name\",\n",
    "        unmanaged_container_model=unmanaged_model_importer.outputs[\"artifact\"],\n",
    "    )\n",
    "    model_upload_op.after(custom_producer_task)\n",
    "\n",
    "    batch_predict_op = ModelBatchPredictOp(\n",
    "        project=PROJECT_ID,\n",
    "        job_display_name=\"batch_predict_job\",\n",
    "        model=model_upload_op.outputs[\"model\"],\n",
    "        gcs_source_uris=[MODEL_DIR + \"/test/test.jsonl\"],\n",
    "        gcs_destination_output_uri_prefix=PIPELINE_ROOT,\n",
    "        instances_format=\"jsonl\",\n",
    "        predictions_format=\"jsonl\",\n",
    "        model_parameters={},\n",
    "        machine_type=DEPLOY_COMPUTE,\n",
    "        starting_replica_count=1,\n",
    "        max_replica_count=1,\n",
    "    )\n",
    "\n",
    "    batch_predict_op.after(model_upload_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e584790f"
   },
   "source": [
    "### 编译和运行管道\n",
    "接下来，编译管道成为一个DAG，然后执行它。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6be28881"
   },
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=\"custom_model_training_spec.yaml\"\n",
    ")\n",
    "\n",
    "DISPLAY_NAME = \"cifar10\"\n",
    "\n",
    "job = aip.PipelineJob(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    template_path=\"custom_model_training_spec.yaml\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    ")\n",
    "\n",
    "job.run(service_account=SERVICE_ACCOUNT)\n",
    "\n",
    "! rm custom_model_training_spec.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87774222c338"
   },
   "source": [
    "查看自定义训练管道结果\n",
    "\n",
    "最后，您可以查看管道中每个任务的工件输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ac26e1314c80"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "PROJECT_NUMBER = job.gca_resource.name.split(\"/\")[1]\n",
    "print(PROJECT_NUMBER)\n",
    "\n",
    "\n",
    "def print_pipeline_output(job, output_task_name):\n",
    "    JOB_ID = job.name\n",
    "    print(JOB_ID)\n",
    "    for _ in range(len(job.gca_resource.job_detail.task_details)):\n",
    "        TASK_ID = job.gca_resource.job_detail.task_details[_].task_id\n",
    "        EXECUTE_OUTPUT = (\n",
    "            PIPELINE_ROOT\n",
    "            + \"/\"\n",
    "            + PROJECT_NUMBER\n",
    "            + \"/\"\n",
    "            + JOB_ID\n",
    "            + \"/\"\n",
    "            + output_task_name\n",
    "            + \"_\"\n",
    "            + str(TASK_ID)\n",
    "            + \"/executor_output.json\"\n",
    "        )\n",
    "        GCP_RESOURCES = (\n",
    "            PIPELINE_ROOT\n",
    "            + \"/\"\n",
    "            + PROJECT_NUMBER\n",
    "            + \"/\"\n",
    "            + JOB_ID\n",
    "            + \"/\"\n",
    "            + output_task_name\n",
    "            + \"_\"\n",
    "            + str(TASK_ID)\n",
    "            + \"/gcp_resources\"\n",
    "        )\n",
    "        EVAL_METRICS = (\n",
    "            PIPELINE_ROOT\n",
    "            + \"/\"\n",
    "            + PROJECT_NUMBER\n",
    "            + \"/\"\n",
    "            + JOB_ID\n",
    "            + \"/\"\n",
    "            + output_task_name\n",
    "            + \"_\"\n",
    "            + str(TASK_ID)\n",
    "            + \"/evaluation_metrics\"\n",
    "        )\n",
    "        if tf.io.gfile.exists(EXECUTE_OUTPUT):\n",
    "            ! gsutil cat $EXECUTE_OUTPUT\n",
    "            return EXECUTE_OUTPUT\n",
    "        elif tf.io.gfile.exists(GCP_RESOURCES):\n",
    "            ! gsutil cat $GCP_RESOURCES\n",
    "            return GCP_RESOURCES\n",
    "        elif tf.io.gfile.exists(EVAL_METRICS):\n",
    "            ! gsutil cat $EVAL_METRICS\n",
    "            return EVAL_METRICS\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "print(\"model-upload\")\n",
    "artifacts = print_pipeline_output(job, \"model-upload\")\n",
    "print(\"\\n\")\n",
    "output = !gsutil cat $artifacts\n",
    "print(output)\n",
    "output = json.loads(output[0])\n",
    "model_id = output[\"artifacts\"][\"model\"][\"artifacts\"][0][\"metadata\"][\"resourceName\"]\n",
    "print(\"model-batch-predict\")\n",
    "artifacts = print_pipeline_output(job, \"model-batch-predict\")\n",
    "print(\"\\n\")\n",
    "output = !gsutil cat $artifacts\n",
    "output = json.loads(output[0])\n",
    "batch_job_id = output[\"artifacts\"][\"batchpredictionjob\"][\"artifacts\"][0][\"metadata\"][\n",
    "    \"resourceName\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2cd7ee5cc21"
   },
   "source": [
    "清理\n",
    "\n",
    "要清理此项目中使用的所有Google Cloud资源，您可以[删除用于本教程的Google Cloud项目](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects)。\n",
    "\n",
    "否则，您可以删除在本教程中创建的各个资源。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c29a5f39e211"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "job.delete()\n",
    "\n",
    "model = aip.Model(model_id)\n",
    "model.delete()\n",
    "\n",
    "batch_job = aip.BatchPredictionJob(batch_job_id)\n",
    "batch_job.delete()\n",
    "\n",
    "\n",
    "delete_bucket = False\n",
    "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
    "    ! gsutil rm -rf {BUCKET_URI}"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "custom_model_training_and_batch_prediction.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
