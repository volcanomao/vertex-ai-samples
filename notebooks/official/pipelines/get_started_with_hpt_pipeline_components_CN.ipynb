{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "copyright"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title:generic,gcp"
   },
   "source": [
    "开始使用Vertex AI超参数调整管道组件\n",
    "\n",
    "*注意：此笔记本使用KFP 1.x和GCPC 1.x。我们建议使用2.x版本*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "overview:mlops"
   },
   "source": [
    "## 概览\n",
    "\n",
    "此教程演示了如何使用Vertex AI超参数调整管道组件。\n",
    "\n",
    "了解更多关于[Vertex AI Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines/introduction)和[Vertex AI超参数调整](https://cloud.google.com/vertex-ai/docs/training/hyperparameter-tuning-overview)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "objective:mlops,stage3,get_started_hpt_pipeline_components"
   },
   "source": [
    "### 目标\n",
    "\n",
    "在本教程中，您将学习如何使用预构建的Google Cloud管道组件来进行Vertex AI超参数调整。\n",
    "\n",
    "本教程使用以下Google Cloud ML服务：\n",
    "\n",
    "- Google Cloud管道组件\n",
    "- Vertex AI数据集、模型和端点资源\n",
    "- Vertex AI超参数调整\n",
    "\n",
    "执行的步骤包括：\n",
    "\n",
    "- 构建一个管道，用于：\n",
    "    - 超参数调整/训练自定义模型。\n",
    "    - 检索经过调整的超参数值和指标以进行优化。\n",
    "    - 如果指标超过指定阈值。\n",
    "      - 获取经过最佳调整的模型的模型构件的位置。\n",
    "      - 将模型构件上传到Vertex AI模型资源。\n",
    "- 执行一个Vertex AI管道。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset:horses_or_humans,icn"
   },
   "source": [
    "数据集\n",
    "\n",
    "本教程使用的数据集是来自[TensorFlow数据集](https://www.tensorflow.org/datasets/catalog/overview)的[马和人类](https://www.tensorflow.org/datasets/catalog/horses_or_humans)。训练模型会预测图像是马还是人类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5e2eba58ad71"
   },
   "source": [
    "费用\n",
    "\n",
    "本教程使用 Google Cloud 的可计费组件：\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "详细了解[Vertex AI 价格](https://cloud.google.com/vertex-ai/pricing)和[Cloud Storage 价格](https://cloud.google.com/storage/pricing)，并使用[Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)根据您预期的使用情况生成费用估算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_mlops"
   },
   "source": [
    "安装所需的包以执行笔记本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LR9HQnyiMoT5"
   },
   "outputs": [],
   "source": [
    "! pip3 install -U tensorflow==2.5 -q\n",
    "! pip3 install -U tensorflow-data-validation==1.2  -q\n",
    "! pip3 install -U tensorflow-transform==1.2  -q\n",
    "! pip3 install -U tensorflow-io==0.18  -q\n",
    "! pip3 install --upgrade google-cloud-aiplatform[tensorboard] -q\n",
    "! pip3 install --upgrade 'google-cloud-pipeline-components<2'  -q\n",
    "! pip3 install --upgrade 'kfp<2' -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e9255e3b156f"
   },
   "source": [
    "仅 Colab 使用：请取消下面单元格的注释以重新启动核心"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0c0b2427998a"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs so that your environment can access the new packages\n",
    "# import IPython\n",
    "\n",
    "# app = IPython.Application.instance()\n",
    "# app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "435b8e413535"
   },
   "source": [
    "### 在开始之前\n",
    "\n",
    "#### 设置您的项目 ID\n",
    "\n",
    "**如果您不知道您的项目 ID**，请尝试以下方法：\n",
    "- 运行 `gcloud config list`\n",
    "- 运行 `gcloud projects list`\n",
    "- 查看支持页面：[查找项目 ID](https://support.google.com/googleapi/answer/7014113)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "be175254a715"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
    "\n",
    "# set the project id\n",
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2e6b8b324ce1"
   },
   "source": [
    "区域\n",
    "\n",
    "您还可以更改由 Vertex AI 使用的 `REGION` 变量。\n",
    "了解有关 [Vertex AI 区域](https://cloud.google.com/vertex-ai/docs/general/locations) 的更多信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "region"
   },
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6c43a8673066"
   },
   "source": [
    "### 验证您的谷歌云账户\n",
    "\n",
    "根据您的Jupyter环境，您可能需要手动进行验证。请按照以下相关说明操作。\n",
    "\n",
    "**1. Vertex AI Workbench**\n",
    "- 无需操作，因为您已经通过验证。\n",
    "\n",
    "**2. 本地JupyterLab实例**，取消注释并运行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fbc9cd30cc4b"
   },
   "outputs": [],
   "source": [
    "# ! gcloud auth login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cd0da2c26879"
   },
   "source": [
    "3. 协作,取消注释并运行:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a336a05c6149"
   },
   "outputs": [],
   "source": [
    "# from google.colab import auth\n",
    "# auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6b2ccc891ed"
   },
   "source": [
    "查看如何将云存储权限授予您的服务帐号，请访问https://cloud.google.com/storage/docs/gsutil/commands/iam#ch-examples。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e5755d1a554f"
   },
   "source": [
    "### 创建一个云存储桶\n",
    "\n",
    "创建一个存储桶来存储诸如数据集等中间文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d2de92accb67"
   },
   "outputs": [],
   "source": [
    "BUCKET_URI = f\"gs://your-bucket-name-{PROJECT_ID}-unique\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_bucket"
   },
   "source": [
    "如果您的存储桶尚不存在：运行以下单元格以创建您的云存储桶。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2smRgc53MoT-"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "set_service_account"
   },
   "source": [
    "#### 服务账户\n",
    "\n",
    "**如果你不知道你的服务账户**，请尝试使用`gcloud`命令在下面执行第二个单元格来获取你的服务账户。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EIivrR-3MoT-"
   },
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT = \"[your-service-account]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "autoset_service_account"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "if (\n",
    "    SERVICE_ACCOUNT == \"\"\n",
    "    or SERVICE_ACCOUNT is None\n",
    "    or SERVICE_ACCOUNT == \"[your-service-account]\"\n",
    "):\n",
    "    # Get your service account from gcloud\n",
    "    if not IS_COLAB:\n",
    "        shell_output = !gcloud auth list 2>/dev/null\n",
    "        SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
    "\n",
    "    if IS_COLAB:\n",
    "        shell_output = ! gcloud projects describe  $PROJECT_ID\n",
    "        project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
    "        SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
    "\n",
    "    print(\"Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "set_service_account:pipelines"
   },
   "source": [
    "## 为Vertex AI Pipelines设置服务账户访问权限\n",
    "\n",
    "运行以下命令，将您的服务账户访问权限授予读取和写入管道工件的存储桶 -- 您只需要对每个服务账户运行一次这些命令。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mtwsjYnIMoT_"
   },
   "outputs": [],
   "source": [
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
    "\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_vars"
   },
   "source": [
    "### 设置变量\n",
    "\n",
    "接下来，设置一些在教程中使用的变量。\n",
    "### 导入库并定义常量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_kfp"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_tf"
   },
   "source": [
    "#### 导入TensorFlow\n",
    "\n",
    "将TensorFlow包导入到你的Python环境中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DtTIHh_KMoUA"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk"
   },
   "source": [
    "### 初始化用于 Python 的 Vertex AI SDK\n",
    "\n",
    "为您的项目和相应的存储桶初始化用于 Python 的 Vertex AI SDK。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sUFJPDW0MoUA"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "accelerators:training,prediction,ngpu,mbsdk"
   },
   "source": [
    "#### 设置硬件加速器\n",
    "\n",
    "您可以为训练和预测设置硬件加速器。\n",
    "\n",
    "设置变量`TRAIN_GPU/TRAIN_NGPU`和`DEPLOY_GPU/DEPLOY_NGPU`以使用支持GPU的容器映像，以及分配给虚拟机（VM）实例的GPU数量。例如，要使用一个支持 GPU 的容器映像，每个 VM 分配 4 个 Nvidia Telsa K80 GPU，您应该指定：\n",
    "\n",
    "（aiplatform.gapic.AcceleratorType.NVIDIA_TESLA_K80, 4）\n",
    "\n",
    "否则，指定`(None, None)`以使用一个在 CPU 上运行的容器映像。\n",
    "\n",
    "了解更多关于[您位置的硬件加速器支持](https://cloud.google.com/vertex-ai/docs/general/locations#accelerators)。\n",
    "\n",
    "*注意*：TF 2.3 之前的 GPU 支持版本会在这个教程中加载自定义模型时失败。这是一个已知问题，在 TF 2.3 中已修复。这是由在服务函数中生成的静态图操作引起的。如果您在自定义模型上遇到这个问题，请使用具有 GPU 支持的 TF 2.3 容器映像。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A6dzi4cXMoUA"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "TRAIN_GPU, TRAIN_NGPU = (None, None)\n",
    "DEPLOY_GPU, DEPLOY_NGPU = (None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "container:prediction"
   },
   "source": [
    "#### 设置预先构建的容器\n",
    "\n",
    "设置用于预测的预先构建的Docker容器镜像。\n",
    "\n",
    "- 将变量`TF`设置为容器镜像的TensorFlow版本。例如，`2-1`表示版本2.1，`1-15`表示版本1.15。以下列表显示了一些可用的预先构建的镜像：\n",
    "\n",
    "要获取最新列表，请参见[用于预测的预先构建容器](https://cloud.google.com/ai-platform-unified/docs/predictions/pre-built-containers)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gxai072KMoUB"
   },
   "outputs": [],
   "source": [
    "TF = \"2.5\".replace(\".\", \"-\")\n",
    "\n",
    "if TF[0] == \"2\":\n",
    "    if DEPLOY_GPU:\n",
    "        DEPLOY_VERSION = \"tf2-gpu.{}\".format(TF)\n",
    "    else:\n",
    "        DEPLOY_VERSION = \"tf2-cpu.{}\".format(TF)\n",
    "else:\n",
    "    if DEPLOY_GPU:\n",
    "        DEPLOY_VERSION = \"tf-gpu.{}\".format(TF)\n",
    "    else:\n",
    "        DEPLOY_VERSION = \"tf-cpu.{}\".format(TF)\n",
    "\n",
    "DEPLOY_IMAGE = \"{}-docker.pkg.dev/vertex-ai/prediction/{}:latest\".format(\n",
    "    REGION.split(\"-\")[0], DEPLOY_VERSION\n",
    ")\n",
    "\n",
    "print(\"Deployment:\", DEPLOY_IMAGE, DEPLOY_GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "machine:training,prediction"
   },
   "source": [
    "#### 设置机器类型\n",
    "\n",
    "接下来，设置用于训练和预测的机器类型。\n",
    "\n",
    "- 设置变量 `TRAIN_COMPUTE` 和 `DEPLOY_COMPUTE` 来配置用于训练和预测的虚拟机的计算资源。\n",
    " - `机器类型`\n",
    "     - `n1-standard`: 每个 vCPU 的内存为 3.75GB。\n",
    "     - `n1-highmem`: 每个 vCPU 的内存为 6.5GB。\n",
    "     - `n1-highcpu`: 每个 vCPU 的内存为 0.9 GB。\n",
    " - `vCPUs`: \\[2, 4, 8, 16, 32, 64, 96 \\] 中的数字\n",
    "\n",
    "*注意：以下机器类型不支持训练:*\n",
    "\n",
    " - `standard`: 2 个 vCPUs\n",
    " - `highcpu`: 2, 4 和 8 个 vCPUs\n",
    "\n",
    "*注意：您也可以使用 n2 和 e2 机器类型进行训练和部署，但它们不支持 GPU*。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LEYjL1ojMoUB"
   },
   "outputs": [],
   "source": [
    "TRAIN_COMPUTE = \"n1-standard-4\"\n",
    "print(\"Train machine type\", TRAIN_COMPUTE)\n",
    "\n",
    "DEPLOY_COMPUTE = \"n1-standard-4\"\n",
    "print(\"Deploy machine type\", DEPLOY_COMPUTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "examine_training_package"
   },
   "source": [
    "### 检查调整包\n",
    "\n",
    "#### 包布局\n",
    "\n",
    "在开始调整之前，您将查看一个Python包是如何为自定义调整工作组装的。解压后，包含以下目录/文件布局。\n",
    "\n",
    "- PKG-INFO\n",
    "- README.md\n",
    "- setup.cfg\n",
    "- setup.py\n",
    "- trainer\n",
    "  - \\_\\_init\\_\\_.py\n",
    "  - task.py\n",
    "\n",
    "文件 `setup.cfg` 和 `setup.py` 是将包安装到Docker镜像操作环境的指令。\n",
    "\n",
    "文件 `trainer/task.py` 是执行自定义调整工作的Python脚本。*注意*，当我们在工作池规范中引用它时，我们用点 (`trainer.task`) 替换目录斜杠，并删除文件后缀 (`.py`)。\n",
    "\n",
    "#### 包装配\n",
    "\n",
    "在接下来的步骤中，您将组装训练包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UIoZdxPqMoUC"
   },
   "outputs": [],
   "source": [
    "# Make folder for Python tuning script\n",
    "! rm -rf custom\n",
    "! mkdir custom\n",
    "\n",
    "# Add package information\n",
    "! touch custom/README.md\n",
    "\n",
    "setup_cfg = \"[egg_info]\\n\\ntag_build =\\n\\ntag_date = 0\"\n",
    "! echo \"$setup_cfg\" > custom/setup.cfg\n",
    "\n",
    "setup_py = \"import setuptools\\n\\nsetuptools.setup(\\n\\n    install_requires=[\\n\\n        'tensorflow==2.5.0',\\n\\n        'tensorflow_datasets==1.3.0',\\n\\n    ],\\n\\n    packages=setuptools.find_packages())\"\n",
    "! echo \"$setup_py\" > custom/setup.py\n",
    "\n",
    "pkg_info = \"Metadata-Version: 1.0\\n\\nName: Horses or Humans image classification\\n\\nVersion: 0.0.0\\n\\nSummary: Demostration tuning script\\n\\nHome-page: www.google.com\\n\\nAuthor: Google\\n\\nAuthor-email: aferlitsch@google.com\\n\\nLicense: Public\\n\\nDescription: Demo\\n\\nPlatform: Vertex\"\n",
    "! echo \"$pkg_info\" > custom/PKG-INFO\n",
    "\n",
    "# Make the training subfolder\n",
    "! mkdir custom/trainer\n",
    "! touch custom/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "taskpy_contents:dataset,horses_or_humans"
   },
   "source": [
    "### 创建Python培训套件的任务脚本\n",
    "\n",
    "接下来，您会创建用于驱动培训套件的`task.py`脚本。一些值得注意的步骤包括：\n",
    "\n",
    "- 命令行参数：\n",
    "    - `model-dir`：用于保存训练模型的位置。在使用Vertex AI自定义训练时，该位置将在环境变量中指定为：`AIP_MODEL_DIR`，\n",
    "    - `epochs`：训练的时期数。\n",
    "    - `learning_rate`：学习速率的超参数。\n",
    "    - `batch_size`：批量大小的超参数。\n",
    "\n",
    "\n",
    "- 数据预处理（`get_data()`）\n",
    "    - 加载并预处理数据集作为`tf.data.Dataset`生成器。\n",
    "\n",
    "\n",
    "- 模型架构（`get_model()`）：\n",
    "    - 构建对应的模型架构。\n",
    "\n",
    "\n",
    "- 训练（`train_model()`）：\n",
    "    - 训练模型。\n",
    "\n",
    "\n",
    "- 模型工件保存\n",
    "    - 保存模型工件的云存储位置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uk4MAGErMoUC"
   },
   "outputs": [],
   "source": [
    "%%writefile custom/trainer/task.py\n",
    "import os\n",
    "os.system('pip install cloudml-hypertune')  # alternaterly, this can be added to the Dockerfile\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import argparse\n",
    "import hypertune\n",
    "\n",
    "\n",
    "def get_args():\n",
    "  '''Parses args. Must include all hyperparameters you want to tune.'''\n",
    "\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument(\n",
    "      '--epochs',\n",
    "      required=True,\n",
    "      type=int,\n",
    "      help='number of epochs')\n",
    "  parser.add_argument(\n",
    "      '--learning_rate',\n",
    "      required=True,\n",
    "      type=float,\n",
    "      help='learning rate')\n",
    "  parser.add_argument(\n",
    "      '--momentum',\n",
    "      required=False,\n",
    "      type=float,\n",
    "      default=0.5,\n",
    "      help='SGD momentum value')\n",
    "  parser.add_argument(\n",
    "      '--batch_size',\n",
    "      required=True,\n",
    "      type=int,\n",
    "      help='the batch size')\n",
    "  parser.add_argument(\n",
    "      '--model-dir',\n",
    "      dest='model_dir',\n",
    "      default=os.getenv('AIP_MODEL_DIR'),\n",
    "      type=str, help='Model dir.')\n",
    "  args = parser.parse_args()\n",
    "  return args\n",
    "\n",
    "def preprocess_data(image, label):\n",
    "  '''Resizes and scales images.'''\n",
    "\n",
    "  image = tf.image.resize(image, (150,150))\n",
    "  return tf.cast(image, tf.float32) / 255., label\n",
    "\n",
    "\n",
    "def get_data():\n",
    "  '''Loads Horses Or Humans dataset and preprocesses data.'''\n",
    "\n",
    "  data, info = tfds.load(name='horses_or_humans', as_supervised=True, with_info=True)\n",
    "\n",
    "  # Create train dataset\n",
    "  train_data = data['train'].map(preprocess_data)\n",
    "  train_data  = train_data.shuffle(1000)\n",
    "  train_data  = train_data.batch(64)\n",
    "\n",
    "  # Create validation dataset\n",
    "  validation_data = data['test'].map(preprocess_data)\n",
    "  validation_data  = validation_data.batch(64)\n",
    "\n",
    "  return train_data, validation_data\n",
    "\n",
    "\n",
    "def get_model(learning_rate, momentum):\n",
    "  '''Defines and complies model.'''\n",
    "\n",
    "  inputs = tf.keras.Input(shape=(150, 150, 3))\n",
    "  x = tf.keras.layers.Conv2D(16, (3, 3), activation='relu')(inputs)\n",
    "  x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "  x = tf.keras.layers.Conv2D(32, (3, 3), activation='relu')(x)\n",
    "  x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "  x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
    "  x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "  x = tf.keras.layers.Flatten()(x)\n",
    "  outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "  model = tf.keras.Model(inputs, outputs)\n",
    "  model.compile(\n",
    "      loss='binary_crossentropy',\n",
    "      optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=momentum),\n",
    "      metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "def train_model(model, train_data, validation_data, epochs, batch_size):\n",
    "\n",
    "  history = model.fit(train_data, epochs=epochs, batch_size=batch_size, validation_data=validation_data)\n",
    "\n",
    "  # DEFINE METRIC\n",
    "  hp_metric = history.history['val_accuracy'][-1]\n",
    "\n",
    "  hpt = hypertune.HyperTune()\n",
    "  hpt.report_hyperparameter_tuning_metric(\n",
    "      hyperparameter_metric_tag='accuracy',\n",
    "      metric_value=hp_metric,\n",
    "      global_step=epochs\n",
    "  )\n",
    "\n",
    "  return model\n",
    "\n",
    "\n",
    "def main():\n",
    "  args = get_args()\n",
    "  train_data, validation_data = get_data()\n",
    "\n",
    "  model = get_model(args.learning_rate, args.momentum)\n",
    "\n",
    "  model = train_model(model, train_data, validation_data, args.epochs, args.batch_size)\n",
    "\n",
    "  model.save(args.model_dir)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tarball_training_script"
   },
   "source": [
    "将调优脚本存储在您的云存储桶中\n",
    "\n",
    "接下来，将调优文件夹打包成压缩的tar文件，然后存储在您的云存储桶中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bHcLQLGkMoUD"
   },
   "outputs": [],
   "source": [
    "! rm -f custom.tar custom.tar.gz\n",
    "! tar cvf custom.tar custom\n",
    "! gzip custom.tar\n",
    "! gsutil cp custom.tar.gz $BUCKET_URI/trainer_horses_or_humans.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_docker_container:training"
   },
   "source": [
    "### 创建一个Docker文件\n",
    "\n",
    "为了使用自己的自定义训练容器，请构建一个Docker文件，并将其嵌入到带有训练脚本的容器中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "write_docker_file:training,tf-dlvm"
   },
   "source": [
    "#### 编写 Docker 文件内容\n",
    "\n",
    "将您的代码容器化的第一步是创建一个 Docker 文件。在您的 Docker 文件中，您将包含运行容器映像所需的所有命令。它将安装您正在使用的所有库，并为训练代码设置入口点。\n",
    "\n",
    "1. 从 TensorFlow 仓库中安装预定义的用于深度学习映像的容器映像。\n",
    "2. 将 Python 训练代码复制进来，稍后会展示。\n",
    "3. 将入口设置为 Python 训练脚本 `trainer/task.py`。注意，`ENTRYPOINT` 命令中省略了 `.py`，因为这是显而易见的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MaMK4AoBMoUE"
   },
   "outputs": [],
   "source": [
    "%%writefile custom/Dockerfile\n",
    "\n",
    "FROM gcr.io/deeplearning-platform-release/tf2-cpu.2-3\n",
    "WORKDIR /root\n",
    "\n",
    "WORKDIR /\n",
    "\n",
    "# Copies the trainer code to the docker image.\n",
    "COPY trainer /trainer\n",
    "\n",
    "# Sets up the entry point to invoke the trainer.\n",
    "ENTRYPOINT [\"python\", \"-m\", \"trainer.task\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "name_container:training"
   },
   "source": [
    "#### 在本地构建容器\n",
    "\n",
    "接下来，在提交到Google容器注册表时，请提供一个您自定义容器的名称。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9qVJGXT2MoUE"
   },
   "outputs": [],
   "source": [
    "TRAIN_IMAGE = \"gcr.io/\" + PROJECT_ID + \"/horses_or_humans:v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "build_container:training"
   },
   "source": [
    "接下来，建造容器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7-dkQP8hMoUE"
   },
   "outputs": [],
   "source": [
    "! docker build custom -t $TRAIN_IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "register_container:training"
   },
   "source": [
    "#### 注册自定义容器\n",
    "\n",
    "当您在本地运行完容器后，请将其推送至Google容器注册表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7-kh6QBLMoUF"
   },
   "outputs": [],
   "source": [
    "! docker push $TRAIN_IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_hpt_pipeline:icn"
   },
   "source": [
    "### 构建超参数调整流水线\n",
    "\n",
    "接下来，构建以下任务的流水线：\n",
    "\n",
    "- 创建/执行一个超参数调整作业\n",
    "- 获取所有试验结果\n",
    "- 获取最佳试验结果\n",
    "- 确定最佳试验结果是否超过一个阈值：\n",
    "    - 检索超参数数值\n",
    "    - 确定最佳模型的云存储位置\n",
    "    - 将最佳模型上传为 Vertex AI 模型资源。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Epzlh8M-MoUF"
   },
   "outputs": [],
   "source": [
    "PIPELINE_ROOT = \"{}/pipeline_root/custom_icn_tuning\".format(BUCKET_URI)\n",
    "\n",
    "\n",
    "@component(packages_to_install=[\"google-cloud-aiplatform\"])\n",
    "def model_dir(base_output_directory: str, best_trial: str) -> str:\n",
    "    from google.cloud.aiplatform_v1.types import study\n",
    "\n",
    "    trial_proto = study.Trial.from_json(best_trial)\n",
    "    model_id = trial_proto.id\n",
    "    return f\"{base_output_directory}/{model_id}/model\"\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=\"hp-tuning\", description=\"Custom image classification hyperparameter tuning\"\n",
    ")\n",
    "def pipeline(\n",
    "    display_name: str,\n",
    "    worker_pool_specs: list,\n",
    "    study_spec_metrics: list,\n",
    "    study_spec_parameters: list,\n",
    "    threshold: float,\n",
    "    deploy_image: str,\n",
    "    max_trial_count: int = 5,\n",
    "    parallel_trial_count: int = 1,\n",
    "    base_output_directory: str = PIPELINE_ROOT,\n",
    "    labels: dict = {},\n",
    "    project: str = PROJECT_ID,\n",
    "    region: str = REGION,\n",
    "):\n",
    "    from google_cloud_pipeline_components.experimental import \\\n",
    "        hyperparameter_tuning_job\n",
    "    from google_cloud_pipeline_components.types import artifact_types\n",
    "    from google_cloud_pipeline_components.v1.hyperparameter_tuning_job import \\\n",
    "        HyperparameterTuningJobRunOp\n",
    "    from google_cloud_pipeline_components.v1.model import ModelUploadOp\n",
    "    from kfp.v2.components import importer_node\n",
    "\n",
    "    tuning_op = HyperparameterTuningJobRunOp(\n",
    "        display_name=display_name,\n",
    "        project=project,\n",
    "        location=region,\n",
    "        worker_pool_specs=worker_pool_specs,\n",
    "        study_spec_metrics=study_spec_metrics,\n",
    "        study_spec_parameters=study_spec_parameters,\n",
    "        max_trial_count=max_trial_count,\n",
    "        parallel_trial_count=parallel_trial_count,\n",
    "        base_output_directory=base_output_directory,\n",
    "    )\n",
    "\n",
    "    trials_op = hyperparameter_tuning_job.GetTrialsOp(\n",
    "        gcp_resources=tuning_op.outputs[\"gcp_resources\"]\n",
    "    )\n",
    "\n",
    "    best_trial_op = hyperparameter_tuning_job.GetBestTrialOp(\n",
    "        trials=trials_op.output, study_spec_metrics=study_spec_metrics\n",
    "    )\n",
    "\n",
    "    threshold_op = hyperparameter_tuning_job.IsMetricBeyondThresholdOp(\n",
    "        trial=best_trial_op.output,\n",
    "        study_spec_metrics=study_spec_metrics,\n",
    "        threshold=threshold,\n",
    "    )\n",
    "\n",
    "    with dsl.Condition(\n",
    "        threshold_op.output == \"true\",\n",
    "        name=\"deploy_decision\",\n",
    "    ):\n",
    "        _ = hyperparameter_tuning_job.GetHyperparametersOp(trial=best_trial_op.output)\n",
    "\n",
    "        model_dir_op = model_dir(base_output_directory, best_trial_op.output)\n",
    "\n",
    "        import_unmanaged_model_op = importer_node.importer(\n",
    "            artifact_uri=model_dir_op.output,\n",
    "            artifact_class=artifact_types.UnmanagedContainerModel,\n",
    "            metadata={\n",
    "                \"containerSpec\": {\n",
    "                    \"imageUri\": DEPLOY_IMAGE,\n",
    "                },\n",
    "            },\n",
    "        ).after(model_dir_op)\n",
    "\n",
    "        _ = ModelUploadOp(\n",
    "            project=project,\n",
    "            display_name=display_name,\n",
    "            unmanaged_container_model=import_unmanaged_model_op.outputs[\"artifact\"],\n",
    "        ).after(import_unmanaged_model_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_hpt_specs"
   },
   "source": [
    "### 创建超参数调整规范\n",
    "\n",
    "接下来，您按照以下方式构建工作池规范、研究的指标和参数规范：\n",
    "\n",
    "**工作池规范**\n",
    "\n",
    "此规范描述了执行超参数研究的机器和容器要求以及扩展性。由于训练模块嵌入在docker镜像中，您可以使用`args`字段来指定任何命令行参数，这些参数不是研究的一部分，传递给训练模块。在这个例子中，您传递了epoch的数量。\n",
    "\n",
    "**参数规范**\n",
    "\n",
    "此规范描述了要调整的超参数以及要调整的值范围。对于每个研究，这些参数的试验值作为命令行参数传递给训练模块，格式为`--<parameter_name>=<trial_value>`。\n",
    "\n",
    "**指标规范**\n",
    "\n",
    "此规范描述了在研究中要评估的指标或指标，以及该指标是最小化还是最大化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aEm7RuwMMoUG"
   },
   "outputs": [],
   "source": [
    "from google_cloud_pipeline_components.experimental import \\\n",
    "    hyperparameter_tuning_job\n",
    "\n",
    "gpu = \"ACCELERATOR_TYPE_UNSPECIFIED\"\n",
    "accelerator_count = 0\n",
    "\n",
    "if TRAIN_GPU:\n",
    "    gpu = TRAIN_GPU.name\n",
    "    accelerator_count = 1\n",
    "\n",
    "else:\n",
    "    gpu = \"ACCELERATOR_TYPE_UNSPECIFIED\"\n",
    "    accelerator_count = (\n",
    "        0  # same problem with accelerator_count, if we keep is as \"None\" its not\n",
    "    )\n",
    "\n",
    "CMDARGS = [\n",
    "    \"--epochs=10\",\n",
    "]\n",
    "\n",
    "# The spec of the worker pools including machine type and Docker image\n",
    "worker_pool_specs = [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": TRAIN_COMPUTE,\n",
    "            \"accelerator_type\": gpu,\n",
    "            \"accelerator_count\": accelerator_count,\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"container_spec\": {\"image_uri\": TRAIN_IMAGE, \"args\": CMDARGS},\n",
    "    }\n",
    "]\n",
    "\n",
    "# List serialized from the dictionary representing metrics to optimize.\n",
    "# The dictionary key is the metric_id, which is reported by your training job,\n",
    "# and the dictionary value is the optimization goal of the metric.\n",
    "metric_spec = hyperparameter_tuning_job.serialize_metrics({\"accuracy\": \"maximize\"})\n",
    "\n",
    "# List serialized from the parameter dictionary. The dictionary\n",
    "# represents parameters to optimize. The dictionary key is the parameter_id,\n",
    "# which is passed into your training job as a command line key word argument, and the\n",
    "# dictionary value is the parameter specification of the metric.\n",
    "parameter_spec = hyperparameter_tuning_job.serialize_parameters(\n",
    "    {\n",
    "        \"learning_rate\": aiplatform.hyperparameter_tuning.DoubleParameterSpec(\n",
    "            min=0.001, max=1, scale=\"log\"\n",
    "        ),\n",
    "        \"batch_size\": aiplatform.hyperparameter_tuning.DiscreteParameterSpec(\n",
    "            values=[16, 32, 64], scale=None\n",
    "        ),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run_pipeline:hpt"
   },
   "source": [
    "### 编译和执行超参数调优管道\n",
    "\n",
    "接下来，您编译管道，然后执行它。管道接受以下参数，这些参数作为字典`parameter_values`传递：\n",
    "\n",
    "- `display_name`：管道作业的人类可读名称。\n",
    "- `worker_pool_specs`：机器和容器，以及自动扩展要求，以及命令行参数。\n",
    "- `study_spec_metrics`：优化研究试验中的度量标准。\n",
    "- `study_spec_parameters`：要调整的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "42YNp9Y9MoUG"
   },
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=\"hp_tune_pipeline_job.json\"\n",
    ")\n",
    "\n",
    "pipeline = aiplatform.PipelineJob(\n",
    "    display_name=\"hp_tuning\",\n",
    "    template_path=\"hp_tune_pipeline_job.json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values={\n",
    "        \"display_name\": \"hp_tuning\",\n",
    "        \"worker_pool_specs\": worker_pool_specs,\n",
    "        \"study_spec_metrics\": metric_spec,\n",
    "        \"study_spec_parameters\": parameter_spec,\n",
    "        \"threshold\": 0.7,\n",
    "        \"deploy_image\": DEPLOY_IMAGE,\n",
    "    },\n",
    "    enable_caching=False,\n",
    ")\n",
    "\n",
    "pipeline.run()\n",
    "\n",
    "! rm -rf hp_tune_pipeline_job.json custom custom.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view_pipleline_results:hpt,horses_or_humans"
   },
   "source": [
    "查看数据管道执行结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4X3jrdX1MoUH"
   },
   "outputs": [],
   "source": [
    "PROJECT_NUMBER = pipeline.gca_resource.name.split(\"/\")[1]\n",
    "print(PROJECT_NUMBER)\n",
    "\n",
    "\n",
    "def print_pipeline_output(job, output_task_name):\n",
    "    JOB_ID = job.name\n",
    "    print(JOB_ID)\n",
    "    for _ in range(len(job.gca_resource.job_detail.task_details)):\n",
    "        TASK_ID = job.gca_resource.job_detail.task_details[_].task_id\n",
    "        EXECUTE_OUTPUT = (\n",
    "            PIPELINE_ROOT\n",
    "            + \"/\"\n",
    "            + PROJECT_NUMBER\n",
    "            + \"/\"\n",
    "            + JOB_ID\n",
    "            + \"/\"\n",
    "            + output_task_name\n",
    "            + \"_\"\n",
    "            + str(TASK_ID)\n",
    "            + \"/executor_output.json\"\n",
    "        )\n",
    "        GCP_RESOURCES = (\n",
    "            PIPELINE_ROOT\n",
    "            + \"/\"\n",
    "            + PROJECT_NUMBER\n",
    "            + \"/\"\n",
    "            + JOB_ID\n",
    "            + \"/\"\n",
    "            + output_task_name\n",
    "            + \"_\"\n",
    "            + str(TASK_ID)\n",
    "            + \"/gcp_resources\"\n",
    "        )\n",
    "        EVAL_METRICS = (\n",
    "            PIPELINE_ROOT\n",
    "            + \"/\"\n",
    "            + PROJECT_NUMBER\n",
    "            + \"/\"\n",
    "            + JOB_ID\n",
    "            + \"/\"\n",
    "            + output_task_name\n",
    "            + \"_\"\n",
    "            + str(TASK_ID)\n",
    "            + \"/evaluation_metrics\"\n",
    "        )\n",
    "        if tf.io.gfile.exists(EXECUTE_OUTPUT):\n",
    "            ! gsutil cat $EXECUTE_OUTPUT\n",
    "            return EXECUTE_OUTPUT\n",
    "        elif tf.io.gfile.exists(GCP_RESOURCES):\n",
    "            ! gsutil cat $GCP_RESOURCES\n",
    "            return GCP_RESOURCES\n",
    "        elif tf.io.gfile.exists(EVAL_METRICS):\n",
    "            ! gsutil cat $EVAL_METRICS\n",
    "            return EVAL_METRICS\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "print(\"hyperparameter-tuning-job\")\n",
    "artifacts = print_pipeline_output(pipeline, \"hyperparameter-tuning-job\")\n",
    "print(\"\\n\\n\")\n",
    "print(\"gettrialsop\")\n",
    "artifacts = print_pipeline_output(pipeline, \"gettrialsop\")\n",
    "print(\"\\n\\n\")\n",
    "print(\"getbesttrialop\")\n",
    "artifacts = print_pipeline_output(pipeline, \"getbesttrialop\")\n",
    "print(\"\\n\\n\")\n",
    "output = !gsutil cat $artifacts\n",
    "output = json.loads(output[0])\n",
    "best_trial = json.loads(output[\"parameters\"][\"Output\"][\"stringValue\"])\n",
    "model_id = best_trial[\"id\"]\n",
    "print(\"BEST MODEL\", model_id)\n",
    "parameters = best_trial[\"parameters\"]\n",
    "batch_size = parameters[0][\"value\"]\n",
    "print(\"BATCH SIZE\", batch_size)\n",
    "learning_rate = parameters[1][\"value\"]\n",
    "print(\"LR\", learning_rate)\n",
    "MODEL_DIR = f\"{PIPELINE_ROOT}/{model_id}/model\"\n",
    "\n",
    "print(\"ismetricbeyondthresholdop\")\n",
    "artifacts = print_pipeline_output(pipeline, \"ismetricbeyondthresholdop\")\n",
    "print(\"\\n\\n\")\n",
    "print(\"deploy-decision\")\n",
    "artifacts = print_pipeline_output(pipeline, \"deploy-decision\")\n",
    "print(\"\\n\\n\")\n",
    "print(\"model-dir\")\n",
    "artifacts = print_pipeline_output(pipeline, \"model-dir\")\n",
    "print(\"\\n\\n\")\n",
    "print(\"model-upload\")\n",
    "artifacts = print_pipeline_output(pipeline, \"model-upload\")\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "delete_pipeline"
   },
   "source": [
    "删除流水线作业\n",
    "\n",
    "在流水线作业完成后，您可以使用`delete()`方法删除流水线作业。在完成之前，可以使用`cancel()`方法取消流水线作业。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hS53o3FcMoUH"
   },
   "outputs": [],
   "source": [
    "pipeline.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleanup:mbsdk"
   },
   "source": [
    "清理\n",
    "\n",
    "要清理此项目中使用的所有Google Cloud资源，您可以删除用于教程的[Google Cloud项目](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects)。\n",
    "\n",
    "否则，您可以删除在本教程中创建的个别资源：\n",
    "\n",
    "- Cloud Storage Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "laAQFM4aoBm3"
   },
   "outputs": [],
   "source": [
    "delete_bucket = False\n",
    "\n",
    "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
    "    ! gsutil rm -r $BUCKET_URI"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "get_started_with_hpt_pipeline_components.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
