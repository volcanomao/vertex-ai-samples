{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "copyright"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title:generic,gcp"
   },
   "source": [
    "# 开始学习使用BigQuery数据集\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/datasets/get_started_bq_datasets.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      在GitHub上查看\n",
    "    </a>\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/datasets/get_started_bq_datasets.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> 在Colab中运行\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/datasets/get_started_bq_datasets.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      在Vertex AI工作台中打开\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "<br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "overview:mlops"
   },
   "source": [
    "## 概述\n",
    "\n",
    "本教程演示了如何在生产环境中使用Vertex AI。本教程涵盖了数据管理：从BigQuery数据集开始入手。\n",
    "\n",
    "Learn more about [BigQuery Datasets](https://cloud.google.com/bigquery/docs/datasets-intro) and [Vertex AI for BigQuery users](https://cloud.google.com/vertex-ai/docs/beginner/bqml)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "objective:mlops,stage1,get_started_bq"
   },
   "source": [
    "### 目标\n",
    "\n",
    "在本教程中，您将学习如何将`BigQuery`用作`Vertex AI`训练的数据集。\n",
    "\n",
    "本教程使用以下Google Cloud ML服务：\n",
    "\n",
    "- `Vertex AI数据集`\n",
    "- `BigQuery数据集`\n",
    "\n",
    "执行的步骤包括：\n",
    "\n",
    "- 从`BigQuery`表创建一个`Vertex AI数据集`资源-- 适用于`AutoML`训练。\n",
    "- 将数据集从`BigQuery`提取到Cloud Storage中的CSV文件-- 适用于`AutoML`或自定义训练。\n",
    "- 从`BigQuery`数据集选择行到`pandas` dataframe-- 适用于自定义训练。\n",
    "- 从`BigQuery`数据集选择行到`tf.data.Dataset`-- 适用于自定义训练`TensorFlow`模型。\n",
    "- 从提取的CSV文件中选择行到`tf.data.Dataset`-- 适用于自定义训练`TensorFlow`模型。\n",
    "- 从CSV文件创建一个`BigQuery`数据集。\n",
    "- 从`BigQuery`表提取数据到`DMatrix`-- 适用于自定义训练`XGBoost`模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "recommendation:mlops,stage1,tabular,bq"
   },
   "source": [
    "### 推荐\n",
    "\n",
    "在Google Cloud上进行端到端（E2E）MLOps时，以下是在BigQuery中处理结构化（表格）数据时的最佳实践：\n",
    "\n",
    "- 对于AutoML训练：\n",
    "  - 使用Vertex AI的 `TabularDataset` 创建一个托管数据集。\n",
    "  - 将BigQuery表用作数据集的输入。\n",
    "  - 在运行AutoML训练管道作业时，指定列和列转换。\n",
    "\n",
    "\n",
    "- 对于自定义训练：\n",
    "  - 对于小型数据集：\n",
    "    - 将BigQuery提取到一个pandas数据框中。\n",
    "    - 在数据框中预处理数据。\n",
    "  - 对于大型数据集：\n",
    "    - TensorFlow模型训练：\n",
    "      - 从BigQuery表创建一个tf.data.Dataset生成器。\n",
    "      - 指定自定义训练的列。\n",
    "      - 对数据进行预处理：\n",
    "        - 在生成器中（上游）\n",
    "        - 在模型中（下游）\n",
    "    - XGBoost模型训练：\n",
    "      - 使用BigQuery ML内置的XGBoost训练。\n",
    "      - 或者，从BigQuery表中提取的CSV文件创建一个DMatrix生成器。\n",
    "    - PyTorch模型训练：\n",
    "        - 将BigQuery提取到一个pandas数据框中。\n",
    "        - 在数据框中预处理数据。\n",
    "        - 从pandas数据框中创建一个DataLoader生成器。\n",
    "\n",
    "\n",
    "- 或者：\n",
    "    - 将BigQuery表提取为CSV文件。\n",
    "    - 预处理CSV文件。\n",
    "    - 从CSV文件创建一个tf.data.Dataset生成器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset:gsod,lrg"
   },
   "source": [
    "### 数据集\n",
    "\n",
    "本教程使用的数据集是来自[BigQuery公共数据集](https://cloud.google.com/bigquery/public-data)的GSOD数据集。在这个数据集的版本中，您考虑年份、月份和日期字段来预测平均每日温度（mean_temp）的值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9e483012a752"
   },
   "source": [
    "### 成本\n",
    "该教程使用 Google Cloud 的可计费组件：\n",
    "\n",
    "- Vertex AI\n",
    "- Cloud Storage\n",
    "- BigQuery\n",
    "\n",
    "了解 [Vertex AI 价格](https://cloud.google.com/vertex-ai/pricing)，[Cloud Storage 价格](https://cloud.google.com/storage/pricing) 和 [BigQuery 价格](https://cloud.google.com/bigquery/pricing)，并使用 [定价计算器](https://cloud.google.com/products/calculator/) 基于您的预计使用情况生成成本估算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_mlops"
   },
   "source": [
    "## 安装\n",
    "\n",
    "安装以下包以执行此笔记本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_mlops"
   },
   "outputs": [],
   "source": [
    "! pip3 install --upgrade --quiet google-cloud-aiplatform \\\n",
    "                                 google-cloud-bigquery \\\n",
    "                                 tensorflow \\\n",
    "                                 tensorflow-io==0.18 \\\n",
    "                                 xgboost \\\n",
    "                                 numpy \\\n",
    "                                 pandas \\\n",
    "                                 pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "restart"
   },
   "source": [
    "### 仅限于 Colab：取消注释以下单元格以重新启动内核"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D-ZBOjErv5mM"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs so that your environment can access the new packages\n",
    "# import IPython\n",
    "\n",
    "# app = IPython.Application.instance()\n",
    "# app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yfEglUHQk9S3"
   },
   "source": [
    "## 在开始之前\n",
    "\n",
    "### 设置您的项目ID\n",
    "\n",
    "**如果您不知道您的项目ID**，请尝试以下操作：\n",
    "* 运行 `gcloud config list`。\n",
    "* 运行 `gcloud projects list`。\n",
    "* 查看支持页面：[查找项目ID](https://support.google.com/googleapi/answer/7014113)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_project_id"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
    "\n",
    "# Set the project id\n",
    "! gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "区域\n",
    "\n",
    "您还可以更改 Vertex AI 使用的 `REGION` 变量。了解有关[Vertex AI 区域](https://cloud.google.com/vertex-ai/docs/general/locations)的更多信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "region"
   },
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcp_authenticate"
   },
   "source": [
    "### 认证您的 Google Cloud 账户\n",
    "\n",
    "根据您的 Jupyter 环境，您可能需要手动认证。请按照以下相关说明操作。\n",
    "\n",
    "**1. Vertex AI Workbench**\n",
    "* 由于您已经过身份验证，所以无需执行任何操作。\n",
    "\n",
    "**2. 本地 JupyterLab 实例，请取消注释并运行：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ce6043da7b33"
   },
   "outputs": [],
   "source": [
    "# ! gcloud auth login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0367eac06a10"
   },
   "source": [
    "3. 协作，取消注释并运行:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "21ad4dbb4a61"
   },
   "outputs": [],
   "source": [
    "# from google.colab import auth\n",
    "# auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c13224697bfb"
   },
   "source": [
    "请查看如何在https://cloud.google.com/storage/docs/gsutil/commands/iam#ch-examples为您的服务帐号授予云存储权限。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bucket:mbsdk"
   },
   "source": [
    "创建一个云存储桶\n",
    "\n",
    "创建一个存储桶，用于存储中间产物，如数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bucket"
   },
   "outputs": [],
   "source": [
    "BUCKET_URI = f\"gs://your-bucket-name-{PROJECT_ID}-unique\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_bucket"
   },
   "source": [
    "只有如果您的存储桶尚不存在：才运行下面的单元格以创建您的云存储存储桶。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_bucket"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_vars"
   },
   "source": [
    "### 导入库并定义常量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aiplatform\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk,region"
   },
   "source": [
    "### 初始化用于 Python 的 Vertex AI SDK\n",
    "\n",
    "为您的项目和对应的存储桶初始化 Python 版的 Vertex AI SDK。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_aip:mbsdk,region"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_bq"
   },
   "source": [
    "创建BigQuery客户端。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_bq"
   },
   "outputs": [],
   "source": [
    "bqclient = bigquery.Client(project=PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_file:u_dataset,bq"
   },
   "source": [
    "请设置变量`IMPORT_FILE`为BigQuery中数据表的位置，并设置`BQ_TABLE`为表ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_file:gsod,bq,lrg"
   },
   "outputs": [],
   "source": [
    "IMPORT_FILE = \"bq://bigquery-public-data.samples.gsod\"\n",
    "BQ_TABLE = \"bigquery-public-data.samples.gsod\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_dataset:tabular,bq,lrg"
   },
   "source": [
    "### 创建数据集\n",
    "\n",
    "#### BigQuery 输入数据\n",
    "\n",
    "接下来，使用 `TabularDataset` 类的 `create` 方法创建 `Dataset` 资源，该方法接受以下参数：\n",
    "\n",
    "- `display_name`：`Dataset` 资源的人类可读的名称。\n",
    "- `bq_source`：将数据项从 BigQuery 表导入到 `Dataset` 资源中。\n",
    "- `labels`：用户定义的元数据。在此示例中，您将存储包含用户定义数据的 Cloud Storage 存储桶的位置。\n",
    "\n",
    "了解有关从 BigQuery 表创建 TabularDataset 的更多信息，请访问[此处](https://cloud.google.com/vertex-ai/docs/datasets/create-dataset-api#aiplatform_create_dataset_tabular_bigquery_sample-python)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_dataset:tabular,bq,lrg"
   },
   "outputs": [],
   "source": [
    "dataset = aiplatform.TabularDataset.create(\n",
    "    display_name=\"NOAA historical weather data\",\n",
    "    bq_source=[IMPORT_FILE],\n",
    "    labels={\"user_metadata\": BUCKET_URI[5:]},\n",
    ")\n",
    "\n",
    "label_column = \"mean_temp\"\n",
    "\n",
    "print(dataset.resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bq_extract"
   },
   "source": [
    "### 将数据集复制到云存储\n",
    "\n",
    "接下来，您可以使用BigQuery提取命令将BigQuery表复制为CSV文件，保存到云存储中。\n",
    "\n",
    "了解更多关于[BigQuery命令行界面](https://cloud.google.com/bigquery/docs/reference/bq-cli-reference)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bq_extract"
   },
   "outputs": [],
   "source": [
    "comps = BQ_TABLE.split(\".\")\n",
    "BQ_PROJECT_DATASET_TABLE = comps[0] + \":\" + comps[1] + \".\" + comps[2]\n",
    "\n",
    "! bq --location=us extract --destination_format CSV $BQ_PROJECT_DATASET_TABLE $BUCKET_URI/mydata*.csv\n",
    "\n",
    "IMPORT_FILES = ! gsutil ls $BUCKET_URI/mydata*.csv\n",
    "\n",
    "print(IMPORT_FILES)\n",
    "\n",
    "EXAMPLE_FILE = IMPORT_FILES[0]\n",
    "\n",
    "! gsutil cat $EXAMPLE_FILE | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_dataset:tabular,lrg"
   },
   "source": [
    "### 创建数据集\n",
    "\n",
    "#### CSV 输入数据\n",
    "\n",
    "接下来，使用 `TabularDataset` 类的 `create` 方法创建 `Dataset` 资源，其参数如下：\n",
    "\n",
    "- `display_name`：`Dataset` 资源的可读名称。\n",
    "- `gcs_source`：一个或多个数据集索引文件的列表，用于将数据项导入 `Dataset` 资源。\n",
    "- `labels`：用户定义的元数据。在本例中，您存储包含用户定义数据的 Cloud Storage 存储桶的位置。\n",
    "\n",
    "了解更多关于[从 CSV 文件创建 TabularDataset](https://cloud.google.com/vertex-ai/docs/datasets/create-dataset-api#aiplatform_create_dataset_tabular_gcs_sample-python)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_dataset:tabular,lrg"
   },
   "outputs": [],
   "source": [
    "gcs_source = IMPORT_FILES\n",
    "\n",
    "dataset = aiplatform.TabularDataset.create(\n",
    "    display_name=\"NOAA historical weather data\",\n",
    "    gcs_source=gcs_source,\n",
    "    labels={\"user_metadata\": BUCKET_URI[5:]},\n",
    ")\n",
    "\n",
    "\n",
    "label_column = \"mean_temp\"\n",
    "\n",
    "print(dataset.resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bq_view"
   },
   "source": [
    "### 创建 BigQuery 数据集的视图\n",
    "\n",
    "或者，您可以创建一个逻辑视图，其中包含 BigQuery 数据集的一部分字段。\n",
    "\n",
    "了解更多关于[创建 BigQuery 视图](https://cloud.google.com/bigquery/docs/views)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7dc142433e50"
   },
   "outputs": [],
   "source": [
    "# Set dataset name and view name in BigQuery\n",
    "BQ_MY_DATASET = \"[your-dataset-name]\"\n",
    "BQ_MY_TABLE = \"[your-view-name]\"\n",
    "\n",
    "# Otherwise, use the default names\n",
    "if (\n",
    "    BQ_MY_DATASET == \"\"\n",
    "    or BQ_MY_DATASET is None\n",
    "    or BQ_MY_DATASET == \"[your-dataset-name]\"\n",
    "):\n",
    "    BQ_MY_DATASET = \"mlops_dataset\"\n",
    "\n",
    "if BQ_MY_TABLE == \"\" or BQ_MY_TABLE is None or BQ_MY_TABLE == \"[your-view-name]\":\n",
    "    BQ_MY_TABLE = \"mlops_view\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bq_view"
   },
   "outputs": [],
   "source": [
    "# Create the resources\n",
    "! bq --location=US mk -d \\\n",
    "$PROJECT_ID:$BQ_MY_DATASET\n",
    "\n",
    "sql_script = f'''\n",
    "CREATE OR REPLACE VIEW `{PROJECT_ID}.{BQ_MY_DATASET}.{BQ_MY_TABLE}`\n",
    "AS SELECT station_number,year,month,day,mean_temp FROM `{BQ_TABLE}`\n",
    "'''\n",
    "print(sql_script)\n",
    "\n",
    "query = bqclient.query(sql_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bq_to_dataframe:gsod"
   },
   "source": [
    "将BigQuery数据集读入pandas dataframe中\n",
    "\n",
    "接下来，您可以使用BigQuery的`list_rows()`和`to_dataframe()`方法，将数据集的样本读入pandas dataframe，如下所示：\n",
    "\n",
    "- `list_rows()`: 对指定的表执行查询，并返回查询结果的行迭代器。可以选择指定：\n",
    "  - `selected_fields`: 返回的字段（列）的子集。\n",
    "  - `max_results`: 要返回的最大行数。与SQL LIMIT命令相同。\n",
    "\n",
    "- `rows.to_dataframe()`: 调用行迭代器，将数据读入pandas dataframe中。\n",
    "\n",
    "了解更多关于[将BigQuery表加载到dataframe中](https://cloud.google.com/bigquery/docs/bigquery-storage-python-pandas)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bq_to_dataframe:gsod"
   },
   "outputs": [],
   "source": [
    "# Download the table.\n",
    "table = bigquery.TableReference.from_string(BQ_TABLE)\n",
    "\n",
    "rows = bqclient.list_rows(\n",
    "    table,\n",
    "    max_results=500,\n",
    "    selected_fields=[\n",
    "        bigquery.SchemaField(\"station_number\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"year\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"month\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"day\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"mean_temp\", \"FLOAT\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "dataframe = rows.to_dataframe()\n",
    "print(dataframe.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bq_to_dataset:gsod"
   },
   "source": [
    "### 将BigQuery数据集读入tf.data.Dataset\n",
    "\n",
    "接下来，您可以使用TensorFlow IO的`BigQueryClient()`和`read_session()`方法将数据集的样本读入tf.data.Dataset，并使用以下参数:\n",
    "\n",
    "- `parent`: 您的项目 ID。\n",
    "- `project_id`: BigQuery 表的项目 ID。\n",
    "- `dataset_id`: BigQuery 数据集的 ID。\n",
    "- `table_id`: 相应 BigQuery 数据集中的表的 ID。\n",
    "- `selected_fields`: 要返回的字段（列）的子集。\n",
    "- `output_types`: 相应字段的输出类型。\n",
    "- `requested_streams`: 并行读取器的数量。\n",
    "\n",
    "了解更多关于[BigQuery TensorFlow reader](https://www.tensorflow.org/io/tutorials/bigquery)的信息。\n",
    "\n",
    "了解更多关于[tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)的信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bq_to_dataset:gsod"
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow_io.bigquery import BigQueryClient\n",
    "\n",
    "feature_names = \"station_number,year,month,day\".split(\",\")\n",
    "\n",
    "target_name = \"mean_temp\"\n",
    "\n",
    "\n",
    "def read_bigquery(project, dataset, table):\n",
    "    tensorflow_io_bigquery_client = BigQueryClient()\n",
    "    read_session = tensorflow_io_bigquery_client.read_session(\n",
    "        parent=\"projects/\" + PROJECT_ID,\n",
    "        project_id=project,\n",
    "        dataset_id=dataset,\n",
    "        table_id=table,\n",
    "        selected_fields=feature_names + [target_name],\n",
    "        output_types=[dtypes.string] + [dtypes.int32] * 3 + [dtypes.float32],\n",
    "        requested_streams=2,\n",
    "    )\n",
    "\n",
    "    dataset = read_session.parallel_read_rows()\n",
    "    return dataset\n",
    "\n",
    "\n",
    "PROJECT, DATASET, TABLE = IMPORT_FILE.split(\"/\")[-1].split(\".\")\n",
    "tf_dataset = read_bigquery(PROJECT, DATASET, TABLE)\n",
    "\n",
    "print(tf_dataset.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "csv_to_dataset:gsod"
   },
   "source": [
    "将CSV文件读入tf.data.Dataset\n",
    "\n",
    "或者，当您的数据存储在CSV文件中时，您可以使用`tf.data.experimental.CsvDataset`将数据集加载到tf.data.Dataset中，使用以下参数：\n",
    "\n",
    "- `filenames`：一个或多个CSV文件的列表。\n",
    "- `header`：CSV文件是否包含标题。\n",
    "- `select_cols`：要返回的字段（列）的子集。\n",
    "- `record_defaults`：相应字段的输出类型。\n",
    "\n",
    "了解更多关于 [tf.data CsvDataset](https://www.tensorflow.org/api_docs/python/tf/data/experimental/CsvDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "csv_to_dataset:gsod"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "feature_names = [\"station_number,year,month,day\".split(\",\")]\n",
    "\n",
    "target_name = \"mean_temp\"\n",
    "\n",
    "tf_dataset = tf.data.experimental.CsvDataset(\n",
    "    filenames=IMPORT_FILES,\n",
    "    header=True,\n",
    "    select_cols=feature_names.append(target_name),\n",
    "    record_defaults=[dtypes.string] + [dtypes.int32] * 3 + [dtypes.float32],\n",
    ")\n",
    "\n",
    "print(tf_dataset.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataframe_to_bq"
   },
   "source": [
    "### 从pandas dataframe创建一个BigQuery数据集\n",
    "\n",
    "您可以使用BigQuery的`create_dataset()`和`load_table_from_dataframe()`方法从pandas dataframe创建一个BigQuery数据集，具体步骤如下：\n",
    "\n",
    "- `create_dataset()`: 创建一个空的BigQuery数据集，有以下参数：\n",
    "  - `dataset_ref`: 从数据集ID创建的`DatasetReference`，例如samples。\n",
    "- `load_table_from_dataframe()`: 将一个或多个CSV文件加载到相应数据集中的表中，具有以下参数：\n",
    "  - `dataframe`: dataframe。\n",
    "  - `table`: 表格的`TableReference`。\n",
    "  - `job_config`: 指定如何加载dataframe数据的规范。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dataframe_to_bq"
   },
   "outputs": [],
   "source": [
    "LOCATION = \"us\"\n",
    "\n",
    "SCHEMA = [\n",
    "    bigquery.SchemaField(\"station_number\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"year\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"month\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"day\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"mean_temp\", \"FLOAT\"),\n",
    "]\n",
    "\n",
    "\n",
    "DATASET_ID = \"samples\"\n",
    "TABLE_ID = \"gsod\"\n",
    "\n",
    "\n",
    "def create_bigquery_dataset(dataset_id):\n",
    "    dataset = bigquery.Dataset(\n",
    "        bigquery.dataset.DatasetReference(PROJECT_ID, dataset_id)\n",
    "    )\n",
    "    dataset.location = \"us\"\n",
    "\n",
    "    try:\n",
    "        dataset = bqclient.create_dataset(dataset)  # API request\n",
    "        return True\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "        if err.code != 409:  # http_client.CONFLICT\n",
    "            raise\n",
    "    return False\n",
    "\n",
    "\n",
    "def load_data_into_bigquery(dataframe, dataset_id, table_id):\n",
    "    create_bigquery_dataset(dataset_id)\n",
    "    dataset = bqclient.dataset(dataset_id)\n",
    "    table = dataset.table(table_id)\n",
    "\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        # Specify a (partial) schema. All columns are always written to the\n",
    "        # table. The schema is used to assist in data type definitions.\n",
    "        schema=[\n",
    "            bigquery.SchemaField(\"station_number\", \"STRING\"),\n",
    "            bigquery.SchemaField(\"year\", \"INTEGER\"),\n",
    "            bigquery.SchemaField(\"month\", \"INTEGER\"),\n",
    "            bigquery.SchemaField(\"day\", \"INTEGER\"),\n",
    "            bigquery.SchemaField(\"mean_temp\", \"FLOAT\"),\n",
    "        ],\n",
    "        # Optionally, set the write disposition. BigQuery appends loaded rows\n",
    "        # to an existing table by default, but with WRITE_TRUNCATE write\n",
    "        # disposition it replaces the table with the loaded data.\n",
    "        write_disposition=\"WRITE_TRUNCATE\",\n",
    "    )\n",
    "\n",
    "    NEW_BQ_TABLE = f\"{PROJECT_ID}.{dataset_id}.{table_id}\"\n",
    "\n",
    "    job = bqclient.load_table_from_dataframe(\n",
    "        dataframe, NEW_BQ_TABLE, job_config=job_config\n",
    "    )  # Make an API request.\n",
    "    job.result()  # Wait for the job to complete.\n",
    "\n",
    "    table = bqclient.get_table(NEW_BQ_TABLE)  # Make an API request.\n",
    "    print(\n",
    "        \"Loaded {} rows and {} columns to {}\".format(\n",
    "            table.num_rows, len(table.schema), NEW_BQ_TABLE\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "load_data_into_bigquery(dataframe, DATASET_ID, TABLE_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "csv_to_bq"
   },
   "source": [
    "### 从CSV文件创建一个BigQuery数据集\n",
    "\n",
    "您可以使用BigQuery的`create_dataset()`和`load_table_from_uri()`方法从CSV文件中创建一个BigQuery数据集，具体步骤如下：\n",
    "\n",
    "- `create_dataset()`: 创建一个空的BigQuery数据集，使用以下参数：\n",
    "  - `dataset_ref`: 从dataset_id创建的`DatasetReference`，例如samples。\n",
    "- `load_table_from_uri()`: 将一个或多个CSV文件加载到相应数据集中的表中，使用以下参数：\n",
    "  - `url`: 一个或多个Cloud Storage存储中的CSV文件集合。\n",
    "  - `table`: 表的`TableReference`。\n",
    "  - `job_config`: 有关如何加载CSV数据的规范。\n",
    "\n",
    "了解更多关于[将CSV数据导入BigQuery](https://www.tensorflow.org/io/tutorials/bigquery#import_census_data_into_bigquery)的信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "csv_to_bq"
   },
   "outputs": [],
   "source": [
    "LOCATION = \"us\"\n",
    "\n",
    "CSV_SCHEMA = [\n",
    "    bigquery.SchemaField(\"station_number\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"wban_number\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"year\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"month\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"day\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"mean_temp\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"num_mean_temp_samples\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"mean_dew_point\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"num_mean_dew_point_samples\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"mean_sealevel_pressure\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"num_mean_sealevel_pressure_samples\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"mean_station_pressure\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"num_mean_station_pressure_samples\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"mean_visibility\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"num_mean_visibility_samples\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"mean_wind_speed\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"num_mean_wind_speed_samples\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"max_sustained_wind_speed\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"max_gust_wind_speed\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"max_temperature\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"max_temperature_explicit\", \"BOOLEAN\"),\n",
    "    bigquery.SchemaField(\"min_temperature\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"min_temperature_explicit\", \"BOOLEAN\"),\n",
    "    bigquery.SchemaField(\"total_percipitation\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"snow_depth\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"fog\", \"BOOLEAN\"),\n",
    "    bigquery.SchemaField(\"rain\", \"BOOLEAN\"),\n",
    "    bigquery.SchemaField(\"snow\", \"BOOLEAN\"),\n",
    "    bigquery.SchemaField(\"hail\", \"BOOLEAN\"),\n",
    "    bigquery.SchemaField(\"thunder\", \"BOOLEAN\"),\n",
    "    bigquery.SchemaField(\"tornado\", \"BOOLEAN\"),\n",
    "]\n",
    "\n",
    "\n",
    "DATASET_ID = \"samples\"\n",
    "TABLE_ID = \"gsod\"\n",
    "\n",
    "\n",
    "def load_data_into_bigquery(url, dataset_id, table_id):\n",
    "    create_bigquery_dataset(dataset_id)\n",
    "    dataset = bqclient.dataset(dataset_id)\n",
    "    table = dataset.table(table_id)\n",
    "\n",
    "    job_config = bigquery.LoadJobConfig()\n",
    "    job_config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE\n",
    "    job_config.source_format = bigquery.SourceFormat.CSV\n",
    "    job_config.schema = CSV_SCHEMA\n",
    "    job_config.skip_leading_rows = 1  # heading\n",
    "\n",
    "    load_job = bqclient.load_table_from_uri(url, table, job_config=job_config)\n",
    "    print(\"Starting job {}\".format(load_job.job_id))\n",
    "\n",
    "    load_job.result()  # Waits for table load to complete.\n",
    "    print(\"Job finished.\")\n",
    "\n",
    "    destination_table = bqclient.get_table(table)\n",
    "    print(\"Loaded {} rows.\".format(destination_table.num_rows))\n",
    "\n",
    "\n",
    "load_data_into_bigquery(IMPORT_FILES, DATASET_ID, TABLE_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bq_to_xgboost"
   },
   "source": [
    "### 将BigQuery表读入XGboost DMatrix\n",
    "\n",
    "目前，BigQuery和开源XGBoost之间没有直接的数据传输连接。BigQuery ML服务具有内置的XGBoost训练模块。\n",
    "\n",
    "或者，您可以将数据提取为pandas dataframe或CSV文件。提取的数据然后在训练模型时作为`DMatrix`对象的输入。\n",
    "\n",
    "了解更多关于[如何开始使用内置XGBoost](https://cloud.google.com/ai-platform/training/docs/algorithms/xgboost-start)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pandas_to_xgboost:gsod"
   },
   "source": [
    "### 将pandas表读入XGboost DMatrix\n",
    "\n",
    "接下来，您将pandas数据框加载到`DMatrix`对象中。XGBoost不支持非数值输入。在加载数据框之前，任何分类列都需要进行独热编码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pandas_to_xgboost:gsod"
   },
   "outputs": [],
   "source": [
    "dataframe[\"station_number\"] = pd.to_numeric(dataframe[\"station_number\"])\n",
    "labels = dataframe[\"mean_temp\"]\n",
    "data = dataframe.drop([\"mean_temp\"], axis=1)\n",
    "\n",
    "dtrain = xgb.DMatrix(data, label=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "csv_to_xgboost:gsod"
   },
   "source": [
    "### 将CSV文件读入XGboost DMatrix\n",
    "\n",
    "目前，XGBoost不支持云存储。如果要使用CSV文件作为输入，您需要将它们下载到本地。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "csv_to_xgboost:gsod"
   },
   "outputs": [],
   "source": [
    "! gsutil cp $EXAMPLE_FILE data.csv\n",
    "\n",
    "dtrain = xgb.DMatrix(\"data.csv?format=csv&label_column=4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleanup:mbsdk"
   },
   "source": [
    "清理\n",
    "\n",
    "要清理此项目中使用的所有谷歌云资源，您可以[删除用于教程的谷歌云项目](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects)。\n",
    "\n",
    "否则，您可以删除此教程中创建的各个资源：\n",
    "\n",
    "- Vertex AI数据集资源\n",
    "- 云存储桶\n",
    "- BigQuery数据集\n",
    "\n",
    "将`delete_storage`设置为_True_以删除在此笔记本中使用的存储资源。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "47ad926d84e8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Delete the dataset using the Vertex dataset object\n",
    "dataset.delete()\n",
    "\n",
    "# Delete the temporary BigQuery dataset\n",
    "! bq rm -r -f $PROJECT_ID:$DATASET_ID\n",
    "\n",
    "delete_storage = False\n",
    "if delete_storage or os.getenv(\"IS_TESTING\"):\n",
    "    # Delete the created GCS bucket\n",
    "    ! gsutil rm -r $BUCKET_URI\n",
    "    # Delete the created BigQuery datasets\n",
    "    ! bq rm -r -f $PROJECT_ID:$BQ_MY_DATASET"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "get_started_bq_datasets.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
