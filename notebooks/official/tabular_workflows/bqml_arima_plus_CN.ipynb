{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "copyright"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "使用Vertex AI Tabular Workflows训练BigQuery ML ARIMA_PLUS模型\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/tabular_workflows/bqml_arima_plus.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> 在Colab中运行\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/tabular_workflows/bqml_arima_plus.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      查看GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/tabular_workflows/bqml_arima_plus.ipynb\">\n",
    "        <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      在Vertex AI工作台中打开\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "<br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "overview:automl"
   },
   "source": [
    "## 概述\n",
    "\n",
    "在本教程中，您将扮演一位商店规划师的角色，必须确定他们将需要为2019年11月的每个产品和商店订购多少库存。您可以通过训练一个BigQuery ML（BQML）[ARIMA_PLUS](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-create-time-series)预测模型来实现这一目标，使用历史销售数据。如果您需要执行多个模型训练的快速迭代，或者需要一个廉价的基准来衡量其他模型，那么BQML ARIMA_PLUS模型将会很有用。\n",
    "\n",
    "了解更多关于[BQML ARIMA+用于表格数据的预测](https://cloud.google.com/vertex-ai/docs/tabular-data/forecasting-arima/overview)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "objective:automl,training,batch_prediction"
   },
   "source": [
    "### 目标\n",
    "\n",
    "在本教程中，您将学习如何使用[Google Cloud Pipeline Components](https://cloud.google.com/vertex-ai/docs/pipelines/components-introduction)（GCPC）中的训练[Vertex AI Pipeline](https://cloud.google.com/vertex-ai/docs/pipelines/introduction)来创建一个 BigQuery ML ARIMA_PLUS 模型，然后使用相应的预测管道进行批量预测。然后，您将使用相同数据训练一个 Vertex AI 预测模型，并比较评估指标。\n",
    "\n",
    "本教程使用以下 Google Cloud ML 服务和资源：\n",
    "\n",
    "- BigQuery\n",
    "- Vertex AI\n",
    "\n",
    "执行的步骤包括：\n",
    "\n",
    "- 训练 BigQuery ML ARIMA_PLUS 模型。\n",
    "- 查看 BigQuery ML 模型评估。\n",
    "- 使用 BigQuery ML 模型进行批量预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset:covid,forecast"
   },
   "source": [
    "### 数据集\n",
    "\n",
    "为了展示使用BigQuery ML和Vertex AI Forecasting之间的权衡，本教程将使用一个合成数据集，其中产品销售依赖于诸多因素，如广告、假期和位置。您可以看到像ARIMA_PLUS这样的单变量模型如何在不明确了解这些因素信息的情况下预测未来销售，以及当了解这些因素时，像Vertex AI Forecasting这样的多变量模型将如何执行。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "costs"
   },
   "source": [
    "###成本\n",
    "\n",
    "本教程使用Google Cloud的计费组件：\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "* BigQuery / BigQuery ML\n",
    "\n",
    "了解[Vertex AI定价](https://cloud.google.com/vertex-ai/pricing)、[Cloud Storage定价](https://cloud.google.com/storage/pricing)和[BigQuery定价](https://cloud.google.com/bigquery/pricing)，并使用[Pricing Calculator](https://cloud.google.com/products/calculator/)根据您的预期使用量生成成本估算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_aip:mbsdk"
   },
   "source": [
    "安装以下所需的软件包以执行此笔记本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pZCCaJsYQEH4"
   },
   "outputs": [],
   "source": [
    "! (pip3 install --upgrade --quiet \\\n",
    "    google-cloud-aiplatform==1.40.0 \\\n",
    "    google-cloud-bigquery[pandas]==3.17.1 \\\n",
    "    google-cloud-pipeline-components==2.9.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "restart"
   },
   "source": [
    "### 仅Colab使用：取消注释以下单元格以重新启动内核"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D-ZBOjErv5mM"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs so that your environment can access the new packages\n",
    "# import IPython\n",
    "\n",
    "# app = IPython.Application.instance()\n",
    "# app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "before_you_begin:nogpu"
   },
   "source": [
    "## 开始之前\n",
    "\n",
    "### 设置您的项目ID\n",
    "\n",
    "**如果您不知道您的项目ID**，请尝试以下操作：\n",
    "* 运行 `gcloud config list`。\n",
    "* 运行 `gcloud projects list`。\n",
    "* 查看支持页面：[查找项目ID](https://support.google.com/googleapi/answer/7014113)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_project_id"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
    "\n",
    "# Set the project id\n",
    "! gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "地区\n",
    "\n",
    "您还可以更改 Vertex AI 使用的 `REGION` 变量。了解有关[Vertex AI 地区](https://cloud.google.com/vertex-ai/docs/general/locations)的更多信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2dw8q9fdQEH5"
   },
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"\n",
    "DATA_REGION = \"US\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcp_authenticate"
   },
   "source": [
    "### 验证您的 Google Cloud 账户\n",
    "\n",
    "根据您的 Jupyter 环境，您可能需要手动进行身份验证。请按照以下相关说明操作。\n",
    "\n",
    "**1. Vertex AI Workbench**\n",
    "* 不需要做任何操作，因为您已经通过验证。\n",
    "\n",
    "**2. 本地 JupyterLab 实例，取消注释并运行：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ce6043da7b33"
   },
   "outputs": [],
   "source": [
    "# ! gcloud auth login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0367eac06a10"
   },
   "source": [
    "3. 合作，去除注释并运行:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "21ad4dbb4a61"
   },
   "outputs": [],
   "source": [
    "# from google.colab import auth\n",
    "# auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c13224697bfb"
   },
   "source": [
    "服务账号或其他\n",
    "* 请参考如何向您的服务账号授予云存储权限的方法，网址为：https://cloud.google.com/storage/docs/gsutil/commands/iam#ch-examples。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bucket:mbsdk"
   },
   "source": [
    "创建一个云存储桶\n",
    "\n",
    "创建一个存储桶来存储诸如数据集等中间产物。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bucket"
   },
   "outputs": [],
   "source": [
    "BUCKET_URI = \"gs://your-bucket-name-unique\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "autoset_bucket"
   },
   "source": [
    "只有在您的存储桶尚不存在时：运行下面的单元格以创建您的云存储存储桶。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "91c46850b49b"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l {REGION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85c4ecfd133a"
   },
   "source": [
    "服务账户\n",
    "\n",
    "您可以使用服务账户来创建Vertex AI管道作业。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "77b01a1fdbb4"
   },
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT = \"[your-service-account]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f936bebda2d4"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "if (\n",
    "    SERVICE_ACCOUNT == \"\"\n",
    "    or SERVICE_ACCOUNT is None\n",
    "    or SERVICE_ACCOUNT == \"[your-service-account]\"\n",
    "):\n",
    "    # Get your service account from gcloud\n",
    "    if not IS_COLAB:\n",
    "        shell_output = !gcloud auth list 2>/dev/null\n",
    "        SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
    "\n",
    "    else:  # IS_COLAB:\n",
    "        shell_output = ! gcloud projects describe  $PROJECT_ID\n",
    "        project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
    "        SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
    "\n",
    "    print(\"Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40ef6967cad3"
   },
   "source": [
    "在Vertex AI Pipelines中设置服务账号访问权限\n",
    "\n",
    "运行以下命令，将您的服务账号访问权限授予读取和写入之前步骤中创建的存储桶中的管道工件。您只需要针对每个服务账号运行此步骤一次。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f88cb0488c08"
   },
   "outputs": [],
   "source": [
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
    "\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_vars"
   },
   "source": [
    "导入库并定义常数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import urllib\n",
    "import uuid\n",
    "\n",
    "from google.cloud import aiplatform, bigquery\n",
    "from google_cloud_pipeline_components.v1.automl.forecasting import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk"
   },
   "source": [
    "初始化用于 Python 的 Vertex AI SDK\n",
    "\n",
    "为您的项目和相应的存储桶初始化 Python 的 Vertex AI SDK。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fm4Pyn1dQEH7"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QNozhksr4Mc-"
   },
   "source": [
    "定义训练和预测数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-TWLAziDiFrL"
   },
   "source": [
    "创建一个BigQuery数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KT1Ik7XqBsxv"
   },
   "outputs": [],
   "source": [
    "arima_dataset_name = \"forecasting_demo_arima\"\n",
    "arima_dataset_path = \".\".join([PROJECT_ID, arima_dataset_name])\n",
    "\n",
    "# Must be same region as TRAINING_DATASET_BQ_PATH.\n",
    "client = bigquery.Client(project=PROJECT_ID)\n",
    "bq_dataset_pre = bigquery.Dataset(arima_dataset_path)\n",
    "bq_dataset_pre.location = DATA_REGION\n",
    "try:\n",
    "    bq_dataset = client.create_dataset(bq_dataset_pre)\n",
    "except:\n",
    "    bq_dataset = client.get_dataset(bq_dataset_pre)\n",
    "print(f\"Created bigquery dataset {arima_dataset_path} in {DATA_REGION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_file:u_dataset,csv"
   },
   "source": [
    "### 在BigQuery中准备训练数据\n",
    "\n",
    "在训练模型之前，您必须首先生成我们的店铺销售数据集。这个数据集将包括多个产品和店铺，并且还将模拟因素如广告和假期效应。数据将被拆分为`TRAIN`、`VALIDATE`、`TEST`和`PREDICT`集，其中最后三个集都为1个月。\n",
    "\n",
    "#### 先定义将创建这个基本销售数据的子查询。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpJxVD8fo3Rj"
   },
   "outputs": [],
   "source": [
    "base_data_query = \"\"\"\n",
    "  WITH \n",
    "\n",
    "    -- Create time series for each product + store with some covariates.\n",
    "    time_series AS (\n",
    "      SELECT\n",
    "        CONCAT(\"id_\", store_id, \"_\", product_id) AS id,\n",
    "        CONCAT('store_', store_id) AS store,\n",
    "        CONCAT('product_', product_id) AS product,\n",
    "        date,\n",
    "        -- Advertise 1/100 products.\n",
    "        IF(\n",
    "          ABS(MOD(FARM_FINGERPRINT(CONCAT(product_id, date)), 100)) = 0,\n",
    "          1,\n",
    "          0\n",
    "        ) AS advertisement,\n",
    "        -- Mark Thanksgiving sales as holiday sales.\n",
    "        IF(\n",
    "          EXTRACT(DAYOFWEEK FROM date) = 6\n",
    "            AND EXTRACT(MONTH FROM date) = 11\n",
    "            AND EXTRACT(DAY FROM date) BETWEEN 23 AND 29,\n",
    "          1,\n",
    "          0\n",
    "        ) AS holiday,\n",
    "        -- Set when each data split ends.\n",
    "        CASE\n",
    "          WHEN date < '2019-09-01' THEN 'TRAIN'\n",
    "          WHEN date < '2019-10-01' THEN 'VALIDATE'\n",
    "          WHEN date < '2019-11-01' THEN 'TEST'\n",
    "          ELSE 'PREDICT'\n",
    "        END AS split,\n",
    "      -- Generate the sales with one SKU per date.\n",
    "      FROM\n",
    "        UNNEST(GENERATE_DATE_ARRAY('2017-01-01', '2019-12-01')) AS date\n",
    "      CROSS JOIN\n",
    "        UNNEST(GENERATE_ARRAY(0, 10)) AS product_id\n",
    "      CROSS JOIN\n",
    "        UNNEST(GENERATE_ARRAY(0, 3)) AS store_id  \n",
    "    ),\n",
    "    \n",
    "    -- Randomly determine factors that contribute to how syntheic sales are calculated. \n",
    "    time_series_sales_factors AS (\n",
    "      SELECT\n",
    "        *,\n",
    "        ABS(MOD(FARM_FINGERPRINT(product), 10)) AS product_factor,\n",
    "        ABS(MOD(FARM_FINGERPRINT(store), 10)) AS store_factor,\n",
    "        [1.6, 0.6, 0.8, 1.0, 1.2, 1.8, 2.0][\n",
    "          ORDINAL(EXTRACT(DAYOFWEEK FROM date))] AS day_of_week_factor,\n",
    "        1 +  SIN(EXTRACT(MONTH FROM date) * 2.0 * 3.14 / 24.0) AS month_factor,    \n",
    "        -- Advertised products have increased sales factors for 5 days.\n",
    "        CASE\n",
    "          WHEN LAG(advertisement, 0) OVER w = 1.0 THEN 1.2\n",
    "          WHEN LAG(advertisement, 1) OVER w = 1.0 THEN 1.8\n",
    "          WHEN LAG(advertisement, 2) OVER w = 1.0 THEN 2.4\n",
    "          WHEN LAG(advertisement, 3) OVER w = 1.0 THEN 3.0\n",
    "          WHEN LAG(advertisement, 4) OVER w = 1.0 THEN 1.4\n",
    "          ELSE 1.0\n",
    "        END AS advertisement_factor,\n",
    "        IF(holiday = 1.0, 2.0, 1.0) AS holiday_factor,\n",
    "        0.001 * ABS(MOD(FARM_FINGERPRINT(CONCAT(product, store, date)), 100)) AS noise_factor\n",
    "      FROM\n",
    "        time_series\n",
    "      WINDOW w AS (PARTITION BY id ORDER BY date)\n",
    "    ),\n",
    "  \n",
    "    -- Use factors to calculate synthetic sales for each time series. \n",
    "    base_data AS (\n",
    "      SELECT\n",
    "        id,\n",
    "        store,\n",
    "        product,\n",
    "        date,\n",
    "        split,\n",
    "        advertisement,\n",
    "        holiday,\n",
    "        (\n",
    "          (1 + store_factor) \n",
    "          * (1 + product_factor) \n",
    "          * (1 + month_factor + day_of_week_factor) \n",
    "          * (\n",
    "            1.0 \n",
    "            + 2.0 * advertisement_factor \n",
    "            + 3.0 * holiday_factor \n",
    "            + 5.0 * noise_factor\n",
    "          )\n",
    "        ) AS sales\n",
    "      FROM\n",
    "        time_series_sales_factors\n",
    "      )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IdWrtxYtqdsh"
   },
   "source": [
    "接下来，将这些基本销售数据转换为您用于训练模型的数据集，以及在提供服务时传递给经过训练的模型的数据集。训练数据集将包括“TRAIN”、“VALIDATE”和“TEST”拆分，而预测数据集将包括“PREDICT”拆分，同时还包括“TEST”拆分以提供上下文信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xvLshoaApviB"
   },
   "outputs": [],
   "source": [
    "TRAINING_DATASET_BQ_PATH = f\"{arima_dataset_path}.train\"\n",
    "PREDICTION_DATASET_BQ_PATH = f\"{arima_dataset_path}.pred\"\n",
    "\n",
    "train_query = f\"\"\"\n",
    "    CREATE OR REPLACE TABLE `{arima_dataset_path}.train` AS\n",
    "    {base_data_query}\n",
    "    SELECT *\n",
    "    FROM base_data\n",
    "    WHERE split != 'PREDICT'\n",
    "\"\"\"\n",
    "client.query(train_query).result()\n",
    "print(f\"Created {TRAINING_DATASET_BQ_PATH}.\")\n",
    "\n",
    "pred_query = f\"\"\"\n",
    "    CREATE OR REPLACE TABLE `{arima_dataset_path}.pred` AS\n",
    "    {base_data_query}\n",
    "    SELECT *\n",
    "    FROM base_data\n",
    "    WHERE split = 'TEST'\n",
    "\n",
    "    UNION ALL\n",
    "\n",
    "    SELECT * EXCEPT (sales), NULL AS sales\n",
    "    FROM base_data\n",
    "    WHERE split = 'PREDICT'\n",
    "\"\"\"\n",
    "client.query(pred_query).result()\n",
    "print(f\"Created {PREDICTION_DATASET_BQ_PATH}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gwr0eDQjpR4c"
   },
   "source": [
    "您可以查看生成的销售数据。在本教程的后续部分中，我们将展示时间序列以及我们的预测。\n",
    "\n",
    "该模型使用从2017年1月到2019年10月的数据进行训练。 \n",
    "\n",
    "#### 查看训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XyBgcsfZpREc"
   },
   "outputs": [],
   "source": [
    "query = f\"SELECT * FROM `{arima_dataset_path}.train` LIMIT 10\"\n",
    "client.query(query).to_dataframe().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F00dL8oEqqVb"
   },
   "source": [
    "用于预测的表格包含2019年11月的数据。它还包括2019年10月的实际数据作为背景信息。\n",
    "\n",
    "#### 查看预测数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yOPL6Cv4qIhZ"
   },
   "outputs": [],
   "source": [
    "query = f\"SELECT * FROM `{arima_dataset_path}.pred` LIMIT 10\"\n",
    "client.query(query).to_dataframe().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tutorial_start:automl"
   },
   "source": [
    "创建一个BigQuery ML ARIMA_PLUS模型\n",
    "\n",
    "现在，您可以开始创建自己的BigQuery ML ARIMA_PLUS模型了。\n",
    "\n",
    "与Vertex AI预测一样，您运行的流水线将使用训练和验证集训练评估模型，并使用回测在测试集上创建评估指标。最后，将生成一个服务模型，该模型使用所有可用数据。\n",
    "\n",
    "**如何估算成本？**\n",
    "\n",
    "回测涉及为测试集中的每个周期训练单个BigQuery ML模型，因此成本是测试集长度的函数，该长度是由窗口策略进行任何下采样后的结果。成本还会乘以训练的候选模型数量，这由`max_order`确定。\n",
    "\n",
    "根据[BQ定价](https://cloud.google.com/bigquery-ml/pricing)，BigQuery ML模型创建成本为每TB 250美元。我们将使用最大阶数为3，这在存在多个时间序列时等于20个候选模型。我们的演示数据集大小为3 MB，并包括31个测试周期。我们的窗口策略的步长为1，因此所有周期都用于评估。\n",
    "\n",
    "在本教程中，流水线的模型创建阶段成本为`3 MB * ($250 / 1024^2) * (31 / 1)周期 * 20个候选模型 = $0.44`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t04OJzrORmJ4"
   },
   "source": [
    "## 创建并运行训练作业\n",
    "使用ARIMA管道训练模型需要执行两个步骤：\n",
    "\n",
    "1. 从GCPC下载训练管道。\n",
    "2. 运行作业。\n",
    "\n",
    "#### 创建训练作业\n",
    "\n",
    "训练管道需要以下参数：\n",
    "\n",
    "- `bigquery_destination_uri`: （可选）BigQuery数据集URI。用于导出指标表和模型。如果未提供，将为用户创建一个。\n",
    "- `data_granularity_unit`: 用于指定时间粒度（小时，天，周，月等）的枚举。\n",
    "- `data_source_csv_filenames`或`data_source_bigquery_table_path`: 分别为存储在GCR中的CSV文件或BigQuery表的URI。\n",
    "- `evaluated_examples_destination_uri`: （可选）BigQuery数据集URI或表URI。用于导出评估示例表。如果未提供，将使用bigquery_destination_uri。\n",
    "- `forecast_horizon`: 预测的周期数。\n",
    "- 数据拆分策略为：\n",
    "   - `predefined_split_key`: 包含`TRAIN`、`VALIDATE`或`TEST`的列，用于表示每行的拆分。\n",
    "   - `training_fraction`、`validation_fraction`和`test_fraction`用于设置在时间列上按时间顺序拆分的比例。\n",
    "   - `timestamp_split_key`加上先前选项中的比例，用于在其它列上执行分数拆分。\n",
    "- 窗口策略为：\n",
    "   - `window_column`: 一个布尔列，决定是否考虑每行在计算评估指标时。\n",
    "   - `window_stride_length`: 每N行将用于计算评估指标。\n",
    "   - `window_max_count`: 降序行，只有给定数量的行用于计算评估指标。\n",
    "- `target_column`: 目标列的名称。\n",
    "- `time_column`: 时间列的名称。\n",
    "- `time_series_identifier_column`: id列的名称。\n",
    "- `max_order`: 介于1到5之间的整数，表示ARIMA_PLUS参数搜索空间的大小。5会得到最高精度的模型，但训练时间/成本也会最长。\n",
    "\n",
    "有关完整参数列表，请参阅GCPC SDK[文档](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-2.9.0/api/v1/automl/forecasting.html#v1.automl.forecasting.get_bqml_arima_train_pipeline_and_parameters)。\n",
    "\n",
    "训练管道的执行可能需要约**20分钟**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qYaWuqQF4mzZ"
   },
   "outputs": [],
   "source": [
    "time_column = \"date\"  # @param {type: \"string\"}\n",
    "time_series_identifier_column = \"id\"  # @param {type: \"string\"}\n",
    "target_column = \"sales\"  # @param {type: \"string\"}\n",
    "forecast_horizon = 30  # @param {type: \"integer\"}\n",
    "data_granularity_unit = \"day\"  # @param {type: \"string\"}\n",
    "split_column = \"split\"  # @param {type: \"string\"}\n",
    "window_stride_length = 1  # @param {type: \"integer\"}\n",
    "max_order = 3  # @param {type: \"integer\"}\n",
    "override_destination = True  # @param {type: \"boolean\"}\n",
    "\n",
    "(\n",
    "    train_job_spec_path,\n",
    "    train_parameter_values,\n",
    ") = utils.get_bqml_arima_train_pipeline_and_parameters(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    root_dir=os.path.join(BUCKET_URI, \"pipeline_root\"),\n",
    "    time_column=time_column,\n",
    "    time_series_identifier_column=time_series_identifier_column,\n",
    "    target_column=target_column,\n",
    "    forecast_horizon=forecast_horizon,\n",
    "    data_granularity_unit=data_granularity_unit,\n",
    "    predefined_split_key=split_column,\n",
    "    data_source_bigquery_table_path=TRAINING_DATASET_BQ_PATH,\n",
    "    window_stride_length=window_stride_length,\n",
    "    bigquery_destination_uri=arima_dataset_path,\n",
    "    override_destination=override_destination,\n",
    "    max_order=max_order,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c6RcJ1RS4xmY"
   },
   "source": [
    "运行培训管道\n",
    "\n",
    "使用Vertex AI Python SDK启动培训管道运行。一旦运行开始，以下单元格会输出一个链接，让您可以监视运行。链接应如下所示： \n",
    "\n",
    "`https://console.cloud.google.com/vertex-ai/locations/[REGION]/pipelines/runs/[DISPLAY_NAME]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yiwcDcmN44BG"
   },
   "outputs": [],
   "source": [
    "# The display name should be unique even if this cell is rerun.\n",
    "DISPLAY_NAME = f\"forecasting-demo-train-{str(uuid.uuid1())}\"\n",
    "\n",
    "job = aiplatform.PipelineJob(\n",
    "    job_id=DISPLAY_NAME,\n",
    "    display_name=DISPLAY_NAME,\n",
    "    pipeline_root=os.path.join(BUCKET_URI, DISPLAY_NAME),\n",
    "    template_path=train_job_spec_path,\n",
    "    parameter_values=train_parameter_values,\n",
    "    enable_caching=False,\n",
    ")\n",
    "job.run(service_account=SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O5t9pqcw3XDs"
   },
   "source": [
    "## 查看模型评估分数\n",
    "在模型训练完成后，您可以查看它的评估分数。\n",
    "\n",
    "#### 评估指标始终通过目标数据集中的`metrics`表报告。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qUFvUSax_Hz-"
   },
   "outputs": [],
   "source": [
    "for task_detail in job.gca_resource.job_detail.task_details:\n",
    "    if task_detail.task_name == \"create-metrics-artifact\":\n",
    "        metrics = task_detail.outputs[\"evaluation_metrics\"].artifacts[0].metadata\n",
    "        break\n",
    "else:\n",
    "    raise ValueError(\"Couldn't find the model evaluation task.\")\n",
    "\n",
    "print(\"Evaluation metrics:\\n\")\n",
    "dict(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jsRVi6AoiUIE"
   },
   "source": [
    "如果您想要计算自己的评估指标，您可以查看用于计算评估指标的预测。\n",
    "\n",
    "#### 查看用于计算评估指标的预测\n",
    "\n",
    "包含所有这些预测的表格称为“评估示例”。在这个表格中，每个不同的“predicted_on_date”代表一个预测窗口的开始期。回测指标利用所有这些窗口。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fLjRzPQRqhbo"
   },
   "outputs": [],
   "source": [
    "query = f\"SELECT * FROM `{arima_dataset_path}.evaluated_examples`\"\n",
    "arima_examples = client.query(query).to_dataframe()\n",
    "arima_examples.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NtoWftFI3ahi"
   },
   "source": [
    "## 创建和运行预测作业\n",
    "\n",
    "### 创建预测作业\n",
    "现在您的 Model 资源已经训练好，您可以使用预测流水线进行批量预测，以下是预测作业的参数：\n",
    "\n",
    "- `bigquery_destination_uri`: （可选）BigQuery 数据集 URI。用于导出指标表和模型。如果未提供，则我们将为用户创建一个。\n",
    "- `data_source_csv_filenames` 或 `data_source_bigquery_table_path`: 分别用于存储在 GCR 中的 CSV 或 BigQuery 表的 URI。\n",
    "- `generate_explanation`: 如果为 True，预测结果表将具有一些额外的 xAI 列。\n",
    "- `model_name`: 要用于预测的现有 BigQuery ML ARIMA_PLUS 模型的名称。\n",
    "\n",
    "有关完整参数列表，请查看 GCPC SDK [文档](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-2.9.0/api/v1/automl/forecasting.html#v1.automl.forecasting.get_bqml_arima_predict_pipeline_and_parameters)。\n",
    "\n",
    "执行预测流水线可能需要大约 **5 分钟**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lDO2aO8o_cwU"
   },
   "outputs": [],
   "source": [
    "# Get the model name programmatically, you can find this by looking at the\n",
    "# execution graph in Vertex AI Pipelines.\n",
    "for task_detail in job.gca_resource.job_detail.task_details:\n",
    "    if task_detail.task_name == \"bigquery-create-model-job\":\n",
    "        model_name = task_detail.outputs[\"model\"].artifacts[0].metadata[\"modelId\"]\n",
    "        break\n",
    "else:\n",
    "    raise ValueError(\"Couldn't find the model training task.\")\n",
    "\n",
    "\n",
    "(\n",
    "    predict_job_spec_path,\n",
    "    predict_parameter_values,\n",
    ") = utils.get_bqml_arima_predict_pipeline_and_parameters(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    model_name=f\"{arima_dataset_path}.{model_name}\",\n",
    "    data_source_bigquery_table_path=PREDICTION_DATASET_BQ_PATH,\n",
    "    bigquery_destination_uri=arima_dataset_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dXf2FyVs3fhW"
   },
   "source": [
    "运行预测流水线\n",
    "\n",
    "使用Vertex AI Python SDK启动预测流水线运行。一旦运行开始，下面的单元格将输出一个链接，允许您监视运行。链接应该看起来像这样：\n",
    "\n",
    "`https://console.cloud.google.com/vertex-ai/locations/[REGION]/pipelines/runs/[DISPLAY_NAME]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NhlOwoIQAKVx"
   },
   "outputs": [],
   "source": [
    "# The display name should be unique even if this cell is rerun.\n",
    "DISPLAY_NAME = f\"forecasting-demo-predict-{str(uuid.uuid1())}\"\n",
    "\n",
    "\n",
    "job = aiplatform.PipelineJob(\n",
    "    job_id=DISPLAY_NAME,\n",
    "    display_name=DISPLAY_NAME,\n",
    "    pipeline_root=os.path.join(BUCKET_URI, DISPLAY_NAME),\n",
    "    template_path=predict_job_spec_path,\n",
    "    parameter_values=predict_parameter_values,\n",
    "    enable_caching=False,\n",
    ")\n",
    "job.run(service_account=SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Vt4LAA2A4qo"
   },
   "source": [
    "获取预测结果\n",
    "\n",
    "接下来，从已完成的批量预测作业中获取结果。这些结果通常会被写入输出数据集中名为`predictions`的表中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v6FpuMHsA8Sz"
   },
   "outputs": [],
   "source": [
    "# Get the prediction table programmatically, you can find this by looking at the\n",
    "# execution graph in Vertex AI Pipelines.\n",
    "for task_detail in job.gca_resource.job_detail.task_details:\n",
    "    if task_detail.task_name == \"bigquery-query-job\":\n",
    "        pred_table = (\n",
    "            task_detail.outputs[\"destination_table\"].artifacts[0].metadata[\"tableId\"]\n",
    "        )\n",
    "        break\n",
    "else:\n",
    "    raise ValueError(\"Couldn't find the prediction task.\")\n",
    "\n",
    "query = f\"SELECT * FROM `{arima_dataset_path}.{pred_table}`\"\n",
    "arima_preds = client.query(query).to_dataframe()\n",
    "arima_preds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NvczFPe0kcgX"
   },
   "source": [
    "## 可视化预测\n",
    "\n",
    "最后，点击以下链接在[数据工作室](https://support.google.com/datastudio/answer/6283323?hl=en)中可视化生成的预测结果。\n",
    "本节中的代码块动态生成一个数据工作室链接，指定了模板、预测位置和生成图表的查询。数据是从之前生成的预测中填充的。\n",
    "\n",
    "您可以在https://datastudio.google.com/c/u/0/reporting/067f70d2-8cd6-4a4c-a099-292acd1053e8 查看使用的模板。这是谷歌专门创建的用于查看预测预测的模板。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ibEs8z38Q9k"
   },
   "outputs": [],
   "source": [
    "def _sanitize_bq_uri(bq_uri: str):\n",
    "    if bq_uri.startswith(\"bq://\"):\n",
    "        bq_uri = bq_uri[5:]\n",
    "    return bq_uri.replace(\":\", \".\")\n",
    "\n",
    "\n",
    "def get_data_studio_link(\n",
    "    batch_prediction_bq_input_uri: str,\n",
    "    batch_prediction_bq_output_uri: str,\n",
    "    time_column: str,\n",
    "    time_series_identifier_column: str,\n",
    "    target_column: str,\n",
    "):\n",
    "    \"\"\"Creates a link that fills in the demo Data Studio template.\"\"\"\n",
    "    batch_prediction_bq_input_uri = _sanitize_bq_uri(batch_prediction_bq_input_uri)\n",
    "    batch_prediction_bq_output_uri = _sanitize_bq_uri(batch_prediction_bq_output_uri)\n",
    "    query = f\"\"\"\n",
    "        SELECT\n",
    "          CAST(input.{time_column} as DATETIME) timestamp_col,\n",
    "          CAST(input.{time_series_identifier_column} as STRING) time_series_identifier_col,\n",
    "          CAST(input.{target_column} as NUMERIC) historical_values,\n",
    "          CAST(predicted_{target_column}.value as NUMERIC) predicted_values,\n",
    "        FROM `{batch_prediction_bq_input_uri}` input\n",
    "        LEFT JOIN `{batch_prediction_bq_output_uri}` output\n",
    "          ON\n",
    "            TIMESTAMP(input.{time_column}) = TIMESTAMP(output.{time_column})\n",
    "            AND CAST(input.{time_series_identifier_column} as STRING) = CAST(\n",
    "              output.{time_series_identifier_column} as STRING)\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"templateId\": \"067f70d2-8cd6-4a4c-a099-292acd1053e8\",\n",
    "        \"ds0.connector\": \"BIG_QUERY\",\n",
    "        \"ds0.projectId\": PROJECT_ID,\n",
    "        \"ds0.billingProjectId\": PROJECT_ID,\n",
    "        \"ds0.type\": \"CUSTOM_QUERY\",\n",
    "        \"ds0.sql\": query,\n",
    "    }\n",
    "    base_url = \"https://datastudio.google.com/c/u/0/reporting\"\n",
    "    url_params = urllib.parse.urlencode({\"params\": json.dumps(params)})\n",
    "    return f\"{base_url}?{url_params}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a91HWQmdByqo"
   },
   "outputs": [],
   "source": [
    "actuals_table = f\"{arima_dataset_path}.actuals\"\n",
    "query = f\"\"\"\n",
    "    CREATE OR REPLACE TABLE `{actuals_table}` AS\n",
    "    {base_data_query}\n",
    "    SELECT *\n",
    "    FROM base_data\n",
    "    WHERE split != 'TRAIN'\n",
    "\"\"\"\n",
    "client.query(query).result()\n",
    "print(f\"Created {actuals_table}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eMDQSc5Fqgco"
   },
   "outputs": [],
   "source": [
    "print(\"Click the link below to view ARIMA predictions:\")\n",
    "print(\n",
    "    get_data_studio_link(\n",
    "        batch_prediction_bq_input_uri=actuals_table,\n",
    "        batch_prediction_bq_output_uri=f\"{arima_dataset_path}.{pred_table}\",\n",
    "        time_column=time_column,\n",
    "        time_series_identifier_column=time_series_identifier_column,\n",
    "        target_column=target_column,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleanup:mbsdk"
   },
   "source": [
    "清理 Vertex AI 和 BigQuery 资源\n",
    "\n",
    "要清理此项目中使用的所有 Google Cloud 资源，您可以删除用于本教程的 Google Cloud 项目。\n",
    "\n",
    "否则，您可以删除此教程中创建的各个资源：\n",
    "\n",
    "- Cloud Storage 存储桶\n",
    "- BigQuery 表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TJa9R2zyQEH-"
   },
   "outputs": [],
   "source": [
    "# Delete output datasets\n",
    "client.delete_dataset(arima_dataset_path, delete_contents=True, not_found_ok=True)\n",
    "\n",
    "delete_bucket = False\n",
    "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
    "    ! gsutil -m rm -r $BUCKET_URI"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "bqml_arima_plus.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
