{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18ebbd838e32"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mThXALJl9Yue"
   },
   "source": [
    "# 表格化工作流程：宽度和深度管道\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/tabular_workflows/wide_and_deep_on_vertex_pipelines.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> 在 Colab 中运行\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/tabular_workflows/wide_and_deep_on_vertex_pipelines.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      在 GitHub 上查看\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/tabular_workflows/wide_and_deep_on_vertex_pipelines.ipynb\">\n",
    "        <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      在 Vertex AI Workbench 中打开\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "<br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fcc745968395"
   },
   "source": [
    "## 概述\n",
    "\n",
    "本笔记本展示了如何使用Vertex AI Tabular Workflows运行Wide & Deep算法。\n",
    "\n",
    "了解更多关于[Wide & Deep的Tabular Workflow](https://cloud.google.com/vertex-ai/docs/tabular-data/tabular-workflows/wide-and-deep)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f887ec5c06c5"
   },
   "source": [
    "### 目标\n",
    "\n",
    "在本教程中，您将学习如何使用Vertex AI Wide & Deep表格工作流创建两个分类模型。每个工作流都是[Vertex AI Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines/introduction)的托管实例。\n",
    "\n",
    "本教程使用以下Google Cloud ML服务和资源：\n",
    "\n",
    "- Vertex AI训练\n",
    "- Vertex管道\n",
    "- 云存储\n",
    "\n",
    "执行的步骤包括：\n",
    "\n",
    "- 创建一个Wide & Deep CustomJob。如果您知道要用于训练的超参数，这是最佳选项。\n",
    "- 创建一个Wide & Deep HyperparameterTuningJob。这使您可以获得适合您数据集的最佳超参数集。\n",
    "\n",
    "训练后，每个管道都会返回一个链接到Vertex Model UI。您可以使用UI部署模型，获得在线预测或运行批量预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eac26958afe8"
   },
   "source": [
    "数据集\n",
    "\n",
    "您将使用的数据集是[银行营销](https://archive.ics.uci.edu/ml/datasets/bank+marketing)。\n",
    "这些数据是来自葡萄牙银行机构的直接营销活动（电话营销）数据。二元分类目标是预测客户是否会订阅定期存款。对于这个笔记本，我们随机选择了原始数据集中90%的行，并将它们保存在一个名为train.csv的文件中，该文件托管在云存储中。要下载该文件，请点击[这里](https://storage.googleapis.com/cloud-samples-data-us-central1/vertex-ai/tabular-workflows/datasets/bank-marketing/train.csv)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "181d4dfbf917"
   },
   "source": [
    "### 成本\n",
    "\n",
    "本教程使用Google Cloud的计费组件：\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "了解[Vertex AI 价格](https://cloud.google.com/vertex-ai/pricing)和[Cloud Storage 价格](https://cloud.google.com/storage/pricing)，并使用[Pricing Calculator](https://cloud.google.com/products/calculator/)根据您的预计使用量生成成本估算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "## 安装\n",
    "\n",
    "安装以下所需的软件包以执行这个笔记本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2b4ef9b72d43"
   },
   "outputs": [],
   "source": [
    "! pip3 install --upgrade --quiet google-cloud-aiplatform \\\n",
    "                                 \"google-cloud-pipeline-components<2.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "restart"
   },
   "source": [
    "只有Colab:取消注释以下单元格以重新启动内核。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bzPxhxS5lugp"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs so that your environment can access the new packages\n",
    "# import IPython\n",
    "\n",
    "# app = IPython.Application.instance()\n",
    "# app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2qpIurSjmpT"
   },
   "source": [
    "## 在开始之前\n",
    "\n",
    "### 设置你的Google Cloud项目\n",
    "\n",
    "**以下步骤是必需的，无论你使用什么笔记本环境。**\n",
    "\n",
    "1. [选择或创建一个Google Cloud项目](https://console.cloud.google.com/cloud-resource-manager)。当你第一次创建一个账户时，你会获得$300的免费信用额度用于支付计算/存储成本。\n",
    "\n",
    "2. [确保为你的项目启用了计费](https://cloud.google.com/billing/docs/how-to/modify-project)。\n",
    "\n",
    "3. [启用以下API：Vertex AI API、Cloud Resource Manager API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,cloudresourcemanager.googleapis.com)。\n",
    "\n",
    "4. 如果你是在本地运行这个笔记本，你需要安装[Cloud SDK](https://cloud.google.com/sdk)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "project_id"
   },
   "source": [
    "将您的项目ID设定为以下值\n",
    "\n",
    "**如果您不知道您的项目ID**，请尝试以下操作：\n",
    "- 运行 `gcloud config list`。\n",
    "- 运行 `gcloud projects list`。\n",
    "- 查看支持页面：[定位项目ID](https://support.google.com/googleapi/answer/7014113)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wsePm9c4jmpT"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
    "\n",
    "# Set the project id\n",
    "! gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a54f9d7c1876"
   },
   "source": [
    "区域\n",
    "\n",
    "您还可以更改 Vertex AI 使用的 `REGION` 变量。了解更多关于 [Vertex AI 区域](https://cloud.google.com/vertex-ai/docs/general/locations)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "region"
   },
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5c0404984792"
   },
   "source": [
    "### 确认您的 Google Cloud 帐户\n",
    "\n",
    "根据您的 Jupyter 环境，您可能需要手动进行身份验证。请按照以下相关指示进行操作。\n",
    "\n",
    "**1. Vertex AI Workbench**\n",
    "* 不需要进行任何操作，因为您已经验证通过。\n",
    "\n",
    "**2. 本地 JupyterLab 实例，取消注释并运行：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nt8cEM2GjmpU"
   },
   "outputs": [],
   "source": [
    "# ! gcloud auth login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XUSL_JcpjmpU"
   },
   "source": [
    "3. 协作，取消注释并运行:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_2zemfGvjmpU"
   },
   "outputs": [],
   "source": [
    "# from google.colab import auth\n",
    "# auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCPJ38n7jmpU"
   },
   "source": [
    "查看如何在https://cloud.google.com/storage/docs/gsutil/commands/iam#ch-examples给您的服务账户授予云存储权限。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bucket:custom"
   },
   "source": [
    "### 创建一个云存储桶\n",
    "\n",
    "创建一个存储桶来存储中间产物，例如数据集。\n",
    "\n",
    "当您使用Cloud SDK提交一个训练作业时，您需要将包含训练代码的Python包上传至一个云存储桶。Vertex AI 会从这个包中运行代码。在本教程中，Vertex AI 还会将训练作业生成的训练模型保存在同一个存储桶中。使用这个模型产物，您可以创建Vertex AI 模型资源并用于预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bucket"
   },
   "outputs": [],
   "source": [
    "BUCKET_URI = \"gs://your-bucket-name-unique\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_bucket"
   },
   "source": [
    "只有在你的存储桶不存在的情况下才能运行以下单元格来创建你的云存储存储桶。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oz8J0vmSlugt"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zebLBGXOky2A"
   },
   "source": [
    "## 有关服务账户和权限的说明\n",
    "\n",
    "**默认情况下不需要配置**，如果遇到任何与权限相关的问题，请确保服务账户具有[TabNet、Wide & Deep 以及 Prophet 文档](https://cloud.google.com/vertex-ai/docs/tabular-data/tabular-workflows/service-accounts#fte-workflow)中列出的所需角色。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fbbc3479a1da"
   },
   "source": [
    "导入库并定义常量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8G6YmJT1yqkV"
   },
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import os\n",
    "import uuid\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "from google.cloud import aiplatform, storage\n",
    "from google_cloud_pipeline_components.experimental.automl.tabular import \\\n",
    "    utils as automl_tabular_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0423f260423"
   },
   "source": [
    "初始化Python的Vertex AI SDK\n",
    "\n",
    "为您的项目初始化Python的Vertex AI SDK。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ad69f2590268"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3LWH3PRF5o2v"
   },
   "source": [
    "### 定义辅助函数\n",
    "定义以下辅助函数：\n",
    "\n",
    "- `get_model_artifacts_path`：从任务详情中获取模型文件路径。\n",
    "- `get_model_uri`：从任务详情中获取模型 URI。\n",
    "- `get_bucket_name_and_path`：获取存储桶名称和路径。\n",
    "- `download_from_gcs`：从存储桶下载内容。\n",
    "- `write_to_gcs`：将内容上传到存储桶。\n",
    "- `get_task_detail`：通过任务名称获取任务详情。\n",
    "- `get_model_name`：从流水线作业 ID 获取模型名称。\n",
    "- `get_evaluation_metrics`：从流水线任务详情中获取评估指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g9FPFT8c5oC0"
   },
   "outputs": [],
   "source": [
    "def get_model_artifacts_path(task_details: List[Dict[str, Any]], task_name: str) -> str:\n",
    "    task = get_task_detail(task_details, task_name)\n",
    "    return task.outputs[\"unmanaged_container_model\"].artifacts[0].uri\n",
    "\n",
    "\n",
    "def get_model_uri(task_details: List[Dict[str, Any]]) -> str:\n",
    "    task = get_task_detail(task_details, \"model-upload\")\n",
    "    # in format https://<location>-aiplatform.googleapis.com/v1/projects/<project_number>/locations/<location>/models/<model_id>\n",
    "    model_id = task.outputs[\"model\"].artifacts[0].uri.split(\"/\")[-1]\n",
    "    return f\"https://console.cloud.google.com/vertex-ai/locations/{REGION}/models/{model_id}?project={PROJECT_ID}\"\n",
    "\n",
    "\n",
    "def get_bucket_name_and_path(uri: str) -> str:\n",
    "    no_prefix_uri = uri[len(\"gs://\") :]\n",
    "    splits = no_prefix_uri.split(\"/\")\n",
    "    return splits[0], \"/\".join(splits[1:])\n",
    "\n",
    "\n",
    "def download_from_gcs(uri: str) -> str:\n",
    "    bucket_name, path = get_bucket_name_and_path(uri)\n",
    "    storage_client = storage.Client(project=PROJECT_ID)\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(path)\n",
    "    return blob.download_as_string()\n",
    "\n",
    "\n",
    "def write_to_gcs(uri: str, content: str):\n",
    "    bucket_name, path = get_bucket_name_and_path(uri)\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(path)\n",
    "    blob.upload_from_string(content)\n",
    "\n",
    "\n",
    "def get_task_detail(\n",
    "    task_details: List[Dict[str, Any]], task_name: str\n",
    ") -> List[Dict[str, Any]]:\n",
    "    for task_detail in task_details:\n",
    "        if task_detail.task_name == task_name:\n",
    "            return task_detail\n",
    "\n",
    "\n",
    "def get_model_name(job_id: str) -> str:\n",
    "    pipeline_task_details = aiplatform.PipelineJob.get(\n",
    "        job_id\n",
    "    ).gca_resource.job_detail.task_details\n",
    "    upload_task_details = get_task_detail(pipeline_task_details, \"model-upload\")\n",
    "    return upload_task_details.outputs[\"model\"].artifacts[0].metadata[\"resourceName\"]\n",
    "\n",
    "\n",
    "def get_evaluation_metrics(\n",
    "    task_details: List[Dict[str, Any]],\n",
    ") -> str:\n",
    "    ensemble_task = get_task_detail(task_details, \"model-evaluation\")\n",
    "    return download_from_gcs(\n",
    "        ensemble_task.outputs[\"evaluation_metrics\"].artifacts[0].uri\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gvNFMRmBegZq"
   },
   "source": [
    "定义训练规范"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7a7332a3f8e2"
   },
   "source": [
    "### 配置数据集\n",
    "\n",
    "您可以定义以下任一参数：\n",
    "\n",
    "- `data_source_csv_filenames`：CSV 数据源。\n",
    "- `data_source_bigquery_table_path`：BigQuery 数据源。\n",
    "\n",
    "***注意***：请注意，数据集的存储位置必须与用于启动训练流水线的服务位置（即 `REGION`）相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b6dd1af1d336"
   },
   "outputs": [],
   "source": [
    "data_source_csv_filenames = \"gs://cloud-samples-data-us-central1/vertex-ai/tabular-workflows/datasets/bank-marketing/train.csv\"\n",
    "data_source_bigquery_table_path = (\n",
    "    None  # @param {type:\"string\"}, format: bq://bq_project.bq_dataset.bq_table\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bf417de96807"
   },
   "source": [
    "### 配置特征转换\n",
    "\n",
    "可通过特征转换引擎（FTE）特定配置指定转换。 FTE支持基于TensorFlow的行级和基于BigQuery的数据集级转换。\n",
    "\n",
    "* 基于TensorFlow的行级转换：\n",
    "  * 完全自动转换：FTE根据数据统计信息自动为每个输入列配置一组内置转换。 可通过训练管道中的`tf_auto_transform_features`设置。\n",
    "  * 全部指定的转换：对输入列上的所有转换明确指定为FTE的内置转换。 还支持在单个列上链接多个转换。 可将这些转换保存为JSON配置文件，并通过训练管道的`tf_transformations_path`参数指定。\n",
    "  * 自定义转换：自定义的，自定义的转换函数，您可以定义和导入自己的转换函数，并与其他FTE的内置转换一起使用。 您可以将自定义转换指定为JSON对象的数组，并通过训练管道的`tf_custom_transformation_definitions`参数传递。\n",
    "\n",
    "* 基于BigQuery的数据集级转换：\n",
    "  * 全部指定的转换：对输入列上的所有转换明确指定为FTE的内置转换。 这些转换可以通过训练管道的`dataset_level_transformations`参数指定为JSON对象的数组。\n",
    "  * 自定义转换：自定义的，自定义的转换函数，您可以定义和导入自己的转换函数，并与其他FTE的内置转换一起使用。 您可以将自定义转换指定为JSON对象的数组，并通过训练管道的`dataset_level_custom_transformation_definitions`参数传递。\n",
    "\n",
    "在下面，通过将输入特征列表指定为传递给训练管道的`tf_auto_transform_features`参数来配置完全自动转换。\n",
    "\n",
    "有关支持的特征转换配置和示例的完整列表，请访问[此处]（https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-1.0.31/google_cloud_pipeline_components.experimental.automl.tabular.html#google_cloud_pipeline_components.experimental.automl.tabular.FeatureTransformEngineOp）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fce334e09df6"
   },
   "outputs": [],
   "source": [
    "auto_transform_features = [\n",
    "    \"age\",\n",
    "    \"job\",\n",
    "    \"marital\",\n",
    "    \"education\",\n",
    "    \"default\",\n",
    "    \"balance\",\n",
    "    \"housing\",\n",
    "    \"loan\",\n",
    "    \"contact\",\n",
    "    \"day\",\n",
    "    \"month\",\n",
    "    \"duration\",\n",
    "    \"campaign\",\n",
    "    \"pdays\",\n",
    "    \"previous\",\n",
    "    \"poutcome\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dgmd7dHi21Sb"
   },
   "source": [
    "### 配置特征选择\n",
    "\n",
    "除了转换之外，您还可以通过Feature Transform Engine应用特征选择，仅使用由支持的算法评估的高排名特征。如果启用了特征选择，它将在数据集级转换之后立即应用，并排除任何未被选中的特征。\n",
    "\n",
    "要启用它，您需要将`run_feature_selection`设置为True。\n",
    "\n",
    "要配置要使用的算法和要选择的特征数量，您需要同时配置`feature_selection_algorithm`和`max_selected_features`参数。\n",
    "\n",
    "有关支持的特征选择算法和配置的完整列表，请访问[这里](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-1.0.31/google_cloud_pipeline_components.experimental.automl.tabular.html#google_cloud_pipeline_components.experimental.automl.tabular.FeatureTransformEngineOp)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "drdGfJ4824pZ"
   },
   "outputs": [],
   "source": [
    "RUN_FEATURE_SELECTION = True  # @param {type:\"boolean\"}\n",
    "\n",
    "FEATURE_SELECTION_ALGORITHM = \"AMI\"  # @param {type:\"string\"}\n",
    "\n",
    "MAX_SELECTED_FEATURES = 10  # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4b28b609b259"
   },
   "source": [
    "### 设置训练配置\n",
    "\n",
    "您需要定义以下内容：\n",
    "\n",
    "- `target_column`: 目标列的名称。\n",
    "- `prediction_type`: 模型要产生的预测类型。'classification' 或 'regression'。\n",
    "- `predefined_split_key`: 预定义拆分列的名称。\n",
    "- `timestamp_split_key`: 时间戳拆分列的名称。\n",
    "- `stratified_split_key`: 分层拆分列的名称。\n",
    "- `training_fraction`: 训练数据的比例。\n",
    "- `validation_fraction`: 验证数据的比例。\n",
    "- `test_fraction`: 测试数据的比例。\n",
    "- `weight_column`: 权重列的名称。\n",
    "- `run_evaluation`: 是否在训练期间运行评估步骤。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eV4JrwB8wAkg"
   },
   "outputs": [],
   "source": [
    "run_evaluation = True  # @param {type:\"boolean\"}\n",
    "prediction_type = \"classification\"\n",
    "target_column = \"deposit\"\n",
    "\n",
    "# Fraction split\n",
    "training_fraction = 0.8\n",
    "validation_fraction = 0.1\n",
    "test_fraction = 0.1\n",
    "\n",
    "timestamp_split_key = None  # timestamp column name when using timestamp split\n",
    "stratified_split_key = None  # target column name when using stratified split\n",
    "training_fraction = 0.8\n",
    "validation_fraction = 0.1\n",
    "test_fraction = 0.1\n",
    "\n",
    "predefined_split_key = None\n",
    "if predefined_split_key:\n",
    "    training_fraction = None\n",
    "    validation_fraction = None\n",
    "    test_fraction = None\n",
    "\n",
    "weight_column = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zyWGg2s09xOk"
   },
   "source": [
    "## VPC相关配置\n",
    "\n",
    "您定义如下：\n",
    "\n",
    "- `dataflow_subnetwork`：Dataflow的完全限定子网络名称，如果为空将使用默认子网络。示例：https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications\n",
    "- `dataflow_use_public_ips`：指定Dataflow工作节点是否使用公共IP地址。\n",
    "\n",
    "如果需要使用自定义Dataflow子网络，可以通过`dataflow_subnetwork`参数进行设置。要求如下：\n",
    "1. `dataflow_subnetwork`必须是完全限定的子网名称。[[参考链接](https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications)]\n",
    "1. 下列服务账户必须在指定的Dataflow子网络上分配[Compute Network User角色](https://cloud.google.com/compute/docs/access/iam#compute.networkUser)：  \n",
    "    1. Compute Engine默认服务账户：PROJECT_NUMBER-compute@developer.gserviceaccount.com  \n",
    "    2. Dataflow服务账户：service-PROJECT_NUMBER@dataflow-service-producer-prod.iam.gserviceaccount.com\n",
    "\n",
    "如果您的项目已启用VPC-SC，请确保：\n",
    "\n",
    "1. VPC-SC中使用的Dataflow子网络已正确配置。\n",
    "   [[参考链接](https://cloud.google.com/dataflow/docs/guides/routes-firewall)]\n",
    "1. `dataflow_use_public_ips`设置为False。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_TePNlLl9v1q"
   },
   "outputs": [],
   "source": [
    "dataflow_subnetwork = \"\"  # @param {type:\"string\"}\n",
    "dataflow_use_public_ips = True  # @param {type:\"boolean\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-iXXE14voyR"
   },
   "source": [
    "## 自定义 Wide & Deep CustomJob 配置并创建流水线\n",
    "\n",
    "如果您确切地知道要用于模型训练的超参数数值，这是最佳选择。它比 HyperparameterTuningJob 使用更少的训练资源。\n",
    "\n",
    "在下面的示例中，您可以配置以下内容：\n",
    "\n",
    "- `root_dir`: 用于流水线组件的根 GCS 目录。\n",
    "- `worker_pool_specs_override`: 用于覆盖训练和评估工作池规范的字典。该字典应该是[这种格式](https://github.com/googleapis/googleapis/blob/4e836c7c257e3e20b1de14d470993a2b1f4736a8/google/cloud/aiplatform/v1beta1/custom_job.proto#L172)。\n",
    "- `learning_rate`: 线性优化器使用的学习率。\n",
    "- `dnn_learning_rate`: 训练模型深部分使用的学习率。\n",
    "- `max_steps`: 运行训练器的步数。\n",
    "- `max_train_secs`: 运行训练器的时间量（以秒为单位）。\n",
    "\n",
    "可以在[这里](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-1.0.23/google_cloud_pipeline_components.experimental.automl.tabular.html#google_cloud_pipeline_components.experimental.automl.tabular.utils.get_tabnet_trainer_pipeline_and_parameters)找到完整的流水线输入和模型超参数列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sG46cXVueb66"
   },
   "outputs": [],
   "source": [
    "pipeline_job_root_dir = os.path.join(BUCKET_URI, \"wide_and_deep_custom_job\")\n",
    "\n",
    "# max_steps and/or max_train_secs must be set. If both are\n",
    "# specified, training will stop after either condition is met.\n",
    "# By default, max_train_secs is set to -1.\n",
    "\n",
    "max_steps = 1000\n",
    "max_train_secs = -1\n",
    "\n",
    "learning_rate = 0.01\n",
    "dnn_learning_rate = 0.01\n",
    "\n",
    "worker_pool_specs_override = [\n",
    "    {\"machine_spec\": {\"machine_type\": \"c2-standard-16\"}}  # Override for TF chief node\n",
    "]\n",
    "\n",
    "# To test GPU training, the worker_pool_specs_override can be specified like this.\n",
    "# worker_pool_specs_override =  [\n",
    "#     {\"machine_spec\": {\n",
    "#       'machine_type': \"n1-highmem-32\",\n",
    "#       \"accelerator_type\": \"NVIDIA_TESLA_V100\",\n",
    "#       \"accelerator_count\": 2\n",
    "#       }\n",
    "#     }\n",
    "#   ]\n",
    "\n",
    "# If your system does not use Python, you can save the JSON file (`template_path`),\n",
    "# and use another programming language to submit the pipeline.\n",
    "(\n",
    "    template_path,\n",
    "    parameter_values,\n",
    ") = automl_tabular_utils.get_wide_and_deep_trainer_pipeline_and_parameters(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    root_dir=pipeline_job_root_dir,\n",
    "    max_steps=max_steps,\n",
    "    max_train_secs=max_train_secs,\n",
    "    learning_rate=learning_rate,\n",
    "    dnn_learning_rate=dnn_learning_rate,\n",
    "    target_column=target_column,\n",
    "    prediction_type=prediction_type,\n",
    "    tf_auto_transform_features=auto_transform_features,\n",
    "    run_feature_selection=RUN_FEATURE_SELECTION,\n",
    "    feature_selection_algorithm=FEATURE_SELECTION_ALGORITHM,\n",
    "    max_selected_features=MAX_SELECTED_FEATURES,\n",
    "    training_fraction=training_fraction,\n",
    "    validation_fraction=validation_fraction,\n",
    "    test_fraction=test_fraction,\n",
    "    data_source_csv_filenames=data_source_csv_filenames,\n",
    "    data_source_bigquery_table_path=data_source_bigquery_table_path,\n",
    "    worker_pool_specs_override=worker_pool_specs_override,\n",
    "    dataflow_use_public_ips=dataflow_use_public_ips,\n",
    "    dataflow_subnetwork=dataflow_subnetwork,\n",
    "    run_evaluation=run_evaluation,\n",
    ")\n",
    "\n",
    "pipeline_job_id = f\"wide-and-deep-{uuid.uuid4()}\"\n",
    "# More info on parameters PipelineJob accepts:\n",
    "# https://cloud.google.com/vertex-ai/docs/pipelines/run-pipeline#create_a_pipeline_run\n",
    "pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name=pipeline_job_id,\n",
    "    template_path=template_path,\n",
    "    job_id=pipeline_job_id,\n",
    "    pipeline_root=pipeline_job_root_dir,\n",
    "    parameter_values=parameter_values,\n",
    "    enable_caching=False,\n",
    ")\n",
    "\n",
    "pipeline_job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cbf2cf98842b"
   },
   "source": [
    "### 前往顶点模型 UI\n",
    "你可以通过下方的链接部署模型、进行在线预测或批量预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "719a784573ce"
   },
   "outputs": [],
   "source": [
    "wide_and_deep_trainer_pipeline_task_details = aiplatform.PipelineJob.get(\n",
    "    pipeline_job_id\n",
    ").gca_resource.job_detail.task_details\n",
    "CUSTOM_JOB_MODEL = get_model_name(pipeline_job_id)\n",
    "print(\"model uri:\", get_model_uri(wide_and_deep_trainer_pipeline_task_details))\n",
    "print(\n",
    "    \"model artifacts:\",\n",
    "    get_model_artifacts_path(\n",
    "        wide_and_deep_trainer_pipeline_task_details, \"wide-and-deep-trainer\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_sF8a2RKtRhg"
   },
   "source": [
    "## 自定义广泛和深度超参数调整作业配置并创建管道\n",
    "\n",
    "为了获得最佳的超参数组合，建议运行一个HyperparameterTuningJob。\n",
    "\n",
    "可以调整的超参数在可选的`study_spec_parameters_override`参数中设置。我们提供了一个称为`get_wide_and_deep_study_spec_parameters_override`的辅助函数来获取这些超参数。该函数返回一个超参数和范围的列表。`study_spec_parameters_override`可以为空，或者可以指定一个或多个这些超参数。对于未在`study_spec_parameters_override`中指定的超参数，我们在管道中设置范围。有关可用于调整的超参数的完整列表，请参阅[这里](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-1.0.23/google_cloud_pipeline_components.experimental.automl.tabular.html#google_cloud_pipeline_components.experimental.automl.tabular.utils.get_wide_and_deep_trainer_pipeline_and_parameters)。\n",
    "\n",
    "除了超参数外，HyperparameterTuningJob在下面的示例中还采用以下值：\n",
    "\n",
    "- `root_dir`：管道组件的根GCS目录。\n",
    "- `worker_pool_specs_override`：用于覆盖训练和评估工作池规范的字典。字典应该是[此格式](https://github.com/googleapis/googleapis/blob/4e836c7c257e3e20b1de14d470993a2b1f4736a8/google/cloud/aiplatform/v1beta1/custom_job.proto#L172)。\n",
    "- `study_spec_metric_id`：优化的指标，可能的值：['loss', 'average_loss', 'rmse', 'mae', 'mql', 'accuracy', 'auc', 'precision', 'recall']。\n",
    "- `study_spec_metric_goal`：指标的优化目标，可能的值：\"MAXIMIZE\"，\"MINIMIZE\"。\n",
    "- `max_trial_count`：所需的总试验数。\n",
    "- `parallel_trial_count`：要并行运行的试验数。\n",
    "- `max_failed_trial_count`：在失败HyperparameterTuningJob之前所需看到的失败试验数。如果设置为0，Vertex AI将决定在整个作业失败之前必须失败的试验数量。\n",
    "- `study_spec_algorithm`：为研究指定的搜索算法。其中之一是'ALGORITHM_UNSPECIFIED'，'GRID_SEARCH'或者' RANDOM_SEARCH'。\n",
    "\n",
    "有关HyperparameterTuningJob参数的完整列表，请参见[这里](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-1.0.23/google_cloud_pipeline_components.experimental.automl.tabular.html#google_cloud_pipeline_components.experimental.automl.tabular.utils.get_wide_and_deep_hyperparameter_tuning_job_pipeline_and_parameters)。\n",
    "\n",
    "可以配置多个试验。管道会返回基于`study_spec_metrics`配置的指标的最佳试验。在下面的示例中，我们返回具有最低损失值的试验。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8hlPps2Rtpq-"
   },
   "outputs": [],
   "source": [
    "pipeline_job_root_dir = os.path.join(\n",
    "    BUCKET_URI, \"wide_and_deep_hyperparameter_tuning_job\"\n",
    ")\n",
    "\n",
    "worker_pool_specs_override = [\n",
    "    {\"machine_spec\": {\"machine_type\": \"c2-standard-16\"}}  # Override for TF chief node\n",
    "]\n",
    "\n",
    "# To test GPU training, the worker_pool_specs_override can be specified like this.\n",
    "# worker_pool_specs_override =  [\n",
    "#    {\n",
    "#       \"machine_spec\":{\n",
    "#          \"machine_type\":\"n1-highmem-32\",\n",
    "#          \"accelerator_type\":\"NVIDIA_TESLA_V100\",\n",
    "#          \"accelerator_count\":2\n",
    "#       }\n",
    "#    }\n",
    "# ]\n",
    "\n",
    "study_spec_metric_id = \"loss\"\n",
    "study_spec_metric_goal = \"MINIMIZE\"\n",
    "\n",
    "# max_steps and/or max_train_secs must be set. If both are\n",
    "# specified, training will stop after either condition is met.\n",
    "# By default, max_train_secs is set to -1 and max_steps is set to\n",
    "# an appropriate range given dataset_size and training budget.\n",
    "study_spec_parameters_override = (\n",
    "    automl_tabular_utils.get_wide_and_deep_study_spec_parameters_override()\n",
    ")\n",
    "\n",
    "# If your system does not use Python, you can save the JSON file (`template_path`),\n",
    "# and use another programming language to submit the pipeline.\n",
    "(\n",
    "    template_path,\n",
    "    parameter_values,\n",
    ") = automl_tabular_utils.get_wide_and_deep_hyperparameter_tuning_job_pipeline_and_parameters(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    root_dir=pipeline_job_root_dir,\n",
    "    target_column=target_column,\n",
    "    prediction_type=prediction_type,\n",
    "    tf_auto_transform_features=auto_transform_features,\n",
    "    run_feature_selection=RUN_FEATURE_SELECTION,\n",
    "    feature_selection_algorithm=FEATURE_SELECTION_ALGORITHM,\n",
    "    max_selected_features=MAX_SELECTED_FEATURES,\n",
    "    training_fraction=training_fraction,\n",
    "    validation_fraction=validation_fraction,\n",
    "    test_fraction=test_fraction,\n",
    "    data_source_csv_filenames=data_source_csv_filenames,\n",
    "    data_source_bigquery_table_path=data_source_bigquery_table_path,\n",
    "    study_spec_metric_id=study_spec_metric_id,\n",
    "    study_spec_metric_goal=study_spec_metric_goal,\n",
    "    study_spec_parameters_override=study_spec_parameters_override,\n",
    "    max_trial_count=1,\n",
    "    parallel_trial_count=1,\n",
    "    max_failed_trial_count=0,\n",
    "    worker_pool_specs_override=worker_pool_specs_override,\n",
    "    dataflow_use_public_ips=dataflow_use_public_ips,\n",
    "    dataflow_subnetwork=dataflow_subnetwork,\n",
    "    run_evaluation=True,\n",
    ")\n",
    "\n",
    "pipeline_job_id = f\"wide-and-deep-hpt-{uuid.uuid4()}\"\n",
    "# More info on parameters PipelineJob accepts:\n",
    "# https://cloud.google.com/vertex-ai/docs/pipelines/run-pipeline#create_a_pipeline_run\n",
    "pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name=pipeline_job_id,\n",
    "    template_path=template_path,\n",
    "    job_id=pipeline_job_id,\n",
    "    pipeline_root=pipeline_job_root_dir,\n",
    "    parameter_values=parameter_values,\n",
    "    enable_caching=False,\n",
    ")\n",
    "\n",
    "pipeline_job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2749d8cec287"
   },
   "source": [
    "### 前往顶点模型 UI\n",
    "通过下面的链接，您可以部署模型并进行在线预测测试，或者运行批量预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "730af2836871"
   },
   "outputs": [],
   "source": [
    "wide_and_deep_hpt_pipeline_task_details = aiplatform.PipelineJob.get(\n",
    "    pipeline_job_id\n",
    ").gca_resource.job_detail.task_details\n",
    "HPT_JOB_MODEL = get_model_name(pipeline_job_id)\n",
    "print(\"model uri:\", get_model_uri(wide_and_deep_hpt_pipeline_task_details))\n",
    "print(\n",
    "    \"model artifacts:\",\n",
    "    get_model_artifacts_path(\n",
    "        wide_and_deep_hpt_pipeline_task_details,\n",
    "        \"get-best-hyperparameter-tuning-job-trial\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43342a43176e"
   },
   "source": [
    "清理顶点和BigQuery资源\n",
    "\n",
    "要清理此项目中使用的所有Google Cloud资源，您可以删除用于本教程的[Google Cloud项目](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects)。\n",
    "\n",
    "否则，您可以删除本教程中创建的各个资源：\n",
    "\n",
    "- Cloud Storage 存储桶\n",
    "- CustomJob 管道生成的模型\n",
    "- HyperparameterTuningJob 管道生成的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ad8d12061a65"
   },
   "outputs": [],
   "source": [
    "# Delete model resources\n",
    "custom_job_model = aiplatform.Model(CUSTOM_JOB_MODEL)\n",
    "hpt_job_model = aiplatform.Model(HPT_JOB_MODEL)\n",
    "custom_job_model.delete()\n",
    "hpt_job_model.delete()\n",
    "\n",
    "# Delete bucket\n",
    "delete_bucket = False\n",
    "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
    "    ! gsutil -m rm -r $BUCKET_URI"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "wide_and_deep_on_vertex_pipelines.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
