{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18ebbd838e32"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mThXALJl9Yue"
   },
   "source": [
    "# 表格工作流程：TabNet管道\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/tabular_workflows/tabnet_on_vertex_pipelines.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> 在Colab中运行\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/tabular_workflows/tabnet_on_vertex_pipelines.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      在GitHub上查看\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/tabular_workflows/tabnet_on_vertex_pipelines.ipynb\">\n",
    "        <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      在Vertex AI Workbench中打开\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "<br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "962e636b5cee"
   },
   "source": [
    "注意：该笔记本已在以下环境中进行了测试：\n",
    "\n",
    "- Python版本 = 3.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fcc745968395"
   },
   "source": [
    "## 概述\n",
    "\n",
    "本笔记本展示了如何使用Vertex AI表格工作流来运行TabNet算法。\n",
    "\n",
    "了解有关[TabNet的表格工作流](https://cloud.google.com/vertex-ai/docs/tabular-data/tabular-workflows/tabnet)的更多信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f887ec5c06c5"
   },
   "source": [
    "### 目标\n",
    "\n",
    "在本教程中，您将学习如何使用Vertex AI TabNet Tabular工作流在表格数据上创建分类模型。每个工作流都是[Vertex AI Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines/introduction)的托管实例。\n",
    "\n",
    "本教程使用以下谷歌云ML服务和资源：\n",
    "\n",
    "- Vertex AI Training\n",
    "- Vertex AI Pipelines\n",
    "- Cloud Storage\n",
    "\n",
    "执行的步骤包括：\n",
    "\n",
    "- 创建TabNet CustomJob。如果您知道要用于训练的超参数，则这是最佳选项。\n",
    "- 创建TabNet HyperparameterTuningJob。这允许您获取适合您数据集的最佳超参数集。\n",
    "\n",
    "培训完成后，每个流水线都将返回一个链接到Vertex Model UI。您可以使用UI部署模型，获取在线预测，或运行批处理预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eac26958afe8"
   },
   "source": [
    "### 数据集\n",
    "\n",
    "您在此笔记本中使用的数据集是[银行营销](https://archive.ics.uci.edu/ml/datasets/bank+marketing)数据集。\n",
    "该数据集包含与葡萄牙银行机构的直接营销活动（电话呼叫）相关的数据。此笔记本中二元分类任务的目标是预测客户是否订阅定期存款。\n",
    "\n",
    "对于此笔记本，随机选择的原始数据集中约90%的行的子集已保存到`train.csv`文件中，并托管在Cloud Storage中。要下载文件，请单击[此处](https://storage.googleapis.com/cloud-samples-data-us-central1/vertex-ai/tabular-workflows/datasets/bank-marketing/train.csv)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "181d4dfbf917"
   },
   "source": [
    "### 成本\n",
    "\n",
    "本教程使用 Google Cloud 的可计费组件：\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "了解 [Vertex AI 价格](https://cloud.google.com/vertex-ai/pricing) 和 [Cloud Storage 价格](https://cloud.google.com/storage/pricing)，使用 [价格计算器](https://cloud.google.com/products/calculator/) 根据您的预期使用量生成成本估算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "## 安装\n",
    "\n",
    "安装以下所需的软件包以执行这个笔记本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2b4ef9b72d43"
   },
   "outputs": [],
   "source": [
    "! pip3 install --upgrade --quiet google-cloud-aiplatform \\\n",
    "                                 google-cloud-pipeline-components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "restart"
   },
   "source": [
    "### 仅限于Colab：取消注释以下单元格以重新启动内核。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bzPxhxS5lugp"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs so that your environment can access the new packages\n",
    "# import IPython\n",
    "\n",
    "# app = IPython.Application.instance()\n",
    "# app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2qpIurSjmpT"
   },
   "source": [
    "## 开始之前\n",
    "\n",
    "### 设置您的 Google Cloud 项目\n",
    "\n",
    "**无论您使用的是笔记本环境，以下步骤都是必须的。**\n",
    "\n",
    "1. [选择或创建一个 Google Cloud 项目](https://console.cloud.google.com/cloud-resource-manager)。当您第一次创建帐户时，您将获得$300的免费信用额用于计算/存储成本。\n",
    "\n",
    "2. [确保您的项目已启用计费](https://cloud.google.com/billing/docs/how-to/modify-project)。\n",
    "\n",
    "3. [启用以下API：Vertex AI API，云资源管理器 API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,cloudresourcemanager.googleapis.com)。\n",
    "\n",
    "4. 如果您正在本地运行此笔记本，您需要安装[Cloud SDK](https://cloud.google.com/sdk)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "project_id"
   },
   "source": [
    "设置您的项目ID\n",
    "\n",
    "**如果您不知道您的项目ID**，请尝试以下操作：\n",
    "* 运行`gcloud config list`。\n",
    "* 运行`gcloud projects list`。\n",
    "* 查看支持页面：[查找项目 ID](https://support.google.com/googleapi/answer/7014113)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wsePm9c4jmpT"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
    "\n",
    "# Set the project id\n",
    "! gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a54f9d7c1876"
   },
   "source": [
    "区域\n",
    "\n",
    "您还可以更改 Vertex AI 使用的 `REGION` 变量。 了解有关[Vertex AI 区域](https://cloud.google.com/vertex-ai/docs/general/locations)的更多信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "region"
   },
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5c0404984792"
   },
   "source": [
    "### 验证您的Google Cloud帐户\n",
    "\n",
    "根据您的Jupyter环境，您可能需要手动进行身份验证。请按照以下相关说明操作。\n",
    "\n",
    "**1. Vertex AI Workbench**\n",
    "* 无需进行任何操作，因为您已经验证过身份。\n",
    "\n",
    "**2. 本地JupyterLab实例，取消注释并运行：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nt8cEM2GjmpU"
   },
   "outputs": [],
   "source": [
    "# ! gcloud auth login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XUSL_JcpjmpU"
   },
   "source": [
    "3. 合作，取消注释并运行:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_2zemfGvjmpU"
   },
   "outputs": [],
   "source": [
    "# from google.colab import auth\n",
    "# auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCPJ38n7jmpU"
   },
   "source": [
    "请参阅如何在https://cloud.google.com/storage/docs/gsutil/commands/iam#ch-examples 上授予云存储权限给您的服务帐户。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bucket:custom"
   },
   "source": [
    "### 创建一个云存储桶\n",
    "\n",
    "创建一个存储桶，用于存储诸如数据集等中间工件。\n",
    "\n",
    "当您使用云SDK提交训练作业时，您需要将包含训练代码的Python软件包上传到一个云存储桶中。Vertex AI会从这个包中运行代码。在本教程中，Vertex AI还会将训练作业产生的训练模型保存在同一个存储桶中。使用这个模型工件，然后您可以创建Vertex AI模型资源，并用于预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bucket"
   },
   "outputs": [],
   "source": [
    "BUCKET_URI = f\"gs://your-bucket-name-{PROJECT_ID}-unique\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_bucket"
   },
   "source": [
    "只有您的存储桶不存在时：运行以下单元格以创建您的云存储存储桶。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oz8J0vmSlugt"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l {REGION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zebLBGXOky2A"
   },
   "source": [
    "### 关于服务账户和权限的注意事项\n",
    "\n",
    "**默认情况下不需要任何配置**，如果遇到任何与权限相关的问题，请确保服务账户具有[TabNet、Wide & Deep、Prophet文档](https://cloud.google.com/vertex-ai/docs/tabular-data/tabular-workflows/service-accounts#fte-workflow)中列出的所需角色。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "44accda192d5"
   },
   "source": [
    "服务账户\n",
    "\n",
    "您可以使用服务账户来创建 Vertex AI Pipeline 作业。如果您不想使用项目的 Compute Engine 服务账户，将 `SERVICE_ACCOUNT` 设置为其他服务账户 ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c65d12a97f45"
   },
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT = \"[your-service-account]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "604ae09ab6d3"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "if (\n",
    "    SERVICE_ACCOUNT == \"\"\n",
    "    or SERVICE_ACCOUNT is None\n",
    "    or SERVICE_ACCOUNT == \"[your-service-account]\"\n",
    "):\n",
    "    # Get your service account from gcloud\n",
    "    if not IS_COLAB:\n",
    "        shell_output = !gcloud auth list 2>/dev/null\n",
    "        SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
    "\n",
    "    else:  # IS_COLAB:\n",
    "        shell_output = ! gcloud projects describe  $PROJECT_ID\n",
    "        project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
    "        SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
    "\n",
    "    print(\"Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d1ecb60964d5"
   },
   "source": [
    "为Vertex AI Pipelines设置服务账户访问权限\n",
    "运行以下命令，为您的服务账户授予读取和写入管道 artifacts 在您在上一步创建的存储桶中的访问权限。您只需要对每个服务账户运行此步骤一次。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a592f0a380c2"
   },
   "outputs": [],
   "source": [
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
    "\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fbbc3479a1da"
   },
   "source": [
    "### 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8G6YmJT1yqkV"
   },
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import os\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "from google.cloud import aiplatform, storage\n",
    "from google_cloud_pipeline_components.preview.automl.tabular import \\\n",
    "    utils as automl_tabular_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0423f260423"
   },
   "source": [
    "### 初始化 Vertex AI SDK 用于 Python\n",
    "\n",
    "为您的项目初始化 Vertex AI SDK 用于 Python。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ad69f2590268"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3LWH3PRF5o2v"
   },
   "source": [
    "## 定义辅助函数\n",
    "定义以下辅助函数：\n",
    "\n",
    "- `get_model_artifacts_path`：从任务详情中获取模型文件路径。\n",
    "- `get_model_uri`：从任务详情中获取模型URI。\n",
    "- `get_bucket_name_and_path`：获取存储桶名称和路径。\n",
    "- `download_from_gcs`：从存储桶下载内容。\n",
    "- `write_to_gcs`：将内容上传到存储桶。\n",
    "- `get_task_detail`：通过任务名称获取任务详情。\n",
    "- `get_model_name`：从管道作业ID中获取模型名称。\n",
    "- `get_evaluation_metrics`：从管道任务详情中获取评估指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g9FPFT8c5oC0"
   },
   "outputs": [],
   "source": [
    "# Get the model artifacts path from task details.\n",
    "\n",
    "\n",
    "def get_model_artifacts_path(task_details: List[Dict[str, Any]], task_name: str) -> str:\n",
    "    task = get_task_detail(task_details, task_name)\n",
    "    return task.outputs[\"unmanaged_container_model\"].artifacts[0].uri\n",
    "\n",
    "\n",
    "# Get the model uri from the task details.\n",
    "def get_model_uri(task_details: List[Dict[str, Any]]) -> str:\n",
    "    task = get_task_detail(task_details, \"model-upload\")\n",
    "    # in format https://<location>-aiplatform.googleapis.com/v1/projects/<project_number>/locations/<location>/models/<model_id>\n",
    "    model_id = task.outputs[\"model\"].artifacts[0].uri.split(\"/\")[-1]\n",
    "    return f\"https://console.cloud.google.com/vertex-ai/locations/{REGION}/models/{model_id}?project={PROJECT_ID}\"\n",
    "\n",
    "\n",
    "# Get the bucket name and path.\n",
    "def get_bucket_name_and_path(uri: str) -> str:\n",
    "    no_prefix_uri = uri[len(\"gs://\") :]\n",
    "    splits = no_prefix_uri.split(\"/\")\n",
    "    return splits[0], \"/\".join(splits[1:])\n",
    "\n",
    "\n",
    "# Get the content from the bucket.\n",
    "def download_from_gcs(uri: str) -> str:\n",
    "    bucket_name, path = get_bucket_name_and_path(uri)\n",
    "    storage_client = storage.Client(project=PROJECT_ID)\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(path)\n",
    "    return blob.download_as_string()\n",
    "\n",
    "\n",
    "# Upload content into the bucket.\n",
    "def write_to_gcs(uri: str, content: str):\n",
    "    bucket_name, path = get_bucket_name_and_path(uri)\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(path)\n",
    "    blob.upload_from_string(content)\n",
    "\n",
    "\n",
    "# Get the task details by using task name.\n",
    "def get_task_detail(\n",
    "    task_details: List[Dict[str, Any]], task_name: str\n",
    ") -> List[Dict[str, Any]]:\n",
    "    for task_detail in task_details:\n",
    "        if task_detail.task_name == task_name:\n",
    "            return task_detail\n",
    "\n",
    "\n",
    "# Get the model name from pipeline job ID.\n",
    "def get_model_name(job_id: str) -> str:\n",
    "    pipeline_task_details = aiplatform.PipelineJob.get(\n",
    "        job_id\n",
    "    ).gca_resource.job_detail.task_details\n",
    "    upload_task_details = get_task_detail(pipeline_task_details, \"model-upload\")\n",
    "    return upload_task_details.outputs[\"model\"].artifacts[0].metadata[\"resourceName\"]\n",
    "\n",
    "\n",
    "# Get the evaluation metrics.\n",
    "def get_evaluation_metrics(\n",
    "    task_details: List[Dict[str, Any]],\n",
    ") -> str:\n",
    "    ensemble_task = get_task_detail(task_details, \"model-evaluation\")\n",
    "    return download_from_gcs(\n",
    "        ensemble_task.outputs[\"evaluation_metrics\"].artifacts[0].uri\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7a7332a3f8e2"
   },
   "source": [
    "## 定义培训规范\n",
    "\n",
    "在创建培训作业之前，在本节中您需要创建以下步骤：\n",
    "\n",
    "1. 配置源数据集。\n",
    "2. 配置特征转换过程。\n",
    "3. 配置特征选择过程。\n",
    "4. 设置运行培训过程所需的参数。\n",
    "\n",
    "### 配置数据集\n",
    "\n",
    "您可以定义以下参数之一：\n",
    "\n",
    "- `data_source_csv_filenames`：CSV数据源。您需要指定`train.csv`文件在数据集部分描述的云存储路径。\n",
    "- `data_source_bigquery_table_path`：BigQuery数据源。由于使用了云存储源，因此将此设置为无。\n",
    "\n",
    "***注意***：请注意数据集的位置必须与用于启动培训管道的服务位置（即 `REGION`）相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b6dd1af1d336"
   },
   "outputs": [],
   "source": [
    "data_source_csv_filenames = \"gs://cloud-samples-data-us-central1/vertex-ai/tabular-workflows/datasets/bank-marketing/train.csv\"\n",
    "data_source_bigquery_table_path = (\n",
    "    None  # @param {type:\"string\"}, format: bq://bq_project.bq_dataset.bq_table\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bf417de96807"
   },
   "source": [
    "### 配置特征转换\n",
    "\n",
    "可以使用Feature Transform Engine（FTE）特定的配置来指定转换。FTE支持基于TensorFlow的行级和基于BigQuery的数据集级转换。\n",
    "\n",
    "* **基于TensorFlow的行级转换**：\n",
    "  * 完全自动转换：FTE根据数据统计信息自动为每个输入列配置一组内置转换。这可以通过训练管线中的`tf_auto_transform_features`进行设置。\n",
    "  * 完全指定的转换：对输入列的所有转换都明确指定为FTE的内置转换。还支持对单个列进行多个转换的串联。这些转换可以保存到JSON配置文件中，并通过训练管线的`tf_transformations_path`参数进行指定。\n",
    "  * 自定义转换：自定义的、自定义的转换功能，您可以定义和导入自己的转换函数，并与其他FTE的内置转换一起使用。您可以将自定义转换指定为JSON对象数组，并通过训练管线的`tf_custom_transformation_definitions`参数传递。\n",
    "\n",
    "* **基于BigQuery的数据集级转换**：\n",
    "  * 完全指定的转换：对输入列的所有转换都明确指定为FTE的内置转换。这些转换可以通过训练管线的`dataset_level_transformations`参数作为JSON对象数组进行指定。\n",
    "  * 自定义转换：自定义的、自定义的转换功能，您可以定义和导入自己的转换函数，并与其他FTE的内置转换一起使用。您可以将自定义转换指定为JSON对象数组，并通过训练管线的`dataset_level_custom_transformation_definitions`参数进行传递。\n",
    "\n",
    "在下面，通过将要传递给训练管线的`tf_auto_transform_features`参数指定为输入特征列表，来配置完全自动转换。\n",
    "\n",
    "了解更多关于[特征转换配置](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-1.0.31/google_cloud_pipeline_components.experimental.automl.tabular.html#google_cloud_pipeline_components.experimental.automl.tabular.FeatureTransformEngineOp)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fce334e09df6"
   },
   "outputs": [],
   "source": [
    "auto_transform_features = [\n",
    "    \"age\",\n",
    "    \"job\",\n",
    "    \"marital\",\n",
    "    \"education\",\n",
    "    \"default\",\n",
    "    \"balance\",\n",
    "    \"housing\",\n",
    "    \"loan\",\n",
    "    \"contact\",\n",
    "    \"day\",\n",
    "    \"month\",\n",
    "    \"duration\",\n",
    "    \"campaign\",\n",
    "    \"pdays\",\n",
    "    \"previous\",\n",
    "    \"poutcome\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-t1fAaCFs8Os"
   },
   "source": [
    "### 配置特征选择\n",
    "\n",
    "除了转换之外，您还可以通过特征转换引擎应用特征选择，仅使用由支持的算法评估的高排名特征。如果启用，它将在数据集级别转换后立即应用，并排除任何未被选定的特征。\n",
    "\n",
    "要启用它，您需要将`run_feature_selection`设置为True。\n",
    "\n",
    "要配置要使用的算法和要选择的特征数量，您需要配置`feature_selection_algorithm`和`max_selected_features`参数。\n",
    "\n",
    "了解更多关于[特征选择算法和配置](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-1.0.31/google_cloud_pipeline_components.experimental.automl.tabular.html#google_cloud_pipeline_components.experimental.automl.tabular.FeatureTransformEngineOp)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YroYjOTJwytk"
   },
   "outputs": [],
   "source": [
    "RUN_FEATURE_SELECTION = True  # @param {type:\"boolean\"}\n",
    "\n",
    "FEATURE_SELECTION_ALGORITHM = \"AMI\"  # @param {type:\"string\"}\n",
    "\n",
    "MAX_SELECTED_FEATURES = 10  # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4b28b609b259"
   },
   "source": [
    "设置训练配置\n",
    "\n",
    "现在，您可以定义以下培训参数：\n",
    "\n",
    "- `target_column`：目标列名称。\n",
    "- `prediction_type`：模型将产生的预测类型。'classification'或'regression'。\n",
    "- `predefined_split_key`：预定义分割列名称。\n",
    "- `timestamp_split_key`：时间戳分割列名称。\n",
    "- `stratified_split_key`：分层分割列名称。\n",
    "- `training_fraction`：培训分数。\n",
    "- `validation_fraction`：验证分数。\n",
    "- `test_fraction`：测试分数。\n",
    "- `weight_column`：权重列名称。\n",
    "- `run_evaluation`：是否在培训过程中运行评估步骤。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eV4JrwB8wAkg"
   },
   "outputs": [],
   "source": [
    "run_evaluation = True  # @param {type:\"boolean\"}\n",
    "prediction_type = \"classification\"\n",
    "target_column = \"deposit\"\n",
    "\n",
    "# Fraction split\n",
    "training_fraction = 0.8\n",
    "validation_fraction = 0.1\n",
    "test_fraction = 0.1\n",
    "\n",
    "timestamp_split_key = None  # timestamp column name when using timestamp split\n",
    "stratified_split_key = None  # target column name when using stratified split\n",
    "\n",
    "predefined_split_key = None\n",
    "if predefined_split_key:\n",
    "    training_fraction = None\n",
    "    validation_fraction = None\n",
    "    test_fraction = None\n",
    "\n",
    "weight_column = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zyWGg2s09xOk"
   },
   "source": [
    "## 为 Dataflow 设置 VPC 配置\n",
    "\n",
    "在本节中，您需要定义以下参数：\n",
    "\n",
    "- `dataflow_subnetwork`：Dataflow 的完全限定子网络名称，当为空时将使用默认子网络。请参阅[示例](https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications)。\n",
    "- `dataflow_use_public_ips`：指定 Dataflow 工作节点是否使用公共 IP 地址。\n",
    "\n",
    "如果需要使用自定义的 Dataflow 子网络，您可以通过`dataflow_subnetwork` 参数进行设置。要求是：\n",
    "1. `dataflow_subnetwork` 必须是完全限定的子网络名称。\n",
    "   [[参考资料](https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications)]\n",
    "1. 以下服务账号必须在指定的 Dataflow 子网络上分配[计算网络用户角色](https://cloud.google.com/compute/docs/access/iam#compute.networkUser) ：\n",
    "    1. Compute Engine 默认服务账号：PROJECT_NUMBER-compute@developer.gserviceaccount.com\n",
    "    1. Dataflow 服务账号：service-PROJECT_NUMBER@dataflow-service-producer-prod.iam.gserviceaccount.com\n",
    "\n",
    "如果您的项目启用了 VPC-SC，请确保以下事项：\n",
    "\n",
    "1. 用于 VPC-SC 的 Dataflow 子网络已经为 Dataflow 正确配置。\n",
    "   参见[参考文档](https://cloud.google.com/dataflow/docs/guides/routes-firewall)。\n",
    "1. `dataflow_use_public_ips` 已设置为 False。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_TePNlLl9v1q"
   },
   "outputs": [],
   "source": [
    "dataflow_subnetwork = \"\"  # @param {type:\"string\"}\n",
    "dataflow_use_public_ips = True  # @param {type:\"boolean\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-iXXE14voyR"
   },
   "source": [
    "定制TabNet CustomJob配置并创建管道\n",
    "\n",
    "如果您确切地知道要在模型训练中使用哪些超参数值，创建TabNet CustomJob是最佳选择。它比HyperparameterTuningJob使用更少的训练资源。\n",
    "\n",
    "在下面的示例中，您可以配置以下关键参数：\n",
    "\n",
    "- `root_dir`：管道组件的根GCS目录。\n",
    "- `worker_pool_specs_override`：用于覆盖训练和评估工作人员池规范的字典。该字典应遵循特定格式。TabNet支持使用CPU和GPU进行训练。\n",
    "- `learning_rate`：线性优化器使用的学习率。\n",
    "- `max_steps`：训练器运行的步数。\n",
    "- `max_train_secs`：训练器运行的秒数。\n",
    "\n",
    "了解有关[管道输入和模型超参数](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-1.0.23/google_cloud_pipeline_components.experimental.automl.tabular.html#google_cloud_pipeline_components.experimental.automl.tabular.utils.get_tabnet_trainer_pipeline_and_parameters)的更多信息。\n",
    "\n",
    "了解有关[创建管道作业所需的参数](https://cloud.google.com/vertex-ai/docs/pipelines/run-pipeline#create_a_pipeline_run)的更多信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sG46cXVueb66"
   },
   "outputs": [],
   "source": [
    "# set a unique display name for your pipeline\n",
    "pipeline_job_id = \"tabnet-unique\"  # @param {type: \"string\"}\n",
    "# set the root dir\n",
    "pipeline_job_root_dir = os.path.join(BUCKET_URI, \"tabnet_custom_job\")\n",
    "# set the worker pool specs\n",
    "worker_pool_specs_override = [\n",
    "    {\"machine_spec\": {\"machine_type\": \"c2-standard-16\"}}  # Override for TF chief node\n",
    "]\n",
    "# set the learning rate\n",
    "learning_rate = 0.01\n",
    "# max_steps and/or max_train_secs must be set. If both are\n",
    "# specified, training stop after either condition is met.\n",
    "# By default, max_train_secs is set to -1.\n",
    "max_steps = 20\n",
    "\n",
    "max_train_secs = -1\n",
    "\n",
    "# To test GPU training, the worker_pool_specs_override can be specified like this.\n",
    "# worker_pool_specs_override =  [\n",
    "#     {\"machine_spec\": {\n",
    "#       'machine_type': \"n1-highmem-32\",\n",
    "#       \"accelerator_type\": \"NVIDIA_TESLA_V100\",\n",
    "#       \"accelerator_count\": 2\n",
    "#       }\n",
    "#     }\n",
    "#   ]\n",
    "\n",
    "# define the pipeline\n",
    "# If your system does not use Python, you can save the JSON file (`template_path`),\n",
    "# and use another programming language to submit the pipeline.\n",
    "(\n",
    "    template_path,\n",
    "    parameter_values,\n",
    ") = automl_tabular_utils.get_tabnet_trainer_pipeline_and_parameters(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    root_dir=pipeline_job_root_dir,\n",
    "    max_steps=max_steps,\n",
    "    max_train_secs=max_train_secs,\n",
    "    learning_rate=learning_rate,\n",
    "    target_column=target_column,\n",
    "    prediction_type=prediction_type,\n",
    "    tf_auto_transform_features=auto_transform_features,\n",
    "    run_feature_selection=RUN_FEATURE_SELECTION,\n",
    "    feature_selection_algorithm=FEATURE_SELECTION_ALGORITHM,\n",
    "    max_selected_features=MAX_SELECTED_FEATURES,\n",
    "    training_fraction=training_fraction,\n",
    "    validation_fraction=validation_fraction,\n",
    "    test_fraction=test_fraction,\n",
    "    data_source_csv_filenames=data_source_csv_filenames,\n",
    "    data_source_bigquery_table_path=data_source_bigquery_table_path,\n",
    "    worker_pool_specs_override=worker_pool_specs_override,\n",
    "    dataflow_use_public_ips=dataflow_use_public_ips,\n",
    "    dataflow_subnetwork=dataflow_subnetwork,\n",
    "    run_evaluation=run_evaluation,\n",
    ")\n",
    "\n",
    "# create the pipeline job\n",
    "training_pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name=pipeline_job_id,\n",
    "    template_path=template_path,\n",
    "    job_id=pipeline_job_id,\n",
    "    pipeline_root=pipeline_job_root_dir,\n",
    "    parameter_values=parameter_values,\n",
    "    enable_caching=False,\n",
    ")\n",
    "\n",
    "# run the pipeline\n",
    "training_pipeline_job.run(service_account=SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cbf2cf98842b"
   },
   "source": [
    "### 前往顶点模型界面\n",
    "\n",
    "通过下面单元格生成的链接，您可以部署模型并进行在线预测或批量预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "719a784573ce"
   },
   "outputs": [],
   "source": [
    "tabnet_trainer_pipeline_task_details = aiplatform.PipelineJob.get(\n",
    "    pipeline_job_id\n",
    ").gca_resource.job_detail.task_details\n",
    "CUSTOM_JOB_MODEL = get_model_name(pipeline_job_id)\n",
    "print(\"model uri:\", get_model_uri(tabnet_trainer_pipeline_task_details))\n",
    "print(\n",
    "    \"model artifacts:\",\n",
    "    get_model_artifacts_path(tabnet_trainer_pipeline_task_details, \"tabnet-trainer\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_sF8a2RKtRhg"
   },
   "source": [
    "## 自定义 TabNet 超参数调优作业配置并创建流水线\n",
    "\n",
    "为了在您的数据集上获得最佳的超参数设置，建议运行一个超参数调优作业。\n",
    "\n",
    "可以调优的超参数通过可选的`study_spec_parameters_override`参数设置。您提供一个名为`get_tabnet_study_spec_parameters_override`的辅助函数以获取这些超参数。在这个辅助函数中，您提供：\n",
    "\n",
    "- `dataset_size_bucket`：'small'（< 1M行），'medium'（1M - 100M行）或'large'（> 100M行）之一。\n",
    "- `training_budget_bucket`：'small'（< \\\\$600），'medium'（\\\\$600 - \\\\$2400）或'large'（> \\\\$2400）之一。\n",
    "- `prediction_type`：模型要生成的预测类型。“classification”或“regression”。\n",
    "\n",
    "然后，您获得超参数和范围的列表。`study_spec_parameters_override`可以为空，或者可以指定一个或多个上述超参数。对于未指定的超参数，您可以在流水线中设置它们的范围。了解有关[可用于调优的超参数](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-1.0.23/google_cloud_pipeline_components.experimental.automl.tabular.html#google_cloud_pipeline_components.experimental.automl.tabular.utils.get_tabnet_trainer_pipeline_and_parameters)的更多信息。\n",
    "\n",
    "除了超参数外，超参数调优作业需要以下值：\n",
    "\n",
    "- `root_dir`：流水线组件的根 GCS 目录。\n",
    "- `worker_pool_specs_override`：用于覆盖训练和评估工作池规格的字典。字典应遵循特定格式。TabNet 支持使用 CPU 和 GPU 进行训练。\n",
    "- `study_spec_metric_id`：要优化的度量标准，可能的值为['loss', 'average_loss', 'rmse', 'mae', 'mql', 'accuracy', 'auc', 'precision', 'recall']。\n",
    "- `study_spec_metric_goal`：度量标准的优化目标，可能的值为\"MAXIMIZE\"，\"MINIMIZE\"。\n",
    "- `max_trial_count`：期望的总试验次数。\n",
    "- `parallel_trial_count`：并行运行的试验次数。\n",
    "- `max_failed_trial_count`：需要在失败之前看到的失败试验次数。如果设置为0，则 Vertex AI 决定整个作业失败前必须有多少次试验失败。\n",
    "- `study_spec_algorithm`：用于研究的搜索算法。其中之一为'ALGORITHM_UNSPECIFIED'，'GRID_SEARCH'或'RANDOM_SEARCH'。\n",
    "\n",
    "了解有关[超参数调优作业参数](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-1.0.23/google_cloud_pipeline_components.experimental.automl.tabular.html#google_cloud_pipeline_components.experimental.automl.tabular.utils.get_tabnet_hyperparameter_tuning_job_pipeline_and_parameters)的更多信息。\n",
    "\n",
    "可以配置多个试验。根据`study_spec_metrics`中指定的度量标准，流水线返回最佳试验。在下面的示例中，您返回具有最低损失值的试验。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8hlPps2Rtpq-"
   },
   "outputs": [],
   "source": [
    "# set a unique display name for pipeline\n",
    "pipeline_job_id = \"tabnet-hpt-unique\"  # @param {type: \"string\"}\n",
    "# set the root dir\n",
    "pipeline_job_root_dir = os.path.join(BUCKET_URI, \"tabnet_hyperparameter_tuning_job\")\n",
    "# set the worker pool specs\n",
    "worker_pool_specs_override = [\n",
    "    {\"machine_spec\": {\"machine_type\": \"c2-standard-16\"}}  # Override for TF chief node\n",
    "]\n",
    "# set the metric\n",
    "study_spec_metric_id = \"loss\"\n",
    "# set the objective for metric\n",
    "study_spec_metric_goal = \"MINIMIZE\"\n",
    "\n",
    "# To test GPU training, the worker_pool_specs_override can be specified like this.\n",
    "# worker_pool_specs_override =  [\n",
    "#    {\n",
    "#       \"machine_spec\":{\n",
    "#          \"machine_type\":\"n1-highmem-32\",\n",
    "#          \"accelerator_type\":\"NVIDIA_TESLA_V100\",\n",
    "#          \"accelerator_count\":2\n",
    "#       }\n",
    "#    }\n",
    "# ]\n",
    "\n",
    "\n",
    "# define the component to get the hyperparameters\n",
    "# max_steps and/or max_train_secs must be set. If both are\n",
    "# specified, training stop after either condition is met.\n",
    "# By default, max_train_secs is set to -1 and max_steps is set to\n",
    "# an appropriate range given dataset_size and training budget.\n",
    "study_spec_parameters_override = (\n",
    "    automl_tabular_utils.get_tabnet_study_spec_parameters_override(\n",
    "        dataset_size_bucket=\"small\",\n",
    "        prediction_type=prediction_type,\n",
    "        training_budget_bucket=\"small\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# define the hyperparameter tuning pipeline\n",
    "# If your system does not use Python, you can save the JSON file (`template_path`),\n",
    "# and use another programming language to submit the pipeline.\n",
    "(\n",
    "    template_path,\n",
    "    parameter_values,\n",
    ") = automl_tabular_utils.get_tabnet_hyperparameter_tuning_job_pipeline_and_parameters(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    root_dir=pipeline_job_root_dir,\n",
    "    target_column=target_column,\n",
    "    prediction_type=prediction_type,\n",
    "    tf_auto_transform_features=auto_transform_features,\n",
    "    run_feature_selection=RUN_FEATURE_SELECTION,\n",
    "    feature_selection_algorithm=FEATURE_SELECTION_ALGORITHM,\n",
    "    max_selected_features=MAX_SELECTED_FEATURES,\n",
    "    training_fraction=training_fraction,\n",
    "    validation_fraction=validation_fraction,\n",
    "    test_fraction=test_fraction,\n",
    "    data_source_csv_filenames=data_source_csv_filenames,\n",
    "    data_source_bigquery_table_path=data_source_bigquery_table_path,\n",
    "    study_spec_metric_id=study_spec_metric_id,\n",
    "    study_spec_metric_goal=study_spec_metric_goal,\n",
    "    study_spec_parameters_override=study_spec_parameters_override,\n",
    "    max_trial_count=1,\n",
    "    parallel_trial_count=1,\n",
    "    max_failed_trial_count=0,\n",
    "    worker_pool_specs_override=worker_pool_specs_override,\n",
    "    dataflow_use_public_ips=dataflow_use_public_ips,\n",
    "    dataflow_subnetwork=dataflow_subnetwork,\n",
    "    run_evaluation=True,\n",
    ")\n",
    "\n",
    "# create the pipeline job\n",
    "tuning_pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name=pipeline_job_id,\n",
    "    template_path=template_path,\n",
    "    job_id=pipeline_job_id,\n",
    "    pipeline_root=pipeline_job_root_dir,\n",
    "    parameter_values=parameter_values,\n",
    "    enable_caching=False,\n",
    ")\n",
    "\n",
    "# run the pipeline job\n",
    "tuning_pipeline_job.run(service_account=SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2749d8cec287"
   },
   "source": [
    "### 前往顶点模型UI\n",
    "\n",
    "通过下面单元格生成的链接，您可以部署模型并进行在线预测或批量预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "730af2836871"
   },
   "outputs": [],
   "source": [
    "tabnet_hpt_pipeline_task_details = aiplatform.PipelineJob.get(\n",
    "    pipeline_job_id\n",
    ").gca_resource.job_detail.task_details\n",
    "HPT_JOB_MODEL = get_model_name(pipeline_job_id)\n",
    "\n",
    "print(\"model uri:\", get_model_uri(tabnet_hpt_pipeline_task_details))\n",
    "print(\n",
    "    \"model artifacts:\",\n",
    "    get_model_artifacts_path(\n",
    "        tabnet_hpt_pipeline_task_details, \"get-best-hyperparameter-tuning-job-trial\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43342a43176e"
   },
   "source": [
    "清理顶点和BigQuery资源\n",
    "\n",
    "要清理此项目中使用的所有Google Cloud资源，您可以[删除用于教程的Google Cloud项目](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects)。\n",
    "\n",
    "否则，您可以删除在此教程中创建的各个资源：\n",
    "\n",
    "- 从CustomJob管道中的管道\n",
    "- 从HyperparameterTuningJob管道中的管道\n",
    "- 从CustomJob管道中的模型\n",
    "- 从HyperparameterTuningJob管道中的模型\n",
    "- Cloud Storage存储桶（将`delete_bucket`设置为True可删除存储桶）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ad8d12061a65"
   },
   "outputs": [],
   "source": [
    "# Delete the training pipeline job\n",
    "training_pipeline_job.delete()\n",
    "\n",
    "# Delete the tuning pipeline job\n",
    "tuning_pipeline_job.delete()\n",
    "\n",
    "# Delete model resources\n",
    "custom_job_model = aiplatform.Model(CUSTOM_JOB_MODEL)\n",
    "hpt_job_model = aiplatform.Model(HPT_JOB_MODEL)\n",
    "custom_job_model.delete()\n",
    "hpt_job_model.delete()\n",
    "\n",
    "# Delete bucket\n",
    "delete_bucket = False\n",
    "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
    "    ! gsutil -m rm -r $BUCKET_URI"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "tabnet_on_vertex_pipelines.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
