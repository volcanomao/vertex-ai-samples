{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Vertex AI - 使用LoRA进行Llama2微调并在TPUv5e上提供服务\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/training/tpuv5e_llama2_pytorch_finetuning_and_serving.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> 在Colab中运行\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/training/tpuv5e_llama2_pytorch_finetuning_and_serving.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      在GitHub上查看\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/notebooks/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/training/tpuv5e_llama2_pytorch_finetuning_and_serving.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "在Vertex AI Workbench中打开\n",
    "    </a> \n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9e4695f5e9dc"
   },
   "source": [
    "# 配额 - 确保在开始之前完成！\n",
    "\n",
    "为了运行此示例，您需要获得以下TPUv5e的配额批准。您可以通过IAM和管理 > 配额 或联系您的Google帐户团队发出请求：\n",
    "\n",
    "aiplatform.googleapis.com/custom_model_serving_tpu_v5e (4-8芯片。Llama2 7B的最少4芯片)\n",
    "aiplatform.googleapis.com/custom_model_training_tpu_v5e (至少16芯片)\n",
    "\n",
    "查看[TPU价格页](https://cloud.google.com/tpu/pricing)了解区域可用性和定价。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## 概述\n",
    "\n",
    "本笔记本演示了使用[LoRA](https://huggingface.co/docs/peft/v0.9.0/en/package_reference/lora#peft.LoraConfig)对Llama2 7B模型进行微调，并在TPUv5e上进行微调和服务。微调基于一个[Hugging Face示例](https://huggingface.co/google/gemma-7b/blob/main/examples/example_fsdp.py)，该示例使用[PyTorch XLA中的完全分片数据并行](https://pytorch.org/blog/scaling-pytorch-models-on-cloud-tpus-with-fsdp/)和[SPMD](https://pytorch.org/blog/pytorch-xla-spmd/)。点击链接了解更多信息。\n",
    "\n",
    "\n",
    "微调通过[Vertex AI自定义训练作业](https://cloud.google.com/vertex-ai/docs/training/create-custom-job)进行。Vertex AI自定义训练作业允许对微调作业进行更高级别的定制和控制。本笔记本中的所有示例都使用参数Low-Rank Adaption [LoRA](https://huggingface.co/docs/peft/en/package_reference/lora)来降低训练和存储成本。\n",
    "\n",
    "本笔记本使用Hex-LLM部署模型，Hex-LLM是一个由Google Cloud开发的基于XLA构建的高效大型语言模型服务解决方案\n",
    "\n",
    "\n",
    "### 目标\n",
    "\n",
    "- 使用Vertex AI自定义训练作业和Vertex Prediction端点微调和部署Llama2模型。\n",
    "- 发送预测请求到您微调的Llama2模型。\n",
    "\n",
    "\n",
    "### 成本\n",
    "\n",
    "本教程使用Google Cloud的收费组件：\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "了解[Vertex AI价格](https://cloud.google.com/vertex-ai/pricing)、[Cloud Storage价格](https://cloud.google.com/storage/pricing)以及使用[Pricing Calculator](https://cloud.google.com/products/calculator/)根据您的预计使用情况生成成本估算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08d289fa873f"
   },
   "source": [
    "### 数据集\n",
    "\n",
    "在这个例子中，您将使用Hugging Face提供的english_quotes数据集来微调模型。有关数据集的详细信息可以在此处找到：https://huggingface.co/datasets/Abirate/english_quotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aed92deeb4a0"
   },
   "source": [
    "成本\n",
    "\n",
    "本教程使用 Google Cloud 的计费组件：\n",
    "\n",
    "Vertex AI（训练、预测、TPUv5e）、Cloud 存储\n",
    "\n",
    "请了解[Vertex AI 定价](https://cloud.google.com/vertex-ai/pricing)，[Cloud 存储定价](https://cloud.google.com/storage/pricing)，[Cloud NL API 定价](https://cloud.google.com/natural-language/pricing)并使用[定价计算器](https://cloud.google.com/products/calculator/)根据您的预期使用情况生成成本估算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "## 安装\n",
    "\n",
    "安装以下必需的软件包以执行此笔记本。\n",
    "\n",
    "运行以下命令安装支持 TPUv5e 的最新 Google 云平台库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2b4ef9b72d43"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# (optional) update gcloud if needed\n",
    "if os.getenv(\"IS_TESTING\"):\n",
    "    ! gcloud components update --quiet\n",
    "\n",
    "! pip3 install --upgrade --quiet google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58707a750154"
   },
   "source": [
    "只有Colab：取消注释下面的单元格以重新启动内核。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f200f10a1da3"
   },
   "outputs": [],
   "source": [
    "# import IPython\n",
    "\n",
    "# app = IPython.Application.instance()\n",
    "# app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF1j6f9HApxa"
   },
   "source": [
    "## 在开始之前\n",
    "\n",
    "### 设置您的 Google Cloud 项目\n",
    "\n",
    "**无论您使用什么笔记本环境，都需要进行以下步骤。**\n",
    "\n",
    "1. [选择或创建一个 Google Cloud 项目](https://console.cloud.google.com/cloud-resource-manager)。当您第一次创建账户时，您会获得 $300 的免费信用额度用于支付计算/存储成本。\n",
    "\n",
    "2. [确保您的项目已启用计费](https://cloud.google.com/billing/docs/how-to/modify-project)。\n",
    "\n",
    "3. [启用 Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com)。\n",
    "\n",
    "4. 如果您在本地运行此笔记本，您需要安装 [Cloud SDK](https://cloud.google.com/sdk)。\n",
    "\n",
    "5. [选择或创建一个 Cloud 存储存储桶](https://cloud.google.com/storage/docs/creating-buckets) 用于存储实验输出。\n",
    "\n",
    "6. [创建一个服务帐号](https://cloud.google.com/iam/docs/service-accounts-create#iam-service-accounts-create-console)，并赋予 `Vertex AI User` 和 `Storage Object Admin` 角色，用于部署微调模型到 Vertex AI 终端点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WReHDGG5g0XY"
   },
   "source": [
    "设置您的项目ID\n",
    "\n",
    "**如果您不知道您的项目ID**，请尝试以下操作：\n",
    "* 运行 `gcloud config list`。\n",
    "* 运行 `gcloud projects list`。\n",
    "* 参考支持页面：[查找项目ID](https://support.google.com/googleapi/answer/7014113)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oM1iC_MfAts1"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
    "\n",
    "# Set the project id\n",
    "! gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "#### 区域\n",
    "\n",
    "您还可以更改 Vertex AI 使用的 `REGION` 变量。欲了解更多关于 [Vertex AI 区域](https://cloud.google.com/vertex-ai/docs/general/locations)。\n",
    "\n",
    "TPUv5e 可在[此处列出的以下区域](https://cloud.google.com/tpu/pricing)中使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "region"
   },
   "outputs": [],
   "source": [
    "REGION = \"us-west1\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBCra4QMA2wR"
   },
   "source": [
    "### 验证您的Google Cloud帐户\n",
    "\n",
    "根据您的Jupyter环境，您可能需要手动验证。请按照以下相关说明操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74ccc9e52986"
   },
   "source": [
    "1. 顶点 AI 工作台\n",
    "* 无需操作，您已经通过身份验证。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "de775a3773ba"
   },
   "source": [
    "2. 本地JupyterLab实例，取消注释并运行："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "254614fa0c46"
   },
   "outputs": [],
   "source": [
    "# ! gcloud auth login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ef21552ccea8"
   },
   "source": [
    "3. 协作，取消注释并运行:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "603adbbf0532"
   },
   "outputs": [],
   "source": [
    "# from google.colab import auth\n",
    "# auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6b2ccc891ed"
   },
   "source": [
    "查看如何在https://cloud.google.com/storage/docs/gsutil/commands/iam#ch-examples为您的服务帐号授予云存储权限。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "960505627ddf"
   },
   "source": [
    "### 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PyQmSRbKA8r-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "创建一个云存储桶\n",
    "创建一个存储桶，用于存储中间产物，如数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "BUCKET_URI = f\"gs://your-bucket-name-{PROJECT_ID}-unique\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "648eb19b83dd"
   },
   "source": [
    "为分阶段、环境和模型构件设置文件夹路径。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "855d6b96f291"
   },
   "outputs": [],
   "source": [
    "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
    "\n",
    "# The service account looks like:\n",
    "# '@.iam.gserviceaccount.com'\n",
    "# Please go to https://cloud.google.com/iam/docs/service-accounts-create#iam-service-accounts-create-console\n",
    "# and create service account with `Vertex AI User` and `Storage Object Admin` roles.\n",
    "# The service account for deploying fine tuned model.\n",
    "SERVICE_ACCOUNT = \"[your-service-account]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EcIXiGsCePi"
   },
   "source": [
    "只有在您的存储桶尚不存在时才能运行以下单元格以创建您的云存储存储桶。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIq7R4HZCfIc"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l {REGION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2931d2e5a94"
   },
   "source": [
    "### 访问Llama 2预训练和微调模型\n",
    "Meta的原始模型被转换为Hugging Face格式，用于微调和在Vertex AI中提供服务。\n",
    "\n",
    "接受模型协议以访问模型：\n",
    "\n",
    "1. 转到Google Cloud控制台中的Vertex AI > Model Garden页面\n",
    "2. 搜索Llama 2\n",
    "3. 查看在模型卡页面弹出的协议\n",
    "4. 接受Llama 2的协议\n",
    "5. 在文档选项卡上，将共享包含Llama 2预训练和微调模型的Cloud Storage存储桶链接\n",
    "6. 将Cloud Storage存储桶链接粘贴到下面，并将其分配给VERTEX_AI_MODEL_GARDEN_LLAMA2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "47330c9cae9f"
   },
   "outputs": [],
   "source": [
    "VERTEX_AI_MODEL_GARDEN_LLAMA2 = \"<Bucket path from documentation tab of Llama 2 in Vertex Model Garden>\"  # This will be shared once click the agreement of LLaMA2 in Vertex AI Model Garden.\n",
    "VERTEX_MODEL_ID = \"llama2-7b-hf\"\n",
    "HF_MODEL_ID = \"meta-llama/Llama-2-7b-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "890b49a97355"
   },
   "outputs": [],
   "source": [
    "assert (\n",
    "    VERTEX_AI_MODEL_GARDEN_LLAMA2\n",
    "), \"Please click the agreement of Llama 2 in Vertex AI Model Garden, and get the GCS path of Llama 2 model artifacts.\"\n",
    "print(\n",
    "    \"Copy Llama 2 model artifacts from\",\n",
    "    VERTEX_AI_MODEL_GARDEN_LLAMA2,\n",
    "    \"to \",\n",
    "    f\"{BUCKET_URI}/{HF_MODEL_ID}\",\n",
    ")\n",
    "\n",
    "# Copy model files to your bucket\n",
    "! gcloud storage cp -R $VERTEX_AI_MODEL_GARDEN_LLAMA2/$VERTEX_MODEL_ID/* $BUCKET_URI/$HF_MODEL_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "source": [
    "### 初始化 Python 的 Vertex AI SDK\n",
    "\n",
    "为您的项目初始化 Python 的 Vertex AI SDK。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "832bbd8cb9ef"
   },
   "source": [
    "### 创建构建包注册库并设置自定义的Docker镜像URI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18fd49b88127"
   },
   "outputs": [],
   "source": [
    "REPOSITORY = \"tpuv5e-training-repository-unique\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c2242d87effa"
   },
   "outputs": [],
   "source": [
    "image_name_train = \"llama2-7b-hf-lora-tuning-tpuv5e\"\n",
    "hostname = f\"{REGION}-docker.pkg.dev\"\n",
    "tag = \"latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9dd94acb26e9"
   },
   "outputs": [],
   "source": [
    "# Register gcloud as a Docker credential helper\n",
    "!gcloud auth configure-docker $REGION-docker.pkg.dev --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5c8e8364a097"
   },
   "outputs": [],
   "source": [
    "# One time or use an existing repository\n",
    "!gcloud artifacts repositories create $REPOSITORY --repository-format=docker \\\n",
    "--location=$REGION --description=\"Vertex TPUv5e training repository\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "16488f1fc702"
   },
   "outputs": [],
   "source": [
    "# Define container image name\n",
    "PYTORCH_TRAIN_DOCKER_URI = (\n",
    "    f\"{hostname}/{PROJECT_ID}/{REPOSITORY}/{image_name_train}:{tag}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0c250872074f"
   },
   "source": [
    "### 定义常用函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "354da31189dc"
   },
   "outputs": [],
   "source": [
    "def get_job_name_with_datetime(prefix: str) -> str:\n",
    "    \"\"\"Gets the job name with date time when triggering training or deployment\n",
    "    jobs in Vertex AI.\n",
    "    \"\"\"\n",
    "    return prefix + datetime.now().strftime(\"_%Y%m%d_%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e11e1f8a6c4c"
   },
   "source": [
    "构建Docker容器文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19d584f9fe62"
   },
   "source": [
    "创建训练师目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "909e93e7fd43"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists(\"trainer\"):\n",
    "    os.makedirs(\"trainer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8a91ddf0cbcb"
   },
   "source": [
    "创建用于自定义容器的Dockerfile。这将安装Hugging Face transformers、datasets、trl 和 peft 用于微调。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "756577886992"
   },
   "outputs": [],
   "source": [
    "%%writefile trainer/Dockerfile\n",
    "# This Dockerfile fine tunes the Llamas2 model using LoRA with PyTorch XLA\n",
    "# Nightly TPU VM docker image\n",
    "FROM us-central1-docker.pkg.dev/tpu-pytorch-releases/docker/xla:nightly_3.10_tpuvm_20240324\n",
    "\n",
    "ENV DEBIAN_FRONTEND=noninteractive\n",
    "\n",
    "# Install basic libs\n",
    "RUN apt-get update && apt-get -y upgrade && apt-get install -y --no-install-recommends \\\n",
    "        cmake \\\n",
    "        curl \\\n",
    "        wget \\\n",
    "        sudo \\\n",
    "        gnupg \\\n",
    "        libsm6 \\\n",
    "        libxext6 \\\n",
    "        libxrender-dev \\\n",
    "        lsb-release \\\n",
    "        ca-certificates \\\n",
    "        build-essential \\\n",
    "        git \\\n",
    "        libgl1\n",
    "\n",
    "# Copy Apache license.\n",
    "RUN wget https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/LICENSE\n",
    "\n",
    "# Install required libs\n",
    "RUN pip install --upgrade pip\n",
    "RUN pip install --upgrade pip\n",
    "RUN pip install transformers==4.38.2 -U\n",
    "RUN pip install datasets==2.18.0\n",
    "RUN pip install trl==0.8.1 peft==0.10.0\n",
    "RUN pip install accelerate==0.28.0\n",
    "RUN pip install --upgrade google-cloud-storage\n",
    "\n",
    "# Copy other licenses.\n",
    "RUN wget -O MIT_LICENSE https://github.com/pytest-dev/pytest/blob/main/LICENSE\n",
    "RUN wget -O BSD_LICENSE https://github.com/pytorch/xla/blob/master/LICENSE\n",
    "RUN wget -O BSD-3_LICENSE https://github.com/pytorch/pytorch/blob/main/LICENSE\n",
    "\n",
    "# Copy install libtpu to PATH above\n",
    "RUN find ./usr/local/lib -name 'libtpu.so' -exec cp {} /lib \\;\n",
    "\n",
    "WORKDIR /\n",
    "COPY train.py train.py\n",
    "ENV PYTHONPATH ./\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ee7ac9469ce"
   },
   "source": [
    "请添加__init__.py文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b0b57ecd02d8"
   },
   "outputs": [],
   "source": [
    "!touch trainer/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c648296e1057"
   },
   "source": [
    "添加train.py文件\n",
    "\n",
    "这段代码来自于LoRA分布式微调代码，可以在这个示例中找到：https://ai.google.dev/gemma/docs/distributed_tuning\n",
    "\n",
    "IMDB TensorFlow数据集用于对Gemma模型进行微调。还添加了额外的逻辑来处理TPUv5e所需的TPU拓扑设置：https://cloud.google.com/tpu/docs/v5e#tpu-v5e-config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1b6a47a6089c"
   },
   "outputs": [],
   "source": [
    "%%writefile trainer/train.py\n",
    "import os, sys\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch_xla\n",
    "import torch_xla.core.xla_model as xm\n",
    "\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, PeftModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "# use spmd\n",
    "import torch_xla.runtime as xr\n",
    "xr.use_spmd()\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    \"--tpu_topology\",\n",
    "    help=\"Topology to use for the TPUv5e (2x2, 2x4, 4x4)\",\n",
    "    default=\"4x4\",\n",
    "    type=str\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--model_name\",\n",
    "    help=\"Llama2 model name (meta-llama/Llama-2-7b-hf, meta-llama/Llama-2-13b-hf)\",\n",
    "    default=\"meta-llama/Llama-2-7b-hf\",\n",
    "    type=str\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--bucket_name\",\n",
    "    help=\"The name of the bucket you copied the Llama2 model files to\",\n",
    "    required=True,\n",
    "    type=str\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--output_folder\",\n",
    "    type=str,\n",
    "    required=True,\n",
    "    help=\"Output folder name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--checkpoint_directory\",\n",
    "    type=str,\n",
    "    default=\"output_ckpt\",\n",
    "    help=\"Checkpoint Directory name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--epochs\",\n",
    "    type=int,\n",
    "    default=10,\n",
    "    help=\"Number of epochs to train\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--merged_model_folder\",\n",
    "    type=str,\n",
    "    default=\"llama2-7b-hf/modelfiles\",\n",
    "    help=\"Checkpoint Directory name\",\n",
    ")\n",
    "args = parser.parse_args()\n",
    "\n",
    "GCS_PREFIX = \"gs://\"\n",
    "\n",
    "def is_gcs_path(input_path: str) -> bool:\n",
    "    return input_path.startswith(GCS_PREFIX)\n",
    "\n",
    "def download_gcs_dir(gcs_dir: str, local_dir: str):\n",
    "    \"\"\"Download files in a GCS directory to a local directory.\n",
    "\n",
    "    For example:\n",
    "    download_gcs_dir(gs://bucket/foo, /tmp/bar)\n",
    "    gs://bucket/foo/a -> /tmp/bar/a\n",
    "    gs://bucket/foo/b/c -> /tmp/bar/b/c\n",
    "\n",
    "    Arguments:\n",
    "    gcs_dir: A string of directory path on GCS.\n",
    "    local_dir: A string of local directory path.\n",
    "    \"\"\"\n",
    "    if not is_gcs_path(gcs_dir):\n",
    "        raise ValueError(f\"{gcs_dir} is not a GCS path starting with gs://.\")\n",
    "\n",
    "    bucket_name = gcs_dir.split(\"/\")[2]\n",
    "    prefix = gcs_dir[len(GCS_PREFIX + bucket_name) :].strip(\"/\")\n",
    "    client = storage.Client()\n",
    "    blobs = client.list_blobs(bucket_name, prefix=prefix)\n",
    "    for blob in blobs:\n",
    "        if blob.name[-1] == \"/\":\n",
    "            continue\n",
    "        file_path = blob.name[len(prefix) :].strip(\"/\")\n",
    "        local_file_path = os.path.join(local_dir, file_path)\n",
    "        os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n",
    "        blob.download_to_filename(local_file_path)\n",
    "        print (f'download of {local_file_path} complete')\n",
    "    print (f'Show all files in directory {os.listdir(local_dir)}')\n",
    "\n",
    "def upload_directory_with_transfer_manager(bucket_name, source_directory, blob_name_prefix, workers=8):\n",
    "    \"\"\"Upload every file in a directory, including all files in subdirectories.\n",
    "\n",
    "    Each blob name is derived from the filename, not including the `directory`\n",
    "    parameter itself. For complete control of the blob name for each file (and\n",
    "    other aspects of individual blob metadata), use\n",
    "    transfer_manager.upload_many() instead.\n",
    "    \"\"\"\n",
    "\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "\n",
    "    # The directory on your computer to upload. Files in the directory and its\n",
    "    # subdirectories will be uploaded. An empty string means \"the current\n",
    "    # working directory\".\n",
    "    # source_directory=\"\"\n",
    "\n",
    "    # blob_name_prefix = prefix for the files being uploaded to GCS\n",
    "    # example: file1 and file2 in a folder uploaded to my-bucket with blob_name_prefix=my-folder/a/\n",
    "    # will be uploaded to gs://my-bucket/my-folder/a/file1 and gs://my-bucket/my-folder/a/file2\n",
    "    \n",
    "    # The maximum number of processes to use for the operation. The performance\n",
    "    # impact of this value depends on the use case, but smaller files usually\n",
    "    # benefit from a higher number of processes. Each additional process occupies\n",
    "    # some CPU and memory resources until finished. Threads can be used instead\n",
    "    # of processes by passing `worker_type=transfer_manager.THREAD`.\n",
    "    # workers=8\n",
    "\n",
    "    from pathlib import Path\n",
    "\n",
    "    from google.cloud.storage import Client, transfer_manager\n",
    "\n",
    "    storage_client = Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    # Generate a list of paths (in string form) relative to the `directory`.\n",
    "    # This can be done in a single list comprehension, but is expanded into\n",
    "    # multiple lines here for clarity.\n",
    "\n",
    "    # First, recursively get all files in `directory` as Path objects.\n",
    "    directory_as_path_obj = Path(source_directory)\n",
    "    paths = directory_as_path_obj.rglob(\"*\")\n",
    "\n",
    "    # Filter so the list only includes files, not directories themselves.\n",
    "    file_paths = [path for path in paths if path.is_file()]\n",
    "\n",
    "    # These paths are relative to the current working directory. Next, make them\n",
    "    # relative to `directory`\n",
    "    relative_paths = [path.relative_to(source_directory) for path in file_paths]\n",
    "\n",
    "    # Finally, convert them all to strings.\n",
    "    string_paths = [str(path) for path in relative_paths]\n",
    "\n",
    "    print(\"Found {} files.\".format(len(string_paths)))\n",
    "\n",
    "    # Start the upload.\n",
    "    print (f\"source directory {source_directory}\")\n",
    "    results = transfer_manager.upload_many_from_filenames(\n",
    "        bucket, string_paths, blob_name_prefix=blob_name_prefix, source_directory=source_directory, max_workers=workers\n",
    "    )\n",
    "\n",
    "    for name, result in zip(string_paths, results):\n",
    "        # The results list is either `None` or an exception for each filename in\n",
    "        # the input list, in order.\n",
    "\n",
    "        if isinstance(result, Exception):\n",
    "            print(\"Failed to upload {} due to exception: {}\".format(name, result))\n",
    "        else:\n",
    "            print(\"Uploaded {} to {}/{}.\".format(name, bucket.name, blob_name_prefix))\n",
    "    \n",
    "def main():\n",
    "    x = args.tpu_topology.split(\"x\")\n",
    "    tpu_topology_x = int(x[0])\n",
    "    tpu_topology_y = int(x[1])\n",
    "    print (f'TPU topology is ({tpu_topology_x}, {tpu_topology_y})')\n",
    "    print (f'Model name is {args.model_name}')\n",
    "    \n",
    "    # Set batch size to 8 for each chip\n",
    "    BATCH_SIZE = 8 * tpu_topology_x * tpu_topology_y\n",
    "    # For anything larger than an 8 chip instance, set the BATCH_SIZE to 128, since we run out of samples\n",
    "    if (tpu_topology_x * tpu_topology_y) >=16:\n",
    "        BATCH_SIZE = 128\n",
    "    \n",
    "    # Set download directory to a tempory folder\n",
    "    DL_DIR=\"/tmp/modelfiles\"\n",
    "    if not os.path.exists(DL_DIR):\n",
    "        os.makedirs(DL_DIR)\n",
    "\n",
    "    print ('Downloading data to temporary folder')\n",
    "    download_gcs_dir (f\"gs://{args.bucket_name}/{args.model_name}\", DL_DIR)\n",
    "    \n",
    "    # Create output folders\n",
    "    if not os.path.exists(f\"/tmp/{args.output_folder}\"):\n",
    "        os.makedirs(f\"/tmp/{args.output_folder}\")\n",
    "    if not os.path.exists(f\"/tmp/{args.checkpoint_directory}\"):\n",
    "        os.makedirs(f\"/tmp/{args.checkpoint_directory}\")\n",
    "\n",
    "    device = xm.xla_device()\n",
    "    \n",
    "    # Set tokenizer parallelism to false to avoid warnings\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(DL_DIR)\n",
    "    print ('Loaded tokenizer')\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(DL_DIR, torch_dtype=torch.bfloat16)\n",
    "    print ('Loaded base model')\n",
    "\n",
    "    # Set LoRA configuration\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\"k_proj\", \"v_proj\"],\n",
    "    )\n",
    "    \n",
    "    # Required when using Llama2, as the tokenizer has no padding\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Load the dataset and format it for training.\n",
    "    data = load_dataset(\"Abirate/english_quotes\", split=\"train\")\n",
    "    max_seq_length = 512\n",
    "    print ('Loaded dataset')\n",
    "\n",
    "    # Set up the FSDP config. To enable FSDP via SPMD, set xla_fsdp_v2 to True.\n",
    "    fsdp_config = {\"fsdp_transformer_layer_cls_to_wrap\": [\n",
    "            \"LlamaDecoderLayer\"\n",
    "        ],\n",
    "        \"xla\": True,\n",
    "        \"xla_fsdp_v2\": True,\n",
    "        \"xla_fsdp_grad_ckpt\": True}\n",
    "\n",
    "    OUTPUT_DIR=f\"/tmp/{args.output_folder}\"\n",
    "    CHECKPOINT_DIR=f\"/tmp/{args.checkpoint_directory}\"\n",
    "\n",
    "    # Finally, set up the trainer and train the model.\n",
    "    trainer = SFTTrainer(\n",
    "        model=base_model,\n",
    "        train_dataset=data,\n",
    "        args=TrainingArguments(\n",
    "            per_device_train_batch_size=BATCH_SIZE,  # This is actually the global batch size for SPMD.\n",
    "            num_train_epochs=args.epochs,\n",
    "            max_steps=-1,\n",
    "            output_dir=OUTPUT_DIR,\n",
    "            optim=\"adafactor\",\n",
    "            logging_steps=1,\n",
    "            dataloader_drop_last = True,  # Required for SPMD.\n",
    "            fsdp=\"full_shard\",\n",
    "            fsdp_config=fsdp_config,\n",
    "        ),\n",
    "        peft_config=lora_config,\n",
    "        dataset_text_field=\"quote\",\n",
    "        max_seq_length=max_seq_length,\n",
    "        packing=True,\n",
    "    )\n",
    "\n",
    "    # train\n",
    "    trainer.train()\n",
    "    \n",
    "    adapter_model_id = \"adapter_model\"\n",
    "    adapter_path = f\"{CHECKPOINT_DIR}/{adapter_model_id}\"\n",
    "    merged_model_id = \"merged_model\"\n",
    "    merged_model_path = f\"{CHECKPOINT_DIR}/{merged_model_id}\"\n",
    "    \n",
    "    trainer.model.to('cpu').save_pretrained(adapter_path)\n",
    "    \n",
    "    # Save the adapter, merged model, and tokenizer\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(DL_DIR, torch_dtype=torch.bfloat16)\n",
    "    peft_model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "    merged_model = peft_model.merge_and_unload()\n",
    "    merged_model.save_pretrained(merged_model_path,safe_serialization=False)\n",
    "    tokenizer.save_pretrained(merged_model_path)\n",
    "    \n",
    "    # Copy merged files to GCS folder\n",
    "    OUTPUT_PREFIX=f\"{args.merged_model_folder}/{merged_model_id}/{xr.process_index()}/\"\n",
    "    upload_directory_with_transfer_manager(bucket_name=args.bucket_name,source_directory=merged_model_path,\n",
    "                                       blob_name_prefix=OUTPUT_PREFIX)\n",
    "    print ('Uploaded merged model files')\n",
    "\n",
    "    # copy adapter files to GCS folder\n",
    "    OUTPUT_PREFIX=f\"{args.merged_model_folder}/{adapter_model_id}/{xr.process_index()}/\"\n",
    "    upload_directory_with_transfer_manager(bucket_name=args.bucket_name,source_directory=adapter_path,\n",
    "                                       blob_name_prefix=OUTPUT_PREFIX)\n",
    "    print ('Uploaded adapter model files')\n",
    "\n",
    "    print ('Exiting job')\n",
    "    sys.exit(0)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pq4iF00YG_4T"
   },
   "source": [
    "使用Vertex AI自定义训练作业进行微调\n",
    "\n",
    "本节演示如何使用PEFT LoRA在Vertex AI自定义训练作业上微调和部署Llama2模型。 LoRA（Low-Rank Adaptation）是PEFT（参数高效微调）的一种方法，其中预训练模型的权重被冻结，并在微调过程中训练表示模型权重变化的秩分解矩阵。 有关LoRA的更多信息，请阅读以下出版物：[Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L. and Chen, W., 2021. Lora: Low-rank adaptation of large language models. *arXiv preprint arXiv:2106.09685*](https://arxiv.org/abs/2106.09685)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1b90914fc81"
   },
   "source": [
    "### 将docker配置为普通用户的运行权限"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "232a259d3edc"
   },
   "outputs": [],
   "source": [
    "!sudo usermod -a -G docker ${USER}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e028d107fbe5"
   },
   "source": [
    "将目录切换到trainer目录以构建docker容器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3a9390f87b66"
   },
   "outputs": [],
   "source": [
    "%cd trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81eb3c13afa9"
   },
   "source": [
    "构建定制的Docker容器并推送到存储库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ee497559677f"
   },
   "outputs": [],
   "source": [
    "!docker build -t $PYTORCH_TRAIN_DOCKER_URI -f Dockerfile ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0715b34162b4"
   },
   "outputs": [],
   "source": [
    "!docker push $PYTORCH_TRAIN_DOCKER_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "586a13cb67b4"
   },
   "source": [
    "回到你的主目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "937b7269c93b"
   },
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08bf867e8f4c"
   },
   "source": [
    "设置GCS文件夹位置和作业配置设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "17392b36e9c0"
   },
   "outputs": [],
   "source": [
    "# Create a GCS folder to store the merged model with the base model and the\n",
    "# fine-tuned LORA adapter.\n",
    "BUCKET_NAME = BUCKET_URI.replace(\"gs://\", \"\")\n",
    "OUTPUT_DIR_NAME = \"output\"\n",
    "CHECKPOINT_DIR_NAME = \"output_chk\"\n",
    "NUM_EPOCHS = 200\n",
    "MERGED_MODEL_FOLDER = \"llama2-7b-hf/modelfiles\"\n",
    "\n",
    "# See machines type to match chips being used\n",
    "# Topologies of 2x2, 2x4, 4x4 = 4, 8, 16 chip settings and use quota from aiplatform.googleapis.com/custom_model_training_tpu_v5e\n",
    "MACHINE_TYPE = \"ct5lp-hightpu-4t\"\n",
    "TPU_TOPOLOGY = \"4x4\"\n",
    "\n",
    "DISPLAY_NAME_PREFIX = f\"llama2-7b-lora-train-{TPU_TOPOLOGY}\"\n",
    "tpuv5e_llama2_peft_job = {\n",
    "    \"display_name\": get_job_name_with_datetime(DISPLAY_NAME_PREFIX),\n",
    "    \"job_spec\": {\n",
    "        \"worker_pool_specs\": [\n",
    "            {\n",
    "                \"machine_spec\": {\n",
    "                    \"machine_type\": MACHINE_TYPE,\n",
    "                    \"tpu_topology\": TPU_TOPOLOGY,\n",
    "                },\n",
    "                \"replica_count\": 1,\n",
    "                \"container_spec\": {\n",
    "                    \"image_uri\": PYTORCH_TRAIN_DOCKER_URI,\n",
    "                    \"args\": [\n",
    "                        f\"--tpu_topology={TPU_TOPOLOGY}\",\n",
    "                        f\"--model_name={HF_MODEL_ID}\",\n",
    "                        f\"--bucket_name={BUCKET_NAME}\",\n",
    "                        f\"--output_folder={OUTPUT_DIR_NAME}\",\n",
    "                        f\"--checkpoint_directory={CHECKPOINT_DIR_NAME}\",\n",
    "                        f\"--epochs={NUM_EPOCHS}\",\n",
    "                        f\"--merged_model_folder={MERGED_MODEL_FOLDER}\",\n",
    "                    ],\n",
    "                },\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "}\n",
    "\n",
    "tpuv5e_llama2_peft_job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0c2bc267c4a5"
   },
   "source": [
    "创建作业客户端并运行作业"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bc9b403c8515"
   },
   "outputs": [],
   "source": [
    "job_client = aiplatform.gapic.JobServiceClient(\n",
    "    client_options=dict(api_endpoint=f\"{REGION}-aiplatform.googleapis.com\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4da63b786392"
   },
   "outputs": [],
   "source": [
    "create_tpuv5e_llama2_peft_job_response = job_client.create_custom_job(\n",
    "    parent=\"projects/{project}/locations/{location}\".format(\n",
    "        project=PROJECT_ID, location=REGION\n",
    "    ),\n",
    "    custom_job=tpuv5e_llama2_peft_job,\n",
    ")\n",
    "print(create_tpuv5e_llama2_peft_job_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fb8dfcc23789"
   },
   "source": [
    "检查工作进度\n",
    "这可能需要20-60分钟或更长时间，具体取决于模型大小。多次运行此单元格以检查进度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f402309d9dbb"
   },
   "outputs": [],
   "source": [
    "get_tpuv5e_llama2_peft_job_response = job_client.get_custom_job(\n",
    "    name=create_tpuv5e_llama2_peft_job_response.name\n",
    ")\n",
    "get_tpuv5e_llama2_peft_job_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eb7f4f1ac160"
   },
   "source": [
    "#### 点击此单元格输出的控制台日志网址以查看您的日志"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "babf40cf7821"
   },
   "outputs": [],
   "source": [
    "job_id = create_tpuv5e_llama2_peft_job_response.name[\n",
    "    create_tpuv5e_llama2_peft_job_response.name.rfind(\"/\") + 1 :\n",
    "]\n",
    "STARTDATE = datetime.today() - timedelta(days=1)\n",
    "STARTDATE = STARTDATE.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "ENDDATE = datetime.today() + timedelta(days=0.1)\n",
    "ENDDATE = ENDDATE.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "print(\n",
    "    f\"https://console.cloud.google.com/logs/query;query=resource.labels.job_id=%22{job_id}%22;cursorTimestamp={ENDDATE}Z;startTime={STARTDATE}Z;endTime={ENDDATE}Z?project={PROJECT_ID}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0f97d1b4fb05"
   },
   "source": [
    "等待直到培训任务完成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a6ecd909fea8"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from google.cloud.aiplatform import gapic as aip\n",
    "\n",
    "while True:\n",
    "    response = job_client.get_custom_job(\n",
    "        name=create_tpuv5e_llama2_peft_job_response.name\n",
    "    )\n",
    "    if response.state != aip.JobState.JOB_STATE_SUCCEEDED:\n",
    "        print(f\"Training is not complete and is in state {response.state.name}\")\n",
    "        if response.state == aip.JobState.JOB_STATE_FAILED:\n",
    "            raise Exception(\"Training Job Failed\")\n",
    "    else:\n",
    "        print(\"Training has completed\")\n",
    "        break\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d52a6bb72aa6"
   },
   "source": [
    "### 部署经过微调的模型\n",
    "本部分将上传模型至模型注册表，并使用Hex-LLM部署模型，这是一个由谷歌云开发的基于XLA构建的高效大型语言模型服务解决方案\n",
    "\n",
    "模型部署步骤将需要15-20分钟完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e1936eee4a7b"
   },
   "outputs": [],
   "source": [
    "HEXLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai-restricted/vertex-vision-model-garden-dockers/hex-llm-serve:20240328_RC01\"\n",
    "\n",
    "# GCS folder path where the merged model files were saved in you bucket\n",
    "# MERGED_MODEL_FOLDER=\"llama2-7b-hf/modelfiles\" set during fine-tuning\n",
    "MERGED_MODEL_PATH = f\"{MERGED_MODEL_FOLDER}/merged_model/0\"\n",
    "GCS_MODEL_PATH = f\"{BUCKET_URI}/{MERGED_MODEL_PATH}\"\n",
    "\n",
    "DISPLAY_NAME_PREFIX = \"llama2-7b-lora-deploy\"  # @param {type:\"string\"}\n",
    "JOB_NAME = get_job_name_with_datetime(DISPLAY_NAME_PREFIX)\n",
    "GCS_MODEL_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c963a9dcf4ac"
   },
   "source": [
    "#### 检查您的GCS目录中的模型文件\n",
    "\n",
    "您的输出应该显示一个类似以下的文件列表\n",
    "```\n",
    "gs://<YOUR-BUCKET>/modelfiles/merged_model/config.json\n",
    "gs://<YOUR-BUCKET>/modelfiles/merged_model/generation_config.json\n",
    "gs://<YOUR-BUCKET>/modelfiles/merged_model/pytorch_model-00001-of-00003.bin\n",
    "gs://<YOUR-BUCKET>/modelfiles/merged_model/pytorch_model-00002-of-00003.bin\n",
    "gs://<YOUR-BUCKET>/modelfiles/merged_model/pytorch_model-00003-of-00003.bin\n",
    "gs://<YOUR-BUCKET>/modelfiles/merged_model/pytorch_model.bin.index.json\n",
    "gs://<YOUR-BUCKET>/modelfiles/merged_model/special_tokens_map.json\n",
    "gs://<YOUR-BUCKET>/modelfiles/merged_model/tokenizer.json\n",
    "gs://<YOUR-BUCKET>/modelfiles/merged_model/tokenizer_config.json\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9159d64417a0"
   },
   "outputs": [],
   "source": [
    "!gsutil ls $GCS_MODEL_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0188453d4c9f"
   },
   "source": [
    "定义用于部署模型的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0931f14c09cb"
   },
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def deploy_model_hexllm(\n",
    "    model_name: str,\n",
    "    model_id: str,\n",
    "    service_account: str,\n",
    "    machine_type: str = \"ct5lp-hightpu-4t\",\n",
    "    max_num_batched_tokens: int = 11264,  # 11264\n",
    "    tokens_pad_multiple: int = 1024,\n",
    "    seqs_pad_multiple: int = 32,\n",
    "    sync: bool = True,\n",
    ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
    "    \"\"\"Deploys models with Hex-LLM on TPU in Vertex AI.\"\"\"\n",
    "    endpoint = aiplatform.Endpoint.create(display_name=f\"{model_name}-endpoint\")\n",
    "\n",
    "    num_tpu_chips = int(machine_type[-2])\n",
    "    hexllm_args = [\n",
    "        \"--host=0.0.0.0\",\n",
    "        \"--port=7080\",\n",
    "        \"--log_level=INFO\",\n",
    "        f\"--model={model_id}\",\n",
    "        \"--load_format=pt\",  # Note: Using Pytorch bin format for weights\n",
    "        f\"--tensor_parallel_size={num_tpu_chips}\",\n",
    "        \"--num_nodes=1\",\n",
    "        \"--use_ray\",\n",
    "        \"--batch_mode=continuous\",\n",
    "        f\"--max_num_batched_tokens={max_num_batched_tokens}\",\n",
    "        f\"--tokens_pad_multiple={tokens_pad_multiple}\",\n",
    "        f\"--seqs_pad_multiple={seqs_pad_multiple}\",\n",
    "    ]\n",
    "\n",
    "    env_vars = {\n",
    "        \"PJRT_DEVICE\": \"TPU\",\n",
    "        \"RAY_DEDUP_LOGS\": \"0\",\n",
    "        \"RAY_USAGE_STATS_ENABLED\": \"0\",\n",
    "    }\n",
    "\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=model_name,\n",
    "        serving_container_image_uri=HEXLLM_DOCKER_URI,\n",
    "        serving_container_command=[\"python\", \"-m\", \"hex_llm.entrypoints.api_server\"],\n",
    "        serving_container_args=hexllm_args,\n",
    "        serving_container_ports=[7080],\n",
    "        serving_container_predict_route=\"/generate\",\n",
    "        serving_container_health_route=\"/ping\",\n",
    "        serving_container_environment_variables=env_vars,\n",
    "        serving_container_shared_memory_size_mb=(16 * 1024),  # 16 GB\n",
    "        serving_container_deployment_timeout=7200,\n",
    "    )\n",
    "\n",
    "    model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        machine_type=machine_type,\n",
    "        deploy_request_timeout=1800,\n",
    "        service_account=service_account,\n",
    "        sync=sync,\n",
    "    )\n",
    "    return model, endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8945e2e6d6ed"
   },
   "source": [
    "部署模型到 Vertex\n",
    "`deploy_model_hexllm` 函数将返回一个指向已添加到 Vertex AI 模型注册表的模型的引用，以及一个将部署模型的新端点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ecd851116c14"
   },
   "outputs": [],
   "source": [
    "print(\"Using model from: \", GCS_MODEL_PATH)\n",
    "model, endpoint = deploy_model_hexllm(\n",
    "    model_name=JOB_NAME,\n",
    "    model_id=GCS_MODEL_PATH,\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    sync=False,\n",
    ")\n",
    "print(\"endpoint_name:\", endpoint.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81650c5e3444"
   },
   "source": [
    "部署模型后，检查日志。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7e31488fc924"
   },
   "outputs": [],
   "source": [
    "ENDPOINT_ID = endpoint.name[endpoint.name.rfind(\"/\") + 1 :]\n",
    "STARTDATE = datetime.today() - timedelta(days=1)\n",
    "STARTDATE = STARTDATE.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "ENDDATE = datetime.today() + timedelta(days=0.1)\n",
    "ENDDATE = ENDDATE.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "print(\n",
    "    f\"https://console.cloud.google.com/logs/query;query=resource.type%3D%22aiplatform.googleapis.com%2FEndpoint%22%20resource.labels.endpoint_id%3D%22{ENDPOINT_ID}%22%20resource.labels.location%3D%22{REGION}%22;startTime={STARTDATE}Z;endTime={ENDDATE}Z?project={PROJECT_ID}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adb2ed90a241"
   },
   "source": [
    "等到终点完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4c92f6cdd08d"
   },
   "outputs": [],
   "source": [
    "endpoint.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3d894b10257a"
   },
   "outputs": [],
   "source": [
    "# (optional) Wait 15 minutes while the model is downloaded and setup\n",
    "if os.getenv(\"IS_TESTING\"):\n",
    "    time.sleep(900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "221a5b47cd7d"
   },
   "source": [
    "注意：整个部署过程可能需要30-40分钟甚至更长时间。在部署成功后（大约15-20分钟），微调模型将从用于训练的GCS存储桶中下载。因此，在模型部署步骤成功后，并且在运行下面的下一步之前，需要额外的约15-20分钟的等待时间（取决于模型大小）。否则，在发送请求到终端时，可能会看到`ServiceUnavailable: 503 502:Bad Gateway`错误。\n",
    "\n",
    "### 一旦部署准备就绪，请发送预测请求\n",
    "\n",
    "部署成功后，您可以发送带有文本提示的请求到终端。第一个请求将花费一两分钟的时间进行模型热身。\n",
    "\n",
    "示例：\n",
    "\n",
    "```\n",
    "Prompt: 提供一个不超过50个字符的90年代三部最好笑的喜剧电影的列表\n",
    "Response:  1) 铁甲侠 2) 史酷比 3) 贝多芬的要求\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0a85ec25e11f"
   },
   "outputs": [],
   "source": [
    "PROMPT = (\n",
    "    \"Provide a list of the 3 best comedy movies in the 90s in 50 characters or less\"\n",
    ")\n",
    "\n",
    "instances = [\n",
    "    {\n",
    "        \"prompt\": PROMPT,\n",
    "        \"max_tokens\": 80,\n",
    "        \"temperature\": 1.0,\n",
    "        \"top_p\": 1.0,\n",
    "        \"top_k\": 1.0,\n",
    "    },\n",
    "]\n",
    "\n",
    "response = endpoint.predict(instances=instances)\n",
    "\n",
    "for prediction in response.predictions:\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "af21a3cff1e0"
   },
   "source": [
    "清理\n",
    "\n",
    "要清理本项目中使用的所有谷歌云资源，您可以删除用于教程的[谷歌云项目](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects)。\n",
    "\n",
    "否则，您可以删除本教程中创建的各个资源。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "911406c1561e"
   },
   "outputs": [],
   "source": [
    "# Delete the train job.\n",
    "job_client.delete_custom_job(name=create_tpuv5e_llama2_peft_job_response.name)\n",
    "\n",
    "# Undeploy model and delete endpoint.\n",
    "endpoint.delete(force=True)\n",
    "\n",
    "# Delete models.\n",
    "model.delete()\n",
    "\n",
    "import os\n",
    "\n",
    "# Delete Cloud Storage objects that were created\n",
    "delete_bucket = False\n",
    "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
    "    ! gsutil -m rm -r $BUCKET_URI"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "tpuv5e_llama2_pytorch_finetuning_and_serving.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
