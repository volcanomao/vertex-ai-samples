{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Vertex AI - 使用LoRA在TPUv5e上进行Gemini分布式调优，服务于L4 GPU\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/training/tpuv5e_gemma_peft_finetuning_and_serving.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> 在Colab中运行（需要更高内存的Colab pro）\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/training/tpuv5e_gemma_peft_finetuning_and_serving.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      在GitHub上查看\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/notebooks/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/training/tpuv5e_gemma_peft_finetuning_and_serving.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "在Vertex AI Workbench中打开\n",
    "    </a>（建议使用带有250GB磁盘的e2-standard-8 CPU）\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## 概述\n",
    "\n",
    "这本笔记本是基于[ai.google.dev上的LoRA调优示例](https://ai.google.dev/gemma/docs/distributed_tuning)。它遵循了一个已经存在的[为GPU上的微调编写的Model Garden示例](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_gemma_finetuning_on_vertex.ipynb)，并已经修改为使用最新的TPUv5e芯片进行训练。它演示了使用[Vertex AI Custom Training Job](https://cloud.google.com/vertex-ai/docs/training/create-custom-job)进行微调和部署Gemma模型。Vertex AI Custom Training Job允许对微调作业进行更高级别的自定义和控制。本笔记本中的所有示例都使用参数有效微调方法[PEFT](https://github.com/huggingface/peft)来降低训练和存储成本。\n",
    "\n",
    "这本笔记本使用[vLLM](https://github.com/vllm-project/vllm) docker部署模型。\n",
    "\n",
    "### 目标\n",
    "\n",
    "- 使用Vertex AI Custom Training Job对Gemma模型进行微调和部署。\n",
    "- 向您微调的Gemma模型发送预测请求。\n",
    "\n",
    "### 成本\n",
    "\n",
    "本教程使用谷歌云的收费组件：\n",
    "\n",
    "* Vertex AI\n",
    "* 云存储\n",
    "\n",
    "了解[Vertex AI定价](https://cloud.google.com/vertex-ai/pricing)，[云存储定价](https://cloud.google.com/storage/pricing)，并使用[定价计算器](https://cloud.google.com/products/calculator/)根据您的预期使用量生成成本估算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08d289fa873f"
   },
   "source": [
    "### 数据集\n",
    "\n",
    "在这个例子中，您将使用来自 TensorFlow 数据集的 IMDB 评论数据集来微调模型。数据集的详细信息可以在这里找到：https://www.tensorflow.org/datasets/catalog/imdb_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aed92deeb4a0"
   },
   "source": [
    "成本\n",
    "\n",
    "本教程使用谷歌云的可计费组件：\n",
    "\n",
    "Vertex AI（训练，TPUv5e，L4 GPU），云存储\n",
    "\n",
    "了解[Vertex AI 价格](https://cloud.google.com/vertex-ai/pricing)，[云存储价格](https://cloud.google.com/storage/pricing)，[云NL API 价格](https://cloud.google.com/natural-language/pricing)，并使用[定价计算器](https://cloud.google.com/products/calculator/)根据您的预期使用量生成一份成本估算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "## 安装\n",
    "\n",
    "安装以下包以执行这个笔记本。\n",
    "\n",
    "运行以下命令来安装支持 TPUv5e 的最新谷歌云平台库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2b4ef9b72d43"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# (optional) update gcloud if needed\n",
    "if os.getenv(\"IS_TESTING\"):\n",
    "    ! gcloud components update --quiet\n",
    "\n",
    "! pip3 install --upgrade --quiet google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58707a750154"
   },
   "source": [
    "只有计算协作：取消下面的单元格注释以重新启动内核。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f200f10a1da3"
   },
   "outputs": [],
   "source": [
    "# import IPython\n",
    "\n",
    "# app = IPython.Application.instance()\n",
    "# app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF1j6f9HApxa"
   },
   "source": [
    "## 在开始之前\n",
    "\n",
    "### 设置您的 Google 云项目\n",
    "\n",
    "**无论您使用什么笔记本环境，下面的步骤都是必需的。**\n",
    "\n",
    "1. [选择或创建一个 Google 云项目](https://console.cloud.google.com/cloud-resource-manager)。当您第一次创建帐户时，您将获得 $300 的免费信用额度，可以用于计算/存储成本。\n",
    "\n",
    "2. [确保您的项目已启用计费功能](https://cloud.google.com/billing/docs/how-to/modify-project)。\n",
    "\n",
    "3. [启用 Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com)。\n",
    "\n",
    "4. 如果您在本地运行此笔记本，您需要安装 [Cloud SDK](https://cloud.google.com/sdk)。\n",
    "\n",
    "5. [选择或创建一个云存储存储桶](https://cloud.google.com/storage/docs/creating-buckets) 用于存储实验结果。\n",
    "\n",
    "6. [创建一个服务帐号](https://cloud.google.com/iam/docs/service-accounts-create#iam-service-accounts-create-console)，并为其指定 `Vertex AI 用户` 和 `存储对象管理员` 角色，以便将优化后的模型部署到 Vertex AI 终端。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "909aa61d1f13"
   },
   "source": [
    "### Kaggle认证\n",
    "Gemma模型由Kaggle托管。要使用Gemma，请在Kaggle上请求访问权限：\n",
    "\n",
    "* 在[kaggle.com](https://www.kaggle.com)上登录或注册\n",
    "* 打开[Gemma模型卡](https://www.kaggle.com/models/google/gemma)，并选择“请求访问权限”\n",
    "* 完成同意表格并接受条款和条件\n",
    "\n",
    "然后，要使用Kaggle API，请创建一个API令牌：\n",
    "\n",
    "* 打开[Kaggle设置](https://www.kaggle.com/settings)\n",
    "* 选择“创建新的令牌”\n",
    "* 会下载一个kaggle.json文件。它包含您的Kaggle凭据。请注意用户名和密钥，以便稍后填写。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WReHDGG5g0XY"
   },
   "source": [
    "设置您的项目ID\n",
    "\n",
    "**如果您不知道您的项目ID**，请尝试以下操作：\n",
    "* 运行 `gcloud config list`。\n",
    "* 运行 `gcloud projects list`。\n",
    "* 查看支持页面：[查找项目ID](https://support.google.com/googleapi/answer/7014113)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oM1iC_MfAts1"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
    "\n",
    "# Set the project id\n",
    "! gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "#### 区域\n",
    "\n",
    "您还可以更改 Vertex AI 使用的“REGION”变量。了解有关[Vertex AI 地区](https://cloud.google.com/vertex-ai/docs/general/locations)的更多信息。\n",
    "\n",
    "TPUv5e 可在[此处列出的以下地区](https://cloud.google.com/tpu/pricing)提供。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "region"
   },
   "outputs": [],
   "source": [
    "REGION = \"us-west1\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBCra4QMA2wR"
   },
   "source": [
    "### 验证您的Google Cloud账户\n",
    "\n",
    "根据您的Jupyter环境，您可能需要手动进行身份验证。请按照以下相关说明操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74ccc9e52986"
   },
   "source": [
    "1. 顶点 AI 工作台\n",
    "* 无需操作，您已经通过身份验证。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "de775a3773ba"
   },
   "source": [
    "2. 本地JupyterLab实例，请取消注释并运行："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "254614fa0c46"
   },
   "outputs": [],
   "source": [
    "# ! gcloud auth login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ef21552ccea8"
   },
   "source": [
    "3. 协作，取消注释并运行："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "603adbbf0532"
   },
   "outputs": [],
   "source": [
    "# from google.colab import auth\n",
    "# auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6b2ccc891ed"
   },
   "source": [
    "请参考https://cloud.google.com/storage/docs/gsutil/commands/iam#ch-examples 网址了解如何向服务账户授予云存储权限。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "960505627ddf"
   },
   "source": [
    "###导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PyQmSRbKA8r-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "### 创建一个云存储桶\n",
    "\n",
    "创建一个存储桶来存储诸如数据集之类的中间文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "BUCKET_URI = f\"gs://your-bucket-name-{PROJECT_ID}-unique\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "648eb19b83dd"
   },
   "source": [
    "设置文件夹路径以用于暂存、环境和模型工件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "855d6b96f291"
   },
   "outputs": [],
   "source": [
    "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
    "MODEL_BUCKET = os.path.join(BUCKET_URI, \"model\")\n",
    "\n",
    "# The service account looks like:\n",
    "# '@.iam.gserviceaccount.com'\n",
    "# Please go to https://cloud.google.com/iam/docs/service-accounts-create#iam-service-accounts-create-console\n",
    "# and create service account with `Vertex AI User` and `Storage Object Admin` roles.\n",
    "# The service account for deploying fine tuned model.\n",
    "SERVICE_ACCOUNT = \"[your-service-account]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EcIXiGsCePi"
   },
   "source": [
    "只有当您的存储桶尚不存在时：运行以下单元格以创建您的云存储存储桶。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIq7R4HZCfIc"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l {REGION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "source": [
    "### 初始化用于Python的Vertex AI SDK\n",
    "\n",
    "为您的项目初始化用于Python的Vertex AI SDK。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7c13158fa7be"
   },
   "source": [
    "选择 Gemma 基础模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d698e6e0c126"
   },
   "outputs": [],
   "source": [
    "# The Gemma base model.\n",
    "base_model = \"google/gemma-2b\"  # @param [\"google/gemma-2b\", \"google/gemma-2b-it\", \"google/gemma-7b\", \"google/gemma-7b-it\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "832bbd8cb9ef"
   },
   "source": [
    "### 创建artifact registry repository，并设置自定义的docker镜像URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18fd49b88127"
   },
   "outputs": [],
   "source": [
    "REPOSITORY = \"tpuv5e-training-repository-unique\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c2242d87effa"
   },
   "outputs": [],
   "source": [
    "image_name_train = \"gemma-lora-tuning-tpuv5e\"\n",
    "hostname = f\"{REGION}-docker.pkg.dev\"\n",
    "tag = \"latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9dd94acb26e9"
   },
   "outputs": [],
   "source": [
    "# Register gcloud as a Docker credential helper\n",
    "!gcloud auth configure-docker $REGION-docker.pkg.dev --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5c8e8364a097"
   },
   "outputs": [],
   "source": [
    "# One time or use an existing repository\n",
    "!gcloud artifacts repositories create $REPOSITORY --repository-format=docker \\\n",
    "--location=$REGION --description=\"Vertex TPUv5e training repository\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "16488f1fc702"
   },
   "outputs": [],
   "source": [
    "# Define container image name\n",
    "KERAS_TRAIN_DOCKER_URI = (\n",
    "    f\"{hostname}/{PROJECT_ID}/{REPOSITORY}/{image_name_train}:{tag}\"\n",
    ")\n",
    "\n",
    "# Set the docker image uri for the vLLM serving container\n",
    "VLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20240220_0936_RC01\"\n",
    "\n",
    "# Set the docker image uri for the model conversion container that converts the fine-tuned model to HF format\n",
    "KERAS_MODEL_CONVERSION_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/jax-keras-model-conversion:20240220_0936_RC01\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0c250872074f"
   },
   "source": [
    "### 定义通用函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "354da31189dc"
   },
   "outputs": [],
   "source": [
    "def get_job_name_with_datetime(prefix: str) -> str:\n",
    "    \"\"\"Gets the job name with date time when triggering training or deployment\n",
    "    jobs in Vertex AI.\n",
    "    \"\"\"\n",
    "    return prefix + datetime.now().strftime(\"_%Y%m%d_%H%M%S\")\n",
    "\n",
    "\n",
    "def deploy_model_vllm(\n",
    "    model_name: str,\n",
    "    model_uri: str,\n",
    "    service_account: str,\n",
    "    machine_type: str = \"g2-standard-8\",\n",
    "    accelerator_type: str = \"NVIDIA_L4\",\n",
    "    accelerator_count: int = 1,\n",
    "    max_model_len: int = 8192,\n",
    "    dtype: str = \"bfloat16\",\n",
    ") -> tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
    "    # Upload the model to \"Model Registry\"\n",
    "    job_name = get_job_name_with_datetime(model_name)\n",
    "    vllm_args = [\n",
    "        \"--host=0.0.0.0\",\n",
    "        \"--port=7080\",\n",
    "        f\"--tensor-parallel-size={accelerator_count}\",\n",
    "        \"--swap-space=16\",\n",
    "        \"--gpu-memory-utilization=0.95\",\n",
    "        f\"--max-model-len={max_model_len}\",\n",
    "        f\"--dtype={dtype}\",\n",
    "        \"--disable-log-stats\",\n",
    "    ]\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=job_name,\n",
    "        artifact_uri=model_uri,\n",
    "        serving_container_image_uri=VLLM_DOCKER_URI,\n",
    "        serving_container_command=[\"python\", \"-m\", \"vllm.entrypoints.api_server\"],\n",
    "        serving_container_args=vllm_args,\n",
    "        serving_container_ports=[7080],\n",
    "        serving_container_predict_route=\"/generate\",\n",
    "        serving_container_health_route=\"/ping\",\n",
    "    )\n",
    "\n",
    "    # Deploy the model to an endpoint to serve \"Online predictions\"\n",
    "    endpoint = aiplatform.Endpoint.create(display_name=f\"{model_name}-endpoint\")\n",
    "    model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        machine_type=machine_type,\n",
    "        accelerator_type=accelerator_type,\n",
    "        accelerator_count=accelerator_count,\n",
    "        deploy_request_timeout=1800,\n",
    "        service_account=service_account,\n",
    "        sync=False,\n",
    "    )\n",
    "\n",
    "    return model, endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e11e1f8a6c4c"
   },
   "source": [
    "构建Docker容器文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19d584f9fe62"
   },
   "source": [
    "#### 创建培训师目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "909e93e7fd43"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists(\"trainer\"):\n",
    "    os.makedirs(\"trainer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c725bbcabe27"
   },
   "source": [
    "#### 为了进行KerasNLP训练和Hex-LLM使用TPUs进行部署，需要Kaggle凭据。\n",
    "将KAGGLE_USERNAME和KAGGLE_KEY设置为环境变量，以便在Vertex Training中使用。\n",
    "根据[这些说明](https://github.com/Kaggle/kaggle-api?tab=readme-ov-file#api-credentials)生成Kaggle用户名和密钥。\n",
    "您需要按照早前提到的说明来查看并接受模型许可证。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0480a14d88e3"
   },
   "outputs": [],
   "source": [
    "KAGGLE_USERNAME = \"your-kaggle-username\"  # @param {type:\"string\", isTemplate:true}\n",
    "KAGGLE_KEY = \"your-kaggle-key\"  # @param {type:\"string\", isTemplate:true}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8a91ddf0cbcb"
   },
   "source": [
    "为自定义容器创建Dockerfile。这将安装JAX[TPU]，Keras和TensorFlow数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "756577886992"
   },
   "outputs": [],
   "source": [
    "%%writefile trainer/Dockerfile\n",
    "# This Dockerfile fine tunes the Gemma model using LoRA with JAX\n",
    "\n",
    "FROM python:3.10\n",
    "\n",
    "ENV DEBIAN_FRONTEND=noninteractive\n",
    "\n",
    "# Install basic libs\n",
    "RUN apt-get update && apt-get -y upgrade && apt-get install -y --no-install-recommends \\\n",
    "        cmake \\\n",
    "        curl \\\n",
    "        wget \\\n",
    "        sudo \\\n",
    "        gnupg \\\n",
    "        libsm6 \\\n",
    "        libxext6 \\\n",
    "        libxrender-dev \\\n",
    "        lsb-release \\\n",
    "        ca-certificates \\\n",
    "        build-essential \\\n",
    "        git \\\n",
    "        libgl1\n",
    "\n",
    "# Copy Apache license.\n",
    "RUN wget https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/LICENSE\n",
    "\n",
    "# Install required libs\n",
    "RUN pip install --upgrade pip\n",
    "RUN pip install jax[tpu]==0.4.25 -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n",
    "RUN pip install tensorflow==2.15.0.post1\n",
    "RUN pip install tensorflow-datasets==4.9.4\n",
    "RUN pip install -q -U keras-nlp==0.8.2\n",
    "RUN pip install keras==3.0.5\n",
    "\n",
    "# Copy other licenses.\n",
    "RUN wget -O MIT_LICENSE https://github.com/pytest-dev/pytest/blob/main/LICENSE\n",
    "RUN wget -O BSD_LICENSE https://github.com/pytorch/xla/blob/master/LICENSE\n",
    "RUN wget -O BSD-3_LICENSE https://github.com/pytorch/pytorch/blob/main/LICENSE\n",
    "\n",
    "ENV KERAS_BACKEND=jax\n",
    "ENV XLA_PYTHON_CLIENT_MEM_FRACTION=0.9\n",
    "ENV TPU_LIBRARY_PATH=/lib/libtpu.so\n",
    "\n",
    "# Copy install libtpu to PATH above\n",
    "RUN find ./usr/local/lib -name 'libtpu.so' -exec cp {} /lib \\;\n",
    "\n",
    "WORKDIR /\n",
    "COPY train.py train.py\n",
    "ENV PYTHONPATH ./\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ee7ac9469ce"
   },
   "source": [
    "请添加 __init__.py 文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b0b57ecd02d8"
   },
   "outputs": [],
   "source": [
    "!touch trainer/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c648296e1057"
   },
   "source": [
    "#### 添加train.py文件\n",
    "这段代码来自于LoRA分布式微调代码的示例：https://ai.google.dev/gemma/docs/distributed_tuning\n",
    "\n",
    "IMDB TensorFlow数据集用于微调Gemma模型。还添加了额外的逻辑，以处理TPUv5e所需的TPU拓扑设置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1b6a47a6089c"
   },
   "outputs": [],
   "source": [
    "%%writefile trainer/train.py\n",
    "import os\n",
    "import argparse\n",
    "import shutil\n",
    "import locale\n",
    "\n",
    "# Model saving variables\n",
    "_ENCODING_FOR_MODEL_SAVING = \"UTF-8\"\n",
    "_VOCABULARY_FILENAME = \"vocabulary.spm\"\n",
    "_TOKENIZER_FILENAME = \"tokenizer.model\"\n",
    "\n",
    "import keras\n",
    "import keras_nlp\n",
    "import tensorflow\n",
    "import tensorflow_datasets as tfds\n",
    "print (keras.__version__)\n",
    "print (tensorflow.__version__)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    \"--tpu_topology\",\n",
    "    help=\"Topology to use for the TPUv5e (1x1, 1x4, 2x2, etc.)\",\n",
    "    type=str\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--model_name\",\n",
    "    help=\"Kaggle model name (gemma_2b_en, gemma_7b_en)\",\n",
    "    type=str\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--output_folder\",\n",
    "    type=str,\n",
    "    required=True,\n",
    "    help=\"Path to the output folder.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--checkpoint_filename\",\n",
    "    type=str,\n",
    "    default=\"fine_tuned.weights.h5\",\n",
    "    help=\"Checkpoint filename.\",\n",
    ")\n",
    "args = parser.parse_args()\n",
    "\n",
    "def main():\n",
    "    x = args.tpu_topology.split(\"x\")\n",
    "    tpu_topology_x = int(x[0])\n",
    "    tpu_topology_y = int(x[1])\n",
    "    print (f'TPU topology is ({tpu_topology_x}, {tpu_topology_y})')\n",
    "    print (f'Model name is {args.model_name}')\n",
    "\n",
    "    device_mesh = keras.distribution.DeviceMesh(\n",
    "        (tpu_topology_x, tpu_topology_y),\n",
    "        [\"batch\", \"model\"],\n",
    "        devices=keras.distribution.list_devices())\n",
    "\n",
    "    model_dim = \"model\"\n",
    "\n",
    "    layout_map = keras.distribution.LayoutMap(device_mesh)\n",
    "    # Weights that match 'token_embedding/embeddings' will be sharded on 8 TPUs\n",
    "    layout_map[\"token_embedding/embeddings\"] = (None, model_dim)\n",
    "    # Regex to match against the query, key and value matrices in the decoder\n",
    "    # attention layers\n",
    "    layout_map[\"decoder_block.*attention.*(query|key|value).*kernel\"] = (\n",
    "        None, model_dim, None)\n",
    "    layout_map[\"decoder_block.*attention_output.*kernel\"] = (\n",
    "        None, None, model_dim)\n",
    "    layout_map[\"decoder_block.*ffw_gating.*kernel\"] = (model_dim, None)\n",
    "    layout_map[\"decoder_block.*ffw_linear.*kernel\"] = (None, model_dim)\n",
    "    model_parallel = keras.distribution.ModelParallel(device_mesh, layout_map,\n",
    "                                                    batch_dim_name=\"batch\")\n",
    "    keras.distribution.set_distribution(model_parallel)\n",
    "    model_name = args.model_name\n",
    "    gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(args.model_name)\n",
    "    print (f'Running inference on the base {args.model_name} model')\n",
    "    lm_output = gemma_lm.generate(\"Prompt: Return 3 things I ask for in this format. \\\n",
    "        Response: 1) item 1 2) item 2 3) item 3. \\\n",
    "        Prompt: List the 3 best comedy movies in the 90s Response: \", max_length=100)\n",
    "    print (lm_output)\n",
    "\n",
    "    # Start training\n",
    "    imdb_train = tfds.load(\n",
    "        \"imdb_reviews\",\n",
    "        split=\"train\",\n",
    "        as_supervised=True,\n",
    "        batch_size=2,\n",
    "    )\n",
    "    # Drop labels.\n",
    "    imdb_train = imdb_train.map(lambda x, y: x)\n",
    "\n",
    "    imdb_train.unbatch().take(1).get_single_element().numpy()\n",
    "\n",
    "    gemma_lm.backbone.enable_lora(rank=4)\n",
    "\n",
    "    # Fine-tune on the IMDb movie reviews dataset.\n",
    "\n",
    "    # Limit the input sequence length to 128 to control memory usage.\n",
    "    gemma_lm.preprocessor.sequence_length = 128\n",
    "    # Use AdamW (a common optimizer for transformer models).\n",
    "    optimizer = keras.optimizers.AdamW(learning_rate=5e-5,weight_decay=0.01,)\n",
    "\n",
    "    # Exclude layernorm and bias terms from decay.\n",
    "    optimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n",
    "\n",
    "    gemma_lm.compile(\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        optimizer=optimizer,\n",
    "        weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "    gemma_lm.summary()\n",
    "    gemma_lm.fit(imdb_train, epochs=1)\n",
    "\n",
    "    print (f'Running inference on the fine-tuned {args.model_name} model')\n",
    "    lm_output = gemma_lm.generate(\"Prompt: Return 3 things I ask for in this format. \\\n",
    "        Response: 1) item 1 2) item 2 3) item 3. \\\n",
    "        Prompt: List the 3 best comedy movies in the 90s Response: \", max_length=100)\n",
    "    print (lm_output) \n",
    "\n",
    "    # Save checkpoint and tokenizer.\n",
    "    print(\"Saving checkpoint and tokenizer.\")\n",
    "    if not os.path.exists(args.output_folder):\n",
    "        os.makedirs(args.output_folder)\n",
    "    locale.getpreferredencoding = lambda: _ENCODING_FOR_MODEL_SAVING\n",
    "    gemma_lm.save_weights(\n",
    "        os.path.join(args.output_folder, args.checkpoint_filename)\n",
    "    )\n",
    "    gemma_lm.preprocessor.tokenizer.save_assets(args.output_folder)\n",
    "\n",
    "    # Copy and rename the tokenizer file.\n",
    "    print(\"Copying tokenizer file.\")\n",
    "    shutil.copy(\n",
    "        os.path.join(args.output_folder, _VOCABULARY_FILENAME),\n",
    "        os.path.join(args.output_folder, _TOKENIZER_FILENAME),\n",
    "    )\n",
    "    print ('Exiting job')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pq4iF00YG_4T"
   },
   "source": [
    "## 使用Vertex AI自定义训练任务进行微调\n",
    "\n",
    "本节演示如何使用PEFT LoRA在Vertex AI自定义训练任务上微调并部署Gemma模型。LoRA（低秩调整）是PEFT（参数高效微调）的一种方法，其中预训练模型权重被冻结，并在微调期间训练表示模型权重变化的秩分解矩阵。请阅读关于LoRA的更多信息，请参阅以下出版物：[Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L. and Chen, W., 2021. Lora: Low-rank adaptation of large language models. *arXiv preprint arXiv:2106.09685*](https://arxiv.org/abs/2106.09685)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1b90914fc81"
   },
   "source": [
    "启用Docker 以普通用户身份运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "232a259d3edc"
   },
   "outputs": [],
   "source": [
    "!sudo usermod -a -G docker ${USER}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e028d107fbe5"
   },
   "source": [
    "转到训练师目录以构建Docker容器####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3a9390f87b66"
   },
   "outputs": [],
   "source": [
    "%cd trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81eb3c13afa9"
   },
   "source": [
    "构建定制的 Docker 容器并推送到工件存储库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ee497559677f"
   },
   "outputs": [],
   "source": [
    "!docker build -t $KERAS_TRAIN_DOCKER_URI -f Dockerfile ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0715b34162b4"
   },
   "outputs": [],
   "source": [
    "!docker push $KERAS_TRAIN_DOCKER_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "586a13cb67b4"
   },
   "source": [
    "改变回你的主目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "937b7269c93b"
   },
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08bf867e8f4c"
   },
   "source": [
    "#### 设置GCS文件夹位置和作业配置设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "17392b36e9c0"
   },
   "outputs": [],
   "source": [
    "# Create a GCS folder to store the merged model with the base model and the\n",
    "# fine-tuned LORA adapter.\n",
    "merged_model_dir = get_job_name_with_datetime(\"gemma-lora-model-tpuv5\")\n",
    "merged_model_output_dir = os.path.join(MODEL_BUCKET, merged_model_dir)\n",
    "merged_model_output_dir_gcsfuse = merged_model_output_dir.replace(\"gs://\", \"/gcs/\")\n",
    "\n",
    "# Set the checkpoint output filename\n",
    "checkpoint_filename = \"fine_tuned.weights.h5\"\n",
    "\n",
    "DISPLAY_NAME_PREFIX = \"gemma-lora-train\"  # @param {type:\"string\"}\n",
    "tpuv5e_gemma_peft_job = {\n",
    "    \"display_name\": get_job_name_with_datetime(DISPLAY_NAME_PREFIX),\n",
    "    \"job_spec\": {\n",
    "        \"worker_pool_specs\": [\n",
    "            {\n",
    "                \"machine_spec\": {\n",
    "                    \"machine_type\": \"ct5lp-hightpu-1t\",\n",
    "                    \"tpu_topology\": \"1x1\",\n",
    "                },\n",
    "                \"replica_count\": 1,\n",
    "                \"container_spec\": {\n",
    "                    \"image_uri\": KERAS_TRAIN_DOCKER_URI,\n",
    "                    \"args\": [\n",
    "                        \"--tpu_topology=1x1\",\n",
    "                        \"--model_name=gemma_2b_en\",\n",
    "                        f\"--output_folder={merged_model_output_dir_gcsfuse}\",\n",
    "                        f\"--checkpoint_filename={checkpoint_filename}\",\n",
    "                    ],\n",
    "                    \"env\": [\n",
    "                        {\"name\": \"KAGGLE_USERNAME\", \"value\": KAGGLE_USERNAME},\n",
    "                        {\"name\": \"KAGGLE_KEY\", \"value\": KAGGLE_KEY},\n",
    "                    ],\n",
    "                },\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "}\n",
    "\n",
    "tpuv5e_gemma_peft_job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0c2bc267c4a5"
   },
   "source": [
    "创建作业客户端并运行作业"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bc9b403c8515"
   },
   "outputs": [],
   "source": [
    "job_client = aiplatform.gapic.JobServiceClient(\n",
    "    client_options=dict(api_endpoint=f\"{REGION}-aiplatform.googleapis.com\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4da63b786392"
   },
   "outputs": [],
   "source": [
    "create_tpuv5e_gemma_peft_job_response = job_client.create_custom_job(\n",
    "    parent=\"projects/{project}/locations/{location}\".format(\n",
    "        project=PROJECT_ID, location=REGION\n",
    "    ),\n",
    "    custom_job=tpuv5e_gemma_peft_job,\n",
    ")\n",
    "print(create_tpuv5e_gemma_peft_job_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fb8dfcc23789"
   },
   "source": [
    "检查工作进展\n",
    "根据模型大小不同，可能需要20-60分钟甚至更长时间。多次运行此单元格以检查进度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f402309d9dbb"
   },
   "outputs": [],
   "source": [
    "get_tpuv5e_gemma_peft_job_response = job_client.get_custom_job(\n",
    "    name=create_tpuv5e_gemma_peft_job_response.name\n",
    ")\n",
    "get_tpuv5e_gemma_peft_job_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eb7f4f1ac160"
   },
   "source": [
    "点击此单元格输出的控制台日志URL，以查看您的日志。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "babf40cf7821"
   },
   "outputs": [],
   "source": [
    "job_id = create_tpuv5e_gemma_peft_job_response.name[\n",
    "    create_tpuv5e_gemma_peft_job_response.name.rfind(\"/\") + 1 :\n",
    "]\n",
    "startdate = datetime.today() - timedelta(days=1)\n",
    "startdate = startdate.strftime(\"%Y-%m-%d\")\n",
    "print(\n",
    "    f\"https://console.cloud.google.com/logs/query;query=resource.labels.job_id=%22{job_id}%22%20timestamp%3E={startdate}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0a1e0dbdf24"
   },
   "source": [
    "### 将微调后的Keras检查点转换为HF格式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ae10c82ba26e"
   },
   "source": [
    "#### 从KerasNLP工具下载转换脚本\n",
    "GitHub仓库网址为https://github.com/keras-team/keras-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b02a8f0a54ab"
   },
   "outputs": [],
   "source": [
    "!wget -nv -nc https://raw.githubusercontent.com/keras-team/keras-nlp/master/tools/gemma/export_gemma_to_hf.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c77580797b26"
   },
   "source": [
    "将微调后的检查点文件下载到本地"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ba0e4080beba"
   },
   "outputs": [],
   "source": [
    "!gcloud storage cp -r $merged_model_output_dir ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52c738bc53fc"
   },
   "source": [
    "#### 为模型转换安装库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dc8e6fc5327e"
   },
   "outputs": [],
   "source": [
    "!pip install torch==2.1\n",
    "!pip install --upgrade keras-nlp\n",
    "!pip install --upgrade keras>=3\n",
    "!pip install --upgrade accelerate sentencepiece transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c27b3cdd841b"
   },
   "source": [
    "运行模型转换脚本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "70e543b66fd1"
   },
   "outputs": [],
   "source": [
    "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "os.environ[\"KAGGLE_USERNAME\"] = KAGGLE_USERNAME\n",
    "os.environ[\"KAGGLE_KEY\"] = KAGGLE_KEY\n",
    "MODEL_SIZE=\"2b\"\n",
    "!KERAS_BACKEND=torch python export_gemma_to_hf.py \\\n",
    "  --weights_file ./$merged_model_dir/fine_tuned.weights.h5 \\\n",
    "  --size $MODEL_SIZE \\\n",
    "  --vocab_path ./$merged_model_dir/vocabulary.spm \\\n",
    "  --output_dir ./$merged_model_dir/fine_tuned_gg_hf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4e81e9115533"
   },
   "source": [
    "#### 将转换后的HF文件复制到GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e2f307e01787"
   },
   "outputs": [],
   "source": [
    "HUGGINGFACE_MODEL_DIR = os.path.join(\"./\", merged_model_dir, \"fine_tuned_gg_hf\")\n",
    "HUGGINGFACE_MODEL_DIR_GCS = os.path.join(merged_model_output_dir, \"fine_tuned_gg_hf\")\n",
    "HUGGINGFACE_MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3016583388ff"
   },
   "outputs": [],
   "source": [
    "!gcloud storage cp $HUGGINGFACE_MODEL_DIR/* $HUGGINGFACE_MODEL_DIR_GCS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1LMBN_gDG_4U"
   },
   "source": [
    "### 部署经过精细调整的模型\n",
    "该部分将模型上传到模型注册表，并使用[vLLM](https://github.com/vllm-project/vllm)在端点上部署它。\n",
    "\n",
    "模型部署步骤将需要15分钟到1小时才能完成，取决于模型的大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e251550b01df"
   },
   "outputs": [],
   "source": [
    "MODEL_NAME_VLLM = get_job_name_with_datetime(prefix=\"gemma-vllm-serve\")\n",
    "\n",
    "# Start with a G2 Series cost-effective configuration\n",
    "if MODEL_SIZE == \"2b\":\n",
    "    machine_type = \"g2-standard-8\"\n",
    "    accelerator_type = \"NVIDIA_L4\"\n",
    "    accelerator_count = 1\n",
    "elif MODEL_SIZE == \"7b\":\n",
    "    machine_type = \"g2-standard-12\"\n",
    "    accelerator_type = \"NVIDIA_L4\"\n",
    "    accelerator_count = 1\n",
    "else:\n",
    "    assert MODEL_SIZE in (\"2b\", \"7b\")\n",
    "\n",
    "# See supported machine/GPU configurations in chosen region:\n",
    "# https://cloud.google.com/vertex-ai/docs/predictions/configure-compute\n",
    "\n",
    "# For even more performance, consider V100 and A100 GPUs\n",
    "# > Nvidia Tesla V100\n",
    "# machine_type = \"n1-standard-8\"\n",
    "# accelerator_type = \"NVIDIA_TESLA_V100\"\n",
    "# > Nvidia Tesla A100\n",
    "# machine_type = \"a2-highgpu-1g\"\n",
    "# accelerator_type = \"NVIDIA_TESLA_A100\"\n",
    "\n",
    "# Larger `max_model_len` values will require more GPU memory\n",
    "max_model_len = 2048\n",
    "\n",
    "model, endpoint = deploy_model_vllm(\n",
    "    MODEL_NAME_VLLM,\n",
    "    HUGGINGFACE_MODEL_DIR_GCS,\n",
    "    SERVICE_ACCOUNT,\n",
    "    machine_type=machine_type,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    "    max_model_len=max_model_len,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c95e08ad63e3"
   },
   "source": [
    "点击此单元格中输出的控制台日志URL，以查看您的日志。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6277300c5531"
   },
   "outputs": [],
   "source": [
    "startdate = datetime.today() - timedelta(days=1)\n",
    "startdate = startdate.strftime(\"%Y-%m-%d\")\n",
    "log_link = \"https://console.cloud.google.com/logs/query;query=resource.type=%22aiplatform.googleapis.com%2FEndpoint%22\"\n",
    "log_link += f\"%20resource.labels.endpoint_id=%22{endpoint.name}%22\"\n",
    "log_link += f\"%20resource.labels.location={REGION}\"\n",
    "log_link += f\"%20timestamp%3E={startdate}\"\n",
    "print(log_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GI363hMzG_4U"
   },
   "source": [
    "注意：总体部署可能需要30-40分钟或更长时间。部署成功后（大约15-20分钟），微调后的模型将从上面训练时使用的GCS存储桶中下载。因此，在模型部署步骤成功后，以及您运行下面的下一步之前，需要额外约15-20分钟（取决于模型大小）的等待时间。否则，当您向端点发送请求时，您可能会看到`ServiceUnavailable: 503 502:Bad Gateway`错误。\n",
    "\n",
    "### 发送预测请求\n",
    "\n",
    "一旦部署成功，您可以使用文本提示向端点发送请求。使用笔记本中先前使用的示例\n",
    "\n",
    "示例：\n",
    "\n",
    "```\n",
    "提示：按照这种格式回答我问的3件事，不要重复我的提示。 响应：1）物品1 2）物品2  3）物品3。列出90年代最好的3部喜剧电影 \n",
    "响应：1）The Cable Guy  2）Scooby-Doo  3）贝多芬要求\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2UYUNn60G_4U"
   },
   "outputs": [],
   "source": [
    "PROMPT = \"Prompt: Return 3 things I ask for in this format and do not repeat my prompt. \\\n",
    "Response: 1) item 1 2) item 2 3) item 3. \\\n",
    "Prompt: List the 3 best comedy movies in the 90s Response: \"\n",
    "\n",
    "instances = [\n",
    "    {\"prompt\": PROMPT},\n",
    "    {\"max_tokens\": 500},\n",
    "    {\"temperature\": 1.0},\n",
    "    {\"top_p\": 1.0},\n",
    "    {\"top_k\": 1.0},\n",
    "]\n",
    "\n",
    "response = endpoint.predict(instances=instances)\n",
    "\n",
    "for prediction in response.predictions:\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "af21a3cff1e0"
   },
   "source": [
    "清理工作\n",
    "\n",
    "要清理此项目中使用的所有Google Cloud资源，您可以[删除用于本教程的Google Cloud项目](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects)。\n",
    "\n",
    "否则，您可以删除在本教程中创建的单个资源："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "911406c1561e"
   },
   "outputs": [],
   "source": [
    "# Delete the train job.\n",
    "job_client.delete_custom_job(name=create_tpuv5e_gemma_peft_job_response.name)\n",
    "\n",
    "# Undeploy model and delete endpoint.\n",
    "endpoint.delete(force=True)\n",
    "\n",
    "# Delete models.\n",
    "model.delete()\n",
    "\n",
    "import os\n",
    "\n",
    "# Delete Cloud Storage objects that were created\n",
    "delete_bucket = False\n",
    "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
    "    ! gsutil -m rm -r $BUCKET_URI"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "tpuv5e_gemma_peft_finetuning_and_serving.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
