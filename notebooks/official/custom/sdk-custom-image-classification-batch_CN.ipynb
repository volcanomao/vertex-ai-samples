{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "copyright"
   },
   "outputs": [],
   "source": [
    "# Copyright 2020 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# 自定义训练和批量预测\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/custom/sdk-custom-image-classification-batch.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> 在Colab中打开\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fofficial%2Fcustom%2Fsdk-custom-image-classification-batch.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://cloud.google.com/ml-engine/images/colab-enterprise-logo-32px.png\" alt=\"Google Cloud Colab Enterprise logo\"><br> 在Colab企业版中打开\n",
    "    </a>\n",
    "  </td>    \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/custom/sdk-custom-image-classification-batch.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> 在Workbench中打开\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/custom/sdk-custom-image-classification-batch.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> 在GitHub上查看\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "overview:custom"
   },
   "source": [
    "## 概述\n",
    "\n",
    "本教程演示了如何使用Python的Vertex AI SDK来训练和部署自定义图像分类模型以进行批量预测。\n",
    "\n",
    "了解更多关于[自定义训练](https://cloud.google.com/vertex-ai/docs/training/custom-training)和[Vertex AI批量预测](https://cloud.google.com/vertex-ai/docs/tabular-data/classification-regression/get-batch-predictions)的信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "objective:custom,training,online_prediction"
   },
   "source": [
    "### 目标\n",
    "\n",
    "在本教程中，您将学习如何使用Vertex AI Training创建一个自定义训练模型，并使用Vertex AI Batch Prediction对训练模型进行批量预测。\n",
    "\n",
    "使用Vertex AI SDK for Python在Docker容器中从Python脚本创建一个自定义训练模型，然后通过发送数据对部署的模型进行预测。 或者，您可以使用`gcloud`命令行工具或在Cloud控制台上在线创建自定义训练模型。\n",
    "\n",
    "本教程使用以下Google Cloud ML服务：\n",
    "\n",
    "- Vertex AI Training\n",
    "- Vertex AI Batch Prediction\n",
    "- Vertex AI Model资源\n",
    "\n",
    "执行的步骤包括：\n",
    "\n",
    "- 创建一个用于训练TensorFlow模型的Vertex AI自定义作业。\n",
    "- 将训练模型工件上传为模型资源。\n",
    "- 进行批量预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset:custom,cifar10,icn"
   },
   "source": [
    "### 数据集\n",
    "\n",
    "本教程使用的数据集是来自[TensorFlow Datasets](https://www.tensorflow.org/datasets/catalog/overview)的[cifar10数据集](https://www.tensorflow.org/datasets/catalog/cifar10)。您将使用的数据集版本已内置于TensorFlow中。训练模型可以预测图像属于十个类别中的哪一类：飞机、汽车、鸟、猫、鹿、狗、青蛙、马、船、卡车。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "costs"
   },
   "source": [
    "### 费用\n",
    "\n",
    "本教程使用谷歌云（GCP）的计费组件：\n",
    "\n",
    "* Vertex AI\n",
    "* 云存储\n",
    "\n",
    "了解[Vertex AI 价格](https://cloud.google.com/vertex-ai/pricing) 和 [云存储 价格](https://cloud.google.com/storage/pricing)，并使用[Pricing 计算器](https://cloud.google.com/products/calculator/)来根据您的预期使用情况生成成本估算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3b1ffd5ab768"
   },
   "source": [
    "开始吧"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aae9ca040eab"
   },
   "source": [
    "### 为Python安装Vertex AI SDK和其他必需的软件包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "23e23ce735c9"
   },
   "outputs": [],
   "source": [
    "! pip3 install --upgrade google-cloud-aiplatform \\\n",
    "                        google-cloud-storage \\\n",
    "                        pillow  \\\n",
    "                        numpy    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ff555b32bab8"
   },
   "source": [
    "### 重新启动运行时（仅限Colab）\n",
    "\n",
    "为了使用新安装的包，您必须在Google Colab上重新启动运行时。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f09b4dff629a"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54c5ef8a8f43"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️内核将重新启动。请等待直到重新启动完成后再继续下一步。⚠️</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "92e68cfc3a90"
   },
   "source": [
    "### 验证您的笔记本环境（仅适用于Colab）\n",
    "\n",
    "在Google Colab上验证您的环境。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "46604f70e831"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "107c51893a64"
   },
   "source": [
    "### 设置谷歌云项目信息并初始化用于 Python 的 Vertex AI SDK\n",
    "\n",
    "要开始使用 Vertex AI，您必须拥有一个现有的谷歌云项目并[启用 Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com)。了解更多关于[设置项目和开发环境](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "294fe4e5a671"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddbea904fbe5"
   },
   "source": [
    "创建一个云存储存储桶\n",
    "\n",
    "创建一个存储桶来存储中间产物，例如数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "751138cf3bd5"
   },
   "outputs": [],
   "source": [
    "BUCKET_URI = f\"gs://your-bucket-name-{PROJECT_ID}-unique\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58cb4f5895f0"
   },
   "source": [
    "如果您的存储桶还不存在：运行以下单元格来创建您的云存储存储桶。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5e1288505682"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l $LOCATION -p $PROJECT_ID $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fb03963cdb69"
   },
   "source": [
    "#### 初始化顶点 AI SDK for Python\n",
    "\n",
    "为您的项目初始化顶点 AI SDK for Python。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a4f61991b160"
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=LOCATION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_vars"
   },
   "source": [
    "### 导入库并定义常量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9dcd3eedadfb"
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "container:training,prediction"
   },
   "source": [
    "### 设置预构建容器\n",
    "\n",
    "Vertex AI提供预构建容器以运行训练和预测。\n",
    "\n",
    "有关最新列表，请参阅[用于训练的预构建容器](https://cloud.google.com/vertex-ai/docs/training/pre-built-containers)和[用于预测的预构建容器](https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1u1mr18jlugv"
   },
   "outputs": [],
   "source": [
    "TRAIN_VERSION = \"tf-cpu.2-9\"\n",
    "DEPLOY_VERSION = \"tf2-cpu.2-9\"\n",
    "\n",
    "TRAIN_IMAGE = \"us-docker.pkg.dev/vertex-ai/training/{}:latest\".format(TRAIN_VERSION)\n",
    "DEPLOY_IMAGE = \"us-docker.pkg.dev/vertex-ai/prediction/{}:latest\".format(DEPLOY_VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tutorial_start:custom"
   },
   "source": [
    "# 教程\n",
    "\n",
    "现在您已经准备好开始使用CIFAR10创建自己的自定义训练模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train_custom_model"
   },
   "source": [
    "训练模型\n",
    "\n",
    "您可以使用容器镜像的两种方式来训练一个定制模型：\n",
    "\n",
    "- **使用Google Cloud的预构建容器**。如果您使用预构建的容器，您还需要指定一个要安装到容器镜像中的Python包。这个Python包包含了您用于训练自定义模型的代码。\n",
    "\n",
    "- **使用您自己的定制容器镜像**。如果您使用自己的容器，容器需要包含您的用于训练自定义模型的代码。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train_custom_job_args"
   },
   "source": [
    "### 定义训练脚本的命令参数\n",
    "\n",
    "准备要传递给训练脚本的命令行参数。\n",
    "- `args`：要传递给相应Python模块的命令行参数。在本例中，它们将是：\n",
    "  - `\"--epochs=\" + EPOCHS`：训练的周期数。\n",
    "  - `\"--steps=\" + STEPS`：每个周期的步骤（批次）数。\n",
    "  - `\"--distribute=\" + TRAIN_STRATEGY\"`：用于单个或分布式训练的训练分发策略。\n",
    "     - `\"single\"`：单个设备。\n",
    "     - `\"mirror\"`：所有GPU设备在单个计算实例上。\n",
    "     - `\"multi\"`：所有GPU设备在所有计算实例上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1npiDcUtlugw"
   },
   "outputs": [],
   "source": [
    "JOB_NAME = \"custom_job_unique\"\n",
    "MODEL_DIR = \"{}/{}\".format(BUCKET_URI, JOB_NAME)\n",
    "\n",
    "\n",
    "TRAIN_STRATEGY = \"single\"\n",
    "\n",
    "EPOCHS = 20\n",
    "STEPS = 100\n",
    "\n",
    "CMDARGS = [\n",
    "    \"--epochs=\" + str(EPOCHS),\n",
    "    \"--steps=\" + str(STEPS),\n",
    "    \"--distribute=\" + TRAIN_STRATEGY,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "taskpy_contents"
   },
   "source": [
    "培训脚本\n",
    "\n",
    "在下一个单元格中，您将编写培训脚本`task.py`的内容。总结如下：\n",
    "\n",
    "- 从环境变量`AIP_MODEL_DIR`获取保存模型工件的目录。这个变量是由训练服务设置的。\n",
    "- 从TF数据集（tfds）加载CIFAR10数据集。\n",
    "- 使用TF.Keras模型API构建模型。\n",
    "- 编译模型（`compile()`）。\n",
    "- 根据参数`args.distribute`设置训练分布策略。\n",
    "- 根据参数`args.epochs`和`args.steps`来训练模型（`fit()`）。\n",
    "- 将已训练的模型保存（`save(MODEL_DIR)`）到指定的模型目录中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "72rUqXNFlugx"
   },
   "outputs": [],
   "source": [
    "%%writefile task.py\n",
    "# Single, Mirror and Multi-Machine Distributed Training for CIFAR-10\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--lr', dest='lr',\n",
    "                    default=0.01, type=float,\n",
    "                    help='Learning rate.')\n",
    "parser.add_argument('--epochs', dest='epochs',\n",
    "                    default=10, type=int,\n",
    "                    help='Number of epochs.')\n",
    "parser.add_argument('--steps', dest='steps',\n",
    "                    default=200, type=int,\n",
    "                    help='Number of steps per epoch.')\n",
    "parser.add_argument('--distribute', dest='distribute', type=str, default='single',\n",
    "                    help='distributed training strategy')\n",
    "args = parser.parse_args()\n",
    "\n",
    "print('Python Version = {}'.format(sys.version))\n",
    "print('TensorFlow Version = {}'.format(tf.__version__))\n",
    "print('TF_CONFIG = {}'.format(os.environ.get('TF_CONFIG', 'Not found')))\n",
    "print('DEVICES', device_lib.list_local_devices())\n",
    "\n",
    "# Single Machine, single compute device\n",
    "if args.distribute == 'single':\n",
    "    if tf.test.is_gpu_available():\n",
    "        strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
    "    else:\n",
    "        strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
    "# Single Machine, multiple compute device\n",
    "elif args.distribute == 'mirror':\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "# Multiple Machine, multiple compute device\n",
    "elif args.distribute == 'multi':\n",
    "    strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n",
    "\n",
    "# Multi-worker configuration\n",
    "print('num_replicas_in_sync = {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "# Preparing dataset\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "def make_datasets_unbatched():\n",
    "  # Scaling CIFAR10 data from (0, 255] to (0., 1.]\n",
    "  def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255.0\n",
    "    return image, label\n",
    "\n",
    "  datasets, info = tfds.load(name='cifar10',\n",
    "                            with_info=True,\n",
    "                            as_supervised=True)\n",
    "  return datasets['train'].map(scale).cache().shuffle(BUFFER_SIZE).repeat()\n",
    "\n",
    "\n",
    "# Build the Keras model\n",
    "def build_and_compile_cnn_model():\n",
    "  model = tf.keras.Sequential([\n",
    "      tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(32, 32, 3)),\n",
    "      tf.keras.layers.MaxPooling2D(),\n",
    "      tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "      tf.keras.layers.MaxPooling2D(),\n",
    "      tf.keras.layers.Flatten(),\n",
    "      tf.keras.layers.Dense(10, activation='softmax')\n",
    "  ])\n",
    "  model.compile(\n",
    "      loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "      optimizer=tf.keras.optimizers.SGD(learning_rate=args.lr),\n",
    "      metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "# Train the model\n",
    "NUM_WORKERS = strategy.num_replicas_in_sync\n",
    "# Here the batch size scales up by number of workers since\n",
    "# `tf.data.Dataset.batch` expects the global batch size.\n",
    "GLOBAL_BATCH_SIZE = BATCH_SIZE * NUM_WORKERS\n",
    "MODEL_DIR = os.getenv(\"AIP_MODEL_DIR\")\n",
    "\n",
    "train_dataset = make_datasets_unbatched().batch(GLOBAL_BATCH_SIZE)\n",
    "\n",
    "with strategy.scope():\n",
    "  # Creation of dataset, and model building/compiling need to be within\n",
    "  # `strategy.scope()`.\n",
    "  model = build_and_compile_cnn_model()\n",
    "\n",
    "model.fit(x=train_dataset, epochs=args.epochs, steps_per_epoch=args.steps)\n",
    "model.save(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train_custom_job"
   },
   "source": [
    "### 训练模型\n",
    "\n",
    "在Vertex AI上定义您的自定义训练作业。\n",
    "\n",
    "使用`CustomTrainingJob`类来定义作业，它接受以下参数：\n",
    "\n",
    "- `display_name`：此训练流水线的用户定义名称。\n",
    "- `script_path`：训练脚本的本地路径。\n",
    "- `container_uri`：训练容器镜像的URI。\n",
    "- `requirements`：脚本的Python软件包依赖项列表。\n",
    "- `model_serving_container_image_uri`：可以为您的模型提供预测的容器的URI - 可以是预构建的容器或自定义容器。\n",
    "\n",
    "使用`run`函数开始训练，它接受以下参数：\n",
    "\n",
    "- `args`：要传递给Python脚本的命令行参数。\n",
    "- `replica_count`：工作器复制品的数量。\n",
    "- `model_display_name`：如果脚本生成托管的`Model`，则为`Model`的显示名称。\n",
    "- `machine_type`：用于训练的机器类型。\n",
    "- `accelerator_type`：硬件加速器类型。\n",
    "- `accelerator_count`：要附加到工作器复制品的加速器数量。\n",
    "\n",
    "`run`函数创建一个训练流水线，用于训练并创建`Model`对象。训练流水线完成后，`run`函数将返回`Model`对象。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mxIxvDdglugx"
   },
   "outputs": [],
   "source": [
    "job = aiplatform.CustomTrainingJob(\n",
    "    display_name=JOB_NAME,\n",
    "    script_path=\"task.py\",\n",
    "    container_uri=TRAIN_IMAGE,\n",
    "    requirements=[\"tensorflow_datasets==1.3.0\"],\n",
    "    model_serving_container_image_uri=DEPLOY_IMAGE,\n",
    ")\n",
    "\n",
    "MODEL_DISPLAY_NAME = \"model_unique\"\n",
    "\n",
    "# Start the training\n",
    "\n",
    "model = job.run(\n",
    "    model_display_name=MODEL_DISPLAY_NAME,\n",
    "    args=CMDARGS,\n",
    "    replica_count=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "make_prediction"
   },
   "source": [
    "发出一个批量预测请求给你部署的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "get_test_item:test"
   },
   "source": [
    "获取测试数据\n",
    "\n",
    "从CIFAR数据集下载图像并预处理。\n",
    "\n",
    "下载测试图像\n",
    "\n",
    "从CIFAR数据集下载提供的一组图像。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E1EQBPGnlugz"
   },
   "outputs": [],
   "source": [
    "# Download the images\n",
    "! gsutil -m cp -r gs://cloud-samples-data/ai-platform-unified/cifar_test_images ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "prepare_test_item:test,image"
   },
   "source": [
    "#### 预处理图像\n",
    "在您能够将数据传递到端点之前，您需要对其进行预处理，以匹配您在`task.py`中定义的自定义模型所期望的格式。\n",
    "\n",
    "`x_test`：\n",
    "通过将每个像素除以255来归一化（重新缩放）像素数据。这将使用 32 位浮点数代替每个单字节整数像素，范围在0到1之间。\n",
    "\n",
    "`y_test`：\n",
    "您可以从图像文件名中提取标签。每个图像的文件名格式为\"image_{LABEL}_{IMAGE_NUMBER}.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cl59KGnXlugz"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Load image data\n",
    "IMAGE_DIRECTORY = \"cifar_test_images\"\n",
    "\n",
    "image_files = [file for file in os.listdir(IMAGE_DIRECTORY) if file.endswith(\".jpg\")]\n",
    "\n",
    "# Decode JPEG images into numpy arrays\n",
    "image_data = [\n",
    "    np.asarray(Image.open(os.path.join(IMAGE_DIRECTORY, file))) for file in image_files\n",
    "]\n",
    "\n",
    "# Scale and convert to expected format\n",
    "x_test = [(image / 255.0).astype(np.float32).tolist() for image in image_data]\n",
    "\n",
    "# Extract labels from image name\n",
    "y_test = [int(file.split(\"_\")[1]) for file in image_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1e29665076f"
   },
   "source": [
    "准备批量预测的数据\n",
    "在您可以运行数据进行批量预测之前，您需要将数据保存成几种可能的格式之一。\n",
    "\n",
    "对于本教程，请使用JSONL，因为它与每个图像目前所在的三维列表兼容。要做到这一点：\n",
    "\n",
    "1. 在一个文件中，将每个实例作为单独的JSON写入一行。\n",
    "2. 将此文件上传到云存储。\n",
    "\n",
    "有关批量预测输入格式的更多详细信息：https://cloud.google.com/vertex-ai/docs/predictions/batch-predictions#batch_request_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3e6b04d29c3b"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "BATCH_PREDICTION_INSTANCES_FILE = \"batch_prediction_instances.jsonl\"\n",
    "\n",
    "BATCH_PREDICTION_GCS_SOURCE = (\n",
    "    BUCKET_URI + \"/batch_prediction_instances/\" + BATCH_PREDICTION_INSTANCES_FILE\n",
    ")\n",
    "\n",
    "# Write instances at JSONL\n",
    "with open(BATCH_PREDICTION_INSTANCES_FILE, \"w\") as f:\n",
    "    for x in x_test:\n",
    "        f.write(json.dumps(x) + \"\\n\")\n",
    "\n",
    "# Upload to Cloud Storage bucket\n",
    "! gsutil cp $BATCH_PREDICTION_INSTANCES_FILE $BATCH_PREDICTION_GCS_SOURCE\n",
    "\n",
    "print(\"Uploaded instances to: \", BATCH_PREDICTION_GCS_SOURCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "send_prediction_request:image"
   },
   "source": [
    "发送预测请求\n",
    "\n",
    "要发起批量预测请求，调用模型对象的`batch_predict`方法，使用以下参数：\n",
    "- `instances_format`：批量预测请求文件的格式：\"jsonl\"、\"csv\"、\"bigquery\"、\"tf-record\"、\"tf-record-gzip\"或\"file-list\"\n",
    "- `prediction_format`：批量预测响应文件的格式：\"jsonl\"、\"csv\"、\"bigquery\"、\"tf-record\"、\"tf-record-gzip\"或\"file-list\"\n",
    "- `job_display_name`：预测作业的可读名称。\n",
    "- `gcs_source`：您的批量预测请求的一个或多个云存储路径列表。\n",
    "- `gcs_destination_prefix`：服务将写入预测结果的云存储路径。\n",
    "- `model_parameters`：用于提供预测结果的其他过滤参数。\n",
    "- `machine_type`：用于训练的计算机类型。\n",
    "- `accelerator_type`：硬件加速器类型。\n",
    "- `accelerator_count`：要附加到工作副本的加速器数量。\n",
    "- `starting_replica_count`：最初提供的计算实例数量。\n",
    "- `max_replica_count`：要扩展到的最大计算实例数量。在本教程中，只提供一个实例。\n",
    "\n",
    "### 计算实例扩展\n",
    "\n",
    "您可以指定单个实例（或节点）来处理批量预测请求。本教程使用单个节点，因此变量`MIN_NODES`和`MAX_NODES`均设置为`1`。\n",
    "\n",
    "如果要使用多个节点来处理批量预测请求，请将`MAX_NODES`设置为您要使用的最大节点数。Vertex AI会自动调整用于提供预测的节点数量，直到达到您设置的最大数量。请参考[定价页面](https://cloud.google.com/vertex-ai/pricing#prediction-prices)了解使用多个节点进行自动扩展的成本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1cf1076178fc"
   },
   "outputs": [],
   "source": [
    "MIN_NODES = 1\n",
    "MAX_NODES = 1\n",
    "\n",
    "# The name of the job\n",
    "BATCH_PREDICTION_JOB_NAME = \"cifar10_batch_prediction_unique\"\n",
    "\n",
    "# Folder in the bucket to write results to\n",
    "DESTINATION_FOLDER = \"batch_prediction_results\"\n",
    "\n",
    "# The Cloud Storage bucket to upload results to\n",
    "BATCH_PREDICTION_GCS_DEST_PREFIX = BUCKET_URI + \"/\" + DESTINATION_FOLDER\n",
    "\n",
    "# Make SDK batch_predict method call\n",
    "batch_prediction_job = model.batch_predict(\n",
    "    instances_format=\"jsonl\",\n",
    "    predictions_format=\"jsonl\",\n",
    "    job_display_name=BATCH_PREDICTION_JOB_NAME,\n",
    "    gcs_source=BATCH_PREDICTION_GCS_SOURCE,\n",
    "    gcs_destination_prefix=BATCH_PREDICTION_GCS_DEST_PREFIX,\n",
    "    model_parameters=None,\n",
    "    starting_replica_count=MIN_NODES,\n",
    "    max_replica_count=MAX_NODES,\n",
    "    machine_type=\"n1-standard-4\",\n",
    "    sync=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "df3b3d1bdd24"
   },
   "source": [
    "### 获取批量预测结果\n",
    "当批量预测处理完成时，您可以最终查看存储在您设置为输出的云存储路径上的预测结果。预测结果将以JSONL格式存在，这是您在创建批量预测作业时指定的格式。预测结果位于以名称prediction开头的子目录中。在该目录中，有一个名为prediction.results-xxxx-of-xxxx的文件。\n",
    "\n",
    "让我们显示内容。您将为每个预测获得一行。该行是对应的CIFAR10类的softmax概率分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2f10b13b2b88"
   },
   "outputs": [],
   "source": [
    "RESULTS_DIRECTORY = \"prediction_results\"\n",
    "RESULTS_DIRECTORY_FULL = RESULTS_DIRECTORY + \"/\" + DESTINATION_FOLDER\n",
    "\n",
    "# Create missing directories\n",
    "os.makedirs(RESULTS_DIRECTORY, exist_ok=True)\n",
    "\n",
    "# Get the Cloud Storage paths for each result\n",
    "! gsutil -m cp -r $BATCH_PREDICTION_GCS_DEST_PREFIX $RESULTS_DIRECTORY\n",
    "\n",
    "# Get most recently modified directory\n",
    "latest_directory = max(\n",
    "    (\n",
    "        os.path.join(RESULTS_DIRECTORY_FULL, d)\n",
    "        for d in os.listdir(RESULTS_DIRECTORY_FULL)\n",
    "    ),\n",
    "    key=os.path.getmtime,\n",
    ")\n",
    "\n",
    "# Get downloaded results in directory\n",
    "results_files = []\n",
    "for dirpath, subdirs, files in os.walk(latest_directory):\n",
    "    for file in files:\n",
    "        if file.startswith(\"prediction.results\"):\n",
    "            results_files.append(os.path.join(dirpath, file))\n",
    "\n",
    "# Consolidate all the results into a list\n",
    "results = []\n",
    "for results_file in results_files:\n",
    "    # Download each result\n",
    "    with open(results_file, \"r\") as file:\n",
    "        results.extend([json.loads(line) for line in file.readlines()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "962b5a11fdae"
   },
   "source": [
    "### 评估结果\n",
    "\n",
    "然后，您可以对预测结果进行快速评估：\n",
    "\n",
    "1. `np.argmax`：将每个置信度列表转换为标签\n",
    "2. 比较预测的标签与实际标签\n",
    "3. 将`正确/总数`计算为`准确率`\n",
    "\n",
    "为了提高准确率，请尝试训练更多个周期。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UywuX7fRlugz"
   },
   "outputs": [],
   "source": [
    "y_predicted = [np.argmax(result[\"prediction\"]) for result in results]\n",
    "\n",
    "correct = sum(y_predicted == np.array(y_test))\n",
    "accuracy = len(y_predicted)\n",
    "print(\n",
    "    f\"Correct predictions = {correct}, Total predictions = {accuracy}, Accuracy = {correct/accuracy}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleanup:custom"
   },
   "source": [
    "清理\n",
    "\n",
    "要清理此项目中使用的所有Google Cloud资源，您可以删除用于本教程的[Google Cloud项目](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects)。\n",
    "\n",
    "否则，您可以删除本教程中创建的个别资源：\n",
    "\n",
    "- 训练作业\n",
    "- 模型\n",
    "- 云存储桶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NNmebHf7lug0"
   },
   "outputs": [],
   "source": [
    "# Warning: Setting this to true will delete everything in your bucket\n",
    "delete_bucket = False\n",
    "\n",
    "# Delete the training job\n",
    "job.delete()\n",
    "\n",
    "# Delete the model\n",
    "model.delete()\n",
    "\n",
    "if delete_bucket:\n",
    "    ! gsutil -m rm -r $BUCKET_URI"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "sdk-custom-image-classification-batch.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
