{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "copyright"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "在BigQuery数据上对TensorFlow模型进行训练\n",
    "\n",
    "在Colab中打开\n",
    "\n",
    "在Colab企业版中打开\n",
    "\n",
    "在Workbench中打开\n",
    "\n",
    "在GitHub上查看"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "overview:custom"
   },
   "source": [
    "## 概述\n",
    "\n",
    "本教程演示了如何使用Python的Vertex AI SDK来训练和部署一个用于在线预测的自定义表格分类模型。\n",
    "\n",
    "了解更多关于[Vertex AI训练](https://cloud.google.com/vertex-ai/docs/training/custom-training)的信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "objective:custom,training,online_prediction"
   },
   "source": [
    "### 目标\n",
    "\n",
    "在这份笔记本中，你将学习如何使用 Vertex AI SDK for Python 在 Docker 容器中从 Python 脚本中创建一个自定义训练模型，然后通过发送数据来获取部署模型的预测。另外，您也可以使用 `gcloud` 命令行工具或在 Cloud 控制台上在线创建自定义训练模型。\n",
    "\n",
    "本教程使用以下 Google Cloud ML 服务和资源：\n",
    "\n",
    "- BigQuery\n",
    "- Cloud Storage\n",
    "- Vertex AI 管理的数据集\n",
    "- Vertex AI 训练\n",
    "- Vertex AI 端点\n",
    "\n",
    "所执行的步骤包括：\n",
    "\n",
    "- 创建一个 Vertex AI 自定义 `TrainingPipeline` 以训练模型。\n",
    "- 训练一个 TensorFlow 模型。\n",
    "- 部署 `Model` 资源到一个服务 `Endpoint` 资源。\n",
    "- 进行预测。\n",
    "- 取消部署 `Model` 资源。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset:custom,cifar10,icn"
   },
   "source": [
    "### 数据集\n",
    "\n",
    "本教程使用的数据集是来自[BigQuery公共数据集](https://cloud.google.com/bigquery/public-data)的企鹅数据集。在本教程中，只使用数据集中的`culmen_length_mm`、`culmen_depth_mm`、`flipper_length_mm`和`body_mass_g`字段来预测企鹅的物种（`species`）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "costs"
   },
   "source": [
    "### 成本\n",
    "\n",
    "本教程使用 Google Cloud 的付费组件：\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "* BigQuery\n",
    "\n",
    "了解有关 [Vertex AI\n",
    "定价](https://cloud.google.com/vertex-ai/pricing)、[Cloud Storage\n",
    "定价](https://cloud.google.com/storage/pricing)、[BigQuery 定价](https://cloud.google.com/bigquery/pricing) 的信息，并使用 [定价\n",
    "计算器](https://cloud.google.com/products/calculator/)\n",
    "根据您的预计使用量生成成本估算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3b1ffd5ab768"
   },
   "source": [
    "开始吧"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dc848186ab0e"
   },
   "source": [
    "### 为Python安装Vertex AI SDK和其他必需的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1fd00fa70a2a"
   },
   "outputs": [],
   "source": [
    "# Install the packages\n",
    "! pip3 install --upgrade --quiet google-cloud-aiplatform \\\n",
    "                        google-cloud-storage \\\n",
    "                        'google-cloud-bigquery[pandas]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ff555b32bab8"
   },
   "source": [
    "重新启动运行时（仅适用于Colab）\n",
    "\n",
    "为了使用新安装的包，您必须在Google Colab上重新启动运行时。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f09b4dff629a"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54c5ef8a8f43"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ 内核将重新启动。请等待完成后再继续下一步。⚠️</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f82e28c631cc"
   },
   "source": [
    "### 在谷歌 Colab 上验证您的笔记本环境\n",
    "\n",
    "在谷歌 Colab 上验证您的环境。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "46604f70e831"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "107c51893a64"
   },
   "source": [
    "### 设置 Google Cloud 项目信息并初始化 Python 的 Vertex AI SDK\n",
    "\n",
    "要开始使用 Vertex AI，您必须拥有现有的 Google Cloud 项目并[启用 Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com)。了解更多关于[设置项目和开发环境](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)的信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3c8049930470"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bucket:custom"
   },
   "source": [
    "创建云存储桶\n",
    "\n",
    "创建一个存储桶，用于存储诸如数据集等中间产物。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bucket"
   },
   "outputs": [],
   "source": [
    "BUCKET_URI = \"gs://your-bucket-name-unique\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_bucket"
   },
   "source": [
    "只有当您的存储桶不存在时，才运行以下单元格以创建您的云存储桶。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oz8J0vmSlugt"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l $LOCATION -p $PROJECT_ID $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4f1319830b6e"
   },
   "source": [
    "### 导入库"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "poijnGfZCFYi"
   },
   "source": [
    "导入Vertex AI Python SDK和其他必需的Python库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b9a0a5a74fa6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from google.cloud import aiplatform, bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "750d53e37094"
   },
   "source": [
    "### 初始化用于Python的顶点 AI SDK\n",
    "\n",
    "为您的项目和相应的存储桶初始化用于Python的顶点 AI SDK。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c9d3ac73dfbc"
   },
   "outputs": [],
   "source": [
    "# Initialize the Vertex AI SDK for Python\n",
    "aiplatform.init(project=PROJECT_ID, location=LOCATION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7c163842eabd"
   },
   "source": [
    "### 初始化BigQuery客户端\n",
    "\n",
    "为您的项目初始化BigQuery Python客户端。\n",
    "\n",
    "要使用BigQuery，请确保您的账号具有“BigQuery用户”角色。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fad2ba1ad7c3"
   },
   "outputs": [],
   "source": [
    "# Set up BigQuery client\n",
    "bq_client = bigquery.Client(project=PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0a2c41bc91a6"
   },
   "source": [
    "### 数据预处理和拆分数据\n",
    "首先，您应该下载并预处理您的数据，用于训练和测试。\n",
    "\n",
    "- 将分类特征转换为数字\n",
    "- 删除未使用的列\n",
    "- 删除无法使用的行\n",
    "- 拆分训练和测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e3a2449cfcf1"
   },
   "outputs": [],
   "source": [
    "LABEL_COLUMN = \"species\"\n",
    "\n",
    "# Define the BigQuery source dataset\n",
    "BQ_SOURCE = \"bigquery-public-data.ml_datasets.penguins\"\n",
    "\n",
    "# Define NA values\n",
    "NA_VALUES = [\"NA\", \".\"]\n",
    "\n",
    "# Download a table\n",
    "table = bq_client.get_table(BQ_SOURCE)\n",
    "df = bq_client.list_rows(table).to_dataframe()\n",
    "\n",
    "# Drop unusable rows\n",
    "df = df.replace(to_replace=NA_VALUES, value=np.NaN).dropna()\n",
    "\n",
    "# Convert categorical columns to numeric\n",
    "df[\"island\"], _ = pd.factorize(df[\"island\"])\n",
    "df[\"species\"], _ = pd.factorize(df[\"species\"])\n",
    "df[\"sex\"], _ = pd.factorize(df[\"sex\"])\n",
    "\n",
    "# Split into a training and holdout dataset\n",
    "df_train = df.sample(frac=0.8, random_state=100)\n",
    "df_holdout = df[~df.index.isin(df_train.index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a39df4692a70"
   },
   "source": [
    "从BigQuery数据集创建一个Vertex AI表格数据集\n",
    "\n",
    "从您的BigQuery训练数据中创建一个Vertex AI表格数据集资源。\n",
    "\n",
    "在此处查看更多信息：https://cloud.google.com/vertex-ai/docs/training/using-managed-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7fa452ee5c75"
   },
   "outputs": [],
   "source": [
    "# Create BigQuery dataset\n",
    "bq_dataset_id = f\"{PROJECT_ID}.dataset_id_unique\"\n",
    "bq_dataset = bigquery.Dataset(bq_dataset_id)\n",
    "bq_client.create_dataset(bq_dataset, exists_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1d26b159106f"
   },
   "outputs": [],
   "source": [
    "dataset = aiplatform.TabularDataset.create_from_dataframe(\n",
    "    df_source=df_train,\n",
    "    staging_path=f\"bq://{bq_dataset_id}.table-unique\",\n",
    "    display_name=\"sample-penguins\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train_custom_model"
   },
   "source": [
    "### 训练模型\n",
    "\n",
    "有两种方式可以使用容器镜像来训练模型：\n",
    "\n",
    "- **使用 Vertex AI 预构建的容器镜像**。如果使用预构建的训练容器镜像，还需指定一个要安装到容器镜像中的 Python 包。这个 Python 包包含了您的训练代码。\n",
    "\n",
    "- **使用自定义的容器镜像**。如果使用自定义的容器，那么容器镜像必须包含您的训练代码。\n",
    "\n",
    "本示范中将使用预构建的容器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train_custom_job_args"
   },
   "source": [
    "### 为训练脚本定义命令参数\n",
    "\n",
    "准备要传递给训练脚本的命令行参数。\n",
    "- `args`：传递给相应 Python 模块的命令行参数。在本例中，它们是：\n",
    "  - `label_column`：要预测的数据中的标签列。\n",
    "  - `epochs`：训练的时期数。\n",
    "  - `batch_size`：训练的批量大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1npiDcUtlugw"
   },
   "outputs": [],
   "source": [
    "JOB_NAME = \"custom_job_unique\"\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "CMDARGS = [\n",
    "    \"--label_column=\" + LABEL_COLUMN,\n",
    "    \"--epochs=\" + str(EPOCHS),\n",
    "    \"--batch_size=\" + str(BATCH_SIZE),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "taskpy_contents"
   },
   "source": [
    "#### 训练脚本\n",
    "\n",
    "在下一个单元格中，编写训练脚本`task.py`的内容。总的来说，脚本执行以下操作：\n",
    "\n",
    "- 使用 BigQuery Python 客户端库从 BigQuery 表中加载数据。\n",
    "- 使用 TF.Keras 模型 API 构建模型。\n",
    "- 编译模型（`compile()`）。\n",
    "- 根据参数`args.distribute`设置训练分发策略。\n",
    "- 根据参数`args.epochs`和`args.batch_size`训练模型(`fit()`)。\n",
    "- 从环境变量`AIP_MODEL_DIR`中获取保存模型文件的目录。该变量是由训练服务设置的。\n",
    "- 将训练完成的模型保存到模型目录中。\n",
    "\n",
    "> **_注意:_** 为了提高模型性能，在训练之前建议对模型的输入进行归一化处理。请查看 TensorFlow 教程 [https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers#numerical_columns](https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers#numerical_columns) 了解详细信息。\n",
    "\n",
    "> **_注意:_** 以下训练代码要求您授予训练帐号“BigQuery Read Session User”角色。请查看 [https://cloud.google.com/vertex-ai/docs/general/access-control#service-agents](https://cloud.google.com/vertex-ai/docs/general/access-control#service-agents) 以了解如何找到该帐号。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "72rUqXNFlugx"
   },
   "outputs": [],
   "source": [
    "%%writefile task.py\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "\n",
    "# Read environmental variables\n",
    "training_data_uri = os.getenv(\"AIP_TRAINING_DATA_URI\")\n",
    "validation_data_uri = os.getenv(\"AIP_VALIDATION_DATA_URI\")\n",
    "test_data_uri = os.getenv(\"AIP_TEST_DATA_URI\")\n",
    "\n",
    "# Read args\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--label_column', required=True, type=str)\n",
    "parser.add_argument('--epochs', default=10, type=int)\n",
    "parser.add_argument('--batch_size', default=10, type=int)\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Set up training variables\n",
    "LABEL_COLUMN = args.label_column\n",
    "\n",
    "# See https://cloud.google.com/vertex-ai/docs/workbench/managed/executor#explicit-project-selection for issues regarding permissions.\n",
    "PROJECT_NUMBER = os.environ[\"CLOUD_ML_PROJECT_ID\"]\n",
    "bq_client = bigquery.Client(project=PROJECT_NUMBER)\n",
    "\n",
    "\n",
    "# Download a table\n",
    "def download_table(bq_table_uri: str):\n",
    "    # Remove bq:// prefix if present\n",
    "    prefix = \"bq://\"\n",
    "    if bq_table_uri.startswith(prefix):\n",
    "        bq_table_uri = bq_table_uri[len(prefix) :]\n",
    "        \n",
    "    # Download the BigQuery table as a dataframe\n",
    "    # This requires the \"BigQuery Read Session User\" role on the custom training service account.\n",
    "    table = bq_client.get_table(bq_table_uri)\n",
    "    return bq_client.list_rows(table).to_dataframe()\n",
    "\n",
    "# Download dataset splits\n",
    "df_train = download_table(training_data_uri)\n",
    "df_validation = download_table(validation_data_uri)\n",
    "df_test = download_table(test_data_uri)\n",
    "\n",
    "def convert_dataframe_to_dataset(\n",
    "    df_train: pd.DataFrame,\n",
    "    df_validation: pd.DataFrame,\n",
    "):\n",
    "    df_train_x, df_train_y = df_train, df_train.pop(LABEL_COLUMN)\n",
    "    df_validation_x, df_validation_y = df_validation, df_validation.pop(LABEL_COLUMN)\n",
    "\n",
    "    y_train = tf.convert_to_tensor(np.asarray(df_train_y).astype(\"float32\"))\n",
    "    y_validation = tf.convert_to_tensor(np.asarray(df_validation_y).astype(\"float32\"))\n",
    "\n",
    "    # Convert to numpy representation\n",
    "    x_train = tf.convert_to_tensor(np.asarray(df_train_x).astype(\"float32\"))\n",
    "    x_test = tf.convert_to_tensor(np.asarray(df_validation_x).astype(\"float32\"))\n",
    "\n",
    "    # Convert to one-hot representation\n",
    "    num_species = len(df_train_y.unique())\n",
    "    y_train = tf.keras.utils.to_categorical(y_train, num_classes=num_species)\n",
    "    y_validation = tf.keras.utils.to_categorical(y_validation, num_classes=num_species)\n",
    "\n",
    "    dataset_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    dataset_validation = tf.data.Dataset.from_tensor_slices((x_test, y_validation))\n",
    "    return (dataset_train, dataset_validation)\n",
    "\n",
    "# Create datasets\n",
    "dataset_train, dataset_validation = convert_dataframe_to_dataset(df_train, df_validation)\n",
    "\n",
    "# Shuffle train set\n",
    "dataset_train = dataset_train.shuffle(len(df_train))\n",
    "\n",
    "def create_model(num_features):\n",
    "    # Create model\n",
    "    Dense = tf.keras.layers.Dense\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            Dense(\n",
    "                100,\n",
    "                activation=tf.nn.relu,\n",
    "                kernel_initializer=\"uniform\",\n",
    "                input_dim=num_features,\n",
    "            ),\n",
    "            Dense(75, activation=tf.nn.relu),\n",
    "            Dense(50, activation=tf.nn.relu),            \n",
    "            Dense(25, activation=tf.nn.relu),\n",
    "            Dense(3, activation=tf.nn.softmax),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Compile Keras model\n",
    "    optimizer = tf.keras.optimizers.RMSprop(lr=0.001)\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=optimizer\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "model = create_model(num_features=dataset_train._flat_shapes[0].dims[0].value)\n",
    "\n",
    "# Set up datasets\n",
    "dataset_train = dataset_train.batch(args.batch_size)\n",
    "dataset_validation = dataset_validation.batch(args.batch_size)\n",
    "\n",
    "# Train the model\n",
    "model.fit(dataset_train, epochs=args.epochs, validation_data=dataset_validation)\n",
    "\n",
    "tf.saved_model.save(model, os.getenv(\"AIP_MODEL_DIR\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train_custom_job"
   },
   "source": [
    "### 训练模型\n",
    "\n",
    "在Vertex AI上定义您的自定义`TrainingPipeline`。\n",
    "\n",
    "使用`CustomTrainingJob`类来定义`TrainingPipeline`。该类接受以下参数：\n",
    "\n",
    "- `display_name`：此训练流程的用户定义名称。\n",
    "- `script_path`：训练脚本的本地路径。\n",
    "- `container_uri`：训练容器镜像的URI。\n",
    "- `requirements`：脚本的Python包依赖项列表。\n",
    "- `model_serving_container_image_uri`：可为模型提供预测的容器的URI，可以是预构建容器或自定义容器。\n",
    "\n",
    "使用`run`函数开始训练。该函数接受以下参数：\n",
    "\n",
    "- `args`：传递给Python脚本的命令行参数。\n",
    "- `replica_count`：工作节点复制品的数量。\n",
    "- `model_display_name`：如果脚本生成托管的`Model`，则为`Model`的展示名称。\n",
    "- `machine_type`：用于训练的机器类型。\n",
    "- `accelerator_type`：硬件加速器类型。\n",
    "- `accelerator_count`：要连接到工作节点复制品的加速器数量。\n",
    "\n",
    "`run`函数创建一个训练流程，用于训练并创建一个`Model`对象。训练流程完成后，`run`函数会返回`Model`对象。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mxIxvDdglugx"
   },
   "outputs": [],
   "source": [
    "job = aiplatform.CustomTrainingJob(\n",
    "    display_name=JOB_NAME,\n",
    "    script_path=\"task.py\",\n",
    "    container_uri=\"us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-8:latest\",\n",
    "    requirements=[\"google-cloud-bigquery[pandas]\", \"protobuf<3.20.0\"],\n",
    "    model_serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-8:latest\",\n",
    ")\n",
    "\n",
    "MODEL_DISPLAY_NAME = \"penguins_model_unique\"\n",
    "\n",
    "# Start the training\n",
    "model = job.run(\n",
    "    dataset=dataset,\n",
    "    model_display_name=MODEL_DISPLAY_NAME,\n",
    "    bigquery_destination=f\"bq://{PROJECT_ID}\",\n",
    "    args=CMDARGS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "deploy_model:dedicated"
   },
   "source": [
    "### 部署模型\n",
    "\n",
    "在使用模型进行预测之前，您必须将其部署到一个`终端点（Endpoint）`。您可以通过调用`Model`资源上的`deploy`函数来完成这个操作。这个操作会执行以下两件事情：\n",
    "\n",
    "1. 为部署`Model`资源创建一个`终端点（Endpoint）`资源。\n",
    "2. 将`Model`资源部署到`终端点（Endpoint）`资源中。\n",
    "\n",
    "该函数接受以下参数：\n",
    "\n",
    "- `deployed_model_display_name`：部署模型的可读名称。\n",
    "- `traffic_split`：将流量分配到此模型的终端点的百分比，指定为一个或多个键/值对的字典。\n",
    "   - 如果只有一个模型，请指定`{ \"0\": 100 }`，其中 \"0\" 指的是正在上传的模型，100 表示 100% 的流量。\n",
    "   - 如果有现有模型在终端点上，需要分配流量，则使用 `model_id` 指定`{ \"0\": 百分比, model_id: 百分比, ... }`，其中 `model_id` 是终端点上现有`DeployedModel`的ID。百分比必须总和为 100。\n",
    "- `machine_type`：用于训练的机器类型。\n",
    "- `accelerator_type`：硬件加速器类型。\n",
    "- `accelerator_count`：要附加到工作节点副本的加速器数。\n",
    "- `starting_replica_count`：初始预留的计算实例数。\n",
    "- `max_replica_count`：要扩展到的最大计算实例数。在本教程中，只会预留一个实例。\n",
    "\n",
    "### 流量分配\n",
    "\n",
    "`traffic_split`参数被指定为一个Python字典。您可以将模型的多个实例部署到一个终端点，然后设置每个实例获得的流量百分比。\n",
    "\n",
    "您可以使用流量分配逐渐将新模型引入生产环境。例如，如果您在生产中有一个现有模型占据了 100% 的流量，您可以将一个新模型部署到同一个终端点，将10% 的流量导向它，并将原始模型的流量减少到 90%。这样可以让您在最小化对大多数用户干扰的同时监视新模型的性能。\n",
    "\n",
    "### 计算实例扩展\n",
    "\n",
    "您可以指定单个实例（或节点）来提供您的在线预测请求。本教程使用单个节点，因此`MIN_NODES`和`MAX_NODES`变量均设置为`1`。\n",
    "\n",
    "如果要使用多个节点来提供您的在线预测请求，请将`MAX_NODES`设置为您想要使用的最大节点数。Vertex AI会自动调整用于提供预测的节点数量，直至达到您设置的最大值。请参考[定价页面](https://cloud.google.com/vertex-ai/pricing#prediction-prices)了解使用多个节点进行自动扩展的成本情况。\n",
    "\n",
    "### 终端点\n",
    "\n",
    "`deploy`方法会等到模型部署完成，并最终返回一个`终端点（Endpoint）`对象。如果这是第一次将模型部署到终端点，则可能需要额外几分钟来完成资源的预留。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WMH7GrYMlugy"
   },
   "outputs": [],
   "source": [
    "DEPLOYED_NAME = \"penguins_deployed_unique\"\n",
    "\n",
    "endpoint = model.deploy(deployed_model_display_name=DEPLOYED_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "make_prediction"
   },
   "source": [
    "发出一个在线预测请求，发送到您部署的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "get_test_item:test"
   },
   "source": [
    "### 准备测试数据\n",
    "\n",
    "将其转换为Python列表，准备测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "67aeea91384a"
   },
   "outputs": [],
   "source": [
    "df_holdout_y = df_holdout.pop(LABEL_COLUMN)\n",
    "df_holdout_x = df_holdout\n",
    "\n",
    "# Convert to list representation\n",
    "holdout_x = np.array(df_holdout_x).tolist()\n",
    "holdout_y = np.array(df_holdout_y).astype(\"float32\").tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "send_prediction_request:image"
   },
   "source": [
    "### 发送预测请求\n",
    "\n",
    "现在您有了测试数据，可以使用它来发送预测请求。使用`Endpoint`对象的`predict`函数，该函数接受以下参数：\n",
    "\n",
    "- `instances`：一组企鹅测量实例。根据您的自定义模型，每个实例应该是一个数字数组。您在上一步中准备了这个列表。\n",
    "\n",
    "`predict`函数返回一个列表，列表中的每个元素对应请求中的一个实例。在每个预测的输出中，您将看到以下内容：\n",
    "\n",
    "- 针对预测的置信水平(`predictions`)，在0到1之间，对应于十个类别中的每一个。\n",
    "\n",
    "然后，您可以对预测结果进行快速评估：\n",
    "1. `np.argmax`：将每个置信水平列表转换为标签\n",
    "2. 打印预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6e20473b09f5"
   },
   "outputs": [],
   "source": [
    "predictions = endpoint.predict(instances=holdout_x)\n",
    "y_predicted = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "y_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "undeploy_model"
   },
   "source": [
    "### 撤销模型\n",
    "\n",
    "要从服务端点资源中撤销所有`Model`资源，可以使用端点的`undeploy_all`方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "khPSAO1tlug0"
   },
   "outputs": [],
   "source": [
    "endpoint.undeploy_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleanup:custom"
   },
   "source": [
    "清理工作\n",
    "\n",
    "要清理此项目中使用的所有Google Cloud资源，您可以[删除用于教程的Google Cloud项目](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects)。\n",
    "\n",
    "否则，您可以删除此教程中创建的个别资源：\n",
    "\n",
    "- 训练作业\n",
    "- 模型\n",
    "- 终端\n",
    "- 云存储桶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NNmebHf7lug0"
   },
   "outputs": [],
   "source": [
    "# Delete the training job\n",
    "job.delete()\n",
    "\n",
    "# Delete the model\n",
    "model.delete()\n",
    "\n",
    "# Delete the endpoint\n",
    "endpoint.delete()\n",
    "\n",
    "# Warning: Setting this to true deletes everything in your bucket\n",
    "delete_bucket = True\n",
    "\n",
    "if delete_bucket:\n",
    "    ! gsutil rm -r $BUCKET_URI"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "overview:custom",
    "objective:custom,training,online_prediction",
    "dataset:custom,cifar10,icn",
    "costs",
    "7c163842eabd",
    "accelerators:training,prediction",
    "container:training,prediction",
    "machine:training,prediction",
    "59f24e7d2269",
    "5c7732822757",
    "train_custom_model",
    "train_custom_job_args",
    "taskpy_contents",
    "train_custom_job",
    "deploy_model:dedicated",
    "make_prediction",
    "get_test_item:test",
    "send_prediction_request:image",
    "undeploy_model",
    "cleanup:custom"
   ],
   "name": "custom-tabular-bq-managed-dataset.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
