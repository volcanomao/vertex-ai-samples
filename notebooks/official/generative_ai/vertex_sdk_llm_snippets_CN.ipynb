{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7d9bbf86da5e"
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9a8301bf64d4"
   },
   "source": [
    "使用Vertex AI SDK与大型语言模型\n",
    "\n",
    "<table align=\"left\">\n",
    "\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/generative_ai/vertex_sdk_llm_snippets.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> 在Colab中打开\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fgithub.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fblob%2Fmain%2Fnotebooks%2Fofficial%2Fgenerative_ai%2Fvertex_sdk_llm_snippets.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> 在Colab Enterprise中打开\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/generative_ai/vertex_sdk_llm_snippets.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> 在Workbench中打开\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/generative_ai/vertex_sdk_llm_snippets.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> 在GitHub上查看\n",
    "    </a>\n",
    "  </td>                                                                                               \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a5d108e0f262"
   },
   "source": [
    "## 概述\n",
    "\n",
    "本教程演示了如何使用Vertex AI SDK通过PaLM API在Vertex AI上运行大型语言模型。您将找到用于测试、调整和部署生成式AI语言模型的示例代码。通过探索内容摘要、情感分析、对话以及文本嵌入和提示调整的示例来开始使用。\n",
    "\n",
    "了解更多关于[PaLM API](https://ai.google/discover/palm2/)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "825a6b6ffb84"
   },
   "source": [
    "### 目标\n",
    "\n",
    "在本教程中，您将学习如何提供文本输入给在Vertex AI上可用的大型语言模型（LLMs），以测试、调整和部署生成式人工智能语言模型。\n",
    "\n",
    "本教程使用以下谷歌云ML服务和资源：\n",
    "\n",
    "- Vertex AI PaLM API\n",
    "- Vertex AI SDK\n",
    "\n",
    "执行的步骤包括：\n",
    "\n",
    "- 使用Vertex AI PaLM API的预测端点接收生成式人工智能响应消息。\n",
    "- 使用文本嵌入终端接收消息的向量表示。\n",
    "- 根据输入/输出训练数据执行LLM的提示调整。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "946810630936"
   },
   "source": [
    "### 成本\n",
    "\n",
    "本教程使用了谷歌云的付费组件：\n",
    "\n",
    "* Vertex AI\n",
    "\n",
    "了解有关[Vertex AI 定价](https://cloud.google.com/vertex-ai/pricing)的信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8925ff9e165e"
   },
   "source": [
    "开始入门"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e150bf471f1b"
   },
   "source": [
    "### 安装 Vertex AI SDK 和其他必要的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a85627de3636"
   },
   "outputs": [],
   "source": [
    "# Install the packages\n",
    "! pip3 install --upgrade --quiet google-cloud-aiplatform \"shapely<2.0.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d98bc9fdd80d"
   },
   "source": [
    "重新启动运行时（仅适用于Colab）\n",
    "\n",
    "要使用新安装的包，您必须重新启动Google Colab上的运行时。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0dbf29389c65"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7b49231643e4"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ 内核将重新启动。请等待直到完成再继续下一步。⚠️</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7176ea64999b"
   },
   "source": [
    "验证您在谷歌Colab上的笔记本环境。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7de6ef0fac42"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e487ec618b5e"
   },
   "source": [
    "### 设置 Google Cloud 项目信息并初始化 Vertex AI SDK\n",
    "\n",
    "要开始使用 Vertex AI，您必须拥有现有的 Google Cloud 项目并[启用 Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com)。了解更多关于[设置项目和开发环境](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)的信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e61aaa036444"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type: \"string\"}\n",
    "\n",
    "import vertexai\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "acedaa0065d1"
   },
   "source": [
    "### 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c9865d6d81fa"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from vertexai.preview.language_models import (ChatModel, InputOutputTextPair,\n",
    "                                              TextEmbeddingModel,\n",
    "                                              TextGenerationModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mOabLACbseoE"
   },
   "source": [
    "### 摘要示例：文本摘要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XmnFXfnhJ9K-"
   },
   "outputs": [],
   "source": [
    "model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
    "response = model.predict(\n",
    "    \"\"\"Provide a summary with about two sentences for the following article:\n",
    "The efficient-market hypothesis (EMH) is a hypothesis in financial \\\n",
    "economics that states that asset prices reflect all available \\\n",
    "information. A direct implication is that it is impossible to \\\n",
    "\"beat the market\" consistently on a risk-adjusted basis since market \\\n",
    "prices should only react to new information. Because the EMH is \\\n",
    "formulated in terms of risk adjustment, it only makes testable \\\n",
    "predictions when coupled with a particular model of risk. As a \\\n",
    "result, research in financial economics since at least the 1990s has \\\n",
    "focused on market anomalies, that is, deviations from specific \\\n",
    "models of risk. The idea that financial market returns are difficult \\\n",
    "to predict goes back to Bachelier, Mandelbrot, and Samuelson, but \\\n",
    "is closely associated with Eugene Fama, in part due to his \\\n",
    "influential 1970 review of the theoretical and empirical research. \\\n",
    "The EMH provides the basic logic for modern risk-based theories of \\\n",
    "asset prices, and frameworks such as consumption-based asset pricing \\\n",
    "and intermediary asset pricing can be thought of as the combination \\\n",
    "of a model of risk with the EMH. Many decades of empirical research \\\n",
    "on return predictability has found mixed evidence. Research in the \\\n",
    "1950s and 1960s often found a lack of predictability (e.g. Ball and \\\n",
    "Brown 1968; Fama, Fisher, Jensen, and Roll 1969), yet the \\\n",
    "1980s-2000s saw an explosion of discovered return predictors (e.g. \\\n",
    "Rosenberg, Reid, and Lanstein 1985; Campbell and Shiller 1988; \\\n",
    "Jegadeesh and Titman 1993). Since the 2010s, studies have often \\\n",
    "found that return predictability has become more elusive, as \\\n",
    "predictability fails to work out-of-sample (Goyal and Welch 2008), \\\n",
    "or has been weakened by advances in trading technology and investor \\\n",
    "learning (Chordia, Subrahmanyam, and Tong 2014; McLean and Pontiff \\\n",
    "2016; Martineau 2021).\n",
    "Summary:\n",
    "\"\"\",\n",
    "    temperature=0.2,\n",
    "    max_output_tokens=256,\n",
    "    top_k=40,\n",
    "    top_p=0.95,\n",
    ")\n",
    "print(f\"Response from Model: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdxJX2dNE7t3"
   },
   "source": [
    "### 分类示例：分类标题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q2fw1_3H-tjX"
   },
   "outputs": [],
   "source": [
    "model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
    "response = model.predict(\n",
    "    \"\"\"What is the topic for a given news headline?\n",
    "- business\n",
    "- entertainment\n",
    "- health\n",
    "- sports\n",
    "- technology\n",
    "\n",
    "Text: Pixel 7 Pro Expert Hands On Review, the Most Helpful Google Phones.\n",
    "The answer is: technology\n",
    "\n",
    "Text: Quit smoking?\n",
    "The answer is: health\n",
    "\n",
    "Text: Roger Federer reveals why he touched Rafael Nadals hand while they were crying\n",
    "The answer is: sports\n",
    "\n",
    "Text: Business relief from Arizona minimum-wage hike looking more remote\n",
    "The answer is: business\n",
    "\n",
    "Text: #TomCruise has arrived in Bari, Italy for #MissionImpossible.\n",
    "The answer is: entertainment\n",
    "\n",
    "Text: CNBC Reports Rising Digital Profit as Print Advertising Falls\n",
    "The answer is:\n",
    "\"\"\",\n",
    "    temperature=0.2,\n",
    "    max_output_tokens=5,\n",
    "    top_k=1,\n",
    "    top_p=0,\n",
    ")\n",
    "print(f\"Response from Model: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKsuhKli70q-"
   },
   "source": [
    "### 分类示例：情感分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ipqROPufuhDr"
   },
   "outputs": [],
   "source": [
    "model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
    "response = model.predict(\n",
    "    \"\"\"I had to compare two versions of Hamlet for my Shakespeare class and \\\n",
    "unfortunately I picked this version. Everything from the acting (the actors \\\n",
    "deliver most of their lines directly to the camera) to the camera shots (all \\\n",
    "medium or close up shots...no scenery shots and very little back ground in the \\\n",
    "shots) were absolutely terrible. I watched this over my spring break and it is \\\n",
    "very safe to say that I feel that I was gypped out of 114 minutes of my \\\n",
    "vacation. Not recommended by any stretch of the imagination.\n",
    "Classify the sentiment of the message: negative\n",
    "\n",
    "Something surprised me about this movie - it was actually original. It was not \\\n",
    "the same old recycled crap that comes out of Hollywood every month. I saw this \\\n",
    "movie on video because I did not even know about it before I saw it at my \\\n",
    "local video store. If you see this movie available - rent it - you will not \\\n",
    "regret it.\n",
    "Classify the sentiment of the message: positive\n",
    "\n",
    "My family has watched Arthur Bach stumble and stammer since the movie first \\\n",
    "came out. We have most lines memorized. I watched it two weeks ago and still \\\n",
    "get tickled at the simple humor and view-at-life that Dudley Moore portrays. \\\n",
    "Liza Minelli did a wonderful job as the side kick - though I\\'m not her \\\n",
    "biggest fan. This movie makes me just enjoy watching movies. My favorite scene \\\n",
    "is when Arthur is visiting his fiancée\\'s house. His conversation with the \\\n",
    "butler and Susan\\'s father is side-spitting. The line from the butler, \\\n",
    "\"Would you care to wait in the Library\" followed by Arthur\\'s reply, \\\n",
    "\"Yes I would, the bathroom is out of the question\", is my NEWMAIL \\\n",
    "notification on my computer.\n",
    "Classify the sentiment of the message: positive\n",
    "\n",
    "This Charles outing is decent but this is a pretty low-key performance. Marlon \\\n",
    "Brando stands out. There\\'s a subplot with Mira Sorvino and Donald Sutherland \\\n",
    "that forgets to develop and it hurts the film a little. I\\'m still trying to \\\n",
    "figure out why Charlie want to change his name.\n",
    "Classify the sentiment of the message: negative\n",
    "\n",
    "Tweet: The Pixel 7 Pro, is too big to fit in my jeans pocket, so I bought \\\n",
    "new jeans.\n",
    "Classify the sentiment of the message: \"\"\",\n",
    "    max_output_tokens=5,\n",
    "    temperature=0.2,\n",
    "    top_k=1,\n",
    "    top_p=0,\n",
    ")\n",
    "print(f\"Response from Model: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dh0tLRggE5H1"
   },
   "source": [
    "提取例子：提取式问答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MPqZ38QDoBeh"
   },
   "outputs": [],
   "source": [
    "model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
    "response = model.predict(\n",
    "    \"\"\"Background: There is evidence that there have been significant changes \\\n",
    "in Amazon rainforest vegetation over the last 21,000 years through the Last \\\n",
    "Glacial Maximum (LGM) and subsequent deglaciation. Analyses of sediment \\\n",
    "deposits from Amazon basin paleo lakes and from the Amazon Fan indicate that \\\n",
    "rainfall in the basin during the LGM was lower than for the present, and this \\\n",
    "was almost certainly associated with reduced moist tropical vegetation cover \\\n",
    "in the basin. There is debate, however, over how extensive this reduction \\\n",
    "was. Some scientists argue that the rainforest was reduced to small, isolated \\\n",
    "refugia separated by open forest and grassland; other scientists argue that \\\n",
    "the rainforest remained largely intact but extended less far to the north, \\\n",
    "south, and east than is seen today. This debate has proved difficult to \\\n",
    "resolve because the practical limitations of working in the rainforest mean \\\n",
    "that data sampling is biased away from the center of the Amazon basin, and \\\n",
    "both explanations are reasonably well supported by the available data.\n",
    "\n",
    "Q: What does LGM stands for?\n",
    "A: Last Glacial Maximum.\n",
    "\n",
    "Q: What did the analysis from the sediment deposits indicate?\n",
    "A: Rainfall in the basin during the LGM was lower than for the present.\n",
    "\n",
    "Q: What are some of scientists arguments?\n",
    "A: The rainforest was reduced to small, isolated refugia separated by open forest and grassland.\n",
    "\n",
    "Q: There have been major changes in Amazon rainforest vegetation over the last how many years?\n",
    "A: 21,000.\n",
    "\n",
    "Q: What caused changes in the Amazon rainforest vegetation?\n",
    "A: The Last Glacial Maximum (LGM) and subsequent deglaciation\n",
    "\n",
    "Q: What has been analyzed to compare Amazon rainfall in the past and present?\n",
    "A: Sediment deposits.\n",
    "\n",
    "Q: What has the lower rainfall in the Amazon during the LGM been attributed to?\n",
    "A:\"\"\",\n",
    "    temperature=0.2,\n",
    "    max_output_tokens=256,\n",
    "    top_k=1,\n",
    "    top_p=0,\n",
    ")\n",
    "print(f\"Response from Model: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vV08j9H-Rbr7"
   },
   "source": [
    "###构思示例：面试问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ofBsWYxt_cl3"
   },
   "outputs": [],
   "source": [
    "model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
    "response = model.predict(\n",
    "    \"Give me ten interview questions for the role of program manager.\",\n",
    "    temperature=0.2,\n",
    "    max_output_tokens=256,\n",
    "    top_k=40,\n",
    "    top_p=0.8,\n",
    ")\n",
    "print(f\"Response from Model: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tpeXOD_bsCAX"
   },
   "source": [
    "### 聊天示例：科学辅导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1my0ebNpsKGT"
   },
   "outputs": [],
   "source": [
    "chat_model = ChatModel.from_pretrained(\"chat-bison@001\")\n",
    "parameters = {\n",
    "    \"temperature\": 0.2,\n",
    "    \"max_output_tokens\": 256,\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": 40,\n",
    "}\n",
    "\n",
    "chat = chat_model.start_chat(\n",
    "    context=\"My name is Miles. You are an astronomer, knowledgeable about the solar system.\",\n",
    "    examples=[\n",
    "        InputOutputTextPair(\n",
    "            input_text=\"How many moons does Mars have?\",\n",
    "            output_text=\"The planet Mars has two moons, Phobos and Deimos.\",\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "response = chat.send_message(\n",
    "    \"How many planets are there in the solar system?\", **parameters\n",
    ")\n",
    "response = chat.send_message(\n",
    "    \"When I learned about the planets in school, there were nine. When did that change?\",\n",
    "    **parameters,\n",
    ")\n",
    "response = chat.send_message(\n",
    "    \"Does Pluto have any moons? What about other dwarf planets?\", **parameters\n",
    ")\n",
    "response = chat.send_message(\"Who chose all of these cool names?!\", **parameters)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ag5Og6Y3xfvM"
   },
   "source": [
    "文本嵌入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sNstE8FOxmm2"
   },
   "outputs": [],
   "source": [
    "model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@001\")\n",
    "embeddings = model.get_embeddings([\"What is life?\"])\n",
    "for embedding in embeddings:\n",
    "    vector = embedding.values\n",
    "    print(f\"Length of Embedding Vector: {len(vector)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CI-dpBTr6LGH"
   },
   "source": [
    "### 列出调整过的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U-MuqTjTA0zk"
   },
   "outputs": [],
   "source": [
    "model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
    "tuned_model_names = model.list_tuned_model_names()\n",
    "print(tuned_model_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gYvbJ9PrFF2g"
   },
   "source": [
    "调整一个模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UoEVAfGCFOnT"
   },
   "outputs": [],
   "source": [
    "def tuning(\n",
    "    project_id: str,\n",
    "    location: str,\n",
    "    model_display_name: str,\n",
    "    training_data: pd.DataFrame,\n",
    "    train_steps: int = 10,\n",
    ") -> TextGenerationModel:\n",
    "    \"\"\"Tune a new model, based on a prompt-response data.\n",
    "\n",
    "    \"training_data\" can be either the GCS URI of a file formatted in JSONL format\n",
    "    (for example: training_data=f'gs://{bucket}/{filename}.jsonl'), or a pandas\n",
    "    DataFrame. Each training example should be JSONL record with two keys, for\n",
    "    example:\n",
    "      {\n",
    "        \"input_text\": <input prompt>,\n",
    "        \"output_text\": <associated output>\n",
    "      },\n",
    "    or the pandas DataFame should contain two columns:\n",
    "      ['input_text', 'output_text']\n",
    "    with rows for each training example.\n",
    "\n",
    "    Args:\n",
    "      project_id: GCP Project ID, used to initialize vertexai\n",
    "      location: GCP Region, used to initialize vertexai\n",
    "      model_display_name: Customized Tuned LLM model name.\n",
    "      training_data: GCS URI of jsonl file or pandas dataframe of training data\n",
    "      train_steps: Number of training steps to use when tuning the model.\n",
    "    \"\"\"\n",
    "    vertexai.init(project=project_id, location=location)\n",
    "    model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
    "\n",
    "    model.tune_model(\n",
    "        training_data=training_data,\n",
    "        # Optional:\n",
    "        model_display_name=model_display_name,\n",
    "        train_steps=train_steps,\n",
    "        tuning_job_location=\"europe-west4\",\n",
    "        tuned_model_location=location,\n",
    "    )\n",
    "\n",
    "    print(model._job.status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "acc05a4e4fe5"
   },
   "source": [
    "清理工作\n",
    "\n",
    "要清理此项目中使用的所有谷歌云资源，可以[删除您用于教程的谷歌云项目](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects)。"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "vertex_sdk_llm_snippets.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
