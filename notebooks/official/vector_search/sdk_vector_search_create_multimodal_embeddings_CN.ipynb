{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "使用Vertex AI多模态嵌入和向量搜索\n",
    "![ ](https://www.google-analytics.com/collect?v=2&tid=G-L6X3ECH596&cid=1&en=page_view&sid=1&dt=sdk_vector_search_create_multimodal_embeddings.ipynb&dl=notebooks%2Fofficial%2Fvector_search%2Fsdk_vector_search_create_multimodal_embeddings.ipynb)\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/vector_search/sdk_vector_search_create_multimodal_embeddings.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"><br> 在Colab中运行\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fofficial%2fvector_search%2fsdk_vector_search_create_multimodal_embeddings.ipynb.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://cloud.google.com/ml-engine/images/colab-enterprise-logo-32px.png\" alt=\"Google Cloud Colab Enterprise logo\"><br> 在Colab Enterprise中打开\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/vector_search/sdk_vector_search_create_multimodal_embeddings.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br>\n",
    "      在Vertex AI Workbench中打开\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/vector_search/sdk_vector_search_create_multimodal_embeddings.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br>\n",
    "      在GitHub上查看\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b0a74aaf1481"
   },
   "source": [
    "## 概览\n",
    "\n",
    "该示例演示了如何使用DiffusionDB数据集和Vertex AI多模态嵌入模型创建文本到图像的嵌入。这些嵌入被上传到Vertex AI矢量搜索服务，这是一个高规模、低延迟的解决方案，可用于在大型语料库中查找相似向量。此外，这是一个完全托管的服务，进一步降低了运营开销。它是构建在由谷歌研究开发的[近似最近邻（ANN）技术](https://ai.googleblog.com/2020/07/announcing-scann-efficient-vector.html)的基础上。\n",
    "\n",
    "要了解更多关于[Vertex AI多模态嵌入](https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-multimodal-embeddings#supported_models)，以及[Vertex AI矢量搜索](https://cloud.google.com/vertex-ai/docs/vector-search/overview)的信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34a4b245e795"
   },
   "source": [
    "### 目标\n",
    "\n",
    "在这个笔记本中，您将学习如何编码自定义文本嵌入，创建一个近似最近邻（ANN）索引，并对索引进行查询。\n",
    "\n",
    "本教程使用以下 Google Cloud ML 服务：\n",
    "\n",
    "- Vertex AI 多模式嵌入\n",
    "- Vertex AI 矢量搜索\n",
    "\n",
    "执行的步骤包括：\n",
    "\n",
    "* 将图像数据集转换为嵌入。\n",
    "* 创建一个索引。\n",
    "* 将嵌入上传到索引。\n",
    "* 创建一个索引端点。\n",
    "* 将索引部署到索引端点。\n",
    "* 执行在线查询。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "### 数据集\n",
    "\n",
    "本教程使用的数据集是[DiffusionDB数据集](https://github.com/poloclub/diffusiondb)。\n",
    "\n",
    "> DiffusionDB是第一个大规模的文本到图像提示数据集。它包含通过Stable Diffusion使用真实用户指定的提示和超参数生成的1400万张图片。这个前所未有的规模和多样性的由人类操控的数据集为理解提示和生成模型之间的相互作用、检测深度伪造，以及设计人机互动工具来帮助用户更轻松地使用这些模型提供了令人兴奋的研究机会。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0f1bea346db"
   },
   "source": [
    "开始吧"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0a51c572aa72"
   },
   "source": [
    "安装Python的Vertex AI SDK和其他必需的程序包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dfbccc635a17"
   },
   "outputs": [],
   "source": [
    "# Install the packages\n",
    "! pip3 install --upgrade google-cloud-aiplatform \\\n",
    "                         google-cloud-storage \\\n",
    "                         google-cloud-vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97473593f37f"
   },
   "source": [
    "重新启动运行时（仅适用于Colab）\n",
    "\n",
    "为了使用新安装的包，您必须在Google Colab上重新启动运行时。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5709d1d57927"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ffcde4d56c00"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️内核即将重新启动。在继续下一步之前，请等待直到完成。⚠️</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7176ea64999b"
   },
   "source": [
    "### 验证您的笔记本环境（仅限Colab）\n",
    "\n",
    "在Google Colab上验证您的环境。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7de6ef0fac42"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ef3990d0482a"
   },
   "source": [
    "### 设置Google Cloud项目信息并为Python初始化Vertex AI SDK\n",
    "\n",
    "要开始使用Vertex AI，您必须拥有现有的Google Cloud项目并[启用Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com)。了解更多关于[设置项目和开发环境](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)的信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "80c0215f05a0"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "创建一个存储桶来存储中间产物，如数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "BUCKET_URI = f\"gs://your-bucket-name-{PROJECT_ID}-unique\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EcIXiGsCePi"
   },
   "source": [
    "如果您的存储桶尚不存在：运行以下单元格以创建您的云存储桶。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIq7R4HZCfIc"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l {LOCATION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lR6Wwv-hCCN-"
   },
   "source": [
    "准备数据\n",
    "\n",
    "使用[DiffusionDB数据集](https://github.com/poloclub/diffusiondb)中的图像提示和图像配对。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9e2a05095082"
   },
   "source": [
    "克隆DiffusionDB存储库###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9wzS85TeB9dG"
   },
   "outputs": [],
   "source": [
    "! git clone https://github.com/poloclub/diffusiondb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "49ee8d2c2df7"
   },
   "source": [
    "### 安装下载数据集所需的依赖项"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ed67efa3c0f3"
   },
   "outputs": [],
   "source": [
    "! pip install -r diffusiondb/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0bc039db70e"
   },
   "source": [
    "下载图像文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7d27d986c61f"
   },
   "outputs": [],
   "source": [
    "# Download image files from 1 to 5. Each file is 1000 images.\n",
    "! python diffusiondb/scripts/download.py -i 1 -r 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a514786370e5"
   },
   "source": [
    "提取图像档案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b43937b6065d"
   },
   "outputs": [],
   "source": [
    "# Unzip all image files\n",
    "image_directory = \"extracted\"\n",
    "\n",
    "! unzip -n 'images/*.zip' -d '{image_directory}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "09a6602bb745"
   },
   "source": [
    "### 载入图像的元数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2cacd9869ee5"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "metadatas = {}\n",
    "for file_name in os.listdir(image_directory):\n",
    "    if file_name.endswith(\".json\"):\n",
    "        with open(os.path.join(image_directory, file_name)) as f:\n",
    "            metadata = json.load(f)\n",
    "            metadatas.update(metadata)\n",
    "\n",
    "image_names = list(metadatas.keys())\n",
    "image_paths = [os.path.join(image_directory, image_name) for image_name in image_names]\n",
    "\n",
    "len(metadatas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2a593ec69048"
   },
   "source": [
    "定义函数来检测显性图像\n",
    "\n",
    "定义一个函数，用于查询Cloud Vision API以检测可能的显性图像。\n",
    "\n",
    "了解更多关于内容检测（SafeSearch）的信息，请参阅[检测显性内容](https://cloud.google.com/vision/docs/detecting-safe-search)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ce31cc893826"
   },
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from google.cloud import vision\n",
    "from google.cloud.vision_v1.types.image_annotator import SafeSearchAnnotation\n",
    "\n",
    "client = vision.ImageAnnotatorClient()\n",
    "\n",
    "\n",
    "def detect_safe_search(path: str) -> Optional[SafeSearchAnnotation]:\n",
    "    \"\"\"Detects unsafe features in the file.\"\"\"\n",
    "\n",
    "    with open(path, \"rb\") as image_file:\n",
    "        content = image_file.read()\n",
    "\n",
    "    image = vision.Image(content=content)\n",
    "\n",
    "    response = client.safe_search_detection(image=image)\n",
    "\n",
    "    if response.error.message:\n",
    "        print(response.error.message)\n",
    "        return None\n",
    "\n",
    "    return response.safe_search_annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e330a70627ed"
   },
   "source": [
    "### 定义SafeSearch注释转换为布尔值\n",
    "定义一个函数，将SafeSearch注释结果转换为布尔值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fc3c32afb383"
   },
   "outputs": [],
   "source": [
    "from google.cloud.vision_v1.types.image_annotator import Likelihood\n",
    "\n",
    "\n",
    "# Returns true if some annotations have a potential safety issues\n",
    "def convert_annotation_to_safety(safe_search_annotation: SafeSearchAnnotation) -> bool:\n",
    "    return all(\n",
    "        [\n",
    "            (safe_level == Likelihood.VERY_UNLIKELY)\n",
    "            or (safe_level == Likelihood.UNLIKELY)\n",
    "            for safe_level in [\n",
    "                safe_search_annotation.adult,\n",
    "                safe_search_annotation.medical,\n",
    "                safe_search_annotation.violence,\n",
    "                safe_search_annotation.racy,\n",
    "            ]\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aded0a519b0c"
   },
   "source": [
    "### 执行速率限制的显式图像检测\n",
    "\n",
    "Cloud Vision对API请求有速率限制。\n",
    "\n",
    "使用速率限制器确保请求在此限制之下。\n",
    "为了更好的性能，可以使用线程池进行并行请求。这超出了本笔记本的范围。\n",
    "\n",
    "了解更多关于[配额和限制](https://cloud.google.com/vision/quotas?hl=zh-cn)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bc753c9264a9"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Create a rate limiter with a limit of 1800 requests per minute\n",
    "seconds_per_job = 1 / (1800 / 60)\n",
    "\n",
    "\n",
    "def process_image(image_path: str) -> Optional[bool]:\n",
    "    try:\n",
    "        annotation = detect_safe_search(image_path)\n",
    "        if annotation:\n",
    "            return convert_annotation_to_safety(safe_search_annotation=annotation)\n",
    "        else:\n",
    "            return None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Process images using ThreadPool\n",
    "is_safe_values_cloud_vision = []\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    futures = []\n",
    "    for img_url in tqdm(image_paths, total=len(image_paths), position=0):\n",
    "        futures.append(executor.submit(process_image, img_url))\n",
    "        time.sleep(seconds_per_job)\n",
    "\n",
    "    for future in futures:\n",
    "        is_safe_values_cloud_vision.append(future.result())\n",
    "# Set Nones to False\n",
    "is_safe_values_cloud_vision = [\n",
    "    is_safe or False for is_safe in is_safe_values_cloud_vision\n",
    "]\n",
    "\n",
    "# Print number of safe images found\n",
    "print(\n",
    "    f\"Safe images = {np.array(is_safe_values_cloud_vision).sum()} out of {len(is_safe_values_cloud_vision)} images\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "00c8234f9828"
   },
   "outputs": [],
   "source": [
    "# Filter images by safety\n",
    "metadatas = [\n",
    "    metadata\n",
    "    for metadata, is_safe in zip(metadatas, is_safe_values_cloud_vision)\n",
    "    if is_safe\n",
    "]\n",
    "image_names = [\n",
    "    image_name\n",
    "    for image_name, is_safe in zip(image_names, is_safe_values_cloud_vision)\n",
    "    if is_safe\n",
    "]\n",
    "image_paths = [\n",
    "    image_path\n",
    "    for image_path, is_safe in zip(image_paths, is_safe_values_cloud_vision)\n",
    "    if is_safe\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3eab0cbaef92"
   },
   "source": [
    "定义编码函数\n",
    "\n",
    "创建一个EmbeddingPredictionClient，封装调用嵌入API的逻辑。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ec925af6f502"
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import time\n",
    "import typing\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google.protobuf import struct_pb2\n",
    "\n",
    "\n",
    "class EmbeddingResponse(typing.NamedTuple):\n",
    "    text_embedding: typing.Sequence[float]\n",
    "    image_embedding: typing.Sequence[float]\n",
    "\n",
    "\n",
    "def load_image_bytes(image_uri: str) -> bytes:\n",
    "    \"\"\"Load image bytes from a remote or local URI.\"\"\"\n",
    "    image_bytes = None\n",
    "    if image_uri.startswith(\"http://\") or image_uri.startswith(\"https://\"):\n",
    "        response = requests.get(image_uri, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            image_bytes = response.content\n",
    "    else:\n",
    "        image_bytes = open(image_uri, \"rb\").read()\n",
    "    return image_bytes\n",
    "\n",
    "\n",
    "class EmbeddingPredictionClient:\n",
    "    \"\"\"Wrapper around Prediction Service Client.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        project: str,\n",
    "        location: str = \"us-central1\",\n",
    "        api_regional_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\n",
    "    ):\n",
    "        client_options = {\"api_endpoint\": api_regional_endpoint}\n",
    "        # Initialize client which is used to create and send requests.\n",
    "        # This client only needs to be created once, and can be reused for multiple requests.\n",
    "        self.client = aiplatform.gapic.PredictionServiceClient(\n",
    "            client_options=client_options\n",
    "        )\n",
    "        self.location = location\n",
    "        self.project = project\n",
    "\n",
    "    def get_embedding(self, text: str = None, image_file: str = None):\n",
    "        if not text and not image_file:\n",
    "            raise ValueError(\"At least one of text or image_file must be specified.\")\n",
    "\n",
    "        # Load image file\n",
    "        image_bytes = None\n",
    "        if image_file:\n",
    "            image_bytes = load_image_bytes(image_file)\n",
    "\n",
    "        instance = struct_pb2.Struct()\n",
    "        if text:\n",
    "            instance.fields[\"text\"].string_value = text\n",
    "\n",
    "        if image_bytes:\n",
    "            encoded_content = base64.b64encode(image_bytes).decode(\"utf-8\")\n",
    "            image_struct = instance.fields[\"image\"].struct_value\n",
    "            image_struct.fields[\"bytesBase64Encoded\"].string_value = encoded_content\n",
    "\n",
    "        instances = [instance]\n",
    "        endpoint = (\n",
    "            f\"projects/{self.project}/locations/{self.location}\"\n",
    "            \"/publishers/google/models/multimodalembedding@001\"\n",
    "        )\n",
    "        response = self.client.predict(endpoint=endpoint, instances=instances)\n",
    "\n",
    "        text_embedding = None\n",
    "        if text:\n",
    "            text_emb_value = response.predictions[0][\"textEmbedding\"]\n",
    "            text_embedding = [v for v in text_emb_value]\n",
    "\n",
    "        image_embedding = None\n",
    "        if image_bytes:\n",
    "            image_emb_value = response.predictions[0][\"imageEmbedding\"]\n",
    "            image_embedding = [v for v in image_emb_value]\n",
    "\n",
    "        return EmbeddingResponse(\n",
    "            text_embedding=text_embedding, image_embedding=image_embedding\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43adc0eefd50"
   },
   "source": [
    "#### 创建用于批处理数据的辅助函数\n",
    "\n",
    "由于数据集可能很大，建议您使用生成器一次将一个批次的数据加载到内存中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d2b9cda3fe9f"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import Callable, Generator, List\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def generate_batches(\n",
    "    inputs: List[str], batch_size: int\n",
    ") -> Generator[List[str], None, None]:\n",
    "    \"\"\"\n",
    "    Generator function that takes a list of strings and a batch size, and yields batches of the specified size.\n",
    "    \"\"\"\n",
    "\n",
    "    for i in range(0, len(inputs), batch_size):\n",
    "        yield inputs[i : i + batch_size]\n",
    "\n",
    "\n",
    "API_IMAGES_PER_SECOND = 2\n",
    "\n",
    "\n",
    "def encode_to_embeddings_chunked(\n",
    "    process_function: Callable[[List[str]], List[Optional[List[float]]]],\n",
    "    items: List[str],\n",
    "    batch_size: int = 1,\n",
    ") -> List[Optional[List[float]]]:\n",
    "    \"\"\"\n",
    "    Function that encodes a list of strings into embeddings using a process function.\n",
    "    It takes a list of strings and returns a list of optional lists of floats.\n",
    "    The data is processed in chunks to prevent out-of-memory errors.\n",
    "    \"\"\"\n",
    "\n",
    "    embeddings_list: List[Optional[List[float]]] = []\n",
    "\n",
    "    # Prepare the batches using a generator\n",
    "    batches = generate_batches(items, batch_size)\n",
    "\n",
    "    seconds_per_job = batch_size / API_IMAGES_PER_SECOND\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for batch in tqdm(batches, total=len(items) // batch_size, position=0):\n",
    "            futures.append(executor.submit(process_function, batch))\n",
    "            time.sleep(seconds_per_job)\n",
    "\n",
    "        for future in futures:\n",
    "            embeddings_list.extend(future.result())\n",
    "    return embeddings_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dd58b70d87f8"
   },
   "source": [
    "创建函数，将嵌入函数包装在try-except和重试逻辑中。\n",
    "\n",
    "这个特定的嵌入模型一次只能处理一张图片，所以输入被验证为长度为1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ae5933cb5a51"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "from typing import List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "from tenacity import retry, stop_after_attempt\n",
    "\n",
    "client = EmbeddingPredictionClient(project=PROJECT_ID)\n",
    "\n",
    "\n",
    "# Use a retry handler in case of failure\n",
    "@retry(reraise=True, stop=stop_after_attempt(3))\n",
    "def encode_texts_to_embeddings_with_retry(text: List[str]) -> List[List[float]]:\n",
    "    assert len(text) == 1\n",
    "\n",
    "    try:\n",
    "        return [client.get_embedding(text=text[0], image_file=None).text_embedding]\n",
    "    except Exception:\n",
    "        raise RuntimeError(\"Error getting embedding.\")\n",
    "\n",
    "\n",
    "def encode_texts_to_embeddings(text: List[str]) -> List[Optional[List[float]]]:\n",
    "    try:\n",
    "        return encode_texts_to_embeddings_with_retry(text=text)\n",
    "    except Exception:\n",
    "        return [None for _ in range(len(text))]\n",
    "\n",
    "\n",
    "@retry(reraise=True, stop=stop_after_attempt(3))\n",
    "def encode_images_to_embeddings_with_retry(image_uris: List[str]) -> List[List[float]]:\n",
    "    assert len(image_uris) == 1\n",
    "\n",
    "    try:\n",
    "        return [\n",
    "            client.get_embedding(text=None, image_file=image_uris[0]).image_embedding\n",
    "        ]\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        raise RuntimeError(\"Error getting embedding.\")\n",
    "\n",
    "\n",
    "def encode_images_to_embeddings(image_uris: List[str]) -> List[Optional[List[float]]]:\n",
    "    try:\n",
    "        return encode_images_to_embeddings_with_retry(image_uris=image_uris)\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        return [None for _ in range(len(image_uris))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0f18ecd0b068"
   },
   "source": [
    "测试编码功能\n",
    "\n",
    "对数据的一个子集进行编码，并查看嵌入和距离度量是否合理。\n",
    "\n",
    "由于没有公开的说明嵌入模型的论文，假设嵌入是使用余弦相似性作为损失函数训练的，因为这是相当常见的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "42f869fd66f5"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Encode a sample subset of images\n",
    "image_paths_filtered = list(image_paths)[:1000]\n",
    "image_embeddings = encode_to_embeddings_chunked(\n",
    "    process_function=encode_images_to_embeddings, items=image_paths_filtered\n",
    ")\n",
    "\n",
    "# Keep only non-None embeddings\n",
    "indexes_to_keep, image_embeddings = zip(\n",
    "    *[\n",
    "        (index, embedding)\n",
    "        for index, embedding in enumerate(image_embeddings)\n",
    "        if embedding is not None\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Processed {len(indexes_to_keep)} embeddings successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "25f78712545e"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def dot_product_distance(\n",
    "    text_embedding: np.ndarray, image_embeddings: np.ndarray\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Compute dot-product distance between text and image embeddings by taking the dot product\"\"\"\n",
    "    return np.dot(text_embedding, image_embeddings.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2595277c886e"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from io import BytesIO\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "text_query = \"Birds in flight\"\n",
    "\n",
    "# Calculate text embedding of query\n",
    "text_embedding = encode_texts_to_embeddings(text=[text_query])[0]\n",
    "\n",
    "try:\n",
    "    print(type(text_embedding))\n",
    "    print(type(text_embedding[0]))\n",
    "\n",
    "    print(type(image_embeddings))\n",
    "    print(type(image_embeddings[0]))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# Calculate distance\n",
    "distances = dot_product_distance(\n",
    "    text_embedding=np.array(text_embedding), image_embeddings=np.array(image_embeddings)\n",
    ")\n",
    "\n",
    "# Set the maximum number of images to display\n",
    "MAX_IMAGES = 20\n",
    "\n",
    "# Sort images and scores by descending order of scores and select the top max_images\n",
    "sorted_data = sorted(\n",
    "    zip(image_paths_filtered, distances), key=lambda x: x[1], reverse=True\n",
    ")[:MAX_IMAGES]\n",
    "\n",
    "# Calculate the number of rows and columns needed to display the images\n",
    "num_cols = 4\n",
    "num_rows = math.ceil(len(sorted_data) / num_cols)\n",
    "\n",
    "\n",
    "# Create a grid of subplots to display the images\n",
    "fig, axs = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(10, 12))\n",
    "\n",
    "# Loop through the top max_images images and display them in the subplots\n",
    "for i, (image_path, distance) in enumerate(sorted_data):\n",
    "    # Calculate the row and column index for the current image\n",
    "    row_idx = i // num_cols\n",
    "    col_idx = i % num_cols\n",
    "\n",
    "    # Check if image_path is a remote URL\n",
    "    if image_path.startswith(\"http://\") or image_path.startswith(\"https://\"):\n",
    "        response = requests.get(image_path)\n",
    "        image = Image.open(BytesIO(response.content))\n",
    "    else:\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "    # Display the image in the current subplot\n",
    "    axs[row_idx, col_idx].imshow(image, cmap=\"gray\")\n",
    "\n",
    "    # Set the title of the subplot to the image index and score\n",
    "    axs[row_idx, col_idx].set_title(f\"Rank {i+1}, Distance = {distance:.2f}\")  # noqa\n",
    "\n",
    "    # Remove ticks from the subplot\n",
    "    axs[row_idx, col_idx].set_xticks([])\n",
    "    axs[row_idx, col_idx].set_yticks([])\n",
    "\n",
    "# Adjust the spacing between subplots and display the plot\n",
    "plt.subplots_adjust(hspace=0.3, wspace=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17f24794c9f0"
   },
   "source": [
    "在创建索引时，保存维度大小以供以后使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "55a128883028"
   },
   "outputs": [],
   "source": [
    "DIMENSIONS = len(text_embedding)\n",
    "\n",
    "print(DIMENSIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQIQSyF9GtSv"
   },
   "source": [
    "保存嵌入式数据以JSONL格式\n",
    "\n",
    "数据必须以JSONL格式进行格式化，这意味着每个嵌入字典被写成一个独立的JSON对象，占据一行。\n",
    "\n",
    "在文档中查看更多信息：[输入数据格式和结构](https://cloud.google.com/vertex-ai/docs/vector-search/setup/format-structure#data-file-formats)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "43aaff23416e"
   },
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "# Create temporary file to write embeddings to\n",
    "embeddings_file = tempfile.NamedTemporaryFile(suffix=\".json\", delete=False)\n",
    "\n",
    "embeddings_file.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "307f468a3ecd"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "BATCH_SIZE = 1000\n",
    "\n",
    "with open(embeddings_file.name, \"a\") as f:\n",
    "    for i in tqdm(range(0, len(image_names), BATCH_SIZE)):\n",
    "        image_names_chunk = image_names[i : i + BATCH_SIZE]\n",
    "        image_paths_chunk = image_paths[i : i + BATCH_SIZE]\n",
    "\n",
    "        embeddings = encode_to_embeddings_chunked(\n",
    "            process_function=encode_images_to_embeddings, items=image_paths_chunk\n",
    "        )\n",
    "\n",
    "        # Append to file\n",
    "        embeddings_formatted = [\n",
    "            json.dumps(\n",
    "                {\n",
    "                    \"id\": str(id),\n",
    "                    \"embedding\": [str(value) for value in embedding],\n",
    "                }\n",
    "            )\n",
    "            + \"\\n\"\n",
    "            for id, embedding in zip(image_names_chunk, embeddings)\n",
    "            if embedding is not None\n",
    "        ]\n",
    "        f.writelines(embeddings_formatted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QuVl8DrWG8NS"
   },
   "source": [
    "上传培训数据到云存储。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3PgsA_vbI8Vg"
   },
   "outputs": [],
   "source": [
    "UNIQUE_FOLDER_NAME = \"embeddings_folder_unique\"\n",
    "EMBEDDINGS_INITIAL_URI = f\"{BUCKET_URI}/{UNIQUE_FOLDER_NAME}/\"\n",
    "! gsutil cp {embeddings_file.name} {EMBEDDINGS_INITIAL_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mglUPwHpJH98"
   },
   "source": [
    "创建向量搜索索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qiIg9b5zJLi1"
   },
   "outputs": [],
   "source": [
    "DISPLAY_NAME = \"multimodal_diffusiondb\"\n",
    "DESCRIPTION = \"Multimodal DiffusionDB Embeddings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y4zooldkGoM4"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=LOCATION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fa5eb5cdb2f0"
   },
   "source": [
    "创建配置\n",
    "\n",
    "有关配置设置的信息，请参阅[管理索引文档](https://cloud.google.com/vertex-ai/docs/vector-search/create-manage-index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xzY7TpUSJcTV"
   },
   "outputs": [],
   "source": [
    "tree_ah_index = aiplatform.MatchingEngineIndex.create_tree_ah_index(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    contents_delta_uri=EMBEDDINGS_INITIAL_URI,\n",
    "    dimensions=DIMENSIONS,\n",
    "    approximate_neighbors_count=150,\n",
    "    distance_measure_type=\"COSINE_DISTANCE\",\n",
    "    leaf_node_embedding_count=500,\n",
    "    leaf_nodes_to_search_percent=7,\n",
    "    description=DESCRIPTION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "17jrQi501QyX"
   },
   "outputs": [],
   "source": [
    "INDEX_RESOURCE_NAME = tree_ah_index.resource_name\n",
    "\n",
    "print(INDEX_RESOURCE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0f1a9fbecabb"
   },
   "source": [
    "使用资源名称，您可以检索现有的索引资源。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ddb70647d98"
   },
   "outputs": [],
   "source": [
    "tree_ah_index = aiplatform.MatchingEngineIndex(index_name=INDEX_RESOURCE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qV2xjAnDDObD"
   },
   "source": [
    "## 使用VPC网络创建一个IndexEndpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BpZQoJyxDlbO"
   },
   "outputs": [],
   "source": [
    "# Retrieve the project number\n",
    "PROJECT_NUMBER = !gcloud projects list --filter=\"PROJECT_ID:'{PROJECT_ID}'\" --format='value(PROJECT_NUMBER)'\n",
    "PROJECT_NUMBER = PROJECT_NUMBER[0]\n",
    "\n",
    "VPC_NETWORK = \"[your-network-name]\"\n",
    "VPC_NETWORK_FULL = \"projects/{}/global/networks/{}\".format(PROJECT_NUMBER, VPC_NETWORK)\n",
    "VPC_NETWORK_FULL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qV2xjAnDDObD"
   },
   "source": [
    "创建一个向量搜索索引端点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QuARXzJVGyQX"
   },
   "outputs": [],
   "source": [
    "my_index_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    description=DISPLAY_NAME,\n",
    "    public_endpoint_enabled=False,\n",
    "    network=VPC_NETWORK_FULL,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "np2cgVuuIe9k"
   },
   "source": [
    "部署索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nLOYTGygIlMK"
   },
   "outputs": [],
   "source": [
    "DEPLOYED_INDEX_ID = \"deployed_index_id_unique\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_uK4WOgqN1NG"
   },
   "outputs": [],
   "source": [
    "my_index_endpoint = my_index_endpoint.deploy_index(\n",
    "    index=tree_ah_index, deployed_index_id=DEPLOYED_INDEX_ID\n",
    ")\n",
    "\n",
    "my_index_endpoint.deployed_indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6LCGvBNvBd8D"
   },
   "source": [
    "## 创建在线查询\n",
    "\n",
    "构建完索引之后，您可以查询已部署的索引以找到最近的邻居。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "48e4e417a496"
   },
   "outputs": [],
   "source": [
    "# Encode query\n",
    "text_embeddings = encode_texts_to_embeddings(text=[\"New York skyline\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A3KYVw5HB-4v"
   },
   "outputs": [],
   "source": [
    "# Define number of neighbors to return\n",
    "NUM_NEIGHBORS = 20\n",
    "\n",
    "response = my_index_endpoint.match(\n",
    "    deployed_index_id=DEPLOYED_INDEX_ID,\n",
    "    queries=text_embeddings,\n",
    "    num_neighbors=NUM_NEIGHBORS,\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1116c7c0d7d"
   },
   "source": [
    "绘制响应并验证图像是否与文本查询相匹配。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6f4ddb7c8d54"
   },
   "outputs": [],
   "source": [
    "# Sort images and scores by descending order of scores and select the top max_images\n",
    "sorted_data = sorted(response[0], key=lambda x: x.distance, reverse=True)\n",
    "\n",
    "# Create a grid of subplots to display the images\n",
    "fig, axs = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(10, 12))\n",
    "\n",
    "# Loop through the top max_images images and display them in the subplots\n",
    "for i, response in enumerate(sorted_data):\n",
    "    image_path = f\"{image_directory}/{response.id}\"\n",
    "    score = response.distance\n",
    "\n",
    "    # Calculate the row and column index for the current image\n",
    "    row_idx = i // num_cols\n",
    "    col_idx = i % num_cols\n",
    "\n",
    "    # Display the image in the current subplot\n",
    "    if os.path.exists(image_path):\n",
    "        image = copy.deepcopy(Image.open(image_path))\n",
    "        axs[row_idx, col_idx].imshow(image, cmap=\"gray\")\n",
    "\n",
    "        # Set the title of the subplot to the image index and score\n",
    "        axs[row_idx, col_idx].set_title(f\"Rank {i+1}, Score = {score:.2f}\")  # noqa\n",
    "\n",
    "        # Remove ticks from the subplot\n",
    "        axs[row_idx, col_idx].set_xticks([])\n",
    "        axs[row_idx, col_idx].set_yticks([])\n",
    "\n",
    "# Adjust the spacing between subplots and display the plot\n",
    "plt.subplots_adjust(hspace=0.3, wspace=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "清理工作\n",
    "\n",
    "要清理此项目中使用的所有Google Cloud资源，您可以[删除用于本教程的Google Cloud项目](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects)。\n",
    "您还可以通过运行以下代码手动删除您创建的资源。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sx_vKniMq9ZX"
   },
   "outputs": [],
   "source": [
    "# Force undeployment of indexes and delete endpoint\n",
    "my_index_endpoint.delete(force=True)\n",
    "\n",
    "# Delete indexes\n",
    "tree_ah_index.delete()\n",
    "\n",
    "# Delete Cloud Storage objects that were created\n",
    "delete_bucket = False\n",
    "if delete_bucket:\n",
    "    ! gsutil -m rm -r $BUCKET_URI"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "sdk_vector_search_create_multimodal_embeddings.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
