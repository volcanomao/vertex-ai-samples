{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "18ebbd838e32"
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mThXALJl9Yue"
   },
   "source": [
    "# 用于预测的表格化工作流程\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/automl/automl_tabular_on_vertex_pipelines.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> 在 Colab 中运行\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/automl/automl_forecasting_on_vertex_pipelines.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      在 GitHub 上查看\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/automl/automl_forecasting_on_vertex_pipelines.ipynb\">\n",
    "        <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      在 Vertex AI Workbench 中打开\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "<br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "962e636b5cee"
   },
   "source": [
    "注意：此笔记本已在以下环境中进行测试：\n",
    "\n",
    "- Python 版本 = 3.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fcc745968395"
   },
   "source": [
    "## 概览\n",
    "\n",
    "本教程演示了您如何使用Vertex AI表格工作流进行预测，训练一个AutoML模型。您可以选择以下模型类型：时间序列密集编码器（TiDE），学习到学习（L2L），序列到序列（Seq2Seq+）和时间融合变换器（TFT）。\n",
    "\n",
    "了解更多关于[Tabular Workflow for Forecasting](https://cloud.google.com/vertex-ai/docs/tabular-data/tabular-workflows/forecasting)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8b54ba90629a"
   },
   "source": [
    "与Vertex预测托管服务相比，预测的表格工作流具有以下优势：\n",
    "1. 支持复合时间序列 id 列。您可以将多个列的组合用作时间序列 id，例如，您可以将`['sku_id']`或`['sku_id', 'store_id']`作为时间序列 id 列。\n",
    "2. 可跳过模型架构搜索。您可以重用先前的模型架构搜索调整结果直接训练模型。\n",
    "3. 硬件定制。您可以覆盖调整和训练步骤的机器规格，以调整训练速度。您还可以控制培训过程的并行度和集成步骤中最终选择试验的数量。\n",
    "4. 一个单一时间序列中支持无限的时间步长。培训数据集中没有3000个时间步长限制。\n",
    "5. 培训数据集没有上限。数据集大小没有100MM行限制或100GB限制。\n",
    "6. 使用 Vertex AI 管道中的所有高级功能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f887ec5c06c5"
   },
   "source": [
    "### 目标\n",
    "\n",
    "在本教程中，您将学习如何使用从[Google Cloud Pipeline Components](https://cloud.google.com/vertex-ai/docs/pipelines/components-introduction)（GCPC）下载的[Vertex AI Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines/introduction)创建AutoML预测模型。 这些管道是由Google维护的Vertex AI Tabular Workflow管道。 这些管道展示了定制Vertex AI Tabular训练过程的不同方法。\n",
    "\n",
    "本教程使用以下Google Cloud ML服务：\n",
    "\n",
    "- AutoML 训练\n",
    "- Vertex AI Pipelines\n",
    "\n",
    "执行的步骤包括：\n",
    "\n",
    "- 使用指定的机器类型创建一个使用TiDE（时间序列密集编码器）算法的训练管道。\n",
    "- 创建一个训练管道，该管道重复使用来自上一个管道的架构搜索结果，以节省TiDE（时间序列密集编码器）的时间。\n",
    "- 创建一个使用Learn-to-learn（L2L）算法的训练管道。\n",
    "- 创建一个使用Seq2seq（序列到序列）算法的训练管道。\n",
    "- 创建一个使用TFT（时间融合变换器）算法的训练管道。\n",
    "- 使用上述步骤中训练的模型执行批量预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eac26958afe8"
   },
   "source": [
    "### 数据集\n",
    "\n",
    "本教程使用了[酒类数据集](https://www.kaggle.com/datasets/residentmario/iowa-liquor-sales)，该数据集预测了中西部地区的酒类销售情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "181d4dfbf917"
   },
   "source": [
    "成本\n",
    "\n",
    "本教程使用了 Google Cloud 的可计费组件：\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "* BigQuery\n",
    "* Dataflow\n",
    "\n",
    "了解 [Vertex AI 价格](https://cloud.google.com/vertex-ai/pricing)、[Cloud Storage 价格](https://cloud.google.com/storage/pricing) 和 [BigQuery 价格](https://cloud.google.com/bigquery)，并使用 [定价计算器](https://cloud.google.com/products/calculator/) 基于您的预期使用量生成成本估算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e85f0288a6df"
   },
   "source": [
    "安装附加包\n",
    "\n",
    "安装谷歌云管道组件（GCPC）SDK不早于`2.3.0`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E7-SzYTR9bo2"
   },
   "outputs": [],
   "source": [
    "!pip3 install --upgrade --quiet google-cloud-pipeline-components==2.3.0 \\\n",
    "                                google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bj5O0S5RTxzY"
   },
   "source": [
    "只有协作：取消对以下单元格的注释以重新启动内核。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "023DMKUaTypt"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs so that your environment can access the new packages\n",
    "# import IPython\n",
    "\n",
    "# app = IPython.Application.instance()\n",
    "# app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yfEglUHQk9S3"
   },
   "source": [
    "## 在开始之前\n",
    "\n",
    "### 设置您的Google Cloud项目\n",
    "\n",
    "**无论您使用什么笔记本环境，以下步骤都是必需的。**\n",
    "\n",
    "1. [选择或创建一个Google Cloud项目](https://console.cloud.google.com/cloud-resource-manager)。\n",
    "\n",
    "2. [确保您的项目已启用计费功能](https://cloud.google.com/billing/docs/how-to/modify-project)。\n",
    "\n",
    "3. [启用Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=ml.googleapis.com,dataflow.googleapis.com,compute_component,storage-component.googleapis.com)。\n",
    "\n",
    "4. 如果您是在本地运行这个笔记本，您需要安装[Cloud SDK](https://cloud.google.com/sdk)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zebLBGXOky2A"
   },
   "source": [
    "##关于服务账户和权限的说明\n",
    "\n",
    "有关权限设置的详细信息，请参考 https://cloud.google.com/vertex-ai/docs/tabular-data/tabular-workflows/service-accounts\n",
    "\n",
    "**默认情况下不需要任何配置**，如果遇到任何与权限相关的问题，请确保上述服务账户具有所需的角色：\n",
    "\n",
    "|服务账户电子邮件|描述|角色|\n",
    "|---|---|---|\n",
    "|PROJECT_NUMBER-compute@developer.gserviceaccount.com|Compute Engine默认服务账户|Dataflow开发者、Dataflow工作者、存储管理员、BigQuery数据编辑器、Vertex AI用户、服务账户用户|\n",
    "|service-PROJECT_NUMBER@gcp-sa-aiplatform.iam.gserviceaccount.com|AI平台服务代理|Vertex AI服务代理|\n",
    "\n",
    "1. 打开 https://console.cloud.google.com/iam-admin/iam。\n",
    "2. 选中“包括Google提供的角色授予”复选框。\n",
    "3. 找到上述电子邮件。\n",
    "4. 授予相应的角色。\n",
    "\n",
    "### 使用来自不同项目的数据源\n",
    "- 对于BQ数据源，为两个服务账户授予“BigQuery数据查看器”角色。\n",
    "- 对于CSV数据源，为两个服务账户授予“存储对象查看器”角色。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95cb7ffd6895"
   },
   "source": [
    "### 设置您的项目ID\n",
    "\n",
    "**如果您不知道您的项目ID**，请尝试以下操作：\n",
    "* 运行 `gcloud config list`。\n",
    "* 运行 `gcloud projects list`。\n",
    "* 查看支持页面：[查找项目ID](https://support.google.com/googleapi/answer/7014113)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cd85f5c794e5"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
    "\n",
    "# Set the project id\n",
    "! gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b12f508d97c6"
   },
   "source": [
    "区域\n",
    "\n",
    "您也可以更改 Vertex AI 使用的 `REGION` 变量。了解有关 [Vertex AI 区域](https://cloud.google.com/vertex-ai/docs/general/locations) 的更多信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8e8b7997de7a"
   },
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eu0e2TRVxjHb"
   },
   "source": [
    "### 认证您的谷歌云账户\n",
    "\n",
    "根据您的Jupyter环境，您可能需要手动进行认证。请按照以下相关说明操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d118c95af93f"
   },
   "source": [
    "1. 顶点 AI 工作台\n",
    "* 不用做任何事，因为你已经通过认证。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3035286fcdda"
   },
   "source": [
    "2. 本地JupyterLab实例，请取消注释并运行："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "455882ec0f11"
   },
   "outputs": [],
   "source": [
    "# ! gcloud auth login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5097f3233d53"
   },
   "source": [
    "3. 协作，取消注释并运行："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2b88e46ac2c8"
   },
   "outputs": [],
   "source": [
    "# from google.colab import auth\n",
    "# auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fcdbb8929927"
   },
   "source": [
    "请参阅如何将云存储权限授予您的服务帐户页面：https://cloud.google.com/storage/docs/gsutil/commands/iam#ch-examples。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OUfwWir9yPNV"
   },
   "source": [
    "创建云存储桶\n",
    "\n",
    "创建一个存储桶来存储中间产物，比如数据集、TF模型检查点、TensorBoard文件等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5325f437b46a"
   },
   "outputs": [],
   "source": [
    "BUCKET_URI = f\"gs://your-bucket-name-{PROJECT_ID}-unique\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ebc0bdb07af"
   },
   "source": [
    "如果您的存储桶尚不存在：运行以下单元格来创建您的云存储存储桶。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0217c63ed87f"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l {REGION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "44accda192d5"
   },
   "source": [
    "#### 服务账号\n",
    "\n",
    "使用服务账号来创建Vertex AI Pipeline作业。如果您不想使用您项目的Compute Engine服务账号，请将`SERVICE_ACCOUNT`设置为另一个服务账号ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "604ae09ab6d3"
   },
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT = \"[your-service-account]\"  # @param {type:\"string\"}\n",
    "\n",
    "if (\n",
    "    SERVICE_ACCOUNT == \"\"\n",
    "    or SERVICE_ACCOUNT is None\n",
    "    or SERVICE_ACCOUNT == \"[your-service-account]\"\n",
    "):\n",
    "    import sys\n",
    "    IS_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "    # Get your service account from gcloud\n",
    "    if not IS_COLAB:\n",
    "        shell_output = !gcloud auth list 2>/dev/null\n",
    "        SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
    "\n",
    "    else:  # IS_COLAB:\n",
    "        shell_output = ! gcloud projects describe  $PROJECT_ID\n",
    "        project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
    "        SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
    "\n",
    "    print(\"Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d1ecb60964d5"
   },
   "source": [
    "设置顶点AI流水线的服务账号访问权限\n",
    "运行以下命令，授予您的服务账号对在上一步中创建的存储桶中的管道工件进行读取和写入的访问权限。您只需要针对每个服务账号运行此步骤一次。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a592f0a380c2"
   },
   "outputs": [],
   "source": [
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fbbc3479a1da"
   },
   "source": [
    "导入库并定义常数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8G6YmJT1yqkV"
   },
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import json\n",
    "import os\n",
    "import uuid\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "from google.cloud import aiplatform, storage\n",
    "from google_cloud_pipeline_components.preview.automl.forecasting import \\\n",
    "    utils as automl_forecasting_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0423f260423"
   },
   "source": [
    "初始化用于 Python 的 Vertex AI SDK。\n",
    "\n",
    "为您的项目初始化 Python 的 Vertex SDK。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ad69f2590268"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Se89OZ0jRWD6"
   },
   "source": [
    "## VPC相关配置\n",
    "\n",
    "如果您需要使用自定义Dataflow子网络，可以通过`dataflow_subnetwork`参数进行设置。要求如下：\n",
    "1. `dataflow_subnetwork`必须是完全限定的子网络名称。\n",
    "   （[示例网络和子网络规格说明](https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications)）\n",
    "1. 以下服务帐户必须在指定的dataflow子网络上分配[计算网络用户角色](https://cloud.google.com/compute/docs/access/iam#compute.networkUser)：                 \n",
    "    1. 计算引擎默认服务帐户：PROJECT_NUMBER-compute@developer.gserviceaccount.com\n",
    "    1. Dataflow服务帐户：service-PROJECT_NUMBER@dataflow-service-producer-prod.iam.gserviceaccount.com\n",
    "\n",
    "如果您的项目已启用VPC-SC，请确保：\n",
    "\n",
    "1. 在VPC-SC中使用的dataflow子网络已针对Dataflow进行正确配置。\n",
    "   [[参考](https://cloud.google.com/dataflow/docs/guides/routes-firewall)]\n",
    "1. `dataflow_use_public_ips`设置为False。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jjYKwvgxRbTn"
   },
   "outputs": [],
   "source": [
    "# Dataflow's fully qualified subnetwork name, when empty the default subnetwork will be used.\n",
    "# Fully qualified subnetwork name is in the form of\n",
    "# https://www.googleapis.com/compute/v1/projects/HOST_PROJECT_ID/regions/REGION_NAME/subnetworks/SUBNETWORK_NAME\n",
    "# reference: https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications\n",
    "dataflow_subnetwork = None  # @param {type:\"string\"}\n",
    "# Specifies whether Dataflow workers use public IP addresses.\n",
    "dataflow_use_public_ips = True  # @param {type:\"boolean\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqSmR2Q3Rphx"
   },
   "source": [
    "准备训练##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3LWH3PRF5o2v"
   },
   "source": [
    "### 定义辅助函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g9FPFT8c5oC0"
   },
   "outputs": [],
   "source": [
    "# Below functions will serve as the utility functions.\n",
    "\n",
    "\n",
    "# Fetch the tuple of GCS bucket and object URI.\n",
    "def get_bucket_name_and_path(uri: str):\n",
    "    no_prefix_uri = uri[len(\"gs://\") :]\n",
    "    splits = no_prefix_uri.split(\"/\")\n",
    "    return splits[0], \"/\".join(splits[1:])\n",
    "\n",
    "\n",
    "# Fetch the content from a GCS object URI.\n",
    "def download_from_gcs(uri: str):\n",
    "    bucket_name, path = get_bucket_name_and_path(uri)\n",
    "    storage_client = storage.Client(project=PROJECT_ID)\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(path)\n",
    "    return blob.download_as_string()\n",
    "\n",
    "\n",
    "# Upload the string content as a GCS object.\n",
    "def write_to_gcs(uri: str, content: str):\n",
    "    bucket_name, path = get_bucket_name_and_path(uri)\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(path)\n",
    "    blob.upload_from_string(content)\n",
    "\n",
    "\n",
    "# This is the example to set non-auto transformations.\n",
    "# For more details about the transformations, please check:\n",
    "# https://cloud.google.com/vertex-ai/docs/datasets/data-types-tabular#transformations\n",
    "def generate_transformation(\n",
    "    auto_column_names: Optional[List[str]] = None,\n",
    "    numeric_column_names: Optional[List[str]] = None,\n",
    "    categorical_column_names: Optional[List[str]] = None,\n",
    "    text_column_names: Optional[List[str]] = None,\n",
    "    timestamp_column_names: Optional[List[str]] = None,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    if auto_column_names is None:\n",
    "        auto_column_names = []\n",
    "    if numeric_column_names is None:\n",
    "        numeric_column_names = []\n",
    "    if categorical_column_names is None:\n",
    "        categorical_column_names = []\n",
    "    if text_column_names is None:\n",
    "        text_column_names = []\n",
    "    if timestamp_column_names is None:\n",
    "        timestamp_column_names = []\n",
    "    return {\n",
    "        \"auto\": auto_column_names,\n",
    "        \"numeric\": numeric_column_names,\n",
    "        \"categorical\": categorical_column_names,\n",
    "        \"text\": text_column_names,\n",
    "        \"timestamp\": timestamp_column_names,\n",
    "    }\n",
    "\n",
    "\n",
    "# Retrieve the data given a task name.\n",
    "def get_task_detail(\n",
    "    task_details: List[Dict[str, Any]], task_name: str\n",
    ") -> List[Dict[str, Any]]:\n",
    "    for task_detail in task_details:\n",
    "        if task_detail.task_name == task_name:\n",
    "            return task_detail\n",
    "\n",
    "\n",
    "# Retrieve the URI of the model.\n",
    "def get_deployed_model_uri(\n",
    "    task_details,\n",
    "):\n",
    "    ensemble_task = get_task_detail(task_details, \"model-upload\")\n",
    "    return ensemble_task.outputs[\"model\"].artifacts[0].uri\n",
    "\n",
    "\n",
    "# Retrieve the feature importance details from GCS.\n",
    "def get_feature_attributions(\n",
    "    task_details,\n",
    "):\n",
    "    ensemble_task = get_task_detail(task_details, \"model-evaluation-2\")\n",
    "    return download_from_gcs(\n",
    "        ensemble_task.outputs[\"evaluation_metrics\"]\n",
    "        .artifacts[0]\n",
    "        .metadata[\"explanation_gcs_path\"]\n",
    "    )\n",
    "\n",
    "\n",
    "# Retrieve the evaluation metrics from GCS.\n",
    "def get_evaluation_metrics(\n",
    "    task_details,\n",
    "):\n",
    "    ensemble_task = get_task_detail(task_details, \"model-evaluation\")\n",
    "    return download_from_gcs(\n",
    "        ensemble_task.outputs[\"evaluation_metrics\"].artifacts[0].uri\n",
    "    )\n",
    "\n",
    "\n",
    "# Pretty print the JSON string.\n",
    "def load_and_print_json(s):\n",
    "    parsed = json.loads(s)\n",
    "    print(json.dumps(parsed, indent=2, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gvNFMRmBegZq"
   },
   "source": [
    "定义培训规范"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eV4JrwB8wAkg"
   },
   "outputs": [],
   "source": [
    "root_dir = os.path.join(BUCKET_URI, f\"automl_forecasting_pipeline/run-{uuid.uuid4()}\")\n",
    "optimization_objective = \"minimize-mae\"\n",
    "time_column = \"date\"\n",
    "time_series_identifier_column = \"store_name\"\n",
    "target_column = \"sale_dollars\"\n",
    "data_source_csv_filenames = None\n",
    "data_source_bigquery_table_path = (\n",
    "    \"bq://bigquery-public-data.iowa_liquor_sales_forecasting.2020_sales_train\"\n",
    ")\n",
    "\n",
    "training_fraction = 0.8\n",
    "validation_fraction = 0.1\n",
    "test_fraction = 0.1\n",
    "\n",
    "predefined_split_key = None\n",
    "if predefined_split_key:\n",
    "    training_fraction = None\n",
    "    validation_fraction = None\n",
    "    test_fraction = None\n",
    "\n",
    "weight_column = None\n",
    "\n",
    "features = [\n",
    "    time_column,\n",
    "    target_column,\n",
    "    \"city\",\n",
    "    \"zip_code\",\n",
    "    \"county\",\n",
    "]\n",
    "\n",
    "available_at_forecast_columns = [time_column]\n",
    "unavailable_at_forecast_columns = [target_column]\n",
    "time_series_attribute_columns = [\"city\", \"zip_code\", \"county\"]\n",
    "forecast_horizon = 150\n",
    "context_window = 150\n",
    "\n",
    "transformations = generate_transformation(auto_column_names=features)\n",
    "\n",
    "# Create a Vertex managed dataset artifact.\n",
    "vertex_dataset = aiplatform.TimeSeriesDataset.create(\n",
    "    bq_source=data_source_bigquery_table_path\n",
    ")\n",
    "vertex_dataset_artifact_id = vertex_dataset.gca_resource.metadata_artifact.split(\"/\")[\n",
    "    -1\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSCTxi48DRxz"
   },
   "source": [
    "支持的API\n",
    "\n",
    "目前，在APIs/SDK中支持四种模型类型，并配备了实用函数：\n",
    "1. `time_series_dense_encoder`（`TiDE`）：`get_time_series_dense_encoder_forecasting_pipeline_and_parameters`\n",
    "2. `learn_to_learn`（`L2L`）：`get_learn_to_learn_forecasting_pipeline_and_parameters`\n",
    "3. `sequence_to_sequence`（`seq2seq`）：`get_sequence_to_sequence_forecasting_pipeline_and_parameters`\n",
    "4. `temporal_fusion_transformer`（`TFT`）：`get_temporal_fusion_transformer_forecasting_pipeline_and_parameters`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C9v6kvuXHCiy"
   },
   "source": [
    "### 高级工作流程\n",
    "\n",
    "以下代码显示了使用API的一般格式：\n",
    "```python\n",
    "# 使用实用程序函数来获取创建 Vertex Pipeline 作业所需的参数。\n",
    "template_path, parameter_values = automl_forecasting_utils.get_${MODEL_TYPE}_forecasting_pipeline_and_parameters(\n",
    "  ...\n",
    ")\n",
    "\n",
    "# 构建 Vertex Pipeline 作业。\n",
    "job = aiplatform.PipelineJob(\n",
    "    ...\n",
    "    location=REGION,  # 在指定区域启动管道作业\n",
    "    template_path=template_path,\n",
    "    ...\n",
    "    pipeline_root=root_dir,\n",
    "    parameter_values=parameter_values,\n",
    "    ...\n",
    ")\n",
    "\n",
    "# 启动 Vertex Pipeline 作业。\n",
    "job.run()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7gA0zRUEHO5b"
   },
   "source": [
    "###效用函数参数\n",
    "\n",
    "所有模型类型的效用函数都有相同的参数。\n",
    "\n",
    "以下以`get_time_series_dense_encoder_forecasting_pipeline_and_parameters`为例：\n",
    "\n",
    "```python\n",
    "def get_time_series_dense_encoder_forecasting_pipeline_and_parameters(\n",
    "    *,\n",
    "    project: str,\n",
    "    location: str,\n",
    "    root_dir: str,\n",
    "    target_column: str,\n",
    "    optimization_objective: str,\n",
    "    transformations: Dict[str, List[str]],\n",
    "    train_budget_milli_node_hours: float,\n",
    "    time_column: str,\n",
    "    time_series_identifier_columns: List[str],\n",
    "    time_series_attribute_columns: Optional[List[str]] = None,\n",
    "    available_at_forecast_columns: Optional[List[str]] = None,\n",
    "    unavailable_at_forecast_columns: Optional[List[str]] = None,\n",
    "    forecast_horizon: Optional[int] = None,\n",
    "    context_window: Optional[int] = None,\n",
    "    evaluated_examples_bigquery_path: Optional[str] = None,\n",
    "    window_predefined_column: Optional[str] = None,\n",
    "    window_stride_length: Optional[int] = None,\n",
    "    window_max_count: Optional[int] = None,\n",
    "    holiday_regions: Optional[List[str]] = None,\n",
    "    stage_1_num_parallel_trials: Optional[int] = None,\n",
    "    stage_1_tuning_result_artifact_uri: Optional[str] = None,\n",
    "    stage_2_num_parallel_trials: Optional[int] = None,\n",
    "    num_selected_trials: Optional[int] = None,\n",
    "    data_source_csv_filenames: Optional[str] = None,\n",
    "    data_source_bigquery_table_path: Optional[str] = None,\n",
    "    predefined_split_key: Optional[str] = None,\n",
    "    training_fraction: Optional[float] = None,\n",
    "    validation_fraction: Optional[float] = None,\n",
    "    test_fraction: Optional[float] = None,\n",
    "    weight_column: Optional[str] = None,\n",
    "    dataflow_service_account: Optional[str] = None,\n",
    "    dataflow_subnetwork: Optional[str] = None,\n",
    "    dataflow_use_public_ips: bool = True,\n",
    "    feature_transform_engine_bigquery_staging_full_dataset_id: str = '',\n",
    "    feature_transform_engine_dataflow_machine_type: str = 'n1-standard-16',\n",
    "    feature_transform_engine_dataflow_max_num_workers: int = 10,\n",
    "    feature_transform_engine_dataflow_disk_size_gb: int = 40,\n",
    "    evaluation_batch_predict_machine_type: str = 'n1-standard-16',\n",
    "    evaluation_batch_predict_starting_replica_count: int = 25,\n",
    "    evaluation_batch_predict_max_replica_count: int = 25,\n",
    "    evaluation_dataflow_machine_type: str = 'n1-standard-16',\n",
    "    evaluation_dataflow_max_num_workers: int = 25,\n",
    "    evaluation_dataflow_disk_size_gb: int = 50,\n",
    "    study_spec_parameters_override: Optional[List[Dict[str, Any]]] = None,\n",
    "    stage_1_tuner_worker_pool_specs_override: Optional[Dict[str, Any]] = None,\n",
    "    stage_2_trainer_worker_pool_specs_override: Optional[Dict[str, Any]] = None,\n",
    "    enable_probabilistic_inference: bool = False,\n",
    "    quantiles: Optional[List[float]] = None,\n",
    "    encryption_spec_key_name: Optional[str] = None,\n",
    "    model_display_name: Optional[str] = None,\n",
    "    model_description: Optional[str] = None,\n",
    "    run_evaluation: bool = True,\n",
    ") -> Tuple[str, Dict[str, Any]]:\n",
    "  \"\"\"返回 l2l_forecasting 管道和格式化参数。\n",
    "\n",
    "  Args:\n",
    "    project: 执行管道组件的 GCP 项目。\n",
    "    location: 运行管道组件的 GCP 区域。\n",
    "    root_dir: 管道组件的根 GCS 目录。\n",
    "    target_column: 目标列名。\n",
    "    optimization_objective: \"minimize-rmse\", \"minimize-mae\", \"minimize-rmsle\",\n",
    "      \"minimize-rmspe\", \"minimize-wape-mae\", \"minimize-mape\", 或\n",
    "      \"minimize-quantile-loss\"。\n",
    "    transformations: 将自动解析和/或类型解析映射到特征列的字典。支持的类型有：auto、categorical、numeric、text 和 timestamp。\n",
    "    train_budget_milli_node_hours: 创建该模型的训练预算，以毫秒节点小时表示，即该字段中的值为 1,000 表示 1 节点小时。\n",
    "    time_column: 表示时间的列。\n",
    "    time_series_identifier_columns: 区分不同时间序列的列。\n",
    "    time_series_attribute_columns: 在同一时间序列中不变的列。\n",
    "    available_at_forecast_columns: 预测时可用列。\n",
    "    unavailable_at_forecast_columns: 预测时不可用列。\n",
    "    forecast_horizon: 预测的时间跨度。\n",
    "    context_window: 上下文窗口的长度。\n",
    "    evaluated_examples_bigquery_path: 用于写入预测示例以进行评估的现有 BigQuery 数据集，格式为\n",
    "      `bq://project.dataset`。需要先创建数据集。\n",
    "    window_predefined_column: 指示每个窗口开始的列。\n",
    "    window_stride_length: 生成窗口的步长。\n",
    "    window_max_count: 将生成的窗口的最大数。\n",
    "    holiday_regions: 应用假日效应的地理区域。\n",
    "    stage_1_num_parallel_trials: 阶段 1 的并行试验次数。\n",
    "    stage_1_tuning_result_artifact_uri: 阶段 1 调整结果存储的 GCS URI。\n",
    "    stage_2_num_parallel_trials: 阶段 2 的并行试验次数。\n",
    "    num_selected_trials: 选定的试验次数。\n",
    "    data_source_csv_filenames: 表示逗号分隔的 CSV 文件名列表的字符串。\n",
    "    data_source_bigquery_table_path: 格式为 bq://bq_project.bq_dataset.bq_table 的 BigQuery 表路径。\n",
    "    predefined_split_key: 预定义的拆分列名。\n",
    "    training_fraction: 训练分数。\n",
    "    validation_fraction: 验证分数。\n",
    "    test_fraction: 测试分数。\n",
    "    weight_column: 权重列名。\n",
    "    dataflow_service_account: 完整的服务帐户名称。\n",
    "    dataflow_subnetwork: Dataflow 子网络。\n",
    "    dataflow_use_public_ips: `True` 表示启用 Dataflow 公共 IP。\n",
    "    feature_transform_engine_bigquery_staging_full_dataset_id: 特征转换引擎暂存数据集的完整 ID。\n",
    "    feature_transform_engine_dataflow_machine_type: 特征转换引擎的 Dataflow 机器类型。\n",
    "    feature_transform_engine_dataflow_max_num_workers: 特征转换引擎的最大 Dataflow\n",
    "      工作者数量。\n",
    "    feature_transform_engine_dataflow_disk_size_gb: 特征转换引擎的 Dataflow\n",
    "      工作者的磁盘大小。\n",
    "    evaluation_batch_predict_machine_type: 评估中批量预测作业的机器类型，如 'n1-standard-16'。\n",
    "    evaluation_batch_predict_starting_replica_count: 在启动时批量预测集群中使用的副本数。\n",
    "    evaluation_batch_predict_max_replica_count: 分布式预测作业可以扩展到的最大副本数。\n",
    "    evaluation_dataflow_machine_type: 评估中 Dataflow 作业的机器类型，如 'n1-standard-16'。\n",
    "    evaluation_dataflow_max_num_workers: Dataflow 工作者的最大数量。\n",
    "    evaluation_dataflow_disk_size_gb: Dataflow 的磁盘空间大小（GB）。\n",
    "    study_spec_parameters_override: 用于覆盖研究规范的列表。\n",
    "    stage_1_tuner_worker_pool_specs_override: 用于覆盖阶段 1 调谐器工作池规范的字典。\n",
    "    stage_2_trainer_worker_pool_specs_override: 用于覆盖阶段 2 训练工作池规范的字典。\n",
    "    enable_probabilistic_inference: 如果启用了概率推断，模型会拟合捕捉预测不确定性的分布。\n",
    "      如果指定了 quantiles，则还会返回分布的分位数。\n",
    "    quantiles: 用于概率推断的分位数。允许使用 0 和 1 之间的值的最多 5 个分位数，表示用于该目标的分位数。分位数必须是唯一的。\n",
    "    encryption_spec_key_name: KMS 密钥名称。\n",
    "    model_display_name: 模型的可选显示名称。\n",
    "    model_description: 可选描述。\n",
    "    run_evaluation: `True` 表示在测试集上评估集成模型。\n",
    "  \"\"\"\n",
    "  ...\n",
    "```\n",
    "\n",
    "\n",
    "### 使用假日地区\n",
    "\n",
    "对于某些用例，区域地区的假日可能会影响预测数据。有关支持的预测假日地区的更多信息，请参阅https://cloud.google.com/vertex-ai/docs/tabular-data/tabular-workflows/forecasting-train#holiday-regions。\n",
    "\n",
    "将字符串列表`holiday_regions`传递给管道参数生成器，以将假日数据纳入您的训练管道中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bb17c3654bcb"
   },
   "source": [
    "## 自定义训练配置\n",
    "\n",
    "您可以使用以下自定义内容创建一个预测管道：\n",
    "- 更改机器类型和调整/训练并行度\n",
    "- 跳过评估\n",
    "- 跳过模型架构搜索\n",
    "\n",
    "您可以重复使用现有的模型架构搜索结果，而不是每次都进行架构搜索。这样可以减少输出模型的变化或训练成本。现有的模型架构搜索结果存储在`automl-forecasting-stage-1-tuner`组件的`tuning_result_output`输出中。您可以使用API以编程方式加载它。\n",
    "\n",
    "```python\n",
    "stage_1_tuner_task = get_task_detail(\n",
    "    pipeline_task_details, \"automl-forecasting-stage-1-tuner\"\n",
    ")\n",
    "\n",
    "stage_1_tuning_result_artifact_uri = (\n",
    "    stage_1_tuner_task.outputs[\"tuning_result_output\"].artifacts[0].uri\n",
    ")\n",
    "```\n",
    "\n",
    "使用以下代码片段自定义训练配置："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dbf9aaca20a3"
   },
   "outputs": [],
   "source": [
    "# Customize the work pool for each trial during tuning.\n",
    "# Only the chief node and the evaluator node are used.\n",
    "# You can change the machine spec for these two nodes.\n",
    "worker_pool_specs_override = [\n",
    "    {\"machine_spec\": {\"machine_type\": \"n1-standard-8\"}},  # override for TF chief node\n",
    "    {},  # override for TF worker node, since it's not used, leave it empty\n",
    "    {},  # override for TF ps node, since it's not used, leave it empty\n",
    "    {\n",
    "        \"machine_spec\": {\"machine_type\": \"n1-standard-4\"}\n",
    "    },  # override for TF evaluator node\n",
    "]\n",
    "\n",
    "# Number of weak models in the final ensemble model.\n",
    "num_selected_trials = 5\n",
    "\n",
    "# Specify the evaluation setup.\n",
    "run_evaluation = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a359fc31ba3b"
   },
   "source": [
    "您可以通过在培训参数中设置参数 `evaluated_examples_bigquery_path` ，将评估的示例从培训导出到BigQuery。BigQuery路径需要指向现有的BigQuery数据集，格式为 `bq://project.dataset`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b2abbcf372ce"
   },
   "outputs": [],
   "source": [
    "# This is ONLY available when `run_evaluation` is set to `True`.\n",
    "evaluated_examples_bigquery_path = f\"bq://{PROJECT_ID}.eval\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-iXXE14voyR"
   },
   "source": [
    "TiDE培训\n",
    "\n",
    "时间序列密集编码器（TiDE）是一种优化的密集DNN编码器-解码器模型，具有出色的模型质量，训练和推断速度快，尤其适用于长上下文和视野。\n",
    "\n",
    "更多详细信息请参见https://ai.googleblog.com/2023/04/recent-advances-in-deep-long-horizon.html\n",
    "\n",
    "在本教程中，运行两次TiDE训练流程：\n",
    "1. 使用模型架构搜索\n",
    "2. 没有模型架构搜索"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3577d591bad1"
   },
   "source": [
    "运行具有模型结构搜索功能的TiDE流水线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sG46cXVueb66"
   },
   "outputs": [],
   "source": [
    "train_budget_milli_node_hours = 250.0  # 15 minutes\n",
    "\n",
    "(\n",
    "    template_path,\n",
    "    parameter_values,\n",
    ") = automl_forecasting_utils.get_time_series_dense_encoder_forecasting_pipeline_and_parameters(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    root_dir=root_dir,\n",
    "    target_column=target_column,\n",
    "    # `minimize-quantile-loss`\n",
    "    optimization_objective=optimization_objective,\n",
    "    transformations=transformations,\n",
    "    train_budget_milli_node_hours=train_budget_milli_node_hours,\n",
    "    # Do not set `data_source_csv_filenames` and\n",
    "    # `data_source_bigquery_table_path` if you want to use Vertex managed\n",
    "    # dataset by commenting out the following two lines.\n",
    "    data_source_csv_filenames=data_source_csv_filenames,\n",
    "    data_source_bigquery_table_path=data_source_bigquery_table_path,\n",
    "    weight_column=weight_column,\n",
    "    predefined_split_key=predefined_split_key,\n",
    "    training_fraction=training_fraction,\n",
    "    validation_fraction=validation_fraction,\n",
    "    test_fraction=test_fraction,\n",
    "    num_selected_trials=num_selected_trials,\n",
    "    time_column=time_column,\n",
    "    time_series_identifier_columns=[time_series_identifier_column],\n",
    "    time_series_attribute_columns=time_series_attribute_columns,\n",
    "    available_at_forecast_columns=available_at_forecast_columns,\n",
    "    unavailable_at_forecast_columns=unavailable_at_forecast_columns,\n",
    "    forecast_horizon=forecast_horizon,\n",
    "    context_window=context_window,\n",
    "    stage_1_tuner_worker_pool_specs_override=worker_pool_specs_override,\n",
    "    dataflow_subnetwork=dataflow_subnetwork,\n",
    "    dataflow_use_public_ips=dataflow_use_public_ips,\n",
    "    run_evaluation=run_evaluation,\n",
    "    # evaluated_examples_bigquery_path=evaluated_examples_bigquery_path,\n",
    "    dataflow_service_account=SERVICE_ACCOUNT,\n",
    "    # Quantile forecast requires `minimize-quantile-loss` as optimization objective.\n",
    "    # quantiles=[0.25, 0.5, 0.9],\n",
    "    # holiday_regions=[\"US\", \"AE\"],\n",
    ")\n",
    "\n",
    "job_id = \"tide-forecasting-{}\".format(uuid.uuid4())\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=job_id,\n",
    "    location=REGION,  # launches the pipeline job in the specified region\n",
    "    template_path=template_path,\n",
    "    job_id=job_id,\n",
    "    pipeline_root=root_dir,\n",
    "    parameter_values=parameter_values,\n",
    "    enable_caching=False,\n",
    "    # Uncomment the following line if you want to use Vertex managed dataset.\n",
    "    # input_artifacts={'vertex_dataset': vertex_dataset_artifact_id},\n",
    ")\n",
    "\n",
    "job.run(service_account=SERVICE_ACCOUNT)\n",
    "\n",
    "\n",
    "pipeline_task_details = job.gca_resource.job_detail.task_details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5F12ZL_uZZ3"
   },
   "source": [
    "不执行模型架构搜索的 TiDE pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c24aa07ead0a"
   },
   "source": [
    "从阶段1调谐器中检索到调谐结果后，您可以使用它来跳过模型架构搜索。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4x4GA5sMuewX"
   },
   "outputs": [],
   "source": [
    "# Retrieve the tuning result output from the previous training pipeline.\n",
    "stage_1_tuner_task = get_task_detail(\n",
    "    pipeline_task_details, \"automl-forecasting-stage-1-tuner\"\n",
    ")\n",
    "\n",
    "stage_1_tuning_result_artifact_uri = (\n",
    "    stage_1_tuner_task.outputs[\"tuning_result_output\"].artifacts[0].uri\n",
    ")\n",
    "\n",
    "train_budget_milli_node_hours = 250.0  # 15 minutes\n",
    "\n",
    "(\n",
    "    template_path,\n",
    "    parameter_values,\n",
    ") = automl_forecasting_utils.get_time_series_dense_encoder_forecasting_pipeline_and_parameters(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    root_dir=root_dir,\n",
    "    target_column=target_column,\n",
    "    optimization_objective=optimization_objective,\n",
    "    transformations=transformations,\n",
    "    train_budget_milli_node_hours=train_budget_milli_node_hours,\n",
    "    data_source_csv_filenames=data_source_csv_filenames,\n",
    "    data_source_bigquery_table_path=data_source_bigquery_table_path,\n",
    "    weight_column=weight_column,\n",
    "    predefined_split_key=predefined_split_key,\n",
    "    training_fraction=training_fraction,\n",
    "    validation_fraction=validation_fraction,\n",
    "    test_fraction=test_fraction,\n",
    "    num_selected_trials=num_selected_trials,\n",
    "    time_column=time_column,\n",
    "    time_series_identifier_columns=[time_series_identifier_column],\n",
    "    time_series_attribute_columns=time_series_attribute_columns,\n",
    "    available_at_forecast_columns=available_at_forecast_columns,\n",
    "    unavailable_at_forecast_columns=unavailable_at_forecast_columns,\n",
    "    forecast_horizon=forecast_horizon,\n",
    "    context_window=context_window,\n",
    "    dataflow_subnetwork=dataflow_subnetwork,\n",
    "    dataflow_use_public_ips=dataflow_use_public_ips,\n",
    "    stage_1_tuning_result_artifact_uri=stage_1_tuning_result_artifact_uri,\n",
    "    run_evaluation=run_evaluation,\n",
    "    # evaluated_examples_bigquery_path=evaluated_examples_bigquery_path,\n",
    "    dataflow_service_account=SERVICE_ACCOUNT,\n",
    ")\n",
    "\n",
    "job_id = \"tide-forecasting-skip-architecture-search-{}\".format(uuid.uuid4())\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=job_id,\n",
    "    location=REGION,  # launches the pipeline job in the specified region\n",
    "    template_path=template_path,\n",
    "    job_id=job_id,\n",
    "    pipeline_root=root_dir,\n",
    "    parameter_values=parameter_values,\n",
    "    enable_caching=False,\n",
    ")\n",
    "\n",
    "job.run(service_account=SERVICE_ACCOUNT)\n",
    "\n",
    "# Get model URI\n",
    "skip_architecture_search_pipeline_task_details = (\n",
    "    job.gca_resource.job_detail.task_details\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xiftLomOwGda"
   },
   "source": [
    "## L2L 培训"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bfe9f2568c7"
   },
   "source": [
    "学以致用（L2L）是广泛范围的时间序列预测用例的好选择。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XSAcS70N1GMN"
   },
   "outputs": [],
   "source": [
    "train_budget_milli_node_hours = 250.0  # 15 minutes\n",
    "\n",
    "(\n",
    "    template_path,\n",
    "    parameter_values,\n",
    ") = automl_forecasting_utils.get_learn_to_learn_forecasting_pipeline_and_parameters(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    root_dir=root_dir,\n",
    "    target_column=target_column,\n",
    "    optimization_objective=optimization_objective,\n",
    "    transformations=transformations,\n",
    "    train_budget_milli_node_hours=train_budget_milli_node_hours,\n",
    "    data_source_csv_filenames=data_source_csv_filenames,\n",
    "    data_source_bigquery_table_path=data_source_bigquery_table_path,\n",
    "    weight_column=weight_column,\n",
    "    predefined_split_key=predefined_split_key,\n",
    "    training_fraction=training_fraction,\n",
    "    validation_fraction=validation_fraction,\n",
    "    test_fraction=test_fraction,\n",
    "    num_selected_trials=num_selected_trials,\n",
    "    time_column=time_column,\n",
    "    time_series_identifier_columns=[time_series_identifier_column],\n",
    "    time_series_attribute_columns=time_series_attribute_columns,\n",
    "    available_at_forecast_columns=available_at_forecast_columns,\n",
    "    unavailable_at_forecast_columns=unavailable_at_forecast_columns,\n",
    "    forecast_horizon=forecast_horizon,\n",
    "    context_window=context_window,\n",
    "    dataflow_subnetwork=dataflow_subnetwork,\n",
    "    dataflow_use_public_ips=dataflow_use_public_ips,\n",
    "    run_evaluation=run_evaluation,\n",
    "    # evaluated_examples_bigquery_path=evaluated_examples_bigquery_path,\n",
    "    dataflow_service_account=SERVICE_ACCOUNT,\n",
    "    # Quantile forecast requires `minimize-quantile-loss` as optimization objective.\n",
    "    # quantiles=[0.25, 0.5, 0.9],\n",
    ")\n",
    "\n",
    "job_id = \"l2l-forecasting-{}\".format(uuid.uuid4())\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=job_id,\n",
    "    location=REGION,  # launches the pipeline job in the specified region\n",
    "    template_path=template_path,\n",
    "    job_id=job_id,\n",
    "    pipeline_root=root_dir,\n",
    "    parameter_values=parameter_values,\n",
    "    enable_caching=False,\n",
    ")\n",
    "\n",
    "job.run(service_account=SERVICE_ACCOUNT)\n",
    "\n",
    "\n",
    "pipeline_task_details = job.gca_resource.job_detail.task_details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1LvCFqFtI-1t"
   },
   "source": [
    "## Seq2seq 训练\n",
    "\n",
    "序列到序列（seq2seq）是进行实验的一个很好的选择。该算法可能会比 AutoML 更快地收敛，因为其架构更简单，并且使用更小的搜索空间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "euK31b8xMJ8i"
   },
   "outputs": [],
   "source": [
    "train_budget_milli_node_hours = 250.0  # 15 minutes\n",
    "\n",
    "(\n",
    "    template_path,\n",
    "    parameter_values,\n",
    ") = automl_forecasting_utils.get_sequence_to_sequence_forecasting_pipeline_and_parameters(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    root_dir=root_dir,\n",
    "    target_column=target_column,\n",
    "    optimization_objective=optimization_objective,\n",
    "    transformations=transformations,\n",
    "    train_budget_milli_node_hours=train_budget_milli_node_hours,\n",
    "    data_source_csv_filenames=data_source_csv_filenames,\n",
    "    data_source_bigquery_table_path=data_source_bigquery_table_path,\n",
    "    weight_column=weight_column,\n",
    "    predefined_split_key=predefined_split_key,\n",
    "    training_fraction=training_fraction,\n",
    "    validation_fraction=validation_fraction,\n",
    "    test_fraction=test_fraction,\n",
    "    num_selected_trials=num_selected_trials,\n",
    "    time_column=time_column,\n",
    "    time_series_identifier_columns=[time_series_identifier_column],\n",
    "    time_series_attribute_columns=time_series_attribute_columns,\n",
    "    available_at_forecast_columns=available_at_forecast_columns,\n",
    "    unavailable_at_forecast_columns=unavailable_at_forecast_columns,\n",
    "    forecast_horizon=forecast_horizon,\n",
    "    context_window=context_window,\n",
    "    dataflow_subnetwork=dataflow_subnetwork,\n",
    "    dataflow_use_public_ips=dataflow_use_public_ips,\n",
    "    run_evaluation=run_evaluation,\n",
    "    # evaluated_examples_bigquery_path=evaluated_examples_bigquery_path,\n",
    "    dataflow_service_account=SERVICE_ACCOUNT,\n",
    "    # Quantile prediction is NOT supported by Seq2seq.\n",
    ")\n",
    "\n",
    "job_id = \"seq2seq-forecasting-{}\".format(uuid.uuid4())\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=job_id,\n",
    "    location=REGION,  # launches the pipeline job in the specified region\n",
    "    template_path=template_path,\n",
    "    job_id=job_id,\n",
    "    pipeline_root=root_dir,\n",
    "    parameter_values=parameter_values,\n",
    "    enable_caching=False,\n",
    ")\n",
    "\n",
    "job.run(service_account=SERVICE_ACCOUNT)\n",
    "\n",
    "\n",
    "pipeline_task_details = job.gca_resource.job_detail.task_details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8PdaaWdJCh-"
   },
   "source": [
    "## TFT培训\n",
    "\n",
    "TFT代表“时间融合变压器”，这是一种基于注意力的DNN模型，旨在通过将模型与一般的多时间段预测任务对齐，实现高准确性和可解释性。\n",
    "\n",
    "使用这个模型，您不需要在提供时显式启用可解释性支持，即可获得每个特征列的特征重要性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZjCEok0IMT-8"
   },
   "outputs": [],
   "source": [
    "train_budget_milli_node_hours = 250.0  # 15 minutes\n",
    "\n",
    "(\n",
    "    template_path,\n",
    "    parameter_values,\n",
    ") = automl_forecasting_utils.get_temporal_fusion_transformer_forecasting_pipeline_and_parameters(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    root_dir=root_dir,\n",
    "    target_column=target_column,\n",
    "    optimization_objective=optimization_objective,\n",
    "    transformations=transformations,\n",
    "    train_budget_milli_node_hours=train_budget_milli_node_hours,\n",
    "    data_source_csv_filenames=data_source_csv_filenames,\n",
    "    data_source_bigquery_table_path=data_source_bigquery_table_path,\n",
    "    weight_column=weight_column,\n",
    "    predefined_split_key=predefined_split_key,\n",
    "    training_fraction=training_fraction,\n",
    "    validation_fraction=validation_fraction,\n",
    "    test_fraction=test_fraction,\n",
    "    # Please note that TFT model will ONLY ensemble the model from\n",
    "    # the top one trial, so `num_selected_trials` can not be set for TFT model.\n",
    "    # num_selected_trials=num_selected_trials,\n",
    "    time_column=time_column,\n",
    "    time_series_identifier_columns=[time_series_identifier_column],\n",
    "    time_series_attribute_columns=time_series_attribute_columns,\n",
    "    available_at_forecast_columns=available_at_forecast_columns,\n",
    "    unavailable_at_forecast_columns=unavailable_at_forecast_columns,\n",
    "    forecast_horizon=forecast_horizon,\n",
    "    context_window=context_window,\n",
    "    dataflow_subnetwork=dataflow_subnetwork,\n",
    "    dataflow_use_public_ips=dataflow_use_public_ips,\n",
    "    run_evaluation=run_evaluation,\n",
    "    # evaluated_examples_bigquery_path=evaluated_examples_bigquery_path,\n",
    "    dataflow_service_account=SERVICE_ACCOUNT,\n",
    "    # Quantile prediction is NOT supported by TFT.\n",
    ")\n",
    "\n",
    "job_id = \"tft-forecasting-{}\".format(uuid.uuid4())\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=job_id,\n",
    "    location=REGION,  # launches the pipeline job in the specified region\n",
    "    template_path=template_path,\n",
    "    job_id=job_id,\n",
    "    pipeline_root=root_dir,\n",
    "    parameter_values=parameter_values,\n",
    "    enable_caching=False,\n",
    ")\n",
    "\n",
    "job.run(service_account=SERVICE_ACCOUNT)\n",
    "\n",
    "\n",
    "pipeline_task_details = job.gca_resource.job_detail.task_details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L38LI3OOwXkV"
   },
   "source": [
    "## 批量预测/解释\n",
    "\n",
    "只需在 `batch_predict` API 中设置 `generate_explanation=True` 即可启用批量解释功能。\n",
    "\n",
    "使用以下代码从管道中检索经过训练的预测模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0bf184357639"
   },
   "outputs": [],
   "source": [
    "upload_model_task = get_task_detail(pipeline_task_details, \"model-upload-2\")\n",
    "\n",
    "forecasting_mp_model_artifact = upload_model_task.outputs[\"model\"].artifacts[0]\n",
    "\n",
    "forecasting_mp_model = aiplatform.Model(\n",
    "    forecasting_mp_model_artifact.metadata[\"resourceName\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9327a8a390a7"
   },
   "source": [
    "一旦您检索到Vertex AI模型，您就可以开始执行批量预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NfssCxqRg1x-"
   },
   "outputs": [],
   "source": [
    "print(f\"Running Batch prediction for model: {forecasting_mp_model.display_name}\")\n",
    "\n",
    "batch_predict_bq_output_uri_prefix = f\"bq://{PROJECT_ID}\"\n",
    "\n",
    "PREDICTION_DATASET_BQ_PATH = (\n",
    "    \"bq://bigquery-public-data:iowa_liquor_sales_forecasting.2021_sales_predict\"\n",
    ")\n",
    "\n",
    "batch_prediction_job = forecasting_mp_model.batch_predict(\n",
    "    job_display_name=\"forecasting_iowa_liquor_sales_forecasting_predictions\",\n",
    "    bigquery_source=PREDICTION_DATASET_BQ_PATH,\n",
    "    instances_format=\"bigquery\",\n",
    "    bigquery_destination_prefix=batch_predict_bq_output_uri_prefix,\n",
    "    predictions_format=\"bigquery\",\n",
    "    # Uncomment the following line to run batch explain:\n",
    "    # generate_explanation=True,\n",
    "    sync=True,\n",
    ")\n",
    "\n",
    "print(batch_prediction_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLDc1X8DVSmm"
   },
   "source": [
    "使用Vertex AI管道作业ID检索已上传的Vertex AI模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MJ_2TaYtTNbO"
   },
   "outputs": [],
   "source": [
    "# Example format of pipeline_job_id: projects/{your-project-id}/locations/us-central1/pipelineJobs/{pipeline-job-id}\n",
    "pipeline_job_id = \"\"  # @param {type:\"string\"}\n",
    "if pipeline_job_id:\n",
    "    job = aiplatform.PipelineJob.get(pipeline_job_id)\n",
    "    pipeline_task_details = job.gca_resource.job_detail.task_details\n",
    "    upload_model_task = get_task_detail(pipeline_task_details, \"model-upload-2\")\n",
    "\n",
    "    forecasting_mp_model_artifact = upload_model_task.outputs[\"model\"].artifacts[0]\n",
    "    forecasting_mp_model = aiplatform.Model(\n",
    "        forecasting_mp_model_artifact.metadata[\"resourceName\"]\n",
    "    )\n",
    "    print(forecasting_mp_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KtcHUmcZIi9g"
   },
   "source": [
    "## 使用父模型上传不同模型版本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6qht5Rdx6fuj"
   },
   "source": [
    "要将此模型上传到父 Vertex AI 模型，您需要父 Vertex AI 模型的 `parent_model_resource_name` resource_name。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6-bzpVGTEq14"
   },
   "outputs": [],
   "source": [
    "# The model resource name can be something like: \"projects/{your-project-id}/locations/us-central1/models/{model-id}\"\n",
    "parent_model_resource_name = \"\"  # @param {type:\"string\"}\n",
    "\n",
    "if parent_model_resource_name:\n",
    "    parent_model_artifact = aiplatform.Artifact.get_with_uri(\n",
    "        \"https://us-central1-aiplatform.googleapis.com/v1/\" + parent_model_resource_name\n",
    "    )\n",
    "    parent_model_artifact_id = str(\n",
    "        parent_model_artifact.gca_resource.name.split(\"artifacts/\")[1]\n",
    "    )\n",
    "\n",
    "    train_budget_milli_node_hours = 250.0  # 15 minutes\n",
    "\n",
    "    (\n",
    "        template_path,\n",
    "        parameter_values,\n",
    "    ) = automl_forecasting_utils.get_time_series_dense_encoder_forecasting_pipeline_and_parameters(\n",
    "        project=PROJECT_ID,\n",
    "        location=REGION,\n",
    "        root_dir=root_dir,\n",
    "        target_column=target_column,\n",
    "        optimization_objective=optimization_objective,\n",
    "        transformations=transformations,\n",
    "        train_budget_milli_node_hours=train_budget_milli_node_hours,\n",
    "        # Do not set `data_source_csv_filenames` and\n",
    "        # `data_source_bigquery_table_path` if you want to use Vertex managed\n",
    "        # dataset by commenting out the following two lines.\n",
    "        data_source_csv_filenames=data_source_csv_filenames,\n",
    "        data_source_bigquery_table_path=data_source_bigquery_table_path,\n",
    "        weight_column=weight_column,\n",
    "        predefined_split_key=predefined_split_key,\n",
    "        training_fraction=training_fraction,\n",
    "        validation_fraction=validation_fraction,\n",
    "        test_fraction=test_fraction,\n",
    "        num_selected_trials=5,\n",
    "        time_column=time_column,\n",
    "        time_series_identifier_columns=[time_series_identifier_column],\n",
    "        time_series_attribute_columns=time_series_attribute_columns,\n",
    "        available_at_forecast_columns=available_at_forecast_columns,\n",
    "        unavailable_at_forecast_columns=unavailable_at_forecast_columns,\n",
    "        forecast_horizon=forecast_horizon,\n",
    "        context_window=context_window,\n",
    "        dataflow_subnetwork=dataflow_subnetwork,\n",
    "        dataflow_use_public_ips=dataflow_use_public_ips,\n",
    "        run_evaluation=run_evaluation,\n",
    "        # evaluated_examples_bigquery_path=evaluated_examples_bigquery_path,\n",
    "        dataflow_service_account=SERVICE_ACCOUNT,\n",
    "        # Quantile forecast requires `minimize-quantile-loss` as optimization objective.\n",
    "        # quantiles=[0.25, 0.5, 0.9],\n",
    "    )\n",
    "\n",
    "    job_id = \"tide-forecasting-with-parent-model-{}\".format(uuid.uuid4())\n",
    "    job = aiplatform.PipelineJob(\n",
    "        display_name=job_id,\n",
    "        location=REGION,  # launches the pipeline job in the specified region\n",
    "        template_path=template_path,\n",
    "        job_id=job_id,\n",
    "        pipeline_root=root_dir,\n",
    "        parameter_values=parameter_values,\n",
    "        enable_caching=False,\n",
    "        input_artifacts={\"parent_model\": parent_model_artifact_id},\n",
    "    )\n",
    "\n",
    "    job.run(service_account=SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Bu0wywvPYkD"
   },
   "source": [
    "将Tabular Workflow for Forecasting集成到现有的KFP管道中\n",
    "\n",
    "这是通过KFP的pipeline-as-component功能实现的。 (链接：https://www.kubeflow.org/docs/components/pipelines/v2/load-and-share-components/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OcWHUuoYPbSi"
   },
   "outputs": [],
   "source": [
    "from kfp import compiler, components, dsl\n",
    "\n",
    "train_budget_milli_node_hours = 250.0  # 15 minutes\n",
    "\n",
    "(\n",
    "    template_path,\n",
    "    parameter_values,\n",
    ") = automl_forecasting_utils.get_time_series_dense_encoder_forecasting_pipeline_and_parameters(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    root_dir=root_dir,\n",
    "    target_column=target_column,\n",
    "    optimization_objective=optimization_objective,\n",
    "    transformations=transformations,\n",
    "    train_budget_milli_node_hours=train_budget_milli_node_hours,\n",
    "    data_source_csv_filenames=data_source_csv_filenames,\n",
    "    data_source_bigquery_table_path=data_source_bigquery_table_path,\n",
    "    weight_column=weight_column,\n",
    "    predefined_split_key=predefined_split_key,\n",
    "    training_fraction=training_fraction,\n",
    "    validation_fraction=validation_fraction,\n",
    "    test_fraction=test_fraction,\n",
    "    num_selected_trials=num_selected_trials,\n",
    "    time_column=time_column,\n",
    "    time_series_identifier_columns=[time_series_identifier_column],\n",
    "    time_series_attribute_columns=time_series_attribute_columns,\n",
    "    available_at_forecast_columns=available_at_forecast_columns,\n",
    "    unavailable_at_forecast_columns=unavailable_at_forecast_columns,\n",
    "    forecast_horizon=forecast_horizon,\n",
    "    context_window=context_window,\n",
    "    dataflow_subnetwork=dataflow_subnetwork,\n",
    "    dataflow_use_public_ips=dataflow_use_public_ips,\n",
    "    run_evaluation=False,\n",
    "    dataflow_service_account=SERVICE_ACCOUNT,\n",
    ")\n",
    "\n",
    "# Load the forecasting pipeline as a sub-pipeline/components which can be used\n",
    "# in a larger KFP pipeline.\n",
    "forecasting_pipeline = components.load_component_from_file(template_path)\n",
    "\n",
    "\n",
    "@dsl.component\n",
    "def print_message(msg: str):\n",
    "    print(\"message:\", msg)\n",
    "\n",
    "\n",
    "# Define a pipeline that follows the below steps:\n",
    "# step_1(print_message) -> step_2(print_message) -> forecasting_pipeline\n",
    "@dsl.pipeline\n",
    "def outer_pipeline(msg_1: str, msg_2: str, ds: dsl.Artifact):\n",
    "    step_1 = print_message(msg=msg_1)\n",
    "    step_2 = print_message(msg=msg_2).after(step_1)\n",
    "    # `vertex_dataset` argument needs to be set/forwarded here to avoid the\n",
    "    # \"missing-argument\" error in KFP pipeline.\n",
    "    forecasting_pipeline(**parameter_values, vertex_dataset=ds).after(step_2)\n",
    "\n",
    "\n",
    "# Compile and save the outer/larger pipeline template.\n",
    "outer_pipeline_template_path = \"./outer_pipeline.yaml\"\n",
    "compiler.Compiler().compile(outer_pipeline, outer_pipeline_template_path)\n",
    "\n",
    "\n",
    "job_id = \"run-forecasting-pipeline-inside-pipeline-{}\".format(uuid.uuid4())\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=job_id,\n",
    "    location=REGION,  # launches the pipeline job in the specified region\n",
    "    template_path=outer_pipeline_template_path,\n",
    "    job_id=job_id,\n",
    "    pipeline_root=root_dir,\n",
    "    parameter_values={\"msg_1\": \"step 1\", \"msg_2\": \"step 2\"},\n",
    "    enable_caching=False,\n",
    ")\n",
    "\n",
    "job.run(service_account=SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2992dc3f6019"
   },
   "source": [
    "清理\n",
    "\n",
    "要清理此项目中使用的所有Google Cloud资源，您可以[删除用于本教程的Google Cloud\n",
    "项目](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects)。\n",
    "\n",
    "否则，您可以删除在本教程中创建的各个资源："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "829cd75564f1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Delete Cloud Storage objects that were created\n",
    "delete_bucket = False\n",
    "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
    "    ! gsutil -m rm -r $BUCKET_URI"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "automl_forecasting_on_vertex_pipelines.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
