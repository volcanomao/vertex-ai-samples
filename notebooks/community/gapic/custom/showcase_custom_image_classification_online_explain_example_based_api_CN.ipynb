{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pS9oVc0bScpN"
   },
   "source": [
    "# 顶点客户端库：使用基于示例的API定制训练图像分类模型进行在线预测，并附带解释\n",
    "\n",
    "<table align=\"left\">\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/gapic/custom/showcase_custom_image_classification_online_explain_example_based_api.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> 在Colab中运行\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/gapic/custom/showcase_custom_image_classification_online_explain_example_based_api.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      在GitHub上查看\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/notebooks/community/gapic/custom/showcase_custom_image_classification_online_explain_example_based_api.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      在Vertex AI Workbench中打开\n",
    "    </a>\n",
    "  </td>                                                                                               \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## 概述\n",
    "\n",
    "本笔记本演示了如何为您的模型获取基于示例的解释。解释可以帮助您回答关于为什么模型作出特定预测的问题，并将其与训练数据中的特征相关联。在这个演示中，我们将讨论以下内容：\n",
    "\n",
    "1. 从Vertex Explainable AI服务中获取基于示例的解释。\n",
    "2. 探索类似示例以理解模型预测的用例。\n",
    "\n",
    "本笔记本的先决条件包括：\n",
    "1. 一个预测模型和从中提取潜在表示（即嵌入）的方法。本笔记本将演示如何为深度神经网络执行此操作。\n",
    "2. 一个Google Cloud项目。\n",
    "3. 一个Google存储桶来托管模型和数据集。\n",
    "\n",
    "一旦这些准备就绪，笔记本的三个主要部分将是：\n",
    "1. 创建并上传启用解释的模型。\n",
    "2. 创建一个`Endpoint`资源并将模型部署到其中。\n",
    "3. 发出解释请求并检查它们。\n",
    "\n",
    "### 数据集\n",
    "\n",
    "在本笔记本中，我们将使用通过[TF Datasets](https://www.tensorflow.org/datasets/catalog/stl10)下载的[STL10数据集](https://cs.stanford.edu/~acoates/stl10/)。这个数据集是[ImageNet数据集](https://www.image-net.org/)的一个子集，只包含10个类别。\n",
    "\n",
    "### 目标\n",
    "\n",
    "本教程使用以下Google Cloud ML服务：\n",
    "\n",
    "- `Vertex AI训练`\n",
    "- `Vertex Explainable AI`\n",
    "- `Vertex AI预测`\n",
    "- `Vertex AI模型`\n",
    "\n",
    "执行的步骤包括：\n",
    "\n",
    "1. 准备训练数据\n",
    "2. 微调图像分类模型以获取嵌入\n",
    "3. 在Vertex AI模型注册表中注册模型\n",
    "4. 将模型部署到Vertex AI Endpoint\n",
    "5. 使用基于示例的解释API请求解释\n",
    "6. 分析结果\n",
    "\n",
    "### 成本\n",
    "\n",
    "本教程使用Google Cloud的收费组件：\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "了解有关[Vertex AI定价](https://cloud.google.com/vertex-ai/pricing)和[Cloud Storage价格](https://cloud.google.com/storage/pricing)的信息，并使用[Pricing Calculator](https://cloud.google.com/products/calculator/)根据您的预期用量生成成本估算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ze4-nDLfK4pw"
   },
   "source": [
    "### 设置本地开发环境\n",
    "\n",
    "**如果您正在使用Colab或Google Cloud笔记本，您可以跳过此步骤**，因为您的环境已经满足运行此笔记本的所有要求。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCuSR8GkAgzl"
   },
   "source": [
    "否则，请确保您的环境满足此笔记本的要求。\n",
    "您需要以下内容：\n",
    "\n",
    "* Google Cloud SDK\n",
    "* Git\n",
    "* Python 3\n",
    "* virtualenv\n",
    "* 在使用 Python 3 的虚拟环境中运行的Jupyter笔记本\n",
    "\n",
    "Google Cloud指南[设置Python开发环境](https://cloud.google.com/python/setup) 和[Jupyter安装指南](https://jupyter.org/install) 提供了满足这些要求的详细说明。以下步骤提供了一套简明的说明：\n",
    "\n",
    "1. [安装和初始化Cloud SDK。](https://cloud.google.com/sdk/docs/)\n",
    "\n",
    "2. [安装Python 3。](https://cloud.google.com/python/setup#installing_python)\n",
    "\n",
    "3. [安装virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv) 并创建一个使用Python 3的虚拟环境。激活虚拟环境。\n",
    "\n",
    "4. 要安装Jupyter，请在终端窗口中运行`pip3 install jupyter`命令。\n",
    "\n",
    "5. 要启动Jupyter，请在终端窗口中运行`jupyter notebook`命令。\n",
    "\n",
    "6. 在Jupyter Notebook仪表板中打开此笔记本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "## 安装\n",
    "\n",
    "安装以下软件包以执行此笔记本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2b4ef9b72d43"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Vertex AI Workbench Notebook product has specific requirements\n",
    "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\") and not os.getenv(\"VIRTUAL_ENV\")\n",
    "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
    "    \"/opt/deeplearning/metadata/env_version\"\n",
    ")\n",
    "\n",
    "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_WORKBENCH_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\"\n",
    "\n",
    "if os.getenv(\"IS_TESTING\"):\n",
    "    ! pip3 install {USER_FLAG} --upgrade numpy tensorflow tensorflow_datasets -q\n",
    "    ! pip3 install {USER_FLAG} --upgrade google-cloud-aiplatform -q\n",
    "else:\n",
    "    ! pip3 install {USER_FLAG} --upgrade numpy==1.21.6 tensorflow==2.8.0 tensorflow_datasets==4.6.0 -q\n",
    "    ! pip3 install {USER_FLAG} --upgrade google-cloud-aiplatform==1.15.0 -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhq5zEbGg0XX"
   },
   "source": [
    "### 重新启动内核\n",
    "\n",
    "在安装/升级软件包之后，您需要重新启动笔记本内核，以便它可以找到这些软件包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EzrelQZ22IZj"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWEdiXsJg0XY"
   },
   "source": [
    "在开始之前"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF1j6f9HApxa"
   },
   "source": [
    "### 设置您的Google Cloud项目\n",
    "\n",
    "**无论您使用哪种笔记本环境，都需要按照以下步骤操作。**\n",
    "\n",
    "1. [选择或创建一个Google Cloud项目](https://console.cloud.google.com/cloud-resource-manager)。当您首次创建账户时，您将获得$300的免费信用额度用于计算/存储成本。\n",
    "\n",
    "1. [确保您的项目已启用计费](https://cloud.google.com/billing/docs/how-to/modify-project)。\n",
    "\n",
    "1. [启用API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,cloudresourcemanager.googleapis.com)。\n",
    "\n",
    "1. 如果您在本地运行此笔记本，您将需要安装[Cloud SDK](https://cloud.google.com/sdk)。\n",
    "\n",
    "1. 在下面的单元格中输入您的项目ID。然后运行该单元格，以确保Cloud SDK对本笔记本中的所有命令使用正确的项目。\n",
    "\n",
    "**注意**：Jupyter会将以`！`开头的行作为shell命令运行，并将以`$`开头的Python变量插入这些命令。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WReHDGG5g0XY"
   },
   "source": [
    "**设置您的项目ID**\n",
    "\n",
    "**如果您不知道您的项目ID**，您可以使用`gcloud`来获取您的项目ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7QEa15y1hofD"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_ID = \"\"\n",
    "\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eTnyUHeOKb-h"
   },
   "source": [
    "否则，请在这里设置您的项目ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_KnmhsDAKc5q"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None:\n",
    "    PROJECT_ID = \"[your-project]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "riG_qUokg0XZ"
   },
   "outputs": [],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aepw5iW5Ko0s"
   },
   "source": [
    "获得您的项目编号\n",
    "\n",
    "现在项目ID已设定，您将获得相应的项目编号。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OXcDgJhlKw9R"
   },
   "outputs": [],
   "source": [
    "shell_output = ! gcloud projects list --filter=\"PROJECT_ID:'{PROJECT_ID}'\" --format='value(PROJECT_NUMBER)'\n",
    "PROJECT_NUMBER = shell_output[0]\n",
    "print(\"Project Number:\", PROJECT_NUMBER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "### 区域\n",
    "\n",
    "您还可以更改 `REGION` 变量，该变量用于本笔记本中的操作。以下是 Vertex AI 支持的区域。我们建议您选择与您最近的区域。\n",
    "\n",
    "- 美洲：`us-central1`\n",
    "- 欧洲：`europe-west4`\n",
    "- 亚太：`asia-east1`\n",
    "\n",
    "您可能无法使用多区域存储桶进行 Vertex AI 训练。并非所有区域都支持所有 Vertex AI 服务。\n",
    "\n",
    "了解有关 [Vertex AI 区域](https://cloud.google.com/vertex-ai/docs/general/locations) 的更多信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sKBTnvJpox9P"
   },
   "outputs": [],
   "source": [
    "REGION = \"[your-region]\"  # @param {type:\"string\"}\n",
    "\n",
    "if REGION == \"[your-region]\":\n",
    "    REGION = \"us-central1\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06571eb4063b"
   },
   "source": [
    "时间戳\n",
    "\n",
    "如果您正在进行实时教程会话，您可能正在使用共享的测试账户或项目。为了避免用户之间在创建的资源上发生名称冲突，您可以为每个实例会话创建一个时间戳，并将其附加到您在本教程中创建的资源名称上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "697568e92bd6"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dr--iN2kAylZ"
   },
   "source": [
    "验证您的 Google Cloud 帐号\n",
    "\n",
    "**如果您正在使用 Vertex AI Workbench 笔记本电脑，您将跳过此步骤**，因为您的环境已经经过验证。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBCra4QMA2wR"
   },
   "source": [
    "如果您正在使用Colab，请运行下面的单元格，并按提示进行OAuth身份验证。\n",
    "\n",
    "否则，请按照以下步骤操作：\n",
    "\n",
    "1. 在Cloud Console中，转到[**创建服务帐号密钥**页面](https://console.cloud.google.com/apis/credentials/serviceaccountkey)。\n",
    "\n",
    "2. 点击 **创建服务帐号**。\n",
    "\n",
    "3. 在**服务帐号名称**字段中输入一个名称，并点击 **创建**。\n",
    "\n",
    "4. 在**将此服务帐号授权访问项目**部分，点击**角色**下拉列表。在过滤框中输入以下角色并选择\n",
    "\n",
    "    - 服务帐号用户\n",
    "    - 存储管理员\n",
    "    - 存储对象管理员\n",
    "    - Vertex AI管理员\n",
    "\n",
    "5. 点击*创建*。将包含您密钥的JSON文件下载到本地环境。\n",
    "\n",
    "6. 在下面的单元格中将您的服务帐号密钥路径输入为`GOOGLE_APPLICATION_CREDENTIALS`变量，然后运行该单元格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PyQmSRbKA8r-"
   },
   "outputs": [],
   "source": [
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "import sys\n",
    "\n",
    "# If on Vertex AI Workbench, then don't execute this code\n",
    "\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\") and not os.getenv(\n",
    "    \"DL_ANACONDA_HOME\"\n",
    "):\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\"):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "### 创建一个云存储存储桶\n",
    "\n",
    "**无论您使用的是哪种笔记本环境，下面的步骤都是必需的。**\n",
    "\n",
    "在下面设置您的云存储存储桶的名称。它必须在所有云存储存储桶中是唯一的。\n",
    "\n",
    "您也可以更改`REGION`变量，该变量用于本笔记本后续操作。我们建议您选择一个[Vertex AI 服务可用的区域](https://cloud.google.com/vertex-ai/docs/general/locations#available_regions)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bHb-DP1ieozQ"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"[your-bucket-name]\"  # @param {type:\"string\"}\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cf221059d072"
   },
   "outputs": [],
   "source": [
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"[your-bucket-name]\":\n",
    "    BUCKET_NAME = PROJECT_ID + \"-aip-\" + TIMESTAMP\n",
    "    BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EcIXiGsCePi"
   },
   "source": [
    "运行以下单元格以创建您的云存储桶。如果该桶已经存在，您将收到一个错误，但这不会影响教程的其余部分。然而，您可能会在此桶中获得未期望的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIq7R4HZCfIc"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucvCsknMCims"
   },
   "source": [
    "最后，通过检查云存储桶的内容来验证访问权限。如果这是一个新的桶，这个单元格不会产生输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vhOb7YnwClBb"
   },
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "set_service_account"
   },
   "source": [
    "获取您的服务账号\n",
    "\n",
    "如果您不想使用项目的计算引擎服务账号，请将`SERVICE_ACCOUNT`设置为另一个服务账号ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vqUH5pk0OWzl"
   },
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT = \"[your-service-account]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "autoset_service_account"
   },
   "outputs": [],
   "source": [
    "if (\n",
    "    SERVICE_ACCOUNT == \"\"\n",
    "    or SERVICE_ACCOUNT is None\n",
    "    or SERVICE_ACCOUNT == \"[your-service-account]\"\n",
    "):\n",
    "    # Get your service account from gcloud\n",
    "    if not IS_COLAB:\n",
    "        shell_output = !gcloud auth list 2>/dev/null\n",
    "        SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
    "\n",
    "    else:  # IS_COLAB:\n",
    "        shell_output = ! gcloud projects describe  $PROJECT_ID\n",
    "        project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
    "        SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
    "\n",
    "    print(\"Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "set_service_account:pipelines"
   },
   "source": [
    "### 设置服务账户访问权限\n",
    "\n",
    "运行以下命令，向您在上一步中创建的存储桶授予服务账户访问权限。您只需要针对每个服务账户运行此步骤一次。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nGfP_UQEOWzm"
   },
   "outputs": [],
   "source": [
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator {BUCKET_URI}\n",
    "\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sYb3wtPucIUR"
   },
   "source": [
    "创建本地目录 \n",
    "接下来，您创建一些本地目录，用于本教程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iHd3PHCFcHKj"
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"data\"\n",
    "DELIVERABLE_PATH = \"deliverables\"\n",
    "\n",
    "! mkdir -m 777 -p {DATA_PATH}\n",
    "! mkdir -m 777 -p {DELIVERABLE_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y9Uo3tifg1kx"
   },
   "source": [
    "### 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRUOFELefqf1"
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import io\n",
    "import json\n",
    "# General\n",
    "import time\n",
    "\n",
    "# Training\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "tfds.disable_progress_bar()\n",
    "# Vertex AI\n",
    "from google.cloud import aiplatform_v1beta1 as vertex_ai_v1beta1\n",
    "from google.cloud.aiplatform_v1beta1.types import io as io_pb2\n",
    "from google.protobuf import json_format\n",
    "from google.protobuf.struct_pb2 import Value\n",
    "from PIL import Image\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### 设置变量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ikn2WUNWPG2B"
   },
   "source": [
    "#### 设定教程变量\n",
    "\n",
    "为教程设定一些变量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-P3O55RyMGtC"
   },
   "outputs": [],
   "source": [
    "# General\n",
    "ENVIRON = \"prod\"\n",
    "DATASET_NAME = (\n",
    "    \"stl10\"  # Will be downloaded from https://www.tensorflow.org/datasets/catalog/stl10\n",
    ")\n",
    "\n",
    "# Model experimentation\n",
    "RAW_DIR = f\"{DATA_PATH}/raw/{DATASET_NAME}\"\n",
    "PREPROCESSED_DIR = f\"{DATA_PATH}/preprocessed/{DATASET_NAME}\"\n",
    "MODEL_DIR = f\"{DELIVERABLE_PATH}/models/mobilenetv2-{DATASET_NAME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NGFeDbldOr6y"
   },
   "source": [
    "设置Vertex AI常量\n",
    "\n",
    "为Vertex设置以下常量：\n",
    "\n",
    "- `API_ENDPOINT`：用于数据集、模型、作业、流水线和端点服务的Vertex API服务端点。\n",
    "- `PARENT`：用于数据集、模型、作业、流水线和端点资源的Vertex位置根路径。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HN-kbHKvOw7G"
   },
   "outputs": [],
   "source": [
    "# API service endpoint\n",
    "API_ENDPOINT = \"{}-aiplatform.googleapis.com\".format(REGION)\n",
    "\n",
    "# Vertex location root path for your dataset, model and endpoint resources\n",
    "PARENT = \"projects/\" + PROJECT_ID + \"/locations/\" + REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "accelerators:training,prediction,cpu"
   },
   "source": [
    "#### 设置硬件加速器\n",
    "\n",
    "设置硬件加速器（例如，GPU）用于训练和预测。\n",
    "\n",
    "设置变量`TRAIN_GPU/TRAIN_NGPU`和`DEPLOY_GPU/DEPLOY_NGPU`，以使用支持GPU的容器映像以及分配给虚拟机（VM）实例的GPU数量。例如，要使用带有4个Nvidia Telsa K80 GPU的GPU容器映像，您可以指定：\n",
    "\n",
    "(aip.AcceleratorType.NVIDIA_TESLA_K80, 4)\n",
    "\n",
    "对于GPU，可用的加速器包括：\n",
    "   - aip.AcceleratorType.NVIDIA_TESLA_K80\n",
    "   - aip.AcceleratorType.NVIDIA_TESLA_P100\n",
    "   - aip.AcceleratorType.NVIDIA_TESLA_P4\n",
    "   - aip.AcceleratorType.NVIDIA_TESLA_T4\n",
    "   - aip.AcceleratorType.NVIDIA_TESLA_V100\n",
    "\n",
    "否则，指定`(None, None)`以使用一个在CPU上运行的容器映像。\n",
    "\n",
    "*注意*：在TF 2.3之前的GPU支持下发布的TF版本将无法在本教程中加载自定义模型。这是一个已知问题，在TF 2.3中已修复——这是由于在serving函数中生成的静态图操作而引起的。如果您在自定义模型中遇到此问题，请使用支持GPU的TF 2.3容器映像。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kVb-KlTfTA89"
   },
   "outputs": [],
   "source": [
    "if os.getenv(\"IS_TESTING_TRAIN_GPU\"):\n",
    "    TRAIN_GPU, TRAIN_NGPU = (\n",
    "        vertex_ai_v1beta1.AcceleratorType.NVIDIA_TESLA_K80,\n",
    "        int(os.getenv(\"IS_TESTING_TRAIN_GPU\")),\n",
    "    )\n",
    "else:\n",
    "    TRAIN_GPU, TRAIN_NGPU = (vertex_ai_v1beta1.AcceleratorType.NVIDIA_TESLA_K80, 1)\n",
    "\n",
    "if os.getenv(\"IS_TESTING_DEPLOY_GPU\"):\n",
    "    DEPLOY_GPU, DEPLOY_NGPU = (\n",
    "        vertex_ai_v1beta1.AcceleratorType.NVIDIA_TESLA_K80,\n",
    "        int(os.getenv(\"IS_TESTING_DEPLOY_GPU\")),\n",
    "    )\n",
    "else:\n",
    "    DEPLOY_GPU, DEPLOY_NGPU = (None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "container:training,prediction"
   },
   "source": [
    "设置预先构建的容器\n",
    "\n",
    "设置用于训练和预测的预先构建的Docker容器镜像。\n",
    "\n",
    "获取最新列表，请查看[用于训练的预构建容器](https://cloud.google.com/vertex-ai/docs/training/pre-built-containers)。\n",
    "\n",
    "获取最新列表，请查看[用于预测的预构建容器](https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CPty8w-cTBtZ"
   },
   "outputs": [],
   "source": [
    "if os.getenv(\"IS_TESTING_TF\"):\n",
    "    TF = os.getenv(\"IS_TESTING_TF\")\n",
    "else:\n",
    "    TF = \"2-8\"\n",
    "\n",
    "if TF[0] == \"2\":\n",
    "    if TRAIN_GPU:\n",
    "        TRAIN_VERSION = \"tf-gpu.{}\".format(TF)\n",
    "    else:\n",
    "        TRAIN_VERSION = \"tf-cpu.{}\".format(TF)\n",
    "    if DEPLOY_GPU:\n",
    "        DEPLOY_VERSION = \"tf2-gpu.{}\".format(TF)\n",
    "    else:\n",
    "        DEPLOY_VERSION = \"tf2-cpu.{}\".format(TF)\n",
    "else:\n",
    "    if TRAIN_GPU:\n",
    "        TRAIN_VERSION = \"tf-gpu.{}\".format(TF)\n",
    "    else:\n",
    "        TRAIN_VERSION = \"tf-cpu.{}\".format(TF)\n",
    "    if DEPLOY_GPU:\n",
    "        DEPLOY_VERSION = \"tf-gpu.{}\".format(TF)\n",
    "    else:\n",
    "        DEPLOY_VERSION = \"tf-cpu.{}\".format(TF)\n",
    "\n",
    "TRAIN_IMAGE = \"{}-docker.pkg.dev/vertex-ai/training/{}:latest\".format(\n",
    "    REGION.split(\"-\")[0], TRAIN_VERSION\n",
    ")\n",
    "DEPLOY_IMAGE = \"{}-docker.pkg.dev/vertex-ai/prediction/{}:latest\".format(\n",
    "    REGION.split(\"-\")[0], DEPLOY_VERSION\n",
    ")\n",
    "\n",
    "print(\"Training:\", TRAIN_IMAGE, TRAIN_GPU, TRAIN_NGPU)\n",
    "print(\"Deployment:\", DEPLOY_IMAGE, DEPLOY_GPU, DEPLOY_NGPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "machine:training,prediction"
   },
   "source": [
    "#### 设置机器类型\n",
    "\n",
    "接下来，设置用于训练和预测的机器类型。\n",
    "\n",
    "- 设置变量`TRAIN_COMPUTE`和`DEPLOY_COMPUTE`以配置用于训练和预测的虚拟机的计算资源。\n",
    " - 机器类型\n",
    "     - `n1-standard`：每个vCPU 3.75GB的内存。\n",
    "     - `n1-highmem`：每个vCPU 6.5GB的内存。\n",
    "     - `n1-highcpu`：每个vCPU 0.9GB的内存。\n",
    " - vCPU数：\\[2, 4, 8, 16, 32, 64, 96\\]\n",
    "\n",
    "*注意：以下内容不支持用于训练*\n",
    "\n",
    " - `standard`：2个vCPUs\n",
    " - `highcpu`：2、4和8个vCPUs\n",
    "\n",
    "*注意：您也可以使用n2和e2的机器类型进行训练和部署，但它们不支持GPU。*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cg2J5hFJOzvP"
   },
   "outputs": [],
   "source": [
    "if os.getenv(\"IS_TESTING_TRAIN_MACHINE\"):\n",
    "    MACHINE_TYPE = os.getenv(\"IS_TESTING_TRAIN_MACHINE\")\n",
    "else:\n",
    "    MACHINE_TYPE = \"n1-standard\"\n",
    "\n",
    "VCPU = \"4\"\n",
    "TRAIN_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
    "print(\"Train machine type\", TRAIN_COMPUTE)\n",
    "\n",
    "if os.getenv(\"IS_TESTING_DEPLOY_MACHINE\"):\n",
    "    MACHINE_TYPE = os.getenv(\"IS_TESTING_DEPLOY_MACHINE\")\n",
    "else:\n",
    "    MACHINE_TYPE = \"n1-standard\"\n",
    "\n",
    "VCPU = \"4\"\n",
    "DEPLOY_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
    "print(\"Deploy machine type\", DEPLOY_COMPUTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hMY97zSZLVj7"
   },
   "source": [
    "### 辅助函数\n",
    "\n",
    "以下是您在教程中使用的一系列辅助函数：\n",
    "\n",
    "1. `create_index_to_name_map`：创建一个从索引到名称的映射，用于标签变量\n",
    "2. `extract_images_and_labels`：从数据集中提取一批图像和标签的函数\n",
    "3. `plot_input_and_neighbors`：绘制输入图像及其邻近图像的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tuhbXWKrLYuk"
   },
   "outputs": [],
   "source": [
    "def create_index_to_name_map(ds_info):\n",
    "    \"\"\"\n",
    "    Creates a map from label name to numerical index.\n",
    "    Args:\n",
    "        ds_info: DatasetInfo object.\n",
    "    Returns:\n",
    "        index_to_name_map: dict. Map from name to index.\n",
    "    \"\"\"\n",
    "    index_to_name = {}\n",
    "    num_classes = ds_info.features[\"label\"].num_classes\n",
    "    names = ds_info.features[\"label\"].names\n",
    "    for i in range(num_classes):\n",
    "        index_to_name[i] = names[i]\n",
    "    return index_to_name\n",
    "\n",
    "\n",
    "def extract_images_and_labels(ds, num_batches):\n",
    "    \"\"\"\n",
    "    Extract images and labels from a dataset.\n",
    "    Args:\n",
    "        ds: A dataset.\n",
    "        num_batches: The number of batches to extract. -1 uses the whole dataset\n",
    "    Returns:\n",
    "        images: A numpy structure of images.\n",
    "        labels: A numpy structure of labels.\n",
    "    \"\"\"\n",
    "    data_slice = ds.take(num_batches)\n",
    "    images = []\n",
    "    labels = []\n",
    "    for image, label in data_slice:\n",
    "        images.append(image)\n",
    "        labels.append(label)\n",
    "    images = tf.concat(images, 0)\n",
    "    labels = tf.concat(labels, 0)\n",
    "    print(f\"Image batch shape: {images.shape}\")\n",
    "    return images.numpy(), labels.numpy()\n",
    "\n",
    "\n",
    "def plot_input_and_neighbors(\n",
    "    val_img_idx,\n",
    "    all_train_images,\n",
    "    val_images,\n",
    "    all_train_labels,\n",
    "    val_labels,\n",
    "    label_index_to_name,\n",
    "    data_with_neighbors,\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot the input image and its neighbors.\n",
    "    Args:\n",
    "        val_img_idx: Index of the input image.\n",
    "        all_train_images: All training images.\n",
    "        val_images: Validation images.\n",
    "        all_train_labels: All training labels.\n",
    "        val_labels: Validation labels.\n",
    "        label_index_to_name: Dictionary mapping label indices to names.\n",
    "        data_with_neighbors: Data with neighbors.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    image = val_images[val_img_idx]\n",
    "    fig = plt.figure(figsize=(24, 12))\n",
    "    ax_list = fig.subplots(3, 5)\n",
    "    ax_list[0, 0].axis(\"off\")\n",
    "    ax_list[0, 1].axis(\"off\")\n",
    "    ax_list[0, 3].axis(\"off\")\n",
    "    ax_list[0, 4].axis(\"off\")\n",
    "    ax = ax_list[0, 2]\n",
    "    class_label = val_labels[val_img_idx]\n",
    "    ax.set_title(\n",
    "        f\"{class_label}:{label_index_to_name[class_label]} (example index: {val_img_idx})\",\n",
    "        fontsize=15,\n",
    "    )\n",
    "    ax.axis(\"off\")\n",
    "    ax.imshow(image.astype(\"uint8\"))\n",
    "\n",
    "    neighbor_list = data_with_neighbors[val_img_idx][\"neighbors\"]\n",
    "    num_neighbors = len(neighbor_list)\n",
    "    for n in range(num_neighbors):\n",
    "        neighbor = neighbor_list[n]\n",
    "        neighbor_idx = int(neighbor[\"neighborId\"])\n",
    "        neighbor_dist = neighbor[\"neighborDistance\"]\n",
    "        ax = ax_list[1 + n // 5, n % 5]\n",
    "        class_label = all_train_labels[neighbor_idx]\n",
    "        ax.set_title(\n",
    "            f\"{class_label}:{label_index_to_name[class_label]} (dist: {neighbor_dist:.3f})\",\n",
    "            fontsize=15,\n",
    "        )\n",
    "        ax.axis(\"off\")\n",
    "        ax.imshow(all_train_images[neighbor_idx].astype(\"uint8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gd6RoZHJYvt8"
   },
   "source": [
    "# 教程\n",
    "\n",
    "Vertex可解释的基于示例的API提供了一个高性能的ANN服务，用于返回类似于新预测/实例的示例。\n",
    "\n",
    "为了利用基于示例的解释服务，您需要按照以下步骤进行操作：\n",
    "\n",
    "1）对整个数据集建立索引：您需要提供一个嵌入模型在GCS存储桶中的路径，训练数据存储在GCS存储桶中，以及基于示例的解释的配置文件\n",
    "\n",
    "2）部署索引和模型：您需要指定要使用的机器和在模型上传组中的模型标识符\n",
    "\n",
    "3）查询类似的示例：您需要进行解释查询，模型将返回类似的示例\n",
    "\n",
    "让我们开始为STL10数据集实验一个自定义模型，您将用它来提取嵌入。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-pUOiVFsZbm4"
   },
   "source": [
    "实验"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDKXS5h2LpNB"
   },
   "source": [
    "### 准备训练数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cDDCBHJbgW7n"
   },
   "source": [
    "下载并可视化数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KGiCMhPeSYdy"
   },
   "outputs": [],
   "source": [
    "split_ds, ds_info = tfds.load(\n",
    "    DATASET_NAME,\n",
    "    split=[\"train\", \"test\"],\n",
    "    as_supervised=True,  # Include labels\n",
    "    with_info=True,\n",
    "    shuffle_files=False,  # ensuring that the data doesn't get shuffled between runs\n",
    "    data_dir=RAW_DIR,\n",
    ")\n",
    "train_ds, validation_ds = split_ds\n",
    "tfds.show_examples(ds=train_ds, ds_info=ds_info);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sGNlBKlWju5t"
   },
   "outputs": [],
   "source": [
    "print(f'Number of classes in the dataset: {ds_info.features[\"label\"].num_classes}')\n",
    "print(f'Label names: {ds_info.features[\"label\"].names}')\n",
    "print(f'Number of examples in training split: {ds_info.splits[\"train\"].num_examples}')\n",
    "print(f'Number of examples in validation split: {ds_info.splits[\"test\"].num_examples}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IRmJEzcdhofK"
   },
   "source": [
    "准备图像\n",
    "\n",
    "将图像缩放到下游模型所预期的大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j3e_Cg4ljyuk"
   },
   "outputs": [],
   "source": [
    "size = (224, 224)\n",
    "train_ds = train_ds.map(lambda x, y: (tf.image.resize(x, size), y))\n",
    "validation_ds = validation_ds.map(lambda x, y: (tf.image.resize(x, size), y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sjds2n3Bj-9w"
   },
   "source": [
    "批量处理和预抓取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t1GjhJODkJRh"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_ds = train_ds.batch(batch_size).prefetch(buffer_size=10)\n",
    "validation_ds = validation_ds.batch(batch_size).prefetch(buffer_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vlhPZStokO90"
   },
   "outputs": [],
   "source": [
    "label_index_to_name = create_index_to_name_map(ds_info)\n",
    "print(label_index_to_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r6xymTJVkdXa"
   },
   "source": [
    "创建并训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WPUESdjzhDWD"
   },
   "source": [
    "微调预训练分类模型的最后一层\n",
    "\n",
    "您可以使用`MobileNetV2` [Keras Application](https://keras.io/api/applications/)深度学习模型，该模型可同时提供预训练权重，用于微调该模型以创建嵌入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lxnUTl_2i4oK"
   },
   "outputs": [],
   "source": [
    "# Each image can be flipped and rotated to generate more training data, and\n",
    "# ensure the model is more robust to such changes, since a rotated \"bird\" should\n",
    "# still be classified as a bird.\n",
    "\n",
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
    "        layers.experimental.preprocessing.RandomRotation(0.1),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fUS5WlTei5BD"
   },
   "outputs": [],
   "source": [
    "size_3d = (224, 224, 3)\n",
    "num_classes = ds_info.features[\"label\"].num_classes\n",
    "\n",
    "# Load an ImageNet model.\n",
    "base_model = keras.applications.MobileNetV2(\n",
    "    weights=\"imagenet\",  # Load weights pre-trained on ImageNet.\n",
    "    input_shape=size_3d,\n",
    "    include_top=False,\n",
    ")  # Do not include the ImageNet classifier at the top.\n",
    "\n",
    "# Freeze the base_model\n",
    "base_model.trainable = False\n",
    "\n",
    "# Create new model on top\n",
    "inputs = keras.Input(shape=size_3d)\n",
    "x = data_augmentation(inputs)  # Apply random data augmentation\n",
    "\n",
    "# Pre-trained Xception weights requires that input be normalized\n",
    "# from (0, 255) to a range (-1., +1.), the normalization layer\n",
    "# does the following, outputs = (inputs - mean) / sqrt(var)\n",
    "norm_layer = keras.layers.experimental.preprocessing.Normalization()\n",
    "mean = np.array([127.5] * 3)\n",
    "var = mean**2\n",
    "\n",
    "# Scale inputs to [-1, +1]\n",
    "x = norm_layer(x)\n",
    "norm_layer.set_weights([mean, var, 0])\n",
    "\n",
    "# The base model contains batchnorm layers. We want to keep them in inference mode\n",
    "# when we unfreeze the base model for fine-tuning, so we make sure that the\n",
    "# base_model is running in inference mode here.\n",
    "x = base_model(x, training=False)\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = keras.layers.Dropout(0.2)(x)  # Regularize with dropout\n",
    "outputs = keras.layers.Dense(num_classes)(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KhhGgjqyeozX"
   },
   "source": [
    "在这个示范中，您可以通过解冻下层并微调来改善预训练模型的潜在表示。但是，您选择放弃这一步骤，以便学习到的表示与预训练模型完全相同，只对softmax层进行简短的训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rO0blIoOjihC"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "epochs = 2\n",
    "history = model.fit(train_ds, epochs=epochs, validation_data=validation_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YkNdPtuNhofN"
   },
   "source": [
    "### 评估模型\n",
    "\n",
    "在训练后绘制模型的`准确率`和`损失率`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FxzsSYunlFGh"
   },
   "outputs": [],
   "source": [
    "x_axis = range(1, epochs + 1)\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x_axis, history.history[\"sparse_categorical_accuracy\"])\n",
    "plt.plot(x_axis, history.history[\"val_sparse_categorical_accuracy\"])\n",
    "plt.title(\"Model Accuracy\", fontsize=20)\n",
    "plt.ylabel(\"Accuracy\", fontsize=15)\n",
    "plt.xlabel(\"Epoch\", fontsize=15)\n",
    "plt.legend([\"Train\", \"Test\"], fontsize=15)\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x_axis, history.history[\"loss\"])\n",
    "plt.plot(x_axis, history.history[\"val_loss\"])\n",
    "plt.title(\"Model Loss\", fontsize=20)\n",
    "plt.ylabel(\"Loss\", fontsize=15)\n",
    "plt.xlabel(\"Epoch\", fontsize=15)\n",
    "plt.legend([\"Train\", \"Test\"], fontsize=15)\n",
    "plt.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "59OE0kYjaqe5"
   },
   "source": [
    "正式化\n",
    "\n",
    "现在您已经实验了模型以创建嵌入，让我们正式化训练以利用 Vertex AI。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MAXCt-Ldplv2"
   },
   "source": [
    "### 设置客户端\n",
    "\n",
    "Vertex AI客户端库以客户端/服务器模型运作。然后，您需要设置客户端以使用不同的服务。\n",
    "\n",
    "在本教程中，您将针对工作流程中的不同步骤使用不同的客户端。因此，请提前设置它们。\n",
    "\n",
    "- 用于`模型`资源的模型服务。\n",
    "- 用于部署的端点服务。\n",
    "- 用于批处理作业和自定义训练的作业服务。\n",
    "- 用于服务的预测服务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vq5UsPfqPiKi"
   },
   "outputs": [],
   "source": [
    "# client options same for all services\n",
    "client_options = {\"api_endpoint\": API_ENDPOINT}\n",
    "\n",
    "\n",
    "def create_job_client():\n",
    "    client = vertex_ai_v1beta1.JobServiceClient(client_options=client_options)\n",
    "    return client\n",
    "\n",
    "\n",
    "def create_model_client():\n",
    "    client = vertex_ai_v1beta1.ModelServiceClient(client_options=client_options)\n",
    "    return client\n",
    "\n",
    "\n",
    "def create_endpoint_client():\n",
    "    client = vertex_ai_v1beta1.EndpointServiceClient(client_options=client_options)\n",
    "    return client\n",
    "\n",
    "\n",
    "def create_prediction_client():\n",
    "    client = vertex_ai_v1beta1.PredictionServiceClient(client_options=client_options)\n",
    "    return client\n",
    "\n",
    "\n",
    "clients = {}\n",
    "clients[\"job\"] = create_job_client()\n",
    "clients[\"model\"] = create_model_client()\n",
    "clients[\"endpoint\"] = create_endpoint_client()\n",
    "clients[\"prediction\"] = create_prediction_client()\n",
    "\n",
    "for client in clients.items():\n",
    "    print(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train_custom_model"
   },
   "source": [
    "训练模型\n",
    "\n",
    "有两种方式可以使用容器镜像来训练一个自定义模型：\n",
    "\n",
    "- **使用谷歌云预构建的容器**。如果使用预构建的容器，你需要额外指定一个要安装到容器镜像中的Python包。这个Python包包含了你用于训练自定义模型的代码。\n",
    "\n",
    "- **使用自己的自定义容器镜像**。如果使用自己的容器，容器需要包含你用于训练自定义模型的代码。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train_custom_job_specification:prebuilt_container"
   },
   "source": [
    "### 准备您的自定义工作规范\n",
    "\n",
    "现在您的客户已经准备好了，您的第一步是为自定义培训工作创建一个工作规范。工作规范将包括以下内容：\n",
    "\n",
    "- `worker_pool_spec`：指定用于培训的机器类型和数量（单个或分布式）\n",
    "- `python_package_spec`：指定要与预构建容器一起安装的Python软件包。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train_custom_job_machine_specification"
   },
   "source": [
    "#### 准备你的机器规格\n",
    "\n",
    "现在为你的自定义训练作业定义机器规格。这告诉 Vertex AI 需要为训练提供什么类型的机器实例。\n",
    "- `machine_type`: 要提供的 GCP 实例的类型 — 例如，n1-standard-8。\n",
    "- `accelerator_type`: （如果有的话）硬件加速器的类型。在本教程中，如果之前设置变量 `TRAIN_GPU != None`，那么你正在使用 GPU；否则将使用 CPU。\n",
    "- `accelerator_count`: 加速器的数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u5hIFmP4OzvR"
   },
   "outputs": [],
   "source": [
    "if TRAIN_GPU:\n",
    "    machine_spec = {\n",
    "        \"machine_type\": TRAIN_COMPUTE,\n",
    "        \"accelerator_type\": TRAIN_GPU,\n",
    "        \"accelerator_count\": TRAIN_NGPU,\n",
    "    }\n",
    "else:\n",
    "    machine_spec = {\"machine_type\": TRAIN_COMPUTE, \"accelerator_count\": 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train_custom_job_disk_specification"
   },
   "source": [
    "准备您的磁盘规格\n",
    "\n",
    "（可选）现在为您的自定义训练作业定义磁盘规格。这告诉 Vertex 在每台机器实例上为训练提供什么类型和大小的磁盘。\n",
    "\n",
    "- `boot_disk_type`：SSD 或标准。SSD 更快，标准更便宜。默认为 SSD。\n",
    "- `boot_disk_size_gb`：磁盘大小（单位为 GB）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1MfgWif9OzvS"
   },
   "outputs": [],
   "source": [
    "DISK_TYPE = \"pd-ssd\"  # [ pd-ssd, pd-standard]\n",
    "DISK_SIZE = 200  # GB\n",
    "\n",
    "disk_spec = {\"boot_disk_type\": DISK_TYPE, \"boot_disk_size_gb\": DISK_SIZE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train_custom_job_worker_pool_specification:prebuilt_container"
   },
   "source": [
    "#### 定义工作池规范\n",
    "\n",
    "接下来，您需要为自定义训练任务定义工作池规范。工作池规范将由以下内容组成：\n",
    "\n",
    "- `replica_count`：要提供的该机器类型的实例数。\n",
    "- `machine_spec`：硬件规范。\n",
    "- `disk_spec`：（可选）磁盘存储规范。\n",
    "\n",
    "- `python_package`：要安装在 VM 实例上的 Python 训练包，以及要调用的 Python 模块，以及 Python 模块的命令行参数。\n",
    "\n",
    "现在让我们深入了解 Python 包规范：\n",
    "\n",
    "- `executor_image_spec`：这是为您的自定义训练任务配置的 Docker 镜像。\n",
    "\n",
    "- `package_uris`：这是要安装在提供实例上的 Python 训练包的位置（URIs）列表。这些位置需要位于 Cloud Storage 存储桶中。这些可以是单个 Python 文件或整个包的 zip（归档）文件。在后一种情况下，作业服务将解压缩（解档）内容到 Docker 镜像中。\n",
    "\n",
    "- `python_module`：用于运行自定义训练任务的 Python 模块（脚本）。在本例中，您将调用 `trainer.task.py` --请注意不需要附加 `.py` 后缀。\n",
    "\n",
    "- `args`：要传递给相应的 Python 模块的命令行参数。在本例中，您将设置：\n",
    "  - `\"--model-dir=\" + MODEL_URI`：存储模型工件的 Cloud Storage 位置。有两种方法可以告诉训练脚本保存模型工件的位置：\n",
    "      - 直接：将 Cloud Storage 位置作为命令行参数传递给训练脚本（设置变量 `DIRECT = True`），\n",
    "      - 间接：服务将 Cloud Storage 位置作为环境变量 `AIP_MODEL_DIR` 传递给训练脚本（设置变量 `DIRECT = False`）。在这种情况下，您在工作规范中告诉服务模型工件位置。\n",
    "  - `\"--epochs=\" + EPOCHS`：训练的时代数。\n",
    "  - `\"--distribute=\" + TRAIN_STRATEGY\"`：用于单个或分布式训练的训练分发策略。\n",
    "     - `\"single\"`：单个设备。\n",
    "     - `\"mirror\"`：单个计算实例上的所有 GPU 设备。\n",
    "     - `\"multi\"`：所有计算实例上的所有 GPU 设备。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PJCovykJOzvS"
   },
   "outputs": [],
   "source": [
    "JOB_NAME = \"custom_job_\" + TIMESTAMP\n",
    "MODEL_URI = f\"{BUCKET_URI}/{DELIVERABLE_PATH}/models/mobilenetv2-{DATASET_NAME}\"\n",
    "\n",
    "if not TRAIN_NGPU or TRAIN_NGPU < 2:\n",
    "    TRAIN_STRATEGY = \"single\"\n",
    "else:\n",
    "    TRAIN_STRATEGY = \"mirror\"\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "DIRECT = True\n",
    "if DIRECT:\n",
    "    CMDARGS = [\n",
    "        \"--model-dir=\" + MODEL_URI,\n",
    "        \"--epochs=\" + str(EPOCHS),\n",
    "        \"--distribute=\" + TRAIN_STRATEGY,\n",
    "    ]\n",
    "else:\n",
    "    CMDARGS = [\n",
    "        \"--epochs=\" + str(EPOCHS),\n",
    "        \"--distribute=\" + TRAIN_STRATEGY,\n",
    "    ]\n",
    "\n",
    "worker_pool_spec = [\n",
    "    {\n",
    "        \"replica_count\": 1,\n",
    "        \"machine_spec\": machine_spec,\n",
    "        \"disk_spec\": disk_spec,\n",
    "        \"python_package_spec\": {\n",
    "            \"executor_image_uri\": TRAIN_IMAGE,\n",
    "            \"package_uris\": [BUCKET_URI + \"/trainer_stl10.tar.gz\"],\n",
    "            \"python_module\": \"trainer.task\",\n",
    "            \"args\": CMDARGS,\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "assemble_custom_job_specification"
   },
   "source": [
    "### 组装工作规范\n",
    "\n",
    "现在组装自定义工作规范的完整描述：\n",
    "\n",
    "- `display_name`：您为这个自定义工作分配的可读名称。\n",
    "- `job_spec`：自定义工作的规范。\n",
    "    - `worker_pool_specs`：机器虚拟机实例的规范。\n",
    "    - `base_output_directory`：这告诉服务在云存储中保存模型工件的位置（当变量 `DIRECT = False` 时）。然后，服务将该位置作为环境变量 `AIP_MODEL_DIR` 传递给训练脚本，路径格式为：\n",
    "\n",
    "                <output_uri_prefix>/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qoY_r-2WOzvS"
   },
   "outputs": [],
   "source": [
    "if DIRECT:\n",
    "    job_spec = {\"worker_pool_specs\": worker_pool_spec}\n",
    "else:\n",
    "    job_spec = {\n",
    "        \"worker_pool_specs\": worker_pool_spec,\n",
    "        \"base_output_directory\": {\"output_uri_prefix\": MODEL_DIR},\n",
    "    }\n",
    "\n",
    "custom_job = {\"display_name\": JOB_NAME, \"job_spec\": job_spec}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "examine_training_package"
   },
   "source": [
    "### 检查培训包\n",
    "\n",
    "#### 包布局\n",
    "\n",
    "在开始培训之前，您将看一下如何为定制培训任务组装Python包。解压后，该包包含以下目录/文件布局。\n",
    "\n",
    "- PKG-INFO\n",
    "- README.md\n",
    "- setup.cfg\n",
    "- setup.py\n",
    "- trainer\n",
    "  - \\_\\_init\\_\\_.py\n",
    "  - task.py\n",
    "\n",
    "文件 `setup.cfg` 和 `setup.py` 是将包安装到 Docker 镜像的操作环境中的指令。\n",
    "\n",
    "文件 `trainer/task.py` 是执行定制培训任务的 Python 脚本。*注意*，当我们在工作池规范中提到它时，我们会用点(`trainer.task`)替换目录斜杠，并且去掉文件后缀(`.py`)。\n",
    "\n",
    "#### 包装配\n",
    "\n",
    "在接下来的单元格中，您将组装培训包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iNcSkITGOzvT"
   },
   "outputs": [],
   "source": [
    "# Make folder for Python training script\n",
    "! rm -rf custom\n",
    "! mkdir custom\n",
    "\n",
    "# Add package information\n",
    "! touch custom/README.md\n",
    "\n",
    "setup_cfg = \"[egg_info]\\n\\ntag_build =\\n\\ntag_date = 0\"\n",
    "! echo \"$setup_cfg\" > custom/setup.cfg\n",
    "\n",
    "setup_py = \"import setuptools\\n\\nsetuptools.setup(\\n\\n    install_requires=[\\n\\n        'tensorflow_datasets==4.0.1',\\n\\n    ],\\n\\n    packages=setuptools.find_packages())\"\n",
    "! echo \"$setup_py\" > custom/setup.py\n",
    "\n",
    "pkg_info = \"Metadata-Version: 1.0\\n\\nName: STL10 image classification\\n\\nVersion: 0.0.0\\n\\nSummary: Demonstration training script\\n\\nHome-page: www.google.com\\n\\nAuthor: Google\\n\\nAuthor-email: googler@google.com\\n\\nLicense: Public\\n\\nDescription: Demo\\n\\nPlatform: Vertex\"\n",
    "! echo \"$pkg_info\" > custom/PKG-INFO\n",
    "\n",
    "# Make the training subfolder\n",
    "! mkdir custom/trainer\n",
    "! touch custom/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "taskpy_contents:cifar10"
   },
   "source": [
    "### 创建任务脚本\n",
    "\n",
    "在下一个单元格中，你需要编写训练脚本 task.py 的内容。我们不会详细讨论，只是让你浏览一下。总结一下：\n",
    "\n",
    "- 从命令行中获取保存模型 artifacts 的目录（`--model_dir`），如果没有指定，则从环境变量 `AIP_MODEL_DIR` 中获取。\n",
    "- 从 TF Datasets（tfds）中加载 STL10 数据集。\n",
    "- 使用 TF.Keras 模型 API 构建模型。\n",
    "- 编译模型 (`compile()`)。\n",
    "- 根据参数 `args.distribute` 设置训练分发策略。\n",
    "- 根据参数 `args.epochs` 训练模型 (`fit()`)，设置 epochs 和 steps。\n",
    "- 将训练好的模型保存到指定的模型目录中（`save(args.model_dir)`）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YogsgSLpOzvT"
   },
   "outputs": [],
   "source": [
    "%%writefile custom/trainer/task.py\n",
    "# Single, Mirror and Multi-Machine Distributed Training for STL10\n",
    "\n",
    "# General\n",
    "import os\n",
    "import argparse\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s', datefmt='%d-%b-%y %H:%M:%S')\n",
    "\n",
    "# Training\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "tfds.disable_progress_bar()\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Variables\n",
    "N_CLASSES = 10\n",
    "SIZE = (224, 224)\n",
    "BUFFER_SIZE = 10\n",
    "BATCH_SIZE = 32\n",
    "SIZE_3D = (224, 224, 3)\n",
    "\n",
    "\n",
    "# Helpers\n",
    "# Get arguments\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--model-dir', dest='model_dir',\n",
    "                        default=os.getenv(\"AIP_MODEL_DIR\"), type=str, help='Model dir.')\n",
    "    parser.add_argument('--epochs', dest='epochs',\n",
    "                        default=2, type=int,\n",
    "                        help='Number of epochs.')\n",
    "    parser.add_argument('--distribute', dest='distribute', type=str, default='single',\n",
    "                        help='distributed training strategy')\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "# Single Machine, single compute device\n",
    "def get_strategy(distribute):\n",
    "    if distribute == 'single':\n",
    "        if tf.config.list_physical_devices('GPU'):\n",
    "            strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
    "        else:\n",
    "            strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
    "\n",
    "    # Single Machine, multiple compute device\n",
    "    elif distribute == 'mirror':\n",
    "        strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "    # Multiple Machine, multiple compute device\n",
    "    elif distribute == 'multi':\n",
    "        strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n",
    "    return strategy\n",
    "\n",
    "\n",
    "# Get train and validation datasets\n",
    "def get_train_val_ds():\n",
    "    split_ds = tfds.load(\n",
    "        'stl10',\n",
    "        split=[\"train\", \"test\"],\n",
    "        as_supervised=True,\n",
    "        shuffle_files=False,\n",
    "    )\n",
    "    train_ds, validation_ds = split_ds\n",
    "    return train_ds, validation_ds\n",
    "\n",
    "\n",
    "# Get preprocessed dataset\n",
    "def preprocess_dataset(train_ds, validation_ds):\n",
    "    train_ds = train_ds.map(lambda x, y: (tf.image.resize(x, SIZE), y))\n",
    "    validation_ds = validation_ds.map(lambda x, y: (tf.image.resize(x, SIZE), y))\n",
    "    return train_ds, validation_ds\n",
    "\n",
    "\n",
    "# Build the model\n",
    "def build_and_compile_model():\n",
    "    # Define data augmentation layers\n",
    "    data_augmentation = keras.Sequential(\n",
    "        [\n",
    "            layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
    "            layers.experimental.preprocessing.RandomRotation(0.1),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Define base model\n",
    "    base_model = keras.applications.MobileNetV2(\n",
    "        weights=\"imagenet\",\n",
    "        input_shape=SIZE_3D,\n",
    "        include_top=False,\n",
    "    )\n",
    "\n",
    "    # Freeze the base_model\n",
    "    base_model.trainable = False\n",
    "\n",
    "    # Create new model on top\n",
    "    inputs = keras.Input(shape=SIZE_3D)\n",
    "    x = data_augmentation(inputs)\n",
    "\n",
    "    # Normalize inputs for Pre-trained Xception weights\n",
    "    norm_layer = keras.layers.experimental.preprocessing.Normalization()\n",
    "    mean = np.array([127.5] * 3)\n",
    "    var = mean ** 2\n",
    "    x = norm_layer(x)\n",
    "    norm_layer.set_weights([mean, var, 0])\n",
    "\n",
    "    # Set inference mode\n",
    "    x = base_model(x, training=False)\n",
    "    x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = keras.layers.Dropout(0.2)(x)  # Regularize with dropout\n",
    "    outputs = keras.layers.Dense(N_CLASSES)(x)\n",
    "\n",
    "    # Build the model\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.summary()\n",
    "\n",
    "    # Compile the model \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Initialize arguments and strategy\n",
    "    logging.info('Initialize arguments and strategy')\n",
    "    train_args = get_args()\n",
    "\n",
    "    # Variables\n",
    "    strategy = get_strategy(train_args.distribute)\n",
    "    epochs = train_args.epochs\n",
    "    model_dir = train_args.model_dir\n",
    "    n_workers = strategy.num_replicas_in_sync\n",
    "    global_batch_size = BATCH_SIZE * n_workers\n",
    "\n",
    "    # Print versions\n",
    "    logging.info('Python Version = {}'.format(sys.version))\n",
    "    logging.info('TensorFlow Version = {}'.format(tf.__version__))\n",
    "    logging.info('Python Version = {}'.format(sys.version))\n",
    "\n",
    "    # Get TRAIN and VAL datasets\n",
    "    logging.info('Get train and val datasets')\n",
    "    train_ds, validation_ds = get_train_val_ds()\n",
    "\n",
    "    # Preprocess TRAIN and VAL datasets\n",
    "    logging.info('Get train and val datasets')\n",
    "    train_ds, validation_ds = preprocess_dataset(train_ds, validation_ds)\n",
    "    train_ds = train_ds.batch(global_batch_size).prefetch(buffer_size=BUFFER_SIZE)\n",
    "    validation_ds = validation_ds.batch(global_batch_size).prefetch(buffer_size=BUFFER_SIZE)\n",
    "\n",
    "    # Train model\n",
    "    logging.info('Build model')\n",
    "    with strategy.scope():\n",
    "        model = build_and_compile_model()\n",
    "\n",
    "    logging.info('Train model')\n",
    "    model.fit(train_ds, epochs=epochs, validation_data=validation_ds)\n",
    "    logging.info(f'Save model in {model_dir}')\n",
    "    model.save(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tarball_training_script"
   },
   "source": [
    "将培训脚本存储在您的云存储桶中\n",
    "\n",
    "接下来，您将培训文件夹打包成压缩的tar文件，然后存储在您的云存储桶中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zEx3hg05OzvT"
   },
   "outputs": [],
   "source": [
    "! rm -f custom.tar custom.tar.gz\n",
    "! tar cvf custom.tar custom\n",
    "! gzip custom.tar\n",
    "! gsutil cp custom.tar.gz $BUCKET_URI/trainer_stl10.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train_custom_job"
   },
   "source": [
    "### 训练模型\n",
    "\n",
    "现在开始在Vertex上训练您的自定义训练作业。使用这个辅助函数`create_custom_job`，它接受以下参数：\n",
    "\n",
    "-`custom_job`：自定义作业的规范。\n",
    "\n",
    "该辅助函数调用作业客户端服务的`create_custom_job`方法，带有以下参数：\n",
    "\n",
    "-`parent`：指向`数据集`，`模型`和`终端`资源的Vertex位置路径。\n",
    "-`custom_job`：自定义作业的规范。\n",
    "\n",
    "您将显示`response`对象中返回的一些字段，其中最感兴趣的两个是：\n",
    "\n",
    "`response.name`：分配给此自定义训练作业的Vertex完全限定标识符。您将保存此标识符以在后续步骤中使用。\n",
    "\n",
    "`response.state`：自定义训练作业的当前状态。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZfcnibeAOzvU"
   },
   "outputs": [],
   "source": [
    "def create_custom_job(custom_job):\n",
    "    response = clients[\"job\"].create_custom_job(parent=PARENT, custom_job=custom_job)\n",
    "    print(\"name:\", response.name)\n",
    "    print(\"display_name:\", response.display_name)\n",
    "    print(\"state:\", response.state)\n",
    "    print(\"create_time:\", response.create_time)\n",
    "    print(\"update_time:\", response.update_time)\n",
    "    return response\n",
    "\n",
    "\n",
    "response = create_custom_job(custom_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "job_id:response"
   },
   "source": [
    "现在获取您创建的自定义工作的唯一标识符。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "goXUwSIQOzvU"
   },
   "outputs": [],
   "source": [
    "# The full unique ID for the custom job\n",
    "job_id = response.name\n",
    "# The short numeric ID for the custom job\n",
    "job_short_id = job_id.split(\"/\")[-1]\n",
    "\n",
    "print(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "get_custom_job"
   },
   "source": [
    "获取有关自定义工作的信息\n",
    "\n",
    "接下来，使用这个辅助函数 `get_custom_job`，它接受以下参数：\n",
    "\n",
    "- `name`：自定义工作的 Vertex 完全限定标识符。\n",
    "\n",
    "辅助函数调用作业客户端服务的 `get_custom_job` 方法，带有以下参数：\n",
    "\n",
    "- `name`：自定义工作的 Vertex 完全限定标识符。\n",
    "\n",
    "如果您还记得，当调用 `create_custom_job` 方法时，在 `response.name` 字段中获取了自定义工作的 Vertex 完全限定标识符，并将标识符保存在变量 `job_id` 中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NecLamxVOzvV"
   },
   "outputs": [],
   "source": [
    "def get_custom_job(name, silent=False):\n",
    "    response = clients[\"job\"].get_custom_job(name=name)\n",
    "    if silent:\n",
    "        return response\n",
    "\n",
    "    print(\"name:\", response.name)\n",
    "    print(\"display_name:\", response.display_name)\n",
    "    print(\"state:\", response.state)\n",
    "    print(\"create_time:\", response.create_time)\n",
    "    print(\"update_time:\", response.update_time)\n",
    "    return response\n",
    "\n",
    "\n",
    "response = get_custom_job(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ijuYBvLOzvW"
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    response = get_custom_job(job_id, True)\n",
    "    if response.state != vertex_ai_v1beta1.JobState.JOB_STATE_SUCCEEDED:\n",
    "        print(\"Training job has not completed:\", response.state)\n",
    "        model_path_to_deploy = None\n",
    "        if response.state == vertex_ai_v1beta1.JobState.JOB_STATE_FAILED:\n",
    "            break\n",
    "    else:\n",
    "        if not DIRECT:\n",
    "            MODEL_DIR = MODEL_DIR + \"/model\"\n",
    "        model_path_to_deploy = MODEL_URI\n",
    "        print(\"Training Time:\", response.update_time - response.create_time)\n",
    "        break\n",
    "    time.sleep(60)\n",
    "\n",
    "print(\"model_to_deploy:\", model_path_to_deploy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6jqOH23PjGK"
   },
   "source": [
    "部署"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "load_saved_model"
   },
   "source": [
    "### 加载保存的模型\n",
    "\n",
    "您的模型以 TensorFlow SavedModel 格式存储在云存储存储桶中。现在从云存储存储桶加载它，然后您可以执行一些操作，如评估模型和进行预测。\n",
    "\n",
    "要加载模型，您可以使用 TF.Keras `model.load_model()` 方法，并向其传递保存模型的云存储路径 -- 由 `MODEL_DIR` 指定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6a8NKCrJOzvW"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(MODEL_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "how_serving_function_works"
   },
   "source": [
    "准备模型用于服务\n",
    "\n",
    "接下来，您将把您的 TF.Keras 模型从自定义作业上传到 Vertex AI 的 `Model` 服务，这将为您的自定义模型创建一个 Vertex `Model` 资源。在上传过程中，您需要定义一个服务函数，将数据转换为模型所需的格式。如果您将编码后的数据发送到 Vertex AI，您的服务函数将确保在将数据作为输入传递给模型之前，在模型服务器上对数据进行解码。\n",
    "\n",
    "服务函数的工作原理\n",
    "\n",
    "当您向在线预测服务器发送请求时，该请求将通过 HTTP 服务器接收。HTTP 服务器从 HTTP 请求内容体中提取预测请求。提取的预测请求将转发到服务函数。对于 Google 预构建的预测容器，请求内容将作为 `tf.string` 传递给服务函数。\n",
    "\n",
    "服务函数由两部分组成：\n",
    "\n",
    "- `预处理函数`：\n",
    "   - 将输入 (`tf.string`) 转换为底层模型的输入形状和数据类型（动态图）。\n",
    "   - 执行与训练底层模型期间相同的数据预处理 -- 例如，归一化，缩放等。\n",
    "- `后处理函数`：\n",
    "   - 将模型输出转换为接收应用程序期望的格式 -- 例如，压缩输出。\n",
    "   - 为接收应用程序打包输出 -- 例如，添加标题，创建 JSON 对象等。\n",
    "\n",
    "预处理和后处理函数均转换为静态图，然后与模型融合。底层模型的输出传递给后处理函数。后处理函数将转换/打包后的输出传回给 HTTP 服务器。HTTP 服务器将输出作为 HTTP 响应内容返回。\n",
    "\n",
    "在为 TF.Keras 模型构建服务函数时需要考虑的一个因素是它们作为静态图运行。这意味着您不能使用需要动态图的 TF 图操作。如果您这样做，您将在服务函数的编译过程中收到一个错误，指示您正在使用不受支持的 EagerTensor。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "serving_function_image:xai"
   },
   "source": [
    "#### 为图像数据创建服务函数\n",
    "\n",
    "为了将图像传递给预测服务，您需要将压缩的（例如JPEG）图像字节编码为base 64格式 - 这样可以使内容在通过网络传输二进制数据时得到安全保护。由于这个部署的模型期望输入数据为原始（未压缩）字节，您需要确保base 64编码的数据在传递给部署的模型之前转换回原始字节。\n",
    "\n",
    "为了解决这个问题，定义一个服务函数（`serving_fn`）并将其附加到模型作为预处理步骤。添加一个`@tf.function`装饰器，使服务函数与底层模型融合在一起（而不是在CPU上游处理）。\n",
    "\n",
    "当您发送预测或解释请求时，请求的内容会被base 64解码为一个Tensorflow字符串（`tf.string`），然后传递给服务函数（`serving_fn`）。服务函数将`tf.string`预处理为原始（未压缩）的numpy字节（`preprocess_fn`），以匹配模型的输入要求：\n",
    "- `io.decode_jpeg`- 解压缩JPG图像，返回一个具有三个通道（RGB）的Tensorflow张量。\n",
    "- `image.convert_image_dtype` - 将整数像素值更改为浮点32，并将像素数据重新缩放在0和1之间。\n",
    "- `image.resize` - 调整图像的尺寸以匹配模型的输入形状。\n",
    "\n",
    "此时，数据可以传递给模型（`m_call`）。\n",
    "\n",
    "#### XAI 签名\n",
    "\n",
    "当服务函数与底层模型一起保存（`tf.saved_model.save`）时，您需要指定服务函数的输入层为 `serving_default` 签名。\n",
    "\n",
    "对于XAI图像模型，您需要保存两个额外的签名来自服务函数：\n",
    "\n",
    "- `xai_preprocess`：服务函数中的预处理函数。\n",
    "- `xai_model`：用于调用模型的具体函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nwoYEzGLOzvX"
   },
   "outputs": [],
   "source": [
    "CONCRETE_INPUT = \"numpy_inputs\"\n",
    "\n",
    "\n",
    "def _preprocess(bytes_input):\n",
    "    \"\"\"\n",
    "    The preprocess function.\n",
    "    Args:\n",
    "        bytes_input: The input image in bytes.\n",
    "    Returns:\n",
    "        The preprocessed image in numpy array.\n",
    "    \"\"\"\n",
    "    decoded = tf.io.decode_jpeg(bytes_input, channels=3)\n",
    "    decoded = tf.image.convert_image_dtype(decoded, tf.float32)\n",
    "    resized = tf.image.resize(decoded, size=(224, 224))\n",
    "    rescale = tf.cast(resized, tf.float32)\n",
    "    return rescale\n",
    "\n",
    "\n",
    "@tf.function(input_signature=[tf.TensorSpec([None], tf.string)])\n",
    "def preprocess_fn(bytes_inputs):\n",
    "    \"\"\"\n",
    "    Preprocess the input image.\n",
    "    Args:\n",
    "        bytes_inputs: A list of raw image bytes.\n",
    "    Returns:\n",
    "        A list of preprocessed images.\n",
    "    \"\"\"\n",
    "    decoded_images = tf.nest.map_structure(\n",
    "        tf.stop_gradient, tf.map_fn(_preprocess, bytes_inputs, dtype=tf.float32)\n",
    "    )\n",
    "    return {CONCRETE_INPUT: decoded_images}\n",
    "\n",
    "\n",
    "@tf.function(\n",
    "    input_signature=[tf.TensorSpec([None], tf.string), tf.TensorSpec([None], tf.string)]\n",
    ")\n",
    "def serving_fn(id, bytes_inputs):\n",
    "    \"\"\"\n",
    "    This function is used to serve the embeddings.\n",
    "    Args:\n",
    "        id: The id of the input.\n",
    "        bytes_inputs: The input image.\n",
    "    Returns:\n",
    "        The output of the model.\n",
    "    \"\"\"\n",
    "    images = preprocess_fn(bytes_inputs)\n",
    "    embedding = m_call(**images)\n",
    "    return {\"id\": id, \"embedding\": embedding}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1K414cl3hlME"
   },
   "source": [
    "提取并上传用于索引创建的嵌入模型\n",
    "\n",
    "如上所述，您需要提供嵌入以便对数据集进行索引。在这种情况下，您可以跳过数据增强层并删除softmax层，以获取先前训练的模型中的嵌入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qNeutW2HmQW5"
   },
   "outputs": [],
   "source": [
    "embedding_model = keras.Sequential()\n",
    "for layer in model.layers[:-1]:  # go through until last layer\n",
    "    print(layer.name)\n",
    "    if \"sequential\" not in layer.name:  # skip data augmentation layer\n",
    "        embedding_model.add(layer)\n",
    "embedding_model.summary()\n",
    "probability_model = keras.Sequential([model, tf.keras.layers.Softmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O7ZQqOlbnpKp"
   },
   "outputs": [],
   "source": [
    "EMBEDDINGS_URI = (\n",
    "    f\"{BUCKET_URI}/{DELIVERABLE_PATH}/embeddings/mobilenetv2-{DATASET_NAME}\"\n",
    ")\n",
    "\n",
    "m_call = tf.function(embedding_model.call).get_concrete_function(\n",
    "    [tf.TensorSpec(shape=[None, 224, 224, 3], dtype=tf.float32, name=CONCRETE_INPUT)]\n",
    ")\n",
    "\n",
    "tf.saved_model.save(\n",
    "    embedding_model,\n",
    "    EMBEDDINGS_URI,\n",
    "    signatures={\n",
    "        \"serving_default\": serving_fn,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "serving_function_signature:xai"
   },
   "source": [
    "获取服务功能签名\n",
    "\n",
    "您可以通过重新加载模型到内存，并查询每个层对应的签名来获取模型的输入和输出层的签名。\n",
    "\n",
    "在进行预测请求时，您需要将请求路由到服务功能而不是模型，因此您需要知道服务功能的输入层名称 -- 这在您进行预测请求时会用到。\n",
    "\n",
    "您还需要知道服务功能的输入和输出层名称以构建解释元数据 -- 这将在后续讨论中详细讨论。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9rLm9QUkOzvY"
   },
   "outputs": [],
   "source": [
    "embedding_model_loaded = tf.saved_model.load(EMBEDDINGS_URI)\n",
    "\n",
    "serving_input = list(\n",
    "    embedding_model_loaded.signatures[\"serving_default\"]\n",
    "    .structured_input_signature[1]\n",
    "    .keys()\n",
    ")[0]\n",
    "print(\"Serving function input:\", serving_input)\n",
    "serving_output = list(\n",
    "    embedding_model_loaded.signatures[\"serving_default\"].structured_outputs.keys()\n",
    ")[0]\n",
    "print(\"Serving function output:\", serving_output)\n",
    "\n",
    "input_name = model.input.name\n",
    "print(\"Model input name:\", input_name)\n",
    "output_name = model.output.name\n",
    "print(\"Model output name:\", output_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4Y9WltYOwIE"
   },
   "source": [
    "### 上传训练数据\n",
    "\n",
    "接下来是上传训练数据。解释信息是从这些数据中提取的。你可以选择较少的批次以加快运行速度，但结果可能不够精确。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KrKfZB_xw0Qu"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(PREPROCESSED_DIR):\n",
    "    os.makedirs(PREPROCESSED_DIR, exist_ok=True)\n",
    "\n",
    "dataset_file = f\"{DATASET_NAME}-train-images.jsonl\"\n",
    "saved_jsonl_path = f\"{PREPROCESSED_DIR}/{dataset_file}\"\n",
    "input_tensor_name = \"bytes_inputs\"  # Must match the serving_fn definition\n",
    "\n",
    "num_batches = -1  # uses the entire dataset\n",
    "start = time.time()\n",
    "all_train_images, all_train_labels = extract_images_and_labels(\n",
    "    train_ds, num_batches=num_batches\n",
    ")\n",
    "end = time.time()\n",
    "print(f\"Time taken to process training data: {end - start:.5f} secs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ALNS_gFnTWjS"
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "with open(saved_jsonl_path, \"w\") as f:\n",
    "    for i, im in enumerate(all_train_images):\n",
    "        img_bytes = io.BytesIO()\n",
    "        image = Image.fromarray(im.astype(np.uint8))\n",
    "        image.save(img_bytes, format=\"PNG\")\n",
    "        json.dump(\n",
    "            {\n",
    "                \"id\": str(i),\n",
    "                \"bytes_inputs\": {\n",
    "                    \"b64\": base64.b64encode(img_bytes.getvalue()).decode(\"utf-8\")\n",
    "                },\n",
    "            },\n",
    "            f,\n",
    "        )\n",
    "        f.write(\"\\n\")\n",
    "! gsutil -m cp {saved_jsonl_path} {BUCKET_URI}\n",
    "end = time.time()\n",
    "print(f\"Time taken to create and upload the training data: {end - start:.5f} secs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "explanation_spec"
   },
   "source": [
    "基于示例的解释规范\n",
    "\n",
    "最后，您需要定义基于示例的解释。在进行预测时获得解释，您必须在将自定义模型上传到 Vertex“模型”资源时启用解释功能并设置相应的设置。这些设置被称为解释元数据，包括：\n",
    "\n",
    "- `parameters`：这是用于对模型进行解释的可解释性算法的规范。在本教程中，您将使用 `Examples`\n",
    "\n",
    "- `metadata`：这是如何在您的自定义模型上应用算法的规范。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6DJ2FcXJc-bI"
   },
   "source": [
    "#### 解释参数\n",
    "\n",
    "让我们首先深入了解解释性算法的设置。\n",
    "\n",
    "#### 基于示例\n",
    "\n",
    "基于示例的解释使得能够基于类比进行数据解释，适用于错误分析、模型调试以及新数据的批量标记。这可以带来更准确和健壮的模型，以及高效的数据标记流程。\n",
    "\n",
    "参数：\n",
    "\n",
    "- `examples`：这个参数允许定义返回提供数据集中最近邻的条件。\n",
    "\n",
    "通过基于示例的解释，您可以获得一个带有相关参数配置的新解释方法。以下是您需要定义的主要属性列表。\n",
    "\n",
    "- `dimensions`：嵌入的维度。\n",
    "- `approximateNeighborsCount`：要返回的邻居数。\n",
    "- `distanceMeasureType`：用于衡量示例相近程度的距离度量标准。您可以在``SQUARED_L2_DISTANCE、L1_DISTANCE、COSINE_DISTANCE 和 DOT_PRODUCT_DISTANCE``之间进行选择。\n",
    "- `featureNormType`：对嵌入进行归一化，使其具有单位长度。您可以在``UNIT_L2_NORM 或 NONE``之间进行选择。\n",
    "- `treeAhConfig`：控制近似质量和速度之间的权衡的参数。有关技术细节，请参阅论文。在幕后，它创建一个浅树，叶节点的数量由leafNodeEmbeddingCount控制，搜索召回/速度权衡由leafNodesToSearchPercent控制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sf_do58iXnK9"
   },
   "outputs": [],
   "source": [
    "DIMENSIONS = embedding_model.output.shape[1]\n",
    "DATASET_FILE_PATH = f\"{BUCKET_URI}/{dataset_file}\"\n",
    "\n",
    "NEAREST_NEIGHBOR_SEARCH_CONFIG = {\n",
    "    \"contentsDeltaUri\": \"\",\n",
    "    \"config\": {\n",
    "        \"dimensions\": DIMENSIONS,\n",
    "        \"approximateNeighborsCount\": 10,\n",
    "        \"distanceMeasureType\": \"SQUARED_L2_DISTANCE\",\n",
    "        \"featureNormType\": \"NONE\",\n",
    "        \"algorithmConfig\": {\n",
    "            \"treeAhConfig\": {\n",
    "                \"leafNodeEmbeddingCount\": 1000,\n",
    "                \"leafNodesToSearchPercent\": 100,\n",
    "            }\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "NUM_NEIGHBORS_TO_RETURN = 10\n",
    "\n",
    "EXAMPLES = vertex_ai_v1beta1.Examples(\n",
    "    nearest_neighbor_search_config=NEAREST_NEIGHBOR_SEARCH_CONFIG,\n",
    "    gcs_source=io_pb2.GcsSource(uris=[DATASET_FILE_PATH]),\n",
    "    neighbor_count=NUM_NEIGHBORS_TO_RETURN,\n",
    ")\n",
    "\n",
    "PARAMETERS = vertex_ai_v1beta1.ExplanationParameters(examples=EXAMPLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Dn_QepydOFv"
   },
   "source": [
    "#### 解释元数据\n",
    "\n",
    "让我们首先深入了解解释元数据，它包括：\n",
    "\n",
    "- `outputs`：它由输出名称到输出元数据的映射表示。在这种情况下，您期望得到嵌入。\n",
    "\n",
    "- `inputs`：它由一个特征的输入的元数据表示。在这种情况下，您有编码图像和与之关联的ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tSMePRWgUaYs"
   },
   "outputs": [],
   "source": [
    "# for encoding parameter, 1 stands for 'IDENTITY'\n",
    "\n",
    "EXPLANATION_INPUTS = {\n",
    "    \"my_input\": vertex_ai_v1beta1.ExplanationMetadata.InputMetadata(\n",
    "        {\n",
    "            \"input_tensor_name\": input_tensor_name,\n",
    "            \"encoding\": vertex_ai_v1beta1.ExplanationMetadata.InputMetadata.Encoding(1),\n",
    "            \"modality\": \"image\",\n",
    "        }\n",
    "    ),\n",
    "    \"id\": vertex_ai_v1beta1.ExplanationMetadata.InputMetadata(\n",
    "        {\n",
    "            \"input_tensor_name\": \"id\",\n",
    "            \"encoding\": vertex_ai_v1beta1.ExplanationMetadata.InputMetadata.Encoding(1),\n",
    "        }\n",
    "    ),\n",
    "}\n",
    "\n",
    "EXPLANATION_OUTPUTS = {\n",
    "    \"embedding\": vertex_ai_v1beta1.ExplanationMetadata.OutputMetadata(\n",
    "        {\"output_tensor_name\": \"embedding\"}\n",
    "    )\n",
    "}\n",
    "\n",
    "EXPLANATION_META_CONFIG = vertex_ai_v1beta1.ExplanationMetadata(\n",
    "    inputs=EXPLANATION_INPUTS, outputs=EXPLANATION_OUTPUTS\n",
    ")\n",
    "\n",
    "EXPLANATION_SPEC = vertex_ai_v1beta1.ExplanationSpec(\n",
    "    parameters=PARAMETERS, metadata=EXPLANATION_META_CONFIG\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upload_the_model:explanation"
   },
   "source": [
    "### 上传模型\n",
    "\n",
    "使用这个辅助函数`upload_model`来上传您的模型，以SavedModel格式存储，并将其上传到`Model`服务，该服务将实例化一个Vertex `Model`资源实例以用于您的模型。一旦完成，您可以像任何其他Vertex `Model`资源实例一样使用`Model`资源实例，例如部署到用于提供预测的`Endpoint`资源。\n",
    "\n",
    "现在让我们深入研究Vertex模型规范`model`。这是一个包含以下字段的字典对象：\n",
    "\n",
    "- `display_name`：`Model`资源的人类可读名称。\n",
    "- `metadata_schema_uri`：由于您的模型是在没有Vertex `Dataset`资源的情况下构建的，因此您可以将其留空(`''`)。\n",
    "- `artificat_uri`：存储嵌入向量的Cloud Storage路径，以SavedModel格式存储。\n",
    "- `container_spec`：这是要安装在`Endpoint`资源上的Docker容器规范，`Model`资源将从中提供预测。使用之前设置的变量`DEPLOY_GPU != None`来使用GPU；否则，只会分配CPU。\n",
    "- `explanation_spec`：这是启用模型可解释性的规范。\n",
    "\n",
    "辅助函数在一个配置中传递这些参数，并调用`Model`客户端服务的`upload_model`方法，该方法接受以下参数：\n",
    "\n",
    "- `parent`：`Dataset`、`Model`和`Endpoint`资源的Vertex根路径。\n",
    "- `model`：Vertex `Model`资源实例的规范。\n",
    "\n",
    "将模型上传到Vertex Model资源将返回一个长时间运行的操作，因为可能需要一些时间。您可以调用response.result()，这是一个同步调用，当Vertex Model资源准备就绪时将返回。\n",
    "\n",
    "辅助函数返回相应的Vertex模型实例`upload_model_response.model`的完全限定标识符。您将保存此标识符，以便在后续步骤中使用变量`model_to_deploy_id`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xATLh3ADhK-w"
   },
   "source": [
    "定义服务容器配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z5ujqbVxhK-w"
   },
   "outputs": [],
   "source": [
    "DEPLOY_IMAGE_URI = \"gcr.io/cloud-aiplatform/prediction/tf2-cpu.2-5:latest\"\n",
    "\n",
    "CONTAINER_CONFIG = {\"image_uri\": DEPLOY_IMAGE_URI}\n",
    "\n",
    "CONTAINER_SPEC = vertex_ai_v1beta1.ModelContainerSpec(CONTAINER_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j0CR8R1KhTBV"
   },
   "source": [
    "定义模型配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7byz4C8wiRRj"
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = f\"similarity-{DATASET_NAME}-{TIMESTAMP}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xh83zjREhTBW"
   },
   "outputs": [],
   "source": [
    "MODEL_CONFIGURATION = {\n",
    "    \"display_name\": MODEL_NAME,\n",
    "    \"artifact_uri\": EMBEDDINGS_URI,\n",
    "    \"metadata_schema_uri\": \"\",\n",
    "    \"container_spec\": CONTAINER_SPEC,\n",
    "    \"explanation_spec\": EXPLANATION_SPEC,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b35eb7277ab4"
   },
   "source": [
    "这个步骤可能需要长达一个小时的时间才能完成。目前，还没有简单的方法来监控这个进展。暴露运行日志和作业状态的功能计划在不久的将来（预览中）实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kmFqtIO5OzvZ"
   },
   "outputs": [],
   "source": [
    "def upload_model(model_configuration):\n",
    "\n",
    "    model = vertex_ai_v1beta1.Model(\n",
    "        display_name=model_configuration[\"display_name\"],\n",
    "        artifact_uri=model_configuration[\"artifact_uri\"],\n",
    "        metadata_schema_uri=model_configuration[\"metadata_schema_uri\"],\n",
    "        explanation_spec=model_configuration[\"explanation_spec\"],\n",
    "        container_spec=model_configuration[\"container_spec\"],\n",
    "    )\n",
    "\n",
    "    response = clients[\"model\"].upload_model(parent=PARENT, model=model)\n",
    "    print(\"Long running operation:\", response.operation.name)\n",
    "    upload_model_response = response.result()\n",
    "    print(\"upload_model_response\")\n",
    "    print(\" model:\", upload_model_response.model)\n",
    "    return upload_model_response.model\n",
    "\n",
    "\n",
    "uploaded_model_id = upload_model(MODEL_CONFIGURATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "get_model"
   },
   "source": [
    "获取`Model`资源信息\n",
    "\n",
    "现在让我们仅获取您的模型的模型信息。使用这个辅助函数`get_model`，带有以下参数：\n",
    "\n",
    "- `name`：`Model`资源的Vertex唯一标识符。\n",
    "\n",
    "这个辅助函数调用Vertex `Model`客户端服务的方法`get_model`，带有以下参数：\n",
    "\n",
    "- `name`：`Model`资源的Vertex唯一标识符。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RtuXNfLdOzva"
   },
   "outputs": [],
   "source": [
    "def get_model(name):\n",
    "    response = clients[\"model\"].get_model(name=name)\n",
    "    print(response)\n",
    "\n",
    "\n",
    "get_model(uploaded_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_endpoint:custom"
   },
   "source": [
    "### 部署 `Model` 资源\n",
    "\n",
    "现在部署训练好的 Vertex 定制 `Model` 资源。这需要两个步骤：\n",
    "\n",
    "1. 为部署 `Model` 资源创建一个 `Endpoint` 资源。\n",
    "\n",
    "2. 将 `Model` 资源部署到 `Endpoint` 资源中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_endpoint"
   },
   "source": [
    "#### 创建一个 `Endpoint` 资源\n",
    "\n",
    "使用这个辅助函数 `create_endpoint` 来创建一个用于部署模型以提供预测的端点，以下是参数：\n",
    "\n",
    "- `display_name`：对于 `Endpoint` 资源的人类可读的名称。\n",
    "\n",
    "辅助函数使用端点客户端服务的 `create_endpoint` 方法，该方法接受以下参数：\n",
    "\n",
    "- `display_name`：对于 `Endpoint` 资源的人类可读的名称。\n",
    "\n",
    "创建一个 `Endpoint` 资源会返回一个长时间运行的操作，因为可能需要一些时间来为服务提供资源进行预伟划。您调用 `response.result()`，这是一个同步调用，当端点资源准备就绪时将返回。辅助函数返回 `Endpoint` 资源的 Vertex 全限定标识符：`response.name`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Qzc7H0jOzva"
   },
   "outputs": [],
   "source": [
    "ENDPOINT_NAME = f\"similarity-{DATASET_NAME}-endpoint-{TIMESTAMP}\"\n",
    "DESCRIPTION = \"An endpoint for the similarity model\"\n",
    "LABELS = {\"env\": ENVIRON, \"status\": \"online\"}\n",
    "\n",
    "\n",
    "def create_endpoint(display_name, description, labels):\n",
    "    endpoint = {\n",
    "        \"display_name\": display_name,\n",
    "        \"description\": description,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "    response = clients[\"endpoint\"].create_endpoint(parent=PARENT, endpoint=endpoint)\n",
    "    print(\"Long running operation:\", response.operation.name)\n",
    "\n",
    "    result = response.result()\n",
    "    print(\"result\")\n",
    "    print(\" name:\", result.name)\n",
    "    print(\" display_name:\", result.display_name)\n",
    "    print(\" description:\", result.description)\n",
    "    print(\" labels:\", result.labels)\n",
    "    print(\" create_time:\", result.create_time)\n",
    "    print(\" update_time:\", result.update_time)\n",
    "    return result\n",
    "\n",
    "\n",
    "result = create_endpoint(ENDPOINT_NAME, DESCRIPTION, LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "endpoint_id:result"
   },
   "source": [
    "现在获得您创建的“Endpoint”资源的唯一标识符。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4pWgr2KkOzva"
   },
   "outputs": [],
   "source": [
    "# The full unique ID for the endpoint\n",
    "endpoint_id = result.name\n",
    "# The short numeric ID for the endpoint\n",
    "endpoint_short_id = endpoint_id.split(\"/\")[-1]\n",
    "\n",
    "print(endpoint_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instance_scaling"
   },
   "source": [
    "#### 计算实例扩展\n",
    "\n",
    "在处理在线预测请求时，您有几种选择来扩展计算实例：\n",
    "\n",
    "- 单个实例：在线预测请求在单个计算实例上处理。\n",
    "  - 将计算实例的最小(`MIN_NODES`)和最大(`MAX_NODES`)数量设置为一。\n",
    "\n",
    "- 手动扩展：在线预测请求会分配到您手动指定的固定数量的计算实例上。\n",
    "  - 将计算实例的最小(`MIN_NODES`)和最大(`MAX_NODES`)数量设置为相同数量的节点。当模型首次部署到实例上时，固定数量的计算实例将被提供，并且在线预测请求会均匀分布在它们之间。\n",
    "\n",
    "- 自动扩展：在线预测请求会分配到可扩展数量的计算实例上。\n",
    "  - 将计算实例的最小(`MIN_NODES`)数量设置为在模型首次部署时提供的计算实例，并在负载情况下予以取消提供，并将最大(`MAX_NODES`)数量设置为根据负载条件提供的计算实例数量。\n",
    "\n",
    "计算实例的最小数量对应于`min_replica_count`字段，最大数量对应于`max_replica_count`字段，这是在您的后续部署请求中的设置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8H20ToPEOzva"
   },
   "outputs": [],
   "source": [
    "MIN_NODES = 1\n",
    "MAX_NODES = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "deploy_model:dedicated,v1beta1"
   },
   "source": [
    "#### 部署 `Model` 资源到 `Endpoint` 资源\n",
    "\n",
    "使用这个辅助函数 `deploy_model` 将模型部署到您为提供预测创建的端点，使用以下参数：\n",
    "\n",
    "- `model`: 要从训练流水线上传（部署）的 `Model` 资源的 Vertex 完全限定标识符。\n",
    "- `deploy_model_display_name`: 部署模型的可读名称。\n",
    "- `endpoint`: 要将 `Model` 资源部署到的 Vertex 完全限定的 `Endpoint` 资源标识符。\n",
    "\n",
    "辅助函数调用 `Endpoint` 客户端服务的方法 `deploy_model`，该方法接受以下参数：\n",
    "\n",
    "- `endpoint`: 要将 `Model` 资源部署到的 Vertex 完全限定的 `Endpoint` 资源标识符。\n",
    "- `deployed_model`: 部署模型的要求。\n",
    "- `traffic_split`: 端点中流向此模型的流量百分比，指定为一个或多个键/值对的字典。\n",
    "   - 如果只有一个模型，则指定为 **{ \"0\": 100 }**，其中 \"0\" 指代要上传的此模型，100 表示流量的 100%。\n",
    "   - 如果端点上存在要分配流量的现有模型，则指定为，其中 `model_id` 是已部署到端点的模型的模型标识符。百分比必须加起来为 100。\n",
    "\n",
    "           { \"0\": percent, model_id: percent, ... }\n",
    "\n",
    "现在让我们更深入地了解 `deployed_model` 参数。该参数被指定为一个 Python 字典，具有最小所需字段：\n",
    "\n",
    "- `model`: 要部署的（上传的） `Model` 资源的 Vertex 完全限定标识符。\n",
    "- `display_name`: 部署模型的可读名称。\n",
    "- `dedicated_resources`: 指的是为提供预测请求而扩展的计算实例（副本）数量。\n",
    "  - `machine_spec`: 要预配的计算实例。使用您之前设置的变量 `DEPLOY_GPU != None` 来使用 GPU；否则只分配 CPU。\n",
    "  - `min_replica_count`: 初始预配的计算实例数量，您之前设置为变量 `MIN_NODES`。\n",
    "  - `max_replica_count`: 可扩展的计算实例最大数量，您之前设置为变量 `MAX_NODES`。\n",
    "- `enable_container_logging`: 这启用容器事件的日志记录，例如执行失败（默认情况下禁用容器日志记录）。通常在调试部署时启用容器日志记录，然后在部署到生产中时禁用。\n",
    "\n",
    "#### 流量分配\n",
    "\n",
    "现在让我们更深入地了解 `traffic_split` 参数。该参数被指定为一个 Python 字典。一开始可能有点困惑。让我解释一下，您可以将模型的多个实例部署到一个端点，然后设置每个实例应该接收的流量百分比。\n",
    "\n",
    "为什么要这样做呢？也许您已经在生产中部署了之前的版本 — 让我们称其为 v1。您在 v2 上得到了更好的模型评估，但在将其部署到生产环境之前，您无法确定它是否真的更好。因此，在流量分配的情况下，您可能希望将 v2 部署到与 v1 相同的端点，但只分配 10% 的流量给它。这样，您可以在不干扰大多数用户的情况下监控其表现，直到最终做出决定。\n",
    "\n",
    "#### 响应\n",
    "\n",
    "该方法返回一个长时间运行的操作 `response`。我们将通过调用 `response.result()` 同步等待操作完成，直到模型部署完毕。如果这是第一次将模型部署到端点，则可能需要额外几分钟来完成资源的预配。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wtq7Xyn2Ozvb"
   },
   "outputs": [],
   "source": [
    "DEPLOYED_NAME = f\"similarity-{DATASET_NAME}-deployed-{TIMESTAMP}\"\n",
    "\n",
    "\n",
    "def deploy_model(\n",
    "    model, deployed_model_display_name, endpoint, traffic_split={\"0\": 100}\n",
    "):\n",
    "\n",
    "    if DEPLOY_GPU:\n",
    "        machine_spec = {\n",
    "            \"machine_type\": DEPLOY_COMPUTE,\n",
    "            \"accelerator_type\": DEPLOY_GPU,\n",
    "            \"accelerator_count\": DEPLOY_NGPU,\n",
    "        }\n",
    "    else:\n",
    "        machine_spec = {\n",
    "            \"machine_type\": DEPLOY_COMPUTE,\n",
    "            \"accelerator_count\": 0,\n",
    "        }\n",
    "\n",
    "    deployed_model = {\n",
    "        \"model\": model,\n",
    "        \"display_name\": deployed_model_display_name,\n",
    "        \"dedicated_resources\": {\n",
    "            \"min_replica_count\": MIN_NODES,\n",
    "            \"max_replica_count\": MAX_NODES,\n",
    "            \"machine_spec\": machine_spec,\n",
    "        },\n",
    "        \"enable_container_logging\": False,\n",
    "    }\n",
    "\n",
    "    response = clients[\"endpoint\"].deploy_model(\n",
    "        endpoint=endpoint, deployed_model=deployed_model, traffic_split=traffic_split\n",
    "    )\n",
    "\n",
    "    print(\"Long running operation:\", response.operation.name)\n",
    "    result = response.result()\n",
    "    print(\"result\")\n",
    "    deployed_model = result.deployed_model\n",
    "    print(\" deployed_model\")\n",
    "    print(\"  id:\", deployed_model.id)\n",
    "    print(\"  model:\", deployed_model.model)\n",
    "    print(\"  display_name:\", deployed_model.display_name)\n",
    "    print(\"  create_time:\", deployed_model.create_time)\n",
    "\n",
    "    return deployed_model.id\n",
    "\n",
    "\n",
    "deployed_model_id = deploy_model(uploaded_model_id, DEPLOYED_NAME, endpoint_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "make_prediction"
   },
   "source": [
    "## 查询类似的例子\n",
    "\n",
    "现在对部署的模型进行在线预测，使用验证数据集的样本来获取类似的例子。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAFuNUq5yr5s"
   },
   "source": [
    "准备验证数据\n",
    "我们将为这些数据发布查询。为了演示目的，我们将选择完整验证数据集的一个小子集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-BgUO1dEzNvU"
   },
   "outputs": [],
   "source": [
    "val_dataset_file = f\"{DATASET_NAME}-val-images.jsonl\"\n",
    "saved_val_jsonl_path = f\"{PREPROCESSED_DIR}/{val_dataset_file}\"\n",
    "\n",
    "num_batches = 10\n",
    "start = time.time()\n",
    "val_images, val_labels = extract_images_and_labels(\n",
    "    validation_ds, num_batches=num_batches\n",
    ")\n",
    "end = time.time()\n",
    "print(f\"Time taken to process validation data: {end - start:.5f} secs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "prepare_test_item:test,image"
   },
   "source": [
    "### 准备请求内容\n",
    "您将以压缩的PNG图像形式发送STL10图像，而不是原始未压缩的字节。您还将字节编码为base 64——这样在通过网络传输二进制数据时，内容就不会被修改。您需要告诉服务二进制代码您的模型部署到了哪里，内容已经被base 64编码，以便它能在服务二进制代码的另一端解码。\n",
    "\n",
    "预测请求中的每个实例都是以下形式的字典条目：\n",
    "\n",
    "{`id`:, `bytes_inputs`: {'b64': content}}\n",
    "\n",
    "- `id`：与图像相关联的唯一标识符。\n",
    "- `bytes_inputs`：包含解码输入的映射。\n",
    "- `'b64'`：指示内容已经以base 64编码的键。\n",
    "- `content`：压缩的JPG图像字节，作为base 64编码的字符串。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wYoLoli7pL2v"
   },
   "outputs": [],
   "source": [
    "val_data = []\n",
    "\n",
    "for i, im in enumerate(val_images):\n",
    "    img_bytes = io.BytesIO()\n",
    "    image = Image.fromarray(im.astype(np.uint8))\n",
    "    image.save(img_bytes, format=\"PNG\")\n",
    "    instance = {\n",
    "        \"id\": str(i),\n",
    "        \"bytes_inputs\": {\"b64\": base64.b64encode(img_bytes.getvalue()).decode(\"utf-8\")},\n",
    "    }\n",
    "    val_data.append(instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "send_explain_request:image"
   },
   "source": [
    "### 发送带有解释请求的预测\n",
    "\n",
    "好的，现在你有了一个测试图片。使用这个辅助函数 `explain_image`，它接受以下参数：\n",
    "\n",
    "- `image`: 作为 numpy 数组的测试图片数据列表。\n",
    "- `endpoint`: `Model` 资源部署的 `Endpoint` 资源的 Vertex 完全限定标识符。\n",
    "- `parameters_dict`: 用于服务的额外参数。\n",
    "- `deployed_model_id`: 当在端点上部署了多个模型时，部署模型的 Vertex 完全限定标识符。否则，如果只有一个模型被部署，可以设为 `None`。\n",
    "\n",
    "这个函数使用预测客户端服务，并以以下参数调用 `explain` 方法：\n",
    "\n",
    "- `endpoint`: `Model` 资源部署的 `Endpoint` 资源的 Vertex 完全限定标识符。\n",
    "- `instances`: 要预测和解释的实例（编码的图片）列表。\n",
    "- `parameters`: 用于服务的额外参数。\n",
    "- `deployed_model_id`: 当在端点上部署了多个模型时，部署模型的 Vertex 完全限定标识符。否则，如果只有一个模型被部署，可以设为 `None`。\n",
    "\n",
    "由于 `predict()` 服务可以接受多个图片（实例），因此你将把单个图片作为一个图片列表发送。最后一步，你将实例列表打包成谷歌的 protobuf 格式 -- 这就是我们传递给 `explain()` 服务的内容。\n",
    "\n",
    "`response` 对象返回一个列表，列表中的每个元素对应请求中的相应图片。你将在每个预测的输出中看到：\n",
    "\n",
    "- `deployed_model_id` -- 执行预测/解释的模型的 Vertex 完全限定标识符。\n",
    "- `predictions` -- 对十个类别的每个预测的置信水平 (`predictions`)，在 0 到 1 之间。\n",
    "- `explanations` -- 每个特征如何影响预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "429YA19P14k7"
   },
   "outputs": [],
   "source": [
    "def explain_image(formatted_data, endpoint, parameters, deployed_model_id):\n",
    "\n",
    "    # The format of each instance should conform to the deployed model's prediction input schema.\n",
    "    instances_list = formatted_data\n",
    "    instances = [\n",
    "        json_format.ParseDict(instance, Value()) for instance in instances_list\n",
    "    ]\n",
    "\n",
    "    response = clients[\"prediction\"].explain(\n",
    "        endpoint=endpoint,\n",
    "        instances=instances,\n",
    "        parameters=parameters,\n",
    "        deployed_model_id=deployed_model_id,\n",
    "    )\n",
    "    print(\"response\")\n",
    "    print(\" deployed_model_id:\", response.deployed_model_id)\n",
    "    predictions = response.predictions\n",
    "    print(\"predictions\")\n",
    "    for prediction in predictions:\n",
    "        print(\" prediction:\", prediction)\n",
    "\n",
    "    explanations = response.explanations\n",
    "    print(\"explanations\")\n",
    "    for explanation in explanations:\n",
    "        print(\" explanation:\", explanation)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m76sOJ4803Qe"
   },
   "outputs": [],
   "source": [
    "# Update input and reformat to match the expected schema.\n",
    "BATCH_SIZE = (\n",
    "    8  # The request payload has a size limit so we need to subbatch our request\n",
    ")\n",
    "NUM_VAL_DATA = 35\n",
    "\n",
    "all_neighbors = []\n",
    "\n",
    "for data_idx in range(0, NUM_VAL_DATA, BATCH_SIZE):\n",
    "    end_idx = min(data_idx + BATCH_SIZE, NUM_VAL_DATA)\n",
    "    formatted_data = val_data[data_idx:end_idx]\n",
    "    response = explain_image(formatted_data, endpoint_id, None, deployed_model_id)\n",
    "    all_neighbors = (\n",
    "        all_neighbors + json_format.MessageToDict(response._pb)[\"explanations\"]\n",
    "    )\n",
    "\n",
    "print(f\"\\nExamples processed: {len(all_neighbors)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RZiJ1ykWIAWR"
   },
   "source": [
    "### 保存输入ID和对应的邻居\n",
    "\n",
    "对于您发送的每个输入图像，我们创建一个包含对应邻居的字典。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ILMgp_4LH9TO"
   },
   "outputs": [],
   "source": [
    "# Save input ids and the corresponding neighbors\n",
    "data_with_neighbors = []\n",
    "input_data_list = val_data[:NUM_VAL_DATA]\n",
    "\n",
    "for i, input_data in enumerate(input_data_list):\n",
    "    neighbor_dict = all_neighbors[i]\n",
    "    neighbor_dict[\"input\"] = input_data[\"id\"]\n",
    "    data_with_neighbors.append(neighbor_dict)\n",
    "\n",
    "DEBUG = False\n",
    "if DEBUG:\n",
    "    val_idx = 0\n",
    "    print(data_with_neighbors[val_idx])\n",
    "    print(data_with_neighbors[val_idx][\"neighbors\"])\n",
    "    print(data_with_neighbors[val_idx][\"input\"])\n",
    "    print(len(data_with_neighbors[val_idx][\"neighbors\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V9Gv-HxLlDOq"
   },
   "source": [
    "### 用解释可视化图像\n",
    "\n",
    "在以下表示中，您将看到每个发送图像的API生成的十个更接近的示例，根据您定义的距离。\n",
    "\n",
    "正如您可以验证的那样，尽管“示例索引”结果接近于分类在同一类别的图像，但在某些情况下，模型错误地识别类别。您可以通过利用距离轻松可视化它们。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x79xz2XrxaNO"
   },
   "outputs": [],
   "source": [
    "val_img_indices = [1, 2, 10, 20, 25, 30]  # images to visually explore\n",
    "for val_img_idx in val_img_indices:\n",
    "    if val_img_idx > NUM_VAL_DATA - 1:\n",
    "        raise ValueError(\n",
    "            f\"Data index {val_img_idx} does not exist in the requested explanations\"\n",
    "        )\n",
    "    plot_input_and_neighbors(\n",
    "        val_img_idx,\n",
    "        all_train_images,\n",
    "        val_images,\n",
    "        all_train_labels,\n",
    "        val_labels,\n",
    "        label_index_to_name,\n",
    "        data_with_neighbors,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aLmS2M6dWsAI"
   },
   "source": [
    "进一步探索\n",
    "如果您想要继续探索，以下是一些想法：\n",
    "1. 隔离模型出错的测试点（将猫误标为鸟），并可视化基于示例的解释，看看是否可以找到任何共同模式。\n",
    "2. 如果通过这种分析，您发现您的训练数据缺少一些典型情况（猫的俯视图像），您可以尝试将这些图像添加到数据集中，看看是否可以提高模型性能。\n",
    "3. [微调](https://keras.io/guides/transfer_learning/)模型的较低层，看看是否可以通过让模型学习更好的潜在表示来提高基于示例的解释的质量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "undeploy_model"
   },
   "source": [
    "## 下线`Model`资源\n",
    "\n",
    "现在从正在提供服务的`Endpoint`资源中下线您的`Model`资源。使用这个辅助函数`undeploy_model`，它接受以下参数：\n",
    "\n",
    "- `deployed_model_id`：当`Model`资源部署到服务端点时，终端服务返回的模型部署标识符。\n",
    "- `endpoint`：`Model`部署到的`Endpoint`资源的Vertex完全限定标识符。\n",
    "\n",
    "此函数调用终端客户服务的`undeploy_model`方法，具有以下参数：\n",
    "\n",
    "- `deployed_model_id`：当`Model`资源部署时，终端服务返回的模型部署标识符。\n",
    "- `endpoint`：`Model`资源部署到的`Endpoint`资源的Vertex完全限定标识符。\n",
    "- `traffic_split`：如何在`Endpoint`资源上分配流量给其余部署的模型。\n",
    "\n",
    "由于这是在`Endpoint`资源上唯一部署的模型，您可以通过将其设置为空来简单地留下`traffic_split`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RgKXH4qa4-5p"
   },
   "outputs": [],
   "source": [
    "def undeploy_model(deployed_model_id, endpoint):\n",
    "    response = clients[\"endpoint\"].undeploy_model(\n",
    "        endpoint=endpoint, deployed_model_id=deployed_model_id, traffic_split={}\n",
    "    )\n",
    "    print(response)\n",
    "\n",
    "\n",
    "undeploy_model(deployed_model_id, endpoint_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleanup"
   },
   "source": [
    "清理工作\n",
    "\n",
    "要清理本项目中使用的所有GCP资源，您可以删除用于教程的[GCP项目](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects)。\n",
    "\n",
    "否则，您可以删除在本教程中创建的单个资源：\n",
    "\n",
    "- 数据集\n",
    "- 流水线\n",
    "- 模型\n",
    "- 端点\n",
    "- 批处理作业\n",
    "- 自定义作业\n",
    "- 超参数调整作业\n",
    "- Cloud存储桶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6vBmkZzP4-5s"
   },
   "outputs": [],
   "source": [
    "delete_model = True\n",
    "delete_endpoint = True\n",
    "delete_customjob = True\n",
    "delete_bucket = True\n",
    "\n",
    "# Delete the model using the Vertex fully qualified identifier for the model\n",
    "try:\n",
    "    if delete_model and \"model_to_deploy_id\" in globals():\n",
    "        clients[\"model\"].delete_model(name=deployed_model_id)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# Delete the endpoint using the Vertex fully qualified identifier for the endpoint\n",
    "try:\n",
    "    if delete_endpoint and \"endpoint_id\" in globals():\n",
    "        clients[\"endpoint\"].delete_endpoint(name=endpoint_id)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# Delete the custom job using the Vertex fully qualified identifier for the custom job\n",
    "try:\n",
    "    if delete_customjob and \"job_id\" in globals():\n",
    "        clients[\"job\"].delete_custom_job(name=job_id)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# Delete bucket\n",
    "if delete_bucket and \"BUCKET_URI\" in globals():\n",
    "    ! gsutil rm -r $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dc1622c1fd2f"
   },
   "outputs": [],
   "source": [
    "! rm custom.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YnT9ASjiocJN"
   },
   "outputs": [],
   "source": [
    "! rm -Rf {DATA_PATH} {DELIVERABLE_PATH} custom"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "showcase_custom_image_classification_online_explain_example_based_api.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
