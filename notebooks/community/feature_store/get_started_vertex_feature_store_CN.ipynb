{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "copyright"
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title:generic,gcp"
   },
   "source": [
    "# 在GCP上进行E2E ML：从Vertex AI Feature Store开始服务\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage6/get_started_vertex_feature_store_serving.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      在GitHub上查看\n",
    "    </a>\n",
    "  </td>\n",
    "    \n",
    "  <td>\n",
    "        <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage6/get_started_vertex_feature_store_serving.ipynb\">\n",
    "        <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> 在Colab中运行\n",
    "        </a>\n",
    "  </td>\n",
    "    \n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/ml_ops/stage6/get_started_vertex_feature_store_serving.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      在Vertex AI Workbench中打开\n",
    "    </a>\n",
    "  </td>\n",
    "    \n",
    "</table>\n",
    "<br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "overview:mlops"
   },
   "source": [
    "## 概述\n",
    "\n",
    "本教程演示了如何在生产环境中使用Google Cloud上的Vertex AI进行端到端的MLOps。本教程涵盖了第3阶段：提供服务：从特征存储开始使用服务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "objective:mlops,stage2,get_started_vertex_feature_store"
   },
   "source": [
    "### 目标\n",
    "\n",
    "在本教程中，您将学习如何使用`Vertex AI Feature Store`来训练模型，以及在进行在线和批量预测时提供特征。\n",
    "\n",
    "本教程使用以下 Google Cloud ML 服务：\n",
    "\n",
    "- `Vertex AI Feature Store`\n",
    "- `Vertex AI Training`\n",
    "- `Vertex AI Prediction`\n",
    "\n",
    "执行的步骤包括：\n",
    "\n",
    "- 创建一个 Vertex AI `Featurestore` 资源。\n",
    "    - 为 `Featurestore` 资源创建 `EntityType` 资源。\n",
    "    - 为每个 `EntityType` 资源创建 `Feature` 资源。\n",
    "- 将特征值（实体数据项）导入到 `Featurestore` 资源中。\n",
    "    - 来自 Cloud Storage 位置。\n",
    "    - 来自 pandas DataFrame。\n",
    "- 从 `Featurestore` 资源执行在线预测。\n",
    "- 从 `Featurestore` 资源执行批量预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset:movies,lbn,avro"
   },
   "source": [
    "### 数据集\n",
    "\n",
    "本笔记本中使用的数据集包含自2018年以来的在线电子商务商店订单数据。此数据集可以在`bigquery-public-data.thelook_ecommerce.order_items`的BigQuery表中公开获取，可以通过在BigQuery中固定bigquery-public-data项目来访问。\n",
    "\n",
    "该表包含与每个订单项有关的各种字段，如订单ID、产品ID、用户ID、状态以及创建时的价格，发货时的价格等。在这些字段中，当前笔记本使用以下字段，假定它们的用途如下描述：\n",
    "\n",
    "* user_id：用户的ID。\n",
    "* product_id：产品的ID。\n",
    "* created_at：用户下订单的时间。\n",
    "* status：订单的状态（已发货、处理中、取消、退货和已完成）。\n",
    "\n",
    "该数据集用于训练一个推荐模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81c777b8ad32"
   },
   "source": [
    "### 费用\n",
    "本教程使用 Google Cloud 的计费组件：\n",
    "\n",
    "- Vertex AI\n",
    "- Cloud Storage\n",
    "- BigQuery\n",
    "\n",
    "了解[Vertex AI 价格](https://cloud.google.com/vertex-ai/pricing)、[Cloud Storage 价格](https://cloud.google.com/storage/pricing) 和[BigQuery 价格](https://cloud.google.com/bigquery/pricing)，并使用 [定价计算器](https://cloud.google.com/products/calculator/) 根据您的预期使用情况生成费用估算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_mlops"
   },
   "source": [
    "## 安装\n",
    "\n",
    "安装以下软件包以进一步运行此笔记本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_mlops"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Vertex AI Workbench Notebook product has specific requirements\n",
    "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\") and not os.getenv(\"VIRTUAL_ENV\")\n",
    "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
    "    \"/opt/deeplearning/metadata/env_version\"\n",
    ")\n",
    "\n",
    "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_WORKBENCH_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\"\n",
    "\n",
    "# Install the dependecies\n",
    "! pip3 install --upgrade google-cloud-aiplatform \\\n",
    "                         google-cloud-bigquery \\\n",
    "                         pyarrow \\\n",
    "                         pandas {USER_FLAG} --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "restart"
   },
   "source": [
    "重新启动内核\n",
    "\n",
    "安装了额外的软件包后，需要重新启动笔记本内核，以便它可以找到这些软件包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "restart"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "project_id"
   },
   "source": [
    "## 开始之前\n",
    "\n",
    "### 设置您的Google Cloud项目\n",
    "\n",
    "**无论您使用什么笔记本环境，都需要按照以下步骤进行操作。**\n",
    "\n",
    "1. [选择或创建一个Google Cloud项目](https://console.cloud.google.com/cloud-resource-manager)。首次创建帐户时，您将获得300美元的免费信用额，用于支付计算/存储成本。\n",
    "\n",
    "1. [确保为您的项目启用了计费](https://cloud.google.com/billing/docs/how-to/modify-project)。\n",
    "\n",
    "1. [启用Vertex AI、计算引擎、Cloud Storage和Cloud Logging API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,compute_component,storage_component,logging)。\n",
    "\n",
    "1. 如果您在本地运行此笔记本，您需要安装[Cloud SDK](https://cloud.google.com/sdk)。\n",
    "\n",
    "1. 在下面的单元格中输入您的项目ID。然后运行该单元格，确保Cloud SDK为本笔记本中的所有命令使用正确的项目。\n",
    "\n",
    "**注意**：Jupyter会将以`!`开头的行视为shell命令，并将以`$`前缀的Python变量插入这些命令中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "project_id"
   },
   "source": [
    "设置您的项目ID\n",
    "\n",
    "**如果您不知道您的项目ID**，您可以通过下面的 `gcloud` 命令获取您的项目ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_project_id"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "autoset_project_id"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID:\", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ba5d513b682"
   },
   "source": [
    "在当前环境中设置默认项目ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_gcloud_project_id"
   },
   "outputs": [],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "#### 地区\n",
    "\n",
    "您也可以更改“REGION”变量，该变量用于本笔记本的其余部分操作。以下是Vertex AI支持的地区。我们建议您选择最靠近您的地区。\n",
    "\n",
    "- 美洲：`us-central1`\n",
    "- 欧洲：`europe-west4`\n",
    "- 亚太：`asia-east1`\n",
    "\n",
    "您可能无法在Vertex AI上使用多地区存储桶进行训练。并非所有地区都支持所有Vertex AI服务。\n",
    "\n",
    "了解更多关于[Vertex AI地区](https://cloud.google.com/vertex-ai/docs/general/locations)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "region"
   },
   "outputs": [],
   "source": [
    "REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
    "\n",
    "if REGION == \"[your-region]\":\n",
    "    REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06571eb4063b"
   },
   "source": [
    "UUID\n",
    "\n",
    "如果您正在进行实时教程会话，则可能正在使用共享的测试帐户或项目。为了避免在创建的资源之间发生名称冲突，您为每个实例会话创建一个 uuid，并将其附加到您在本教程中创建的资源的名称上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4e166d927e36"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "\n",
    "# Generate a uuid of a specifed length(default=8)\n",
    "def generate_uuid(length: int = 8) -> str:\n",
    "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
    "\n",
    "\n",
    "UUID = generate_uuid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29b110b44457"
   },
   "source": [
    "### 验证您的Google Cloud帐户\n",
    "\n",
    "**如果您正在使用Vertex AI Workbench笔记本**，则您的环境已经得到验证。跳过这一步。\n",
    "\n",
    "**如果您正在使用Colab**，运行下面的单元格，并按照提示进行认证您的帐户。\n",
    "\n",
    "**否则**，请按照以下步骤操作：\n",
    "\n",
    "在Cloud控制台中，转到[创建服务账户密钥](https://console.cloud.google.com/apis/credentials/serviceaccountkey)页面。\n",
    "\n",
    "1. **单击创建服务帐户**。\n",
    "\n",
    "2. 在**服务帐户名称**字段中输入名称，然后单击**创建**。\n",
    "\n",
    "3. 在**授予此服务帐户访问项目**部分，单击角色下拉列表。在过滤框中输入\"Vertex\"，并选择**Vertex管理员**。在过滤框中输入\"Storage Object Admin\"，并选择**Storage对象管理员**。\n",
    "\n",
    "4. 单击创建。包含您密钥的JSON文件将下载到本地环境中。\n",
    "\n",
    "5. 在下面的单元格中将您的服务账户密钥路径输入为GOOGLE_APPLICATION_CREDENTIALS变量，并运行该单元格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "89788a802687"
   },
   "outputs": [],
   "source": [
    "# IMPORTANT - If you are using Vertex AI Workbench Notebooks, your environment is already authenticated. Skip this step.\n",
    "\n",
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# If on Vertex AI Workbench, then don't execute this code\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\") and not os.getenv(\n",
    "    \"DL_ANACONDA_HOME\"\n",
    "):\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\"):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bucket:mbsdk"
   },
   "source": [
    "### 创建一个云存储桶\n",
    "\n",
    "**无论您使用的是哪种笔记本环境，以下步骤都是必需的。**\n",
    "\n",
    "当您初始化 Python 版 Vertex AI SDK 时，您需要指定一个云存储中转桶。这个中转桶是您的数据集和模型资源在会话之间保留的位置。\n",
    "\n",
    "在下面设置您的云存储桶的名称。桶的名称必须在所有谷歌云项目中全局唯一，甚至包括您所在组织之外的项目。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bucket"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"[your-bucket-name]\"  # @param {type:\"string\"}\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "autoset_bucket"
   },
   "outputs": [],
   "source": [
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"[your-bucket-name]\":\n",
    "    BUCKET_NAME = PROJECT_ID + \"vai-\" + UUID\n",
    "    BUCKET_URI = \"gs://\" + BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_bucket"
   },
   "source": [
    "只有您的存储桶尚不存在时：运行以下单元格以创建您的云存储存储桶。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_bucket"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "validate_bucket"
   },
   "source": [
    "最后，通过检查存储桶的内容来验证访问您的云存储桶。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "validate_bucket"
   },
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "342ddaf59298"
   },
   "source": [
    "为特征存储设置桶访问权限"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5103ef1b1aa2"
   },
   "outputs": [],
   "source": [
    "! gsutil uniformbucketlevelaccess set on {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_vars"
   },
   "source": [
    "导入库并定义常量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "import google.cloud.aiplatform as aiplatform\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_bq"
   },
   "source": [
    "初始化顶点 AI 和 BigQuery 客户端"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_bq"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)\n",
    "bqclient = bigquery.Client(project=PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "container:training,prediction"
   },
   "source": [
    "设置预构建的容器\n",
    "\n",
    "设置用于训练和预测的预构建Docker容器映像。\n",
    "\n",
    "\n",
    "有关最新列表，请参见[用于训练的预构建容器](https://cloud.google.com/ai-platform-unified/docs/training/pre-built-containers)。\n",
    "\n",
    "\n",
    "有关最新列表，请参见[用于预测的预构建容器](https://cloud.google.com/ai-platform-unified/docs/predictions/pre-built-containers)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "container:training,prediction"
   },
   "outputs": [],
   "source": [
    "TF = \"2.8\".replace(\".\", \"-\")\n",
    "TRAIN_VERSION = \"tf-cpu.{}\".format(TF)\n",
    "DEPLOY_VERSION = \"tf2-cpu.{}\".format(TF)\n",
    "\n",
    "TRAIN_IMAGE = \"{}-docker.pkg.dev/vertex-ai/training/{}:latest\".format(\n",
    "    REGION.split(\"-\")[0], TRAIN_VERSION\n",
    ")\n",
    "DEPLOY_IMAGE = \"{}-docker.pkg.dev/vertex-ai/prediction/{}:latest\".format(\n",
    "    REGION.split(\"-\")[0], DEPLOY_VERSION\n",
    ")\n",
    "\n",
    "print(\"Training:\", TRAIN_IMAGE)\n",
    "print(\"Deployment:\", DEPLOY_IMAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "machine:training"
   },
   "source": [
    "#### 设置机器类型\n",
    "\n",
    "接下来，设置用于训练的机器类型。\n",
    "\n",
    "- 设置变量`TRAIN_COMPUTE`/`DEPLOY_COMPUTE` 来配置用于训练和预测的虚拟机的计算资源。\n",
    " - `机器类型`\n",
    "     - `n1-standard`：每个虚拟 CPU 3.75GB 的内存。\n",
    "     - `n1-highmem`：每个虚拟 CPU 6.5GB 的内存。\n",
    "     - `n1-highcpu`：每个虚拟 CPU 0.9GB 的内存。\n",
    " - `虚拟 CPU`：数量为 \\[2, 4, 8, 16, 32, 64, 96 \\]\n",
    "\n",
    "*注意：以下机型不支持用于训练：*\n",
    "\n",
    " - `standard`：2 个虚拟 CPU\n",
    " - `highcpu`：2、4 和 8 个虚拟 CPU\n",
    "\n",
    "*注意：您也可以使用 n2 和 e2 机型进行训练和部署，但它们不支持 GPU。*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "machine:training"
   },
   "outputs": [],
   "source": [
    "MACHINE_TYPE = \"n1-standard\"\n",
    "\n",
    "VCPU = \"4\"\n",
    "TRAIN_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
    "print(\"Train machine type\", TRAIN_COMPUTE)\n",
    "DEPLOY_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
    "print(\"Deploy machine type\", DEPLOY_COMPUTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "featurestore_intro"
   },
   "source": [
    "## 引言：Vertex AI 功能存储介绍\n",
    "\n",
    "假设您有一个推荐模型，用于预测要在收银机收据背面打印的优惠券。现在，如果该模型只是在单个交易实例（购买了什么和多少）上进行训练，那么（过去）您会使用Apriori算法。\n",
    "\n",
    "但现在我们有关于客户的历史数据（比如按信用卡号索引）。比如迄今为止的总购买金额，每笔交易的平均购买金额，按产品类别购买的频率等。我们使用这些“丰富的数据”来训练一个推荐系统。\n",
    "\n",
    "现在是进行实时预测的时候了。您从收银机得到一笔交易，但它只有信用卡号和这笔交易。它没有模型所需的丰富数据。在提供服务的过程中，信用卡号被用作特征存储中获取模型所需的丰富数据的索引。\n",
    "\n",
    "另一方面，假设模型训练时使用的丰富数据是时间戳为6月1日的。当前交易发生在6月15日。假设用户在6月1日至15日之间进行了其他交易，并且丰富的数据在特征存储中持续更新。但模型是基于6月1日的数据进行训练的。特征存储知道版本号，并将6月1日的版本提供给模型（而不是当前的6月15日）。否则，如果使用6月15日的数据，就会出现训练和提供服务之间的偏差。\n",
    "\n",
    "这里的另一个问题是数据漂移。事情会发生变化，突然某一天，每个人都在购买卫生纸！现有的丰富数据的分布与部署模型训练时的分布发生了显著变化。特征存储可以检测到分布变化/阈值的变化，并触发重新训练模型的通知。\n",
    "\n",
    "了解更多关于[Vertex AI 功能存储 API](https://cloud.google.com/vertex-ai/docs/featurestore)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "featurestore_datamodel:movies"
   },
   "source": [
    "## Vertex AI 特征存储数据模型\n",
    "\n",
    "Vertex AI 特征存储使用以下三个重要的层次概念组织数据：\n",
    "\n",
    "        Featurestore（特征存储） -> EntityType（实体类型） -> Feature（特征）\n",
    "\n",
    "- `Featurestore`：存储特征的地方。\n",
    "- `EntityType`：在一个 `Featurestore` 下，`EntityType` 描述了要被建模的对象，可以是真实的也可以是虚拟的。\n",
    "- `Feature`：在一个 `EntityType` 下，`Feature` 描述了 `EntityType` 的属性。\n",
    "\n",
    "了解有关[Vertex AI 特征存储数据模型](https://cloud.google.com/vertex-ai/docs/featurestore/concepts)的更多信息。\n",
    "\n",
    "在这个电子商务示例中，您将创建一个名为 ecomm_recommendation 的 `Featurestore` 资源。这个 `Featurestore` 资源有两个实体类型：\n",
    "- `users`：这个实体类型具有 `product_id` 和 `rating` 特征。\n",
    "- `products`：这个实体类型具有 `user_list` 和 `product_name` 特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "featurestore_create"
   },
   "source": [
    "## 创建 `Featurestore` 资源\n",
    "\n",
    "首先，使用以下参数使用 `Featurestore.create()` 方法为数据集创建一个 `Featurestore`：\n",
    "\n",
    "- `featurestore_id`：特征存储的名称。\n",
    "- `online_store_fixed_node_count`：特征存储中在线服务的配置设置。\n",
    "- `project`：项目 ID。\n",
    "- `location`：位置（区域）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "featurestore_create"
   },
   "outputs": [],
   "source": [
    "# Represents featurestore resource path.\n",
    "FEATURESTORE_NAME = \"ecomm_recommendation\" + UUID\n",
    "\n",
    "featurestore = aiplatform.Featurestore.create(\n",
    "    featurestore_id=FEATURESTORE_NAME,\n",
    "    online_store_fixed_node_count=1,\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    ")\n",
    "\n",
    "print(featurestore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "featurestore_get"
   },
   "source": [
    "获取“Featurestore”资源\n",
    "\n",
    "您可以使用“Featurestore（）”初始化程序获取您项目中指定的“Featurestore”资源，具有以下参数：\n",
    "\n",
    "- `featurestore_name`：`Featurestore`资源的名称。\n",
    "- `project`：项目ID。\n",
    "- `location`：位置（区域）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "featurestore_get"
   },
   "outputs": [],
   "source": [
    "featurestore = aiplatform.Featurestore(\n",
    "    featurestore_name=FEATURESTORE_NAME, project=PROJECT_ID, location=REGION\n",
    ")\n",
    "print(featurestore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "featurestore_create:entity_type"
   },
   "source": [
    "为您的`Featurestore`资源创建实体类型\n",
    "\n",
    "接下来，您可以使用`create_entity_type()`方法为您的`Featurestore`资源创建`EntityType`资源，使用以下参数：\n",
    "\n",
    "- `entity_type_id`：`EntityType`资源的名称。\n",
    "- `description`：实体类型的描述。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "featurestore_create:entity_type"
   },
   "outputs": [],
   "source": [
    "for name, description in [\n",
    "    (\"users\", \"Description of the user\"),\n",
    "    (\"products\", \"Description of the product\"),\n",
    "]:\n",
    "    entity_type = featurestore.create_entity_type(\n",
    "        entity_type_id=name, description=description\n",
    "    )\n",
    "    print(entity_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "featurestore_create:feature"
   },
   "source": [
    "### 为您的`EntityType`资源添加`Feature`资源\n",
    "\n",
    "接下来，您使用`create_feature()`方法为您的`Featurestore`资源中的每个`EntityType`资源创建`Feature`资源，参数如下：\n",
    "\n",
    "- `feature_id`：`Feature`资源的名称。\n",
    "- `description`：特征的描述。\n",
    "- `value_type`：特征的数据类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "featurestore_create:feature,movies"
   },
   "outputs": [],
   "source": [
    "def create_features(featurestore_name, entity_name, features):\n",
    "    entity_type = aiplatform.EntityType(\n",
    "        entity_type_name=entity_name, featurestore_id=featurestore_name\n",
    "    )\n",
    "\n",
    "    for feature in features:\n",
    "        feature = entity_type.create_feature(\n",
    "            feature_id=feature[0], description=feature[1], value_type=feature[2]\n",
    "        )\n",
    "        print(feature)\n",
    "\n",
    "\n",
    "create_features(\n",
    "    FEATURESTORE_NAME,\n",
    "    \"users\",\n",
    "    [\n",
    "        (\"product_id\", \"product description\", \"INT64\"),\n",
    "        (\"rating\", \"rating of the product\", \"DOUBLE\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "create_features(\n",
    "    FEATURESTORE_NAME,\n",
    "    \"products\",\n",
    "    [\n",
    "        (\"users_list\", \"List of user ids who bought product\", \"STRING_ARRAY\"),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05351390b8ce"
   },
   "source": [
    "## 对数据集执行特征工程\n",
    "\n",
    "接下来，对公开的BigQuery数据集执行特征工程，然后将其导入特征存储。\n",
    "\n",
    "### 将BigQuery数据集加载到数据框中\n",
    "\n",
    "* 将数据从BigQuery加载到pandas数据框中。\n",
    "* 选择要使用的列。\n",
    "    - 用户ID\n",
    "    - 产品ID\n",
    "    - 创建时间\n",
    "    - 状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6d47e0aa0547"
   },
   "outputs": [],
   "source": [
    "query_string = \"\"\"\n",
    "SELECT\n",
    "    CAST(user_id AS STRING) AS user_id,\n",
    "    product_id,\n",
    "    created_at,\n",
    "    status\n",
    "FROM\n",
    "    `bigquery-public-data.thelook_ecommerce.order_items`\n",
    "\"\"\"\n",
    "\n",
    "df_bq_table = bqclient.query(query_string).result().to_dataframe()\n",
    "\n",
    "print(df_bq_table.shape)\n",
    "df_bq_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60e6b3f9ea64"
   },
   "source": [
    "### 派生一个新的列评分\n",
    "\n",
    "接下来，您添加一个用于评分的新列。由于评分是数值型的，您可以从现有的状态列中派生出它们，具体做法如下：\n",
    "\n",
    "- 将状态字符串值映射到数值范围（0..4）。\n",
    "- 将值归一化在0和1之间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c34184515a49"
   },
   "outputs": [],
   "source": [
    "# map the status to a rating\n",
    "rating_map = {\n",
    "    \"Cancelled\": 0,\n",
    "    \"Returned\": 1,\n",
    "    \"Processing\": 2,\n",
    "    \"Shipped\": 3,\n",
    "    \"Complete\": 4,\n",
    "}\n",
    "\n",
    "df_bq_table[\"rating\"] = df_bq_table[\"status\"].map(rating_map)\n",
    "print(df_bq_table.head())\n",
    "\n",
    "# Normalize the ratings\n",
    "min_rating = min(df_bq_table[\"rating\"])\n",
    "max_rating = max(df_bq_table[\"rating\"])\n",
    "\n",
    "df_bq_table[\"rating\"] = (\n",
    "    df_bq_table[\"rating\"]\n",
    "    .apply(lambda x: (x - min_rating) / (max_rating - min_rating))\n",
    "    .values\n",
    ")\n",
    "print(df_bq_table.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3102e6ef28d7"
   },
   "source": [
    "### 过滤数据集\n",
    "\n",
    "接下来，将数据集过滤为仅包含上周购买产品的用户，并删除“状态”列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7709f81a9d82"
   },
   "outputs": [],
   "source": [
    "PAST_WEEK_DATE = datetime.now() - pd.to_timedelta(\"7day\")\n",
    "\n",
    "df_filtered = df_bq_table[\n",
    "    (df_bq_table[\"created_at\"] < PAST_WEEK_DATE.isoformat() + \"Z\")\n",
    "].reset_index()\n",
    "\n",
    "result = df_filtered.groupby([\"product_id\"])[\"user_id\"].apply(list).to_dict()\n",
    "\n",
    "df_prod_user_list = pd.DataFrame(result.items(), columns=[\"product_id\", \"users_list\"])\n",
    "df_prod_user_list[\"product_id\"] = df_prod_user_list[\"product_id\"].astype(\"string\")\n",
    "print(df_prod_user_list.head())\n",
    "\n",
    "df_bq_table.drop(\"status\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0706ab43559"
   },
   "source": [
    "### 将预处理数据重新导入BigQuery\n",
    "\n",
    "#### 为预处理数据创建目标表。\n",
    "\n",
    "接下来，您需要创建一个BigQuery数据集，随后将在其中添加预处理数据的表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ceba9301ca2d"
   },
   "outputs": [],
   "source": [
    "DESTINATION_DATASET = f\"product_recommendation_{UUID}\"\n",
    "\n",
    "USERS_SOURCE_TABLE_NAME = \"user_prod_rating_data\"\n",
    "USERS_SOURCE_TABLE_URI = (\n",
    "    f\"bq://{PROJECT_ID}.{DESTINATION_DATASET}.{USERS_SOURCE_TABLE_NAME}\"\n",
    ")\n",
    "\n",
    "PRODUCTS_SOURCE_TABLE_NAME = \"prod_users_list_data\"\n",
    "PRODUCTS_SOURCE_TABLE_URI = (\n",
    "    f\"bq://{PROJECT_ID}.{DESTINATION_DATASET}.{PRODUCTS_SOURCE_TABLE_NAME}\"\n",
    ")\n",
    "\n",
    "# Create destination dataset\n",
    "dataset_id = \"{}.{}\".format(PROJECT_ID, DESTINATION_DATASET)\n",
    "dataset = bigquery.Dataset(dataset_id)\n",
    "dataset.location = REGION\n",
    "dataset = bqclient.create_dataset(dataset)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1bb6cbbcb472"
   },
   "source": [
    "#### 为筛选后的数据集创建表格\n",
    "\n",
    "接下来，您可以创建一个表格并加载筛选后的数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e4d8f864367a"
   },
   "outputs": [],
   "source": [
    "# Create a table\n",
    "schema = [\n",
    "    bigquery.SchemaField(\"user_id\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"product_id\", \"INT64\"),\n",
    "    bigquery.SchemaField(\"created_at\", \"TIMESTAMP\"),\n",
    "    bigquery.SchemaField(\"rating\", \"FLOAT\"),\n",
    "]\n",
    "\n",
    "table_id = f\"{PROJECT_ID}.{DESTINATION_DATASET}.{USERS_SOURCE_TABLE_NAME}\"\n",
    "table = bigquery.Table(table_id, schema=schema)\n",
    "bqclient.create_table(table, exists_ok=True)\n",
    "\n",
    "\n",
    "# Load data to BQ\n",
    "job = bqclient.load_table_from_dataframe(df_bq_table, table_id)\n",
    "print(job.errors, job.state)\n",
    "while job.running():\n",
    "    from time import sleep\n",
    "\n",
    "    sleep(30)\n",
    "    print(\"Running ...\")\n",
    "print(job.errors, job.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d71d1dec847c"
   },
   "source": [
    "创建用于产品用户列表的表格。\n",
    "\n",
    "创建新的表格，用于产品用户列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5b119a73a445"
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "# Create a table\n",
    "schema = [\n",
    "    bigquery.SchemaField(\"product_id\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"users_list\", \"STRING\", \"REPEATED\"),\n",
    "]\n",
    "table_id = f\"{PROJECT_ID}.{DESTINATION_DATASET}.{PRODUCTS_SOURCE_TABLE_NAME}\"\n",
    "table = bigquery.Table(table_id, schema=schema)\n",
    "bqclient.create_table(table, exists_ok=True)\n",
    "\n",
    "# Load data to BQ\n",
    "job = bqclient.load_table_from_dataframe(df_prod_user_list, table_id)\n",
    "print(job.errors, job.state)\n",
    "while job.running():\n",
    "    sleep(30)\n",
    "    print(\"Running ...\")\n",
    "print(job.errors, job.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "featurestore_import:movies,avro"
   },
   "source": [
    "## 将特征数据导入到您的`Featurestore`资源中\n",
    "\n",
    "接下来，您将导入您的`Featurestore`资源的特征数据。一旦导入，您可以将这些特征值用于在线和离线（批处理）服务。\n",
    "\n",
    "### 数据布局\n",
    "\n",
    "每个导入的`EntityType`资源数据必须有一个ID。此外，每个`EntityType`资源数据项可以选择性地具有一个时间戳，指定特征值生成的时间。\n",
    "\n",
    "在导入时，在您的请求中指定以下内容：\n",
    "\n",
    "- 数据源格式：BigQuery表/Avro/CSV/Pandas数据框\n",
    "- 数据源URL\n",
    "- 目标：要导入的特征存储/实体类型/特征\n",
    "\n",
    "在本教程中，模式如下：\n",
    "\n",
    "    对于用户实体：\n",
    "    模式 = {\n",
    "        \"name\": \"users\",\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"name\":\"product_id\",\n",
    "                \"type\":[\"null\",\"integer\"]\n",
    "            },\n",
    "            {\n",
    "                \"name\":\"rating\",\n",
    "                \"type\":[\"null\",\"double\"]\n",
    "                },\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    对于产品实体：\n",
    "    模式 = {\n",
    "        \"name\": \"products\",\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"name\":\"users_list\",\n",
    "                \"type\":[\"null\",\"string_array\"]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "### 从BigQuery导入特征值\n",
    "\n",
    "您可以使用`ingest_from_bq()`方法导入`EntityType`资源的特征值，参数如下：\n",
    "\n",
    "- `entity_id_field`：父`EntityType`资源的标识符名称。\n",
    "- `feature_ids`：要添加到`EntityType`资源中的`Feature`资源数据的标识符名称列表。\n",
    "- `feature_time`：用于输入特征的时间戳字段。\n",
    "- `bq_source_uri`：要从中导入数据的BigQuery表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "featurestore_import:movies,avro"
   },
   "outputs": [],
   "source": [
    "entity_type = featurestore.get_entity_type(\"users\")\n",
    "response = entity_type.ingest_from_bq(\n",
    "    entity_id_field=\"user_id\",\n",
    "    feature_ids=[\"product_id\", \"rating\"],\n",
    "    feature_time=\"created_at\",\n",
    "    bq_source_uri=f\"bq://{PROJECT_ID}.{DESTINATION_DATASET}.{USERS_SOURCE_TABLE_NAME}\",\n",
    ")\n",
    "print(response)\n",
    "\n",
    "\n",
    "def past_6days():\n",
    "    return datetime.now() - timedelta(days=6)\n",
    "\n",
    "\n",
    "entity_type = featurestore.get_entity_type(\"products\")\n",
    "response = entity_type.ingest_from_bq(\n",
    "    entity_id_field=\"product_id\",\n",
    "    feature_ids=[\"users_list\"],\n",
    "    feature_time=past_6days(),\n",
    "    bq_source_uri=f\"bq://{PROJECT_ID}.{DESTINATION_DATASET}.{PRODUCTS_SOURCE_TABLE_NAME}\",\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "featurestore_serving"
   },
   "source": [
    "顶点 AI 特征存储服务\n",
    "\n",
    "顶点 AI 特征存储服务为从 `Featurestore` 资源中提供特征提供以下两种服务：\n",
    "\n",
    "- 在线提供 - 低延迟提供小批量特征（预测）。\n",
    "\n",
    "- 批量提供 - 高吞吐量提供大批量特征（训练和预测）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "featurestore_serving:batch"
   },
   "source": [
    "批量服务\n",
    "\n",
    "Vertex AI特征商店的批量服务功能专门针对实时高吞吐量大批量特征的服务优化，通常用于训练模型或批量预测。\n",
    "\n",
    "可以将批量服务发送到以下目的地：\n",
    "\n",
    "- BigQuery表\n",
    "- 云存储位置\n",
    "- 数据帧"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "featurestore_serving:batch,output,movies"
   },
   "source": [
    "### 输出数据集\n",
    "\n",
    "在这个笔记本中，您将使用来自您的特征存储中以CSV格式存储在Google Cloud Storage中的数据来训练一个模型。\n",
    "\n",
    "### 用例\n",
    "\n",
    "**任务** 是准备一个数据集来训练一个模型，该模型为给定用户推荐产品。为了实现这一目标，您需要两组输入：\n",
    "\n",
    "* 特征：您已经导入到特征存储中。\n",
    "* 标签：记录在案的实际数据，即评分。\n",
    "\n",
    "具体来说，实际观测数据在表1中描述，期望的数据集在表2中描述。表2中的每一行都是根据表1中的实体ID和时间戳连接导入的特征值从Vertex AI特征存储中获取的结果。在这个示例中，已选择从`users`中的`product_id`和`rating`特征进行批量训练。\n",
    "\n",
    "batch_serve_to_df方法将表1作为read_instances_df参数的输入，连接所有需要的特征值从特征存储中，并返回用于训练的表2。\n",
    "\n",
    "<h4 align=\"center\">表1. 实际数据</h4>\n",
    "\n",
    "users | timestamp            \n",
    "----- | -------------------- \n",
    "87228 | 2022-07-01T00:00:00Z \n",
    "16173 | 2022-07-01T18:09:43Z \n",
    "...   | ...      | ...     \n",
    "\n",
    "\n",
    "<h4 align=\"center\">表2. batch_serve_to_df生成的预期训练数据（正样本）</h4>\n",
    "\n",
    "feature_timestamp            | entity_type_users | product_id | rating |\n",
    "-------------------- | ----------------- | --------------- | ---------------- |\n",
    "2022-07-01T00:00:00Z | 87228 | 4567 | 0.5 |\n",
    "2022-07-01T00:00:00Z | 16173 | 5490 | 0.75 |\n",
    "... | ... | ... | ... | ...  \n",
    "\n",
    "#### 为什么要有时间戳?\n",
    "\n",
    "注意表2中有一个`timestamp`列。这表示观察到实际数据的时间。这是为了避免数据不一致。\n",
    "\n",
    "例如，表2中的第一行表示ID为`87228`的用户在`2022-07-01T00:00:00Z`购买了产品。特征存储保留所有时间戳的特征值，但在批量服务期间仅获取给定时间戳时的特征值。\n",
    "\n",
    "### 批量服务至数据框\n",
    "\n",
    "组装请求，指定以下信息：\n",
    "\n",
    "* 标签数据在哪里，即表1。\n",
    "* 要读取哪些特征，即表1中的列名。\n",
    "\n",
    "接下来，使用batch_serve_to_df从特征存储中获取数据框，并将其存储到一个CSV文件中，该文件将用于在Vertex AI中训练推荐模型。\n",
    "\n",
    "* 将entityType Id（`users`）和`timestamp`列导出为csv到创建的GCS桶中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lwNAh1Ysifal"
   },
   "outputs": [],
   "source": [
    "from datetime import timezone\n",
    "\n",
    "past_week_date = (datetime.now() - pd.to_timedelta(\"7day\")).isoformat() + \"Z\"\n",
    "df_sorted = df_bq_table.sort_values(\"created_at\", ascending=False, ignore_index=True)\n",
    "df_sorted.rename(columns={\"user_id\": \"users\"}, inplace=True)\n",
    "df_sorted = df_sorted[df_sorted[\"created_at\"] <= past_week_date].reset_index()\n",
    "df_sorted[\"created_at\"] = df_sorted[\"created_at\"].astype(str)\n",
    "df_sorted[\"timestamp\"] = df_sorted[\"created_at\"].map(\n",
    "    lambda x: datetime.fromisoformat(x).astimezone(timezone.utc)\n",
    ")\n",
    "df_batch = df_sorted[[\"users\", \"timestamp\"]]\n",
    "\n",
    "df_batch.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "featurestore_serving:batch,read,movies"
   },
   "source": [
    "### 批量读取特征值\n",
    "\n",
    "您可以使用`batch_serve_to_df`方法将实体数据项批量提供给DataFrame，参数如下：\n",
    "\n",
    "- `serving_feature_ids`：要提供的实体类型和相应特征的字典。\n",
    "- `read_instances_uri`：要从中读取实体数据项的云存储位置。\n",
    "\n",
    "输出存储在一个BigQuery表中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "featurestore_serving:batch,read,movies"
   },
   "outputs": [],
   "source": [
    "batch_serve = featurestore.batch_serve_to_df(\n",
    "    serving_feature_ids={\"users\": [\"product_id\", \"rating\"]}, read_instances_df=df_batch\n",
    ")\n",
    "\n",
    "batch_serve.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f07844d4f372"
   },
   "source": [
    "将数据框数据导出为CSV文件\n",
    "\n",
    "接下来，您将数据框数据导出到云存储中的CSV文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4853bcf78d19"
   },
   "outputs": [],
   "source": [
    "CSV_FILE = f\"{BUCKET_URI}/data.csv\"\n",
    "\n",
    "batch_serve.to_csv(CSV_FILE, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11a04964fcba"
   },
   "source": [
    "## 训练一个推荐模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TGBVQPxSifan"
   },
   "source": [
    "在这个部分，您使用`batch_serve_to_df`方法的数据来训练一个为给定用户推荐产品的定制模型。\n",
    "\n",
    "您可以使用Vertex AI SDK for Python在Docker容器中的Python脚本中创建一个定制训练的模型，然后通过发送数据获取部署模型的预测。\n",
    "\n",
    "执行的步骤包括：\n",
    "\n",
    "- 训练一个Vertex AI定制的`TrainingPipeline`来训练一个TensorFlow模型。\n",
    "- 部署`Model`资源到服务`Endpoint`资源。\n",
    "- 进行预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zLruf4blifao"
   },
   "source": [
    "### 训练模型\n",
    "\n",
    "您可以使用容器镜像有两种方式来训练模型：\n",
    "\n",
    "- **使用 Vertex AI 预构建的容器**。如果您使用预构建的训练容器，您还必须指定一个要安装到容器镜像中的 Python 包。这个 Python 包包含您的训练代码。\n",
    "\n",
    "- **使用自定义的容器镜像**。如果您使用自己的容器，容器镜像必须包含您的训练代码。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s-l9xyOXifao"
   },
   "source": [
    "### 为训练脚本定义命令参数\n",
    "\n",
    "准备要传递给训练脚本的命令行参数。\n",
    "- `args`：要传递给相应 Python 模块的命令行参数。在这个例子中，它们是：\n",
    "  - `\"--epochs=\" + EPOCHS`：用于训练的周期数。\n",
    "  - `\"--batch_size=\" + BATCH_SIZE`：用于训练的批次大小。\n",
    "  - `\"--training_data=\" + GCS_PATH`：来自特征存储的包含训练数据的 csv 文件的路径。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Ebg4UFWifap"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "CMDARGS = [\n",
    "    \"--epochs=\" + str(EPOCHS),\n",
    "    \"--batch_size=\" + str(BATCH_SIZE),\n",
    "    \"--training_data=\" + CSV_FILE,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKAkzoDjifap"
   },
   "source": [
    "#### 训练脚本\n",
    "\n",
    "接下来，您将编写训练脚本 `task.py` 的内容。总结起来，该脚本执行以下操作：\n",
    "\n",
    "- 从 Google Cloud 存储加载 csv 数据。\n",
    "- 使用 TF.Keras 模型 API 构建模型。\n",
    "- 编译模型（`compile()`）。\n",
    "- 根据参数 `args.epochs` 和 `args.batch_size` 训练模型（`fit()`）。\n",
    "- 从环境变量 `AIP_MODEL_DIR` 获取保存模型工件的目录。该变量由[训练服务设置](https://cloud.google.com/vertex-ai/docs/training/code-requirements#environment-variables)。\n",
    "- 将训练好的模型保存到模型目录中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GXPR1CVgifap"
   },
   "outputs": [],
   "source": [
    "%%writefile task.py\n",
    "\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Read args\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--epochs', dest='epochs',\n",
    "                    default=10, type=int,\n",
    "                    help='Number of epochs.')\n",
    "parser.add_argument('--batch_size', dest='batch_size',\n",
    "                    default=10, type=int,\n",
    "                    help='Batch size.')\n",
    "parser.add_argument('--training_data', dest='training_data', type=str,\n",
    "                    help=\"URI of the training data in BQ\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "# Collect the arguments\n",
    "training_data_uri = args.training_data\n",
    "\n",
    "# Set up training variables\n",
    "LABEL_COLUMN = \"rating\"\n",
    "UNUSED_COLUMNS = [\"timestamp\",\"entity_type_users\",\"product_id\"]\n",
    "NA_VALUES = [\"NA\", \".\", \" \", \"\", \"null\", \"NaN\"]\n",
    "\n",
    "# # Possible categorical values\n",
    "RATING = [0,1,2,3,4]\n",
    "\n",
    "df_train = pd.read_csv(training_data_uri)\n",
    "\n",
    "# Remove NA values\n",
    "def clean_dataframe(df):\n",
    "    return df.replace(to_replace=NA_VALUES, value=np.NaN).dropna()\n",
    "\n",
    "df_train = clean_dataframe(df_train)\n",
    "\n",
    "user_ids = df_train[\"entity_type_users\"].unique().tolist()\n",
    "user2user_encoded = {x: i for i, x in enumerate(user_ids)}\n",
    "\n",
    "product_ids = df_train[\"product_id\"].unique().tolist()\n",
    "product2product_encoded = {x: i for i, x in enumerate(product_ids)}\n",
    "\n",
    "df_train[\"user\"] = df_train[\"entity_type_users\"].map(user2user_encoded)\n",
    "df_train[\"product\"] = df_train[\"product_id\"].map(product2product_encoded)\n",
    "NUM_USERS = len(user2user_encoded)\n",
    "NUM_PRODUCTS = len(product2product_encoded)\n",
    "\n",
    "\n",
    "def convert_dataframe_to_dataset(\n",
    "    df_train,\n",
    "):\n",
    "    NUMERIC_COLUMNS = [\"entity_type_users\",\"product_id\",\"rating\"]\n",
    "    df_train[NUMERIC_COLUMNS] = df_train[NUMERIC_COLUMNS].astype(\"float32\")\n",
    "    df_train = df_train.drop(columns=UNUSED_COLUMNS)\n",
    "\n",
    "    df_train_x, df_train_y = df_train, df_train.pop(LABEL_COLUMN)\n",
    "\n",
    "    y_train = np.asarray(df_train_y).astype(\"float32\")\n",
    "\n",
    "    # Convert to numpy representation\n",
    "    x_train = np.asarray(df_train_x)\n",
    "\n",
    "    dataset_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    return dataset_train\n",
    "\n",
    "# Create datasets\n",
    "dataset_train = convert_dataframe_to_dataset(df_train)\n",
    "\n",
    "# Shuffle train set\n",
    "dataset_train = dataset_train.shuffle(len(df_train))\n",
    "\n",
    "EMBEDDING_SIZE = 50\n",
    "class RecommenderNet(tf.keras.Model):\n",
    "        def __init__(self, num_users, num_products, embedding_size, **kwargs):\n",
    "            super(RecommenderNet, self).__init__(**kwargs)\n",
    "            self.num_users = num_users\n",
    "            self.num_products = num_products\n",
    "            self.embedding_size = embedding_size\n",
    "            self.user_embedding = tf.keras.layers.Embedding(\n",
    "                num_users,\n",
    "                embedding_size,\n",
    "                embeddings_initializer=\"he_normal\",\n",
    "                embeddings_regularizer=tf.keras.regularizers.l2(1e-6),\n",
    "            )\n",
    "            self.user_bias = tf.keras.layers.Embedding(num_users, 1)\n",
    "            self.product_embedding = tf.keras.layers.Embedding(\n",
    "                num_products,\n",
    "                embedding_size,\n",
    "                embeddings_initializer=\"he_normal\",\n",
    "                embeddings_regularizer=tf.keras.regularizers.l2(1e-6),\n",
    "            )\n",
    "            self.product_bias = tf.keras.layers.Embedding(num_products, 1)\n",
    "\n",
    "        def call(self, inputs):\n",
    "            user_vector = self.user_embedding(inputs[:, 0])\n",
    "            user_bias = self.user_bias(inputs[:, 0])\n",
    "            product_vector = self.product_embedding(inputs[:, 1])\n",
    "            product_bias = self.product_bias(inputs[:, 1])\n",
    "            dot_user_product = tf.tensordot(user_vector, product_vector, 2)\n",
    "            # Add all the components (including bias)\n",
    "            x = dot_user_product + user_bias + product_bias\n",
    "            # The sigmoid activation forces the rating to between 0 and 1\n",
    "            return tf.nn.sigmoid(x)\n",
    "\n",
    "def create_model(num_users,num_products):\n",
    "    # Create model\n",
    "        model = RecommenderNet(num_users, num_products, EMBEDDING_SIZE)\n",
    "        model.compile(\n",
    "            loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        )\n",
    "        return model\n",
    "\n",
    "\n",
    "model = create_model(num_users=NUM_USERS,num_products=NUM_PRODUCTS)\n",
    "\n",
    "dataset_train = dataset_train.batch(args.batch_size)\n",
    "\n",
    "# Train the model\n",
    "model.fit(dataset_train, epochs=args.epochs)\n",
    "\n",
    "tf.saved_model.save(model, os.getenv(\"AIP_MODEL_DIR\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZJ0ic_oifaq"
   },
   "source": [
    "### 训练模型\n",
    "\n",
    "使用`CustomTrainingJob`类来定义`TrainingPipeline`。该类接受以下参数：\n",
    "\n",
    "- `display_name`：此训练流程的用户定义名称。\n",
    "- `script_path`：训练脚本的本地路径。\n",
    "- `container_uri`：训练容器镜像的URI。\n",
    "- `requirements`：脚本的Python软件包依赖列表。\n",
    "- `model_serving_container_image_uri`：可以为您的模型提供预测的容器的URI —— 可以是预构建的容器或自定义容器。\n",
    "\n",
    "使用`run`函数开始训练。该函数接受以下参数：\n",
    "\n",
    "- `args`：要传递给Python脚本的命令行参数。\n",
    "- `replica_count`：worker副本的数量。\n",
    "- `model_display_name`：如果脚本生成托管的`Model`，则为`Model`的显示名称。\n",
    "- `machine_type`：用于训练的机器类型。\n",
    "- `accelerator_type`：硬件加速器类型。\n",
    "- `accelerator_count`：要连接到worker副本的加速器数量。\n",
    "\n",
    "`run`函数创建一个训练流程，训练并创建一个`Model`对象。训练流程完成后，`run`函数将返回`Model`对象。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mxGdxjxkifaq"
   },
   "outputs": [],
   "source": [
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "DEPLOYED_NAME = f\"product-recommender-{UUID}-{TIMESTAMP}\"\n",
    "\n",
    "job = aiplatform.CustomTrainingJob(\n",
    "    display_name=DEPLOYED_NAME,\n",
    "    script_path=\"task.py\",\n",
    "    container_uri=TRAIN_IMAGE,\n",
    "    requirements=[\"google-cloud-bigquery>=2.20.0\", \"db-dtypes\"],\n",
    "    model_serving_container_image_uri=DEPLOY_IMAGE,\n",
    ")\n",
    "\n",
    "# Start the training\n",
    "model = job.run(\n",
    "    model_display_name=DEPLOYED_NAME,\n",
    "    args=CMDARGS,\n",
    "    replica_count=1,\n",
    "    machine_type=TRAIN_COMPUTE,\n",
    "    accelerator_count=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "090a3916e770"
   },
   "source": [
    "### 部署模型\n",
    "\n",
    "接下来，您可以将训练好的模型部署到`Endpoint`中。您可以通过在`Model`资源上调用`deploy`函数来实现这一点。这将执行两项操作：\n",
    "\n",
    "1. 为部署`Model`资源创建一个`Endpoint`资源。\n",
    "2. 将`Model`资源部署到`Endpoint`资源中。\n",
    "\n",
    "该函数接受以下参数：\n",
    "\n",
    "- `deployed_model_display_name`：部署模型的可读名称。\n",
    "- `traffic_split`：在端点上发送到该模型的流量百分比，指定为一个或多个键/值对的字典。\n",
    "   - 如果只有一个模型，那么请指定`{ \"0\": 100 }`，其中\"0\"指的是这个模型被上传，100表示100%的流量。\n",
    "   - 如果端点上已有现有模型，流量将被拆分，则使用`model_id`指定`{ \"0\": 百分比, model_id: 百分比, ... }`，其中`model_id`是端点上现有`DeployedModel`的ID。百分比必须总和为100。\n",
    "- `machine_type`：用于训练的机器类型。\n",
    "- `accelerator_type`：硬件加速器类型。\n",
    "- `accelerator_count`：要附加到工作实例的加速器数量。\n",
    "- `starting_replica_count`：最初配置的计算实例数量。\n",
    "- `max_replica_count`：要扩展到的最大计算实例数量。在本教程中，只配置一个实例。\n",
    "\n",
    "#### 流量拆分\n",
    "\n",
    "`traffic_split`参数指定为Python字典。您可以将模型的多个实例部署到一个端点，并设置流量比例分配到每个实例。\n",
    "\n",
    "您可以使用流量拆分逐渐将新模型引入到生产环境中。例如，如果您已经在生产中拥有一个模型负责100%的流量，您可以部署一个新模型到同一个端点，将10%的流量引导到它，将原始模型的流量减少到90%。这样可以在最小化对大多数用户的干扰的同时监视新模型的性能。\n",
    "\n",
    "#### 计算实例扩展\n",
    "\n",
    "您可以指定单个实例（或节点）来提供您的在线预测请求服务。本教程使用单个节点，所以变量`MIN_NODES`和`MAX_NODES`都设置为`1`。\n",
    "\n",
    "如果您想要使用多个节点来提供在线预测请求服务，请将`MAX_NODES`设置为您希望使用的节点的最大数量。Vertex AI会自动调整用于提供预测的节点数量，最多达到您设置的最大数量。请参考[定价页面](https://cloud.google.com/vertex-ai/pricing#prediction-prices)来了解使用多个节点进行自动缩放的成本。\n",
    "\n",
    "#### Endpoint\n",
    "\n",
    "该方法将阻塞直到模型部署完成，并最终返回一个`Endpoint`对象。如果这是第一次将模型部署到端点，则可能需要几分钟额外来完成资源的配置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e5d1a523a28f"
   },
   "outputs": [],
   "source": [
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "DEPLOYED_NAME = f\"product-recommender-{UUID}-{TIMESTAMP}\"\n",
    "\n",
    "TRAFFIC_SPLIT = {\"0\": 100}\n",
    "\n",
    "MIN_NODES = 1\n",
    "MAX_NODES = 1\n",
    "\n",
    "endpoint = model.deploy(\n",
    "    deployed_model_display_name=DEPLOYED_NAME,\n",
    "    traffic_split=TRAFFIC_SPLIT,\n",
    "    machine_type=DEPLOY_COMPUTE,\n",
    "    min_replica_count=MIN_NODES,\n",
    "    max_replica_count=MAX_NODES,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90afa6cebc21"
   },
   "source": [
    "做一个预测\n",
    "最后，您对部署到端点的推荐模型进行在线预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9f156d4f16ec"
   },
   "source": [
    "### 准备测试项目\n",
    "您可以使用数据集的测试切片中的测试项目。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7466f7676f8c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set up training variables\n",
    "LABEL_COLUMN = \"rating\"\n",
    "UNUSED_COLUMNS = [\"timestamp\", \"entity_type_users\", \"product_id\"]\n",
    "NA_VALUES = [\"NA\", \".\", \" \", \"\", \"null\", \"NaN\"]\n",
    "\n",
    "# # Possible categorical values\n",
    "RATING = [0, 1, 2, 3, 4]\n",
    "\n",
    "df_test = pd.read_csv(CSV_FILE)\n",
    "\n",
    "\n",
    "# Remove NA values\n",
    "def clean_dataframe(df):\n",
    "    return df.replace(to_replace=NA_VALUES, value=np.NaN).dropna()\n",
    "\n",
    "\n",
    "df_test = clean_dataframe(df_test)\n",
    "\n",
    "user_ids = df_test[\"entity_type_users\"].unique().tolist()\n",
    "user2user_encoded = {x: i for i, x in enumerate(user_ids)}\n",
    "product_ids = df_test[\"product_id\"].unique().tolist()\n",
    "product_encoded2product = {i: x for i, x in enumerate(product_ids)}\n",
    "product2product_encoded = {x: i for i, x in enumerate(product_ids)}\n",
    "\n",
    "df_test[\"user\"] = df_test[\"entity_type_users\"].map(user2user_encoded)\n",
    "df_test[\"product\"] = df_test[\"product_id\"].map(product2product_encoded)\n",
    "\n",
    "sample = df_test.sample(1)\n",
    "user_id = sample[\"user\"].values[0]\n",
    "products_bought = sample[\"product\"].to_list()\n",
    "products_not_bought = (\n",
    "    df_test[~df_test[\"product\"].isin(products_bought)][\"product\"].unique().tolist()\n",
    ")\n",
    "\n",
    "instances_input = [[float(user_id), k] for k in products_not_bought]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ab821131e70"
   },
   "source": [
    "发送预测请求\n",
    "接下来，您发送预测请求。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ccc139e500f2"
   },
   "outputs": [],
   "source": [
    "prediction = endpoint.predict(instances=instances_input)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bb9625a4b323"
   },
   "source": [
    "获取前10个产品推荐\n",
    "基于推荐模型预测的评分，我们为选定的`user_id`选择了前10个产品。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "532ed9990a40"
   },
   "outputs": [],
   "source": [
    "predictions_array = np.array(\n",
    "    [prediction.predictions[k][0] for k in range(len(prediction.predictions))]\n",
    ")\n",
    "top_rating_indices = predictions_array.argsort()[-10:][::-1]\n",
    "top_predictions = predictions_array[top_rating_indices]\n",
    "top_10_products = [\n",
    "    int(product_encoded2product.get(instances_input[k][1])) for k in top_rating_indices\n",
    "]\n",
    "print(top_10_products)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "delete_bq_dataset"
   },
   "source": [
    "清理工作\n",
    "### 删除 BigQuery 数据集\n",
    "\n",
    "使用方法 `delete_dataset()` 来删除一个 BigQuery 数据集以及其所有表，将参数 `delete_contents` 设置为 `True`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "delete_bq_dataset"
   },
   "outputs": [],
   "source": [
    "DESTINATION_DATASET = f\"product_recommendation_{UUID}\"\n",
    "dataset_id = \"{}.{}\".format(PROJECT_ID, DESTINATION_DATASET)\n",
    "dataset = bigquery.Dataset(dataset_id)\n",
    "bqclient.delete_dataset(dataset, delete_contents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "featurestore_delete"
   },
   "source": [
    "### 删除 `Featurestore` 资源\n",
    "\n",
    "您可以使用 `delete()` 方法删除指定的 `Featurestore` 资源，需要传入以下参数：\n",
    "\n",
    "- `force`: 一个标志，指示是否删除非空的 `Featurestore` 资源。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "featurestore_delete"
   },
   "outputs": [],
   "source": [
    "featurestore.delete(force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "04d0a00fb30c"
   },
   "source": [
    "删除Vertex AI `Model`和`Endpoint`\n",
    "\n",
    "接下来，部署并删除Vertex AI Model和Endpoint资源。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "342bff50ac09"
   },
   "outputs": [],
   "source": [
    "endpoint.undeploy_all()\n",
    "endpoint.delete()\n",
    "model.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "473a3ebd9014"
   },
   "source": [
    "### 删除 Google Cloud 存储桶\n",
    "最终，您删除了谷歌云存储桶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7207f0612a0c"
   },
   "outputs": [],
   "source": [
    "! gsutil -m rm -r $BUCKET_URI\n",
    "! gsutil rb $BUCKET_URI"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "get_started_vertex_feature_store.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
