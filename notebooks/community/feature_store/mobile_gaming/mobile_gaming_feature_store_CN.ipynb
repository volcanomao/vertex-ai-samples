{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "<table align=\"left\">\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/vertex-ai-samples/blob/main/notebooks/community/feature_store/mobile_gaming/mobile_gaming_feature_store.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> 在 Colab 中运行\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/feature_store/mobile_gaming/mobile_gaming_feature_store.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      在 GitHub 上查看\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/feature_store/mobile_gaming/mobile_gaming_feature_store.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      在 Vertex AI 工作台中打开\n",
    "    </a>\n",
    "  </td>                                                                                               \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## 概述\n",
    "\n",
    "想象一下，您是数据科学团队的成员，正在处理与[使用 Google Analytics 4（GA4）和 BigQuery ML 进行游戏开发者的流失预测](https://cloud.google.com/blog/topics/developers-practitioners/churn-prediction-game-developers-using-google-analytics-4-ga4-and-bigquery-ml)博客中报告的相同移动游戏应用程序相关的工作。\n",
    "\n",
    "业务希望将该信息实时应用于游戏中，以立即采取干预行动，以防止流失。具体来说，对于每个玩家，他们希望根据客户人口统计信息、行为信息和回归的倾向性来提供类似新物品或奖励包的游戏激励。\n",
    "\n",
    "去年，Google Cloud 宣布推出 Vertex AI，这是一个托管的机器学习（ML）平台，允许数据科学团队加快 ML 模型的部署和维护。该平台的构建块之一是 Vertex AI 功能存储库，提供了一个托管服务，用于低延迟可扩展的特征服务。此外，它是一个集中式特征存储库，具有易于搜索和发现功能的简单 API，并具有特征监控功能，以跟踪漂移等质量问题。\n",
    "\n",
    "在这个笔记本中，我们将展示 Vertex AI 功能存储库在准备投入生产的场景中的作用，用户在最后一次参与活动的前 24 小时内的活动以及游戏平台将使用这些活动来提高用户体验。下面是系统的高级图片\n",
    "\n",
    "<img src=\"./assets/mobile_gaming_architecture_1.png\">\n",
    "\n",
    "### 数据集\n",
    "\n",
    "数据集是来自一个名为“Flood It!”（Android、iOS）的实际移动游戏应用程序的公共样本导出数据。\n",
    "\n",
    "### 目标\n",
    "\n",
    "在以下笔记本中，您将了解 Vertex AI 功能存储库：\n",
    "\n",
    "1. 提供一个集中式特征存储库，具有易于搜索和发现功能的简单 API，以及获取它们进行训练/服务。\n",
    "\n",
    "2. 通过低延迟可扩展的特征服务，简化在线预测模型的部署。\n",
    "\n",
    "3. 通过执行点时间查找来获取历史数据进行训练，以缓解训练服务偏差和数据泄漏。\n",
    "\n",
    "**请注意，我们假设您已经知道如何设置 Vertex AI 特征存储库。如果您不知道，请查看[这个详细的笔记本](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/feature_store/gapic-feature-store.ipynb)。**\n",
    "\n",
    "### 成本\n",
    "\n",
    "本教程使用 Google Cloud 的计费组件：\n",
    "\n",
    "* Vertex AI\n",
    "* BigQuery\n",
    "* Cloud Storage\n",
    "\n",
    "了解[Vertex AI 定价](https://cloud.google.com/vertex-ai/pricing)和[Cloud Storage 定价](https://cloud.google.com/storage/pricing)，并使用[Pricing 计算器](https://cloud.google.com/products/calculator/)根据您的预期使用量生成成本估算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ze4-nDLfK4pw"
   },
   "source": [
    "### 设置本地开发环境\n",
    "\n",
    "**如果您正在使用Colab或Vertex AI Workbench笔记本电脑**，您的环境已经满足运行此笔记本的所有要求。您可以跳过这一步。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCuSR8GkAgzl"
   },
   "source": [
    "如果不符合条件，请确保您的环境符合本笔记本的要求。\n",
    "您需要以下内容：\n",
    "\n",
    "* Google Cloud SDK\n",
    "* Git\n",
    "* Python 3\n",
    "* virtualenv\n",
    "* 在使用Python 3的虚拟环境中运行的Jupyter笔记本\n",
    "\n",
    "Google Cloud的[设置Python开发环境指南](https://cloud.google.com/python/setup)和[Jupyter安装指南](https://jupyter.org/install)提供了满足这些要求的详细说明。以下步骤提供一套简洁的说明：\n",
    "\n",
    "1. [安装并初始化Cloud SDK。](https://cloud.google.com/sdk/docs/)\n",
    "\n",
    "1. [安装Python 3。](https://cloud.google.com/python/setup#installing_python)\n",
    "\n",
    "1. [安装virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv)\n",
    "   并创建一个使用Python 3的虚拟环境。激活虚拟环境。\n",
    "\n",
    "1. 要安装Jupyter，可以在终端中运行 `pip3 install jupyter` 命令。\n",
    "\n",
    "1. 要启动Jupyter，可以在终端中运行 `jupyter notebook` 命令。\n",
    "\n",
    "1. 在Jupyter Notebook Dashboard中打开此笔记本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "### 安装额外的包\n",
    "\n",
    "安装在您的笔记本环境中尚未安装的额外包依赖项，例如XGBoost。使用每个包的最新主要GA版本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2b4ef9b72d43"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Vertex AI Workbench Notebook product has specific requirements\n",
    "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\")\n",
    "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
    "    \"/opt/deeplearning/metadata/env_version\"\n",
    ")\n",
    "\n",
    "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_WORKBENCH_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_vr6BYED_5my"
   },
   "outputs": [],
   "source": [
    "! pip3 install {USER_FLAG} --upgrade pip -q\n",
    "! pip3 install {USER_FLAG} --upgrade google-cloud-aiplatform==1.11.0 -q --no-warn-conflicts\n",
    "! pip3 install {USER_FLAG} git+https://github.com/googleapis/python-aiplatform.git@main # For features monitoring\n",
    "! pip3 install {USER_FLAG} --upgrade google-cloud-bigquery==2.24.0 -q --no-warn-conflicts\n",
    "! pip3 install {USER_FLAG} --upgrade xgboost==1.1.1 -q --no-warn-conflicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhq5zEbGg0XX"
   },
   "source": [
    "### 重新启动内核\n",
    "\n",
    "在安装了额外的包之后，您需要重新启动笔记本内核，以便它可以找到这些包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EzrelQZ22IZj"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWEdiXsJg0XY"
   },
   "source": [
    "## 在你开始之前"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF1j6f9HApxa"
   },
   "source": [
    "### 设置您的 Google Cloud 项目\n",
    "\n",
    "**无论您使用什么笔记本环境，都需要执行以下步骤。**\n",
    "\n",
    "1. [选择或创建 Google Cloud 项目](https://console.cloud.google.com/cloud-resource-manager)。当您首次创建帐户时，您可以获得$300的免费信用额度，用于支付计算/存储成本。\n",
    "\n",
    "1. [确保项目已启用计费](https://cloud.google.com/billing/docs/how-to/modify-project)。\n",
    "\n",
    "1. [启用 API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,notebooks.googleapis.com,)。\n",
    "\n",
    "1. 如果您在本地运行此笔记本，您需要安装[Cloud SDK](https://cloud.google.com/sdk)。\n",
    "\n",
    "1. 在下面的单元格中输入您的项目 ID。然后运行该单元格，确保 Cloud SDK 在此笔记本中的所有命令中使用正确的项目。\n",
    "\n",
    "**注意**：Jupyter 运行以 `!` 作为前缀的行作为 shell 命令，并将以 `$` 作为前缀的 Python 变量插入这些命令中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WReHDGG5g0XY"
   },
   "source": [
    "#### 设置您的项目ID\n",
    "\n",
    "**如果您不知道您的项目ID**，您可以使用`gcloud`来获取您的项目ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oM1iC_MfAts1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_ID = \"\"\n",
    "\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJYoRfYng0XZ"
   },
   "source": [
    "否则，请在这里设置您的项目ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "riG_qUokg0XZ"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None:\n",
    "    PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_gcloud_project_id"
   },
   "outputs": [],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23988890fef6"
   },
   "source": [
    "#### 获取您的项目编号（可选）\n",
    "\n",
    "现在项目ID已设置，您将获得相应的项目编号。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2d6950574e1d"
   },
   "outputs": [],
   "source": [
    "shell_output = ! gcloud projects list --filter=\"PROJECT_ID:'{PROJECT_ID}'\" --format='value(PROJECT_NUMBER)'\n",
    "PROJECT_NUMBER = shell_output[0]\n",
    "print(\"Project Number:\", PROJECT_NUMBER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "#### 区域\n",
    "\n",
    "您还可以更改“REGION”变量，该变量用于笔记本的其余操作。以下是Vertex AI支持的区域。我们建议您选择最靠近您的区域。\n",
    "\n",
    "- 美洲：`us-central1`\n",
    "- 欧洲：`europe-west4`\n",
    "- 亚太地区：`asia-east1`\n",
    "\n",
    "您可能无法使用多区域存储桶进行 Vertex AI 的训练。并非所有区域都支持所有 Vertex AI 服务。\n",
    "\n",
    "了解更多关于[Vertex AI 区域](https://cloud.google.com/vertex-ai/docs/general/locations)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jIcZV7-C2RrX"
   },
   "outputs": [],
   "source": [
    "REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
    "\n",
    "if REGION == \"[your-region]\":\n",
    "    REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06571eb4063b"
   },
   "source": [
    "时间戳\n",
    "\n",
    "如果您正在进行实时教程会话，您可能正在使用共享的测试账户或项目。为了避免用户在创建的资源之间发生名称冲突，您可以为每个实例会话创建一个时间戳，并将其附加到您在本教程中创建的资源名称之后。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "697568e92bd6"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dr--iN2kAylZ"
   },
   "source": [
    "### 验证您的 Google Cloud 账户\n",
    "\n",
    "**如果您正在使用 Vertex AI Workbench 笔记本**，您的环境已经通过验证。请跳过此步骤。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBCra4QMA2wR"
   },
   "source": [
    "如果您正在使用Colab，请运行下面的单元格，并按照提示进行oAuth身份验证。\n",
    "\n",
    "否则，请按照以下步骤操作：\n",
    "\n",
    "1. 在Cloud控制台中，转到[**创建服务帐号密钥**页面](https://console.cloud.google.com/apis/credentials/serviceaccountkey)。\n",
    "\n",
    "2. 单击**创建服务帐号**。\n",
    "\n",
    "3. 在**服务帐号名称**字段中输入名称，然后单击**创建**。\n",
    "\n",
    "4. 在**将此服务帐号授予项目访问权限**部分，单击**角色**下拉菜单并添加以下角色：\n",
    "   - BigQuery管理员\n",
    "   - 存储管理员\n",
    "   - 存储对象管理员\n",
    "   - Vertex AI管理员\n",
    "   - Vertex AI特征存储管理员\n",
    "\n",
    "5. 单击*创建*。包含您密钥的JSON文件将下载到您的本地环境。\n",
    "\n",
    "6. 在下面的单元格中将您的服务帐号密钥路径输入为`GOOGLE_APPLICATION_CREDENTIALS`变量，然后运行该单元格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PyQmSRbKA8r-"
   },
   "outputs": [],
   "source": [
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# If on Vertex AI Workbench, then don't execute this code\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\") and not os.getenv(\n",
    "    \"DL_ANACONDA_HOME\"\n",
    "):\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\"):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "### 创建一个云存储桶\n",
    "\n",
    "**以下步骤是必需的，无论您使用的是哪种笔记本环境。**\n",
    "\n",
    "在下面设置您的云存储桶的名称。它必须在所有云存储桶中是唯一的。\n",
    "\n",
    "您也可以更改`REGION`变量，该变量在本笔记本的其余部分中使用。确保[选择一个支持 Vertex AI 服务的区域](https://cloud.google.com/vertex-ai/docs/general/locations#available_regions)。您不能使用多区域存储桶来训练 Vertex AI。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"[your-bucket-name]\"  # @param {type:\"string\"}\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cf221059d072"
   },
   "outputs": [],
   "source": [
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"[your-bucket-name]\":\n",
    "    BUCKET_NAME = PROJECT_ID + \"-aip-\" + TIMESTAMP\n",
    "    BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EcIXiGsCePi"
   },
   "source": [
    "只有在您的存储桶不存在时才运行以下单元格以创建您的云存储桶。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIq7R4HZCfIc"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucvCsknMCims"
   },
   "source": [
    "最后，通过检查其内容来验证对云存储桶的访问权限。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vhOb7YnwClBb"
   },
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "set_service_account"
   },
   "source": [
    "服务账户（可选）\n",
    "\n",
    "如果您不想使用项目的Compute Engine服务账户，请将 `SERVICE_ACCOUNT` 设置为另一个服务账户ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MQVV9haf2Rra"
   },
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT = \"[your-service-account]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "autoset_service_account"
   },
   "outputs": [],
   "source": [
    "if (\n",
    "    SERVICE_ACCOUNT == \"\"\n",
    "    or SERVICE_ACCOUNT is None\n",
    "    or SERVICE_ACCOUNT == \"[your-service-account]\"\n",
    "):\n",
    "    # Get your service account from gcloud\n",
    "    if not IS_COLAB:\n",
    "        shell_output = !gcloud auth list 2>/dev/null\n",
    "        SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
    "\n",
    "    else:  # IS_COLAB:\n",
    "        shell_output = ! gcloud projects describe  $PROJECT_ID\n",
    "        project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
    "        SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
    "\n",
    "    print(\"Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "set_service_account:pipelines"
   },
   "source": [
    "设置服务帐户访问权限\n",
    "\n",
    "运行以下命令以授予您的服务帐户访问权限。每个服务帐户只需运行此步骤一次。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4UpQThc2Rrb"
   },
   "outputs": [],
   "source": [
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
    "\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "utO91mebwEmw"
   },
   "source": [
    "创建一个BigQuery数据集\n",
    "\n",
    "您可以创建BigQuery数据集来存储演示中的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8615339fa4ca"
   },
   "outputs": [],
   "source": [
    "BQ_DATASET = \"Mobile_Gaming\"  # @param {type:\"string\"}\n",
    "LOCATION = \"US\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G3G2BXqswb_J"
   },
   "outputs": [],
   "source": [
    "!bq mk --location=$LOCATION --dataset $PROJECT_ID:$BQ_DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRUOFELefqf1"
   },
   "outputs": [],
   "source": [
    "# General\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Data Science\n",
    "import pandas as pd\n",
    "# Vertex AI and its Feature Store\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.aiplatform import Feature, Featurestore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tgP_N3MSpnd3"
   },
   "source": [
    "### 定义常量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b_k2ejxQsET8"
   },
   "outputs": [],
   "source": [
    "# Data Engineering and Feature Engineering\n",
    "TODAY = \"2022-06-16\"\n",
    "LABEL_TABLE = f\"label_table_{TODAY}\".replace(\"-\", \"\")\n",
    "FEATURES_TABLE = f\"wide_features_table_{TODAY}\"  # @param {type:\"string\"}\n",
    "FEATURESTORE_ID = \"mobile_gaming\"  # @param {type:\"string\"}\n",
    "ENTITY_TYPE_ID = \"user\"\n",
    "\n",
    "# Vertex AI Feature store\n",
    "ONLINE_STORE_NODES_COUNT = 5\n",
    "ENTITY_ID = \"user\"\n",
    "API_ENDPOINT = f\"{REGION}-aiplatform.googleapis.com\"\n",
    "FEATURE_TIME = \"timestamp\"\n",
    "ENTITY_ID_FIELD = \"user_pseudo_id\"\n",
    "BQ_SOURCE_URI = f\"bq://{PROJECT_ID}.{BQ_DATASET}.{FEATURES_TABLE}\"\n",
    "GCS_DESTINATION_PATH = f\"data/features/train_features_{TODAY}\".replace(\"-\", \"\")\n",
    "GCS_DESTINATION_OUTPUT_URI = f\"{BUCKET_URI}/{GCS_DESTINATION_PATH}\"\n",
    "SERVING_FEATURE_IDS = {\"user\": [\"*\"]}\n",
    "READ_INSTANCES_TABLE = f\"ground_truth_{TODAY}\".replace(\"-\", \"\")\n",
    "READ_INSTANCES_URI = f\"bq://{PROJECT_ID}.{BQ_DATASET}.{READ_INSTANCES_TABLE}\"\n",
    "\n",
    "# Vertex AI Training\n",
    "BASE_CPU_IMAGE = \"us-docker.pkg.dev/vertex-ai/training/scikit-learn-cpu.0-23:latest\"\n",
    "DATASET_NAME = f\"churn_mobile_gaming_{TODAY}\".replace(\"-\", \"\")\n",
    "TRAIN_JOB_NAME = f\"xgb_classifier_training_{TODAY}\".replace(\"-\", \"\")\n",
    "MODEL_NAME = f\"churn_xgb_classifier_{TODAY}\".replace(\"-\", \"\")\n",
    "MODEL_PACKAGE_PATH = \"train_package\"\n",
    "TRAINING_MACHINE_TYPE = \"n1-standard-4\"\n",
    "TRAINING_REPLICA_COUNT = 1\n",
    "DATA_PATH = f\"{GCS_DESTINATION_OUTPUT_URI}/000000000000.csv\".replace(\"gs://\", \"/gcs/\")\n",
    "MODEL_PATH = f\"model/{TODAY}\".replace(\"-\", \"\")\n",
    "MODEL_DIR = f\"{BUCKET_URI}/{MODEL_PATH}\".replace(\"gs://\", \"/gcs/\")\n",
    "\n",
    "# Vertex AI Prediction\n",
    "DESTINATION_URI = f\"{BUCKET_URI}/{MODEL_PATH}\"\n",
    "VERSION = \"v1\"\n",
    "SERVING_CONTAINER_IMAGE_URI = (\n",
    "    \"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.0-23:latest\"\n",
    ")\n",
    "ENDPOINT_NAME = \"mobile_gaming_churn\"\n",
    "DEPLOYED_MODEL_NAME = f\"churn_xgb_classifier_{VERSION}\"\n",
    "MODEL_DEPLOYED_NAME = \"churn_xgb_classifier_v1\"\n",
    "SERVING_MACHINE_TYPE = \"n1-highcpu-4\"\n",
    "MIN_NODES = 1\n",
    "MAX_NODES = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hY99VsxQBOU9"
   },
   "outputs": [],
   "source": [
    "# Sampling distributions for categorical features implemented in\n",
    "# https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/model_monitoring/model_monitoring.ipynb\n",
    "\n",
    "LANGUAGE = [\n",
    "    \"en-us\",\n",
    "    \"en-gb\",\n",
    "    \"ja-jp\",\n",
    "    \"en-au\",\n",
    "    \"en-ca\",\n",
    "    \"de-de\",\n",
    "    \"en-in\",\n",
    "    \"en\",\n",
    "    \"fr-fr\",\n",
    "    \"pt-br\",\n",
    "    \"es-us\",\n",
    "    \"zh-tw\",\n",
    "    \"zh-hans-cn\",\n",
    "    \"es-mx\",\n",
    "    \"nl-nl\",\n",
    "    \"fr-ca\",\n",
    "    \"en-za\",\n",
    "    \"vi-vn\",\n",
    "    \"en-nz\",\n",
    "    \"es-es\",\n",
    "]\n",
    "\n",
    "OS = [\"IOS\", \"ANDROID\", \"null\"]\n",
    "COUNTRY = [\n",
    "    \"United States\",\n",
    "    \"India\",\n",
    "    \"Japan\",\n",
    "    \"Canada\",\n",
    "    \"Australia\",\n",
    "    \"United Kingdom\",\n",
    "    \"Germany\",\n",
    "    \"Mexico\",\n",
    "    \"France\",\n",
    "    \"Brazil\",\n",
    "    \"Taiwan\",\n",
    "    \"China\",\n",
    "    \"Saudi Arabia\",\n",
    "    \"Pakistan\",\n",
    "    \"Egypt\",\n",
    "    \"Netherlands\",\n",
    "    \"Vietnam\",\n",
    "    \"Philippines\",\n",
    "    \"South Africa\",\n",
    "]\n",
    "\n",
    "USER_IDS = [\n",
    "    \"C8685B0DFA2C4B4E6E6EA72894C30F6F\",\n",
    "    \"A976A39B8E08829A5BC5CD3827C942A2\",\n",
    "    \"DD2269BCB7F8532CD51CB6854667AF51\",\n",
    "    \"A8F327F313C9448DFD5DE108DAE66100\",\n",
    "    \"8BE7BF90C971453A34C1FF6FF2A0ACAE\",\n",
    "    \"8375B114AFAD8A31DE54283525108F75\",\n",
    "    \"4AD259771898207D5869B39490B9DD8C\",\n",
    "    \"51E859FD9D682533C094B37DC85EAF87\",\n",
    "    \"8C33815E0A269B776AAB4B60A4F7BC63\",\n",
    "    \"D7EA8E3645EFFBD6443946179ED704A6\",\n",
    "    \"58F3D672BBC613680624015D5BC3ADDB\",\n",
    "    \"FF955E4CA27C75CE0BEE9FC89AD275A3\",\n",
    "    \"22DC6A6AE86C0AA33EBB8C3164A26925\",\n",
    "    \"BC10D76D02351BD4C6F6F5437EE5D274\",\n",
    "    \"19DEEA6B15B314DB0ED2A4936959D8F9\",\n",
    "    \"C2D17D9066EE1EB9FAE1C8A521BFD4E5\",\n",
    "    \"EFBDEC168A2BF8C727B060B2E231724E\",\n",
    "    \"E43D3AB2F9B9055C29373523FAF9DB9B\",\n",
    "    \"BBDCBE2491658165B7F20540DE652E3A\",\n",
    "    \"6895EEFC23B59DB13A9B9A7EED6A766F\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTjqqE0jafnn"
   },
   "source": [
    "### 帮手"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9dJ_TGTVak7u"
   },
   "outputs": [],
   "source": [
    "def run_bq_query(query: str):\n",
    "    \"\"\"\n",
    "    An helper function to run a BigQuery job\n",
    "    Args:\n",
    "        query: a formatted SQL query\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        job = bq_client.query(query)\n",
    "        _ = job.result()\n",
    "    except RuntimeError as error:\n",
    "        print(error)\n",
    "\n",
    "\n",
    "def upload_model(\n",
    "    display_name: str,\n",
    "    serving_container_image_uri: str,\n",
    "    artifact_uri: str,\n",
    "    sync: bool = True,\n",
    ") -> vertex_ai.Model:\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        display_name: The name of Vertex AI Model artefact\n",
    "        serving_container_image_uri: The uri of the serving image\n",
    "        artifact_uri: The uri of artefact to import\n",
    "        sync:\n",
    "\n",
    "    Returns: Vertex AI Model\n",
    "\n",
    "    \"\"\"\n",
    "    model = vertex_ai.Model.upload(\n",
    "        display_name=display_name,\n",
    "        artifact_uri=artifact_uri,\n",
    "        serving_container_image_uri=serving_container_image_uri,\n",
    "        sync=sync,\n",
    "    )\n",
    "    model.wait()\n",
    "    print(model.display_name)\n",
    "    print(model.resource_name)\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_endpoint(display_name: str) -> vertex_ai.Endpoint:\n",
    "    \"\"\"\n",
    "    An utility to create a Vertex AI Endpoint\n",
    "    Args:\n",
    "        display_name: The name of Endpoint\n",
    "\n",
    "    Returns: Vertex AI Endpoint\n",
    "\n",
    "    \"\"\"\n",
    "    endpoint = vertex_ai.Endpoint.create(display_name=display_name)\n",
    "\n",
    "    print(endpoint.display_name)\n",
    "    print(endpoint.resource_name)\n",
    "    return endpoint\n",
    "\n",
    "\n",
    "def deploy_model(\n",
    "    model: vertex_ai.Model,\n",
    "    machine_type: str,\n",
    "    endpoint: vertex_ai.Endpoint = None,\n",
    "    deployed_model_display_name: str = None,\n",
    "    min_replica_count: int = 1,\n",
    "    max_replica_count: int = 1,\n",
    "    sync: bool = True,\n",
    ") -> vertex_ai.Model:\n",
    "    \"\"\"\n",
    "    An helper function to deploy a Vertex AI Endpoint\n",
    "    Args:\n",
    "        model: A Vertex AI Model\n",
    "        machine_type: The type of machine to serve the model\n",
    "        endpoint: An Vertex AI Endpoint\n",
    "        deployed_model_display_name: The name of the model\n",
    "        min_replica_count: Minimum number of serving replicas\n",
    "        max_replica_count: Max number of serving replicas\n",
    "        sync: Whether to execute method synchronously\n",
    "\n",
    "    Returns: vertex_ai.Model\n",
    "\n",
    "    \"\"\"\n",
    "    model_deployed = model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        deployed_model_display_name=deployed_model_display_name,\n",
    "        machine_type=machine_type,\n",
    "        min_replica_count=min_replica_count,\n",
    "        max_replica_count=max_replica_count,\n",
    "        sync=sync,\n",
    "    )\n",
    "\n",
    "    model_deployed.wait()\n",
    "\n",
    "    print(model_deployed.display_name)\n",
    "    print(model_deployed.resource_name)\n",
    "    return model_deployed\n",
    "\n",
    "\n",
    "def endpoint_predict_sample(\n",
    "    instances: list, endpoint: vertex_ai.Endpoint\n",
    ") -> vertex_ai.models.Prediction:\n",
    "    \"\"\"\n",
    "    An helper function to get prediction from Vertex AI Endpoint\n",
    "    Args:\n",
    "        instances: The list of instances to score\n",
    "        endpoint: An Vertex AI Endpoint\n",
    "\n",
    "    Returns:\n",
    "        vertex_ai.models.Prediction\n",
    "\n",
    "    \"\"\"\n",
    "    prediction = endpoint.predict(instances=instances)\n",
    "    print(prediction)\n",
    "    return prediction\n",
    "\n",
    "\n",
    "def generate_online_sample() -> dict:\n",
    "    \"\"\"\n",
    "    An helper function to generate a sample of online features\n",
    "    Returns:\n",
    "        online_sample: dict of online features\n",
    "    \"\"\"\n",
    "    online_sample = {}\n",
    "    online_sample[\"entity_id\"] = random.choices(USER_IDS)\n",
    "    online_sample[\"country\"] = random.choices(COUNTRY)\n",
    "    online_sample[\"operating_system\"] = random.choices(OS)\n",
    "    online_sample[\"language\"] = random.choices(LANGUAGE)\n",
    "    return online_sample\n",
    "\n",
    "\n",
    "def simulate_prediction(endpoint: vertex_ai.Endpoint, n_requests: int, latency: int):\n",
    "    \"\"\"\n",
    "    An helper function to simulate online prediction with customer entity type\n",
    "        - format entities for prediction\n",
    "        - retrieve static features with a singleton lookup operations from Vertex AI Feature store\n",
    "        - run the prediction request and get back the result\n",
    "    Args:\n",
    "        endpoint: Vertex AI Endpoint object\n",
    "        n_requests: number of requests to run\n",
    "        latency: latency in seconds\n",
    "    Returns:\n",
    "        vertex_ai.models.Prediction\n",
    "    \"\"\"\n",
    "    for i in range(n_requests):\n",
    "        online_sample = generate_online_sample()\n",
    "        online_features = pd.DataFrame.from_dict(online_sample)\n",
    "        entity_ids = online_features[\"entity_id\"].tolist()\n",
    "\n",
    "        customer_aggregated_features = user_entity_type.read(\n",
    "            entity_ids=entity_ids,\n",
    "            feature_ids=[\n",
    "                \"cnt_user_engagement\",\n",
    "                \"cnt_level_start_quickplay\",\n",
    "                \"cnt_level_end_quickplay\",\n",
    "                \"cnt_level_complete_quickplay\",\n",
    "                \"cnt_level_reset_quickplay\",\n",
    "                \"cnt_post_score\",\n",
    "                \"cnt_spend_virtual_currency\",\n",
    "                \"cnt_ad_reward\",\n",
    "                \"cnt_challenge_a_friend\",\n",
    "                \"cnt_completed_5_levels\",\n",
    "                \"cnt_use_extra_steps\",\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        prediction_sample_df = pd.merge(\n",
    "            customer_aggregated_features.set_index(\"entity_id\"),\n",
    "            online_features.set_index(\"entity_id\"),\n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "        ).reset_index(drop=True)\n",
    "\n",
    "        # prediction_sample = prediction_sample_df.to_dict(\"records\")\n",
    "        prediction_instance = prediction_sample_df.values.tolist()\n",
    "        prediction = endpoint.predict(prediction_instance)\n",
    "        print(\n",
    "            f\"Prediction request: user_id - {entity_ids} - values - {prediction_instance} - prediction - {prediction[0]}\"\n",
    "        )\n",
    "        time.sleep(latency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MewDhNbrXf_N"
   },
   "source": [
    "# 设定实时场景\n",
    "\n",
    "为了进行实时流失预测，您需要\n",
    "\n",
    "1. 收集关于用户事件和行为的历史数据\n",
    "2. 设计您的数据模型，构建特征并将它们投入特征存储中，以便同时为脱机训练和在线服务。\n",
    "3. 定义流失，并获取用于训练流失模型的数据\n",
    "4. 在规模上训练模型\n",
    "5. 部署模型到终端点，并实时生成预测分数\n",
    "\n",
    "您将在下文详细介绍这些步骤。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "source": [
    "### 初始化 Python 的 Vertex AI SDK\n",
    "\n",
    "为您的项目和相应的存储桶初始化 Python 的 Vertex AI SDK。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "poLJ0fV52Rrc"
   },
   "outputs": [],
   "source": [
    "vertex_ai.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ffd54e97270"
   },
   "source": [
    "### 初始化Python的BigQuery SDK\n",
    "\n",
    "为您的项目和相应的存储桶初始化Python的BigQuery AI SDK。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9f48872501fb"
   },
   "outputs": [],
   "source": [
    "bq_client = bigquery.Client(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WnUQO2IHC9pZ"
   },
   "source": [
    "识别用户并构建您的特征\n",
    "\n",
    "这一部分，我们将从Vertex AI Feature Store中提取静态特征。具体来说，我们将涵盖以下步骤：\n",
    "\n",
    "1. 识别用户，使用**BigQuery**处理人口统计特征和行为特征，在过去24小时内进行处理。\n",
    "\n",
    "2. 设置特征存储\n",
    "\n",
    "3. 使用**Vertex AI Feature Store**和SDK注册特征。\n",
    "\n",
    "下面有一幅图显示了整个过程。\n",
    "\n",
    "原始数据集包含我们无法直接在特征存储中提取的原始事件数据。我们需要预处理原始数据，以获取用户特征。\n",
    "\n",
    "**请注意我们模拟这些转换在不同的时间点（今天和明天）。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e9zIrwhpDF2q"
   },
   "source": [
    "### 标签、人口统计和行为转换\n",
    "\n",
    "该部分基于 Minhaz Kazi 和 Polong Lin 撰写的[《使用Google Analytics 4（GA4）和BigQuery ML为游戏开发者预测流失》](https://cloud.google.com/blog/topics/developers-practitioners/churn-prediction-game-developers-using-google-analytics-4-ga4-and-bigquery-ml?utm_source=linkedin&utm_medium=unpaidsoc&utm_campaign=FY21-Q2-Google-Cloud-Tech-Blog&utm_content=google-analytics-4&utm_term=-)博客文章。\n",
    "\n",
    "您将对其进行调整，将批次流失预测（使用首次参与用户的最初24小时内的特征）转换为实时流失预测（使用最后参与用户的最初6小时内的特征）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RQX5m8UiC_px"
   },
   "outputs": [],
   "source": [
    "features_sql_query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE\n",
    "  `{PROJECT_ID}.{BQ_DATASET}.{FEATURES_TABLE}` AS\n",
    "WITH\n",
    "\n",
    "  # query to extract demographic data for each user ---------------------------------------------------------\n",
    "  get_demographic_data AS (\n",
    "  SELECT * EXCEPT (row_num)\n",
    "  FROM (\n",
    "    SELECT\n",
    "      user_pseudo_id,\n",
    "      geo.country as country,\n",
    "      device.operating_system as operating_system,\n",
    "      device.language as language,\n",
    "      ROW_NUMBER() OVER (PARTITION BY user_pseudo_id ORDER BY event_timestamp DESC) AS row_num\n",
    "    FROM `firebase-public-project.analytics_153293282.events_*`)\n",
    "  WHERE row_num = 1),\n",
    "\n",
    "  # query to extract behavioral data for each user ----------------------------------------------------------\n",
    "  get_behavioral_data AS (\n",
    "  SELECT\n",
    "    event_timestamp,\n",
    "    user_pseudo_id,\n",
    "    SUM(IF(event_name = 'user_engagement', 1, 0)) OVER (PARTITION BY user_pseudo_id ORDER BY event_timestamp ASC RANGE BETWEEN 21600000000 PRECEDING\n",
    "      AND CURRENT ROW ) AS cnt_user_engagement,\n",
    "    SUM(IF(event_name = 'level_start_quickplay', 1, 0)) OVER (PARTITION BY user_pseudo_id ORDER BY event_timestamp ASC RANGE BETWEEN 21600000000 PRECEDING\n",
    "      AND CURRENT ROW ) AS cnt_level_start_quickplay,\n",
    "    SUM(IF(event_name = 'level_end_quickplay', 1, 0)) OVER (PARTITION BY user_pseudo_id ORDER BY event_timestamp ASC RANGE BETWEEN 21600000000 PRECEDING\n",
    "      AND CURRENT ROW ) AS cnt_level_end_quickplay,\n",
    "    SUM(IF(event_name = 'level_complete_quickplay', 1, 0)) OVER (PARTITION BY user_pseudo_id ORDER BY event_timestamp ASC RANGE BETWEEN 21600000000 PRECEDING\n",
    "      AND CURRENT ROW ) AS cnt_level_complete_quickplay,\n",
    "    SUM(IF(event_name = 'level_reset_quickplay', 1, 0)) OVER (PARTITION BY user_pseudo_id ORDER BY event_timestamp ASC RANGE BETWEEN 21600000000 PRECEDING\n",
    "      AND CURRENT ROW ) AS cnt_level_reset_quickplay,\n",
    "    SUM(IF(event_name = 'post_score', 1, 0)) OVER (PARTITION BY user_pseudo_id ORDER BY event_timestamp ASC RANGE BETWEEN 21600000000 PRECEDING\n",
    "      AND CURRENT ROW ) AS cnt_post_score,\n",
    "    SUM(IF(event_name = 'spend_virtual_currency', 1, 0)) OVER (PARTITION BY user_pseudo_id ORDER BY event_timestamp ASC RANGE BETWEEN 21600000000 PRECEDING\n",
    "      AND CURRENT ROW ) AS cnt_spend_virtual_currency,\n",
    "    SUM(IF(event_name = 'ad_reward', 1, 0)) OVER (PARTITION BY user_pseudo_id ORDER BY event_timestamp ASC RANGE BETWEEN 21600000000 PRECEDING\n",
    "      AND CURRENT ROW ) AS cnt_ad_reward,\n",
    "    SUM(IF(event_name = 'challenge_a_friend', 1, 0)) OVER (PARTITION BY user_pseudo_id ORDER BY event_timestamp ASC RANGE BETWEEN 21600000000 PRECEDING\n",
    "      AND CURRENT ROW ) AS cnt_challenge_a_friend,\n",
    "    SUM(IF(event_name = 'completed_5_levels', 1, 0)) OVER (PARTITION BY user_pseudo_id ORDER BY event_timestamp ASC RANGE BETWEEN 21600000000 PRECEDING\n",
    "      AND CURRENT ROW ) AS cnt_completed_5_levels,\n",
    "    SUM(IF(event_name = 'use_extra_steps', 1, 0)) OVER (PARTITION BY user_pseudo_id ORDER BY event_timestamp ASC RANGE BETWEEN 21600000000 PRECEDING\n",
    "      AND CURRENT ROW ) AS cnt_use_extra_steps,\n",
    "  FROM (\n",
    "    SELECT\n",
    "      e.*\n",
    "    FROM\n",
    "      `firebase-public-project.analytics_153293282.events_*` AS e\n",
    "    )\n",
    ")\n",
    "\n",
    "SELECT\n",
    "    -- PARSE_TIMESTAMP('%Y-%m-%d %H:%M:%S', CONCAT('{TODAY}', ' ', STRING(TIME_TRUNC(CURRENT_TIME(), SECOND))), 'UTC') as timestamp,\n",
    "    TIMESTAMP_ADD(PARSE_TIMESTAMP('%Y-%m-%d %H:%M:%S', FORMAT_TIMESTAMP('%Y-%m-%d %H:%M:%S', TIMESTAMP_MICROS(beh.event_timestamp))), INTERVAL 1351 DAY) AS timestamp,\n",
    "    dem.*,\n",
    "    CAST(IFNULL(beh.cnt_user_engagement, 0) AS FLOAT64)  AS cnt_user_engagement,\n",
    "    CAST(IFNULL(beh.cnt_level_start_quickplay, 0) AS FLOAT64) AS cnt_level_start_quickplay,\n",
    "    CAST(IFNULL(beh.cnt_level_end_quickplay, 0) AS FLOAT64) AS cnt_level_end_quickplay,\n",
    "    CAST(IFNULL(beh.cnt_level_complete_quickplay, 0) AS FLOAT64) AS cnt_level_complete_quickplay,\n",
    "    CAST(IFNULL(beh.cnt_level_reset_quickplay, 0) AS FLOAT64) AS cnt_level_reset_quickplay,\n",
    "    CAST(IFNULL(beh.cnt_post_score, 0) AS FLOAT64) AS cnt_post_score,\n",
    "    CAST(IFNULL(beh.cnt_spend_virtual_currency, 0) AS FLOAT64) AS cnt_spend_virtual_currency,\n",
    "    CAST(IFNULL(beh.cnt_ad_reward, 0) AS FLOAT64) AS cnt_ad_reward,\n",
    "    CAST(IFNULL(beh.cnt_challenge_a_friend, 0) AS FLOAT64) AS cnt_challenge_a_friend,\n",
    "    CAST(IFNULL(beh.cnt_completed_5_levels, 0) AS FLOAT64) AS cnt_completed_5_levels,\n",
    "    CAST(IFNULL(beh.cnt_use_extra_steps, 0) AS FLOAT64) AS cnt_use_extra_steps,\n",
    "FROM\n",
    "  get_demographic_data dem\n",
    "LEFT OUTER JOIN \n",
    "  get_behavioral_data beh\n",
    "ON\n",
    "  dem.user_pseudo_id = beh.user_pseudo_id\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oGYxLCSnD068"
   },
   "outputs": [],
   "source": [
    "run_bq_query(features_sql_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xQLIlsTCD_nk"
   },
   "source": [
    "## 创建一个Vertex AI功能存储库并输入您的特征\n",
    "\n",
    "现在您有一个包含大量特征的宽表格。现在是将它们导入到特征存储库中的时候了。\n",
    "\n",
    "在继续之前，您可能会有一个问题：为什么在这种情况下我需要一个特征存储库呢？\n",
    "\n",
    "其中一个原因是要使这些特征能够跨团队访问，只需计算一次，就可以多次重复使用。为了实现这一点，您还需要能够随时监控这些特征以确保其新鲜度，并在需要时进行新的特征工程运行以对其进行刷新。\n",
    "\n",
    "如果这不是您的情况，我将在接下来的部分中提供更多关于为什么您应该考虑使用功能存储库的原因。现在请继续跟随我。\n",
    "\n",
    "其中一个最重要的事情与其数据模型有关。正如您在下面的图片中所看到的，Vertex AI Feature Store按照以下顺序层次化地组织资源：`Featurestore -> EntityType -> Feature`。您必须在将数据导入到Vertex AI功能存储库之前创建这些资源。\n",
    "\n",
    "在我们的情况下，我们将创建一个名为**mobile_gaming**的功能存储资源，其中包含**user**实体类型以及其所有关联的**特征**，例如国家或用户向朋友发起挑战的次数（cnt_challenge_a_friend）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VR7BJEozED_Q"
   },
   "source": [
    "### 创建特征存储，```mobile_gaming```\n",
    "\n",
    "您需要创建一个`featurestore`资源来包含实体类型、特征和特征值。在您的情况下，您将称其为`mobile_gaming`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vUFqtYU-EDTR"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    mobile_gaming_feature_store = Featurestore.create(\n",
    "        featurestore_id=FEATURESTORE_ID,\n",
    "        online_store_fixed_node_count=ONLINE_STORE_NODES_COUNT,\n",
    "        labels={\"team\": \"dataoffice\", \"app\": \"mobile_gaming\"},\n",
    "        sync=True,\n",
    "    )\n",
    "except RuntimeError as error:\n",
    "    print(error)\n",
    "else:\n",
    "    FEATURESTORE_RESOURCE_NAME = mobile_gaming_feature_store.resource_name\n",
    "    print(f\"Feature store created: {FEATURESTORE_RESOURCE_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mUlCwfdpEHJG"
   },
   "source": [
    "### 创建```User```实体类型及其特性\n",
    "\n",
    "您可以定义自己的实体类型，表示您决定引用特性的一个或多个级别。在您的情况下，它将有一个`user`实体。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PnCU1wBND3W7"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    user_entity_type = mobile_gaming_feature_store.create_entity_type(\n",
    "        entity_type_id=ENTITY_ID, description=\"User Entity\", sync=True\n",
    "    )\n",
    "except RuntimeError as error:\n",
    "    print(error)\n",
    "else:\n",
    "    USER_ENTITY_RESOURCE_NAME = user_entity_type.resource_name\n",
    "    print(\"Entity type name is\", USER_ENTITY_RESOURCE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bT9LXzu1EOvW"
   },
   "source": [
    "### 设置特征监控\n",
    "\n",
    "请注意，Vertex AI 特征存储具有[特征监控功能](https://cloud.google.com/vertex-ai/docs/featurestore/monitoring)。这是预览版，因此您需要使用比我们在此笔记本中迄今为止使用的更低级别的 v1beta1 Python API。\n",
    "\n",
    "目前设置最简单的方法是使用[控制台 UI](https://console.cloud.google.com/vertex-ai/features)。为了完整起见，以下是使用 v1beta1 SDK 进行此操作的示例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8WBlYUkOERaI"
   },
   "outputs": [],
   "source": [
    "from google.cloud.aiplatform_v1beta1 import \\\n",
    "    FeaturestoreServiceClient as v1beta1_FeaturestoreServiceClient\n",
    "from google.cloud.aiplatform_v1beta1.types import \\\n",
    "    entity_type as v1beta1_entity_type_pb2\n",
    "from google.cloud.aiplatform_v1beta1.types import \\\n",
    "    featurestore_monitoring as v1beta1_featurestore_monitoring_pb2\n",
    "from google.cloud.aiplatform_v1beta1.types import \\\n",
    "    featurestore_service as v1beta1_featurestore_service_pb2\n",
    "from google.protobuf.duration_pb2 import Duration\n",
    "\n",
    "v1beta1_admin_client = v1beta1_FeaturestoreServiceClient(\n",
    "    client_options={\"api_endpoint\": API_ENDPOINT}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "92X4-7PFETj5"
   },
   "outputs": [],
   "source": [
    "v1beta1_admin_client.update_entity_type(\n",
    "    v1beta1_featurestore_service_pb2.UpdateEntityTypeRequest(\n",
    "        entity_type=v1beta1_entity_type_pb2.EntityType(\n",
    "            name=v1beta1_admin_client.entity_type_path(\n",
    "                PROJECT_ID, REGION, FEATURESTORE_ID, ENTITY_ID\n",
    "            ),\n",
    "            monitoring_config=v1beta1_featurestore_monitoring_pb2.FeaturestoreMonitoringConfig(\n",
    "                snapshot_analysis=v1beta1_featurestore_monitoring_pb2.FeaturestoreMonitoringConfig.SnapshotAnalysis(\n",
    "                    monitoring_interval=Duration(seconds=86400),  # 1 day\n",
    "                ),\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxAuZjt3EWFo"
   },
   "source": [
    "### 创建特征\n",
    "\n",
    "为了接收特征，您需要提供特征配置并将其创建为特征商店资源。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hRXO2I5VEYwt"
   },
   "source": [
    "创建特性配置\n",
    "\n",
    "为简单起见，我以声明方式创建了配置。当然，我们可以创建一个帮助函数，从Bigquery模式中构建它。\n",
    "还要注意，我们希望可以动态地传递一些特性。在这种情况下，国家、操作系统和语言看起来非常适合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K26NEYZIEbvE"
   },
   "outputs": [],
   "source": [
    "feature_configs = {\n",
    "    \"country\": {\n",
    "        \"value_type\": \"STRING\",\n",
    "        \"description\": \"The country of customer\",\n",
    "        \"labels\": {\"status\": \"passed\"},\n",
    "    },\n",
    "    \"operating_system\": {\n",
    "        \"value_type\": \"STRING\",\n",
    "        \"description\": \"The operating system of device\",\n",
    "        \"labels\": {\"status\": \"passed\"},\n",
    "    },\n",
    "    \"language\": {\n",
    "        \"value_type\": \"STRING\",\n",
    "        \"description\": \"The language of device\",\n",
    "        \"labels\": {\"status\": \"passed\"},\n",
    "    },\n",
    "    \"cnt_user_engagement\": {\n",
    "        \"value_type\": \"DOUBLE\",\n",
    "        \"description\": \"A variable of user engagement level\",\n",
    "        \"labels\": {\"status\": \"passed\"},\n",
    "    },\n",
    "    \"cnt_level_start_quickplay\": {\n",
    "        \"value_type\": \"DOUBLE\",\n",
    "        \"description\": \"A variable of user engagement with start level\",\n",
    "        \"labels\": {\"status\": \"passed\"},\n",
    "    },\n",
    "    \"cnt_level_end_quickplay\": {\n",
    "        \"value_type\": \"DOUBLE\",\n",
    "        \"description\": \"A variable of user engagement with end level\",\n",
    "        \"labels\": {\"status\": \"passed\"},\n",
    "    },\n",
    "    \"cnt_level_complete_quickplay\": {\n",
    "        \"value_type\": \"DOUBLE\",\n",
    "        \"description\": \"A variable of user engagement with complete status\",\n",
    "        \"labels\": {\"status\": \"passed\"},\n",
    "    },\n",
    "    \"cnt_level_reset_quickplay\": {\n",
    "        \"value_type\": \"DOUBLE\",\n",
    "        \"description\": \"A variable of user engagement with reset status\",\n",
    "        \"labels\": {\"status\": \"passed\"},\n",
    "    },\n",
    "    \"cnt_post_score\": {\n",
    "        \"value_type\": \"DOUBLE\",\n",
    "        \"description\": \"A variable of user score\",\n",
    "        \"labels\": {\"status\": \"passed\"},\n",
    "    },\n",
    "    \"cnt_spend_virtual_currency\": {\n",
    "        \"value_type\": \"DOUBLE\",\n",
    "        \"description\": \"A variable of user virtual amount\",\n",
    "        \"labels\": {\"status\": \"passed\"},\n",
    "    },\n",
    "    \"cnt_ad_reward\": {\n",
    "        \"value_type\": \"DOUBLE\",\n",
    "        \"description\": \"A variable of user reward\",\n",
    "        \"labels\": {\"status\": \"passed\"},\n",
    "    },\n",
    "    \"cnt_challenge_a_friend\": {\n",
    "        \"value_type\": \"DOUBLE\",\n",
    "        \"description\": \"A variable of user challenges with friends\",\n",
    "        \"labels\": {\"status\": \"passed\"},\n",
    "    },\n",
    "    \"cnt_completed_5_levels\": {\n",
    "        \"value_type\": \"DOUBLE\",\n",
    "        \"description\": \"A variable of user level 5 completed\",\n",
    "        \"labels\": {\"status\": \"passed\"},\n",
    "    },\n",
    "    \"cnt_use_extra_steps\": {\n",
    "        \"value_type\": \"DOUBLE\",\n",
    "        \"description\": \"A variable of user extra steps\",\n",
    "        \"labels\": {\"status\": \"passed\"},\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FjzMd1XbEfdo"
   },
   "source": [
    "使用`batch_create_features`方法创建特征\n",
    "\n",
    "一旦您有了特征配置，您可以使用`batch_create_features`方法创建特征资源。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nqlgCDI9pbCD"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    user_entity_type.batch_create_features(feature_configs=feature_configs, sync=True)\n",
    "except RuntimeError as error:\n",
    "    print(error)\n",
    "else:\n",
    "    for feature in user_entity_type.list_features():\n",
    "        print(\"\")\n",
    "        print(f\"The resource name of {feature.name} feature is\", feature.resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7zpFV7wAppkC"
   },
   "source": [
    "搜索功能\n",
    "\n",
    "Vertex AI Feature存储支持搜索功能。以下是一个简单示例，展示如何根据特征名称对特征进行过滤。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BJXYLLOfppCL"
   },
   "outputs": [],
   "source": [
    "feature_query = \"feature_id:cnt_user_engagement\"\n",
    "searched_features = Feature.search(query=feature_query)\n",
    "searched_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "is9C_6-QpxG3"
   },
   "source": [
    "摄入特性\n",
    "\n",
    "在那个时候，您创建与特性库相关的所有资源。在您可以将特性值用于在线/离线服务之前，只需导入特性值即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4QgjW2vhsEUE"
   },
   "outputs": [],
   "source": [
    "FEATURES_IDS = [feature.name for feature in user_entity_type.list_features()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U5wE5h4GN_aA"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    user_entity_type.ingest_from_bq(\n",
    "        feature_ids=FEATURES_IDS,\n",
    "        feature_time=FEATURE_TIME,\n",
    "        bq_source_uri=BQ_SOURCE_URI,\n",
    "        entity_id_field=ENTITY_ID_FIELD,\n",
    "        disable_online_serving=False,\n",
    "        worker_count=10,\n",
    "        sync=False,\n",
    "    )\n",
    "except RuntimeError as error:\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8lCMpDPGp-oQ"
   },
   "source": [
    "使用Vertex AI Training和Endpoints训练和部署实时流失ML模型\n",
    "\n",
    "现在您已经拥有了您的特征，并且几乎准备好训练我们的流失模型。\n",
    "\n",
    "以下是一个高层次的图片\n",
    "\n",
    "<img src=\"./assets/train_model_4.png\">\n",
    "\n",
    "让我们深入了解这个过程的每个步骤。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VMrvnuyjqGfY"
   },
   "source": [
    "使用BigQuery和Vertex AI Feature存储库，使用时间点查询获取训练数据\n",
    "\n",
    "如上所述，在实时流失预测中，定义您的模型要预测的标签非常重要。\n",
    "\n",
    "假设您决定在接下来的一个小时内预测流失概率。现在您有了标签。下一步是定义您的训练样本。但让我们考虑一下。\n",
    "\n",
    "在这个实时流失系统中，您有大量的交易可以用来计算那些随时间不断变化并不断收集的特征。这意味着您始终可以获得新鲜数据来重建特征。取决于您何时决定计算一个特征或另一个特征，您可能会得到一组在时间上不对齐的特征。\n",
    "\n",
    "当您有标签可用时，非常难以确定哪组特征包含与您要预测的标签相关的最新历史信息。当您无法保证时，您的模型的表现将受到严重影响，因为当其实时进行时，您无法提供数据和标签的代表性特征。因此，您需要一种在标签可用之前获取您随时间计算的最新特征的方法，以避免这种信息偏移。\n",
    "\n",
    "**使用Vertex AI Feature存储库，您可以通过时间点查找功能值对应于特定时间戳，**在我们的情况下，这将是与您的模型要预测的标签相关联的时间戳。通过这种方式，您将避免数据泄漏，并获得最新的特征来训练您的模型。\n",
    "\n",
    "让我们看看如何做到这一点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RE_Pvmu-qdDt"
   },
   "source": [
    "### 定义在特定时间点阅读实例的查询\n",
    "\n",
    "首先，您需要定义在特定时间点阅读实例的集合，以便生成您的训练样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bUDVw7l-qF2x"
   },
   "outputs": [],
   "source": [
    "read_instances_query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE\n",
    "  `{PROJECT_ID}.{BQ_DATASET}.{READ_INSTANCES_TABLE}` AS\n",
    "WITH\n",
    "\n",
    "  # get training threshold ----------------------------------------------------------------------------------\n",
    "  get_training_threshold AS (\n",
    "  SELECT\n",
    "    (MAX(event_timestamp) - 10800000000) AS training_thrs\n",
    "  FROM\n",
    "    `firebase-public-project.analytics_153293282.events_*`\n",
    "  WHERE\n",
    "    event_name=\"user_engagement\"\n",
    "    AND\n",
    "    TIMESTAMP_ADD(PARSE_TIMESTAMP('%Y-%m-%d %H:%M:%S', FORMAT_TIMESTAMP('%Y-%m-%d %H:%M:%S', TIMESTAMP_MICROS(event_timestamp))), INTERVAL 1351 DAY) < '{TODAY}'),\n",
    "\n",
    "  # query to create label -----------------------------------------------------------------------------------\n",
    "  get_label AS (\n",
    "  SELECT\n",
    "    user_pseudo_id,\n",
    "    user_last_engagement,\n",
    "    #label = 1 if last_touch within last hour hr else 0\n",
    "  IF\n",
    "    (user_last_engagement < (\n",
    "      SELECT\n",
    "        training_thrs\n",
    "      FROM\n",
    "        get_training_threshold),\n",
    "      1,\n",
    "      0 ) AS churned\n",
    "  FROM (\n",
    "    SELECT\n",
    "      user_pseudo_id,\n",
    "      MAX(event_timestamp) AS user_last_engagement\n",
    "    FROM\n",
    "      `firebase-public-project.analytics_153293282.events_*`\n",
    "    WHERE\n",
    "      event_name=\"user_engagement\"\n",
    "    AND\n",
    "    TIMESTAMP_ADD(PARSE_TIMESTAMP('%Y-%m-%d %H:%M:%S', FORMAT_TIMESTAMP('%Y-%m-%d %H:%M:%S', TIMESTAMP_MICROS(event_timestamp))), INTERVAL 1351 DAY) < '{TODAY}'\n",
    "    GROUP BY\n",
    "      user_pseudo_id )\n",
    "  GROUP BY\n",
    "    1,\n",
    "    2),\n",
    "\n",
    "  # query to create class weights --------------------------------------------------------------------------------\n",
    "  get_class_weights AS (\n",
    "  SELECT\n",
    "    CAST(COUNT(*) / (2*(COUNT(*) - SUM(churned))) AS STRING) AS class_weight_zero,\n",
    "    CAST(COUNT(*) / (2*SUM(churned)) AS STRING) AS class_weight_one,\n",
    "  FROM\n",
    "    get_label )\n",
    "\n",
    "SELECT\n",
    "  user_pseudo_id as user,\n",
    "  PARSE_TIMESTAMP('%Y-%m-%d %H:%M:%S', CONCAT('{TODAY}', ' ', STRING(TIME_TRUNC(CURRENT_TIME(), SECOND))), 'UTC') as timestamp,\n",
    "  churned AS churned,\n",
    "  CASE\n",
    "      WHEN churned = 0 THEN ( SELECT class_weight_zero FROM get_class_weights)\n",
    "      ELSE ( SELECT class_weight_one\n",
    "       FROM get_class_weights)\n",
    "    END AS class_weights\n",
    "FROM\n",
    "  get_label \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTB3853wcYm6"
   },
   "source": [
    "### 创建BigQuery实例表\n",
    "\n",
    "您将这些实例存储在一个BigQuery表中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0PRgsZl09vJ0"
   },
   "outputs": [],
   "source": [
    "run_bq_query(read_instances_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SwSsdk2AcdTf"
   },
   "source": [
    "### 为批量训练提供功能\n",
    "\n",
    "然后使用 `batch_serve_to_gcs` 来生成您的训练样本，并将其存储为csv文件在目标云存储桶中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LnP0Q5XtwfYM"
   },
   "outputs": [],
   "source": [
    "mobile_gaming_feature_store.batch_serve_to_gcs(\n",
    "    gcs_destination_output_uri_prefix=GCS_DESTINATION_OUTPUT_URI,\n",
    "    gcs_destination_type=\"csv\",\n",
    "    serving_feature_ids=SERVING_FEATURE_IDS,\n",
    "    read_instances_uri=READ_INSTANCES_URI,\n",
    "    pass_through_fields=[\"churned\", \"class_weights\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vjvv7NKnYfcU"
   },
   "source": [
    "使用Training Pipelines在Vertex AI上训练一个自定义模型\n",
    "\n",
    "现在我们已经生成了训练样本，我们使用Vertex AI SDK使用Vertex AI Training来训练一个新版本的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gm8PKhFfoHO4"
   },
   "source": [
    "创建培训套餐和培训样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zAYW_N_Ewc_5"
   },
   "outputs": [],
   "source": [
    "!rm -Rf train_package #if train_package already exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OZLVGnWzBVER"
   },
   "outputs": [],
   "source": [
    "!mkdir -m 777 -p trainer data/ingest data/raw model config\n",
    "!gsutil -m cp -r $GCS_DESTINATION_OUTPUT_URI/*.csv data/ingest\n",
    "!head -n 2000 data/ingest/*.csv > data/raw/sample.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZLmh54sm8m0l"
   },
   "source": [
    "创建训练脚本\n",
    "\n",
    "您创建训练脚本以训练一个XGboost模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oTp85Qfpwc_6"
   },
   "outputs": [],
   "source": [
    "!touch trainer/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PVGEf68fDSax"
   },
   "outputs": [],
   "source": [
    "%%writefile trainer/task.py\n",
    "import os\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import yaml\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def get_args():\n",
    "    \"\"\"\n",
    "    Get arguments from command line.\n",
    "    Returns:\n",
    "        args: parsed arguments\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--data_path',\n",
    "        required=False,\n",
    "        default=os.getenv('AIP_TRAINING_DATA_URI'),\n",
    "        type=str,\n",
    "        help='path to read data')\n",
    "    parser.add_argument(\n",
    "        '--learning_rate',\n",
    "        required=False,\n",
    "        default=0.01,\n",
    "        type=int,\n",
    "        help='number of epochs')\n",
    "    parser.add_argument(\n",
    "        '--model_dir',\n",
    "        required=False,\n",
    "        default=os.getenv('AIP_MODEL_DIR'),\n",
    "        type=str,\n",
    "        help='dir to store saved model')\n",
    "    parser.add_argument(\n",
    "        '--config_path',\n",
    "        required=False,\n",
    "        default='../config.yaml',\n",
    "        type=str,\n",
    "        help='path to read config file')\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def ingest_data(data_path, data_model_params):\n",
    "    \"\"\"\n",
    "    Ingest data\n",
    "    Args:\n",
    "        data_path: path to read data\n",
    "        data_model_params: data model parameters\n",
    "    Returns:\n",
    "        df: dataframe\n",
    "    \"\"\"\n",
    "    # read training data\n",
    "    df = pd.read_csv(data_path, sep=',',\n",
    "                     dtype={col: 'string' for col in data_model_params['categorical_features']})\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess_data(df, data_model_params):\n",
    "    \"\"\"\n",
    "    Preprocess data\n",
    "    Args:\n",
    "        df: dataframe\n",
    "        data_model_params: data model parameters\n",
    "    Returns:\n",
    "        df: dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    # convert nan values because pd.NA ia not supported by SimpleImputer\n",
    "    # bug in sklearn 0.23.1 version: https://github.com/scikit-learn/scikit-learn/pull/17526\n",
    "    # decided to skip NAN values for now\n",
    "    df.replace({pd.NA: np.nan}, inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # get features and labels\n",
    "    x = df[data_model_params['numerical_features'] + data_model_params['categorical_features'] + [\n",
    "        data_model_params['weight_feature']]]\n",
    "    y = df[data_model_params['target']]\n",
    "\n",
    "    # train-test split\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y,\n",
    "                                                        test_size=data_model_params['train_test_split']['test_size'],\n",
    "                                                        random_state=data_model_params['train_test_split'][\n",
    "                                                            'random_state'])\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "\n",
    "def build_pipeline(learning_rate, model_params):\n",
    "    \"\"\"\n",
    "    Build pipeline\n",
    "    Args:\n",
    "        learning_rate: learning rate\n",
    "        model_params: model parameters\n",
    "    Returns:\n",
    "        pipeline: pipeline\n",
    "    \"\"\"\n",
    "    # build pipeline\n",
    "    pipeline = Pipeline([\n",
    "        # ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('encoder', OneHotEncoder(handle_unknown='ignore')),\n",
    "        ('model', xgb.XGBClassifier(learning_rate=learning_rate,\n",
    "                                    use_label_encoder=False, #deprecated and breaks Vertex AI predictions\n",
    "                                    **model_params))\n",
    "    ])\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "def main():\n",
    "    print('Starting training...')\n",
    "    args = get_args()\n",
    "    data_path = args.data_path\n",
    "    learning_rate = args.learning_rate\n",
    "    model_dir = args.model_dir\n",
    "    config_path = args.config_path\n",
    "\n",
    "    # read config file\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    f.close()\n",
    "    data_model_params = config['data_model_params']\n",
    "    model_params = config['model_params']\n",
    "\n",
    "    # ingest data\n",
    "    print('Reading data...')\n",
    "    data_df = ingest_data(data_path, data_model_params)\n",
    "\n",
    "    # preprocess data\n",
    "    print('Preprocessing data...')\n",
    "    x_train, x_test, y_train, y_test = preprocess_data(data_df, data_model_params)\n",
    "    sample_weight = x_train.pop(data_model_params['weight_feature'])\n",
    "    sample_weight_eval_set = x_test.pop(data_model_params['weight_feature'])\n",
    "\n",
    "    # train lgb model\n",
    "    print('Training model...')\n",
    "    xgb_pipeline = build_pipeline(learning_rate, model_params)\n",
    "    # need to use fit_transform to get the encoded eval data\n",
    "    x_train_transformed = xgb_pipeline[:-1].fit_transform(x_train)\n",
    "    x_test_transformed = xgb_pipeline[:-1].transform(x_test)\n",
    "    xgb_pipeline[-1].fit(x_train_transformed, y_train,\n",
    "                         sample_weight=sample_weight,\n",
    "                         eval_set=[(x_test_transformed, y_test)],\n",
    "                         sample_weight_eval_set=[sample_weight_eval_set],\n",
    "                         eval_metric='error',\n",
    "                         early_stopping_rounds=50,\n",
    "                         verbose=True)\n",
    "    # save model\n",
    "    print('Saving model...')\n",
    "    model_path = Path(model_dir)\n",
    "    model_path.mkdir(parents=True, exist_ok=True)\n",
    "    joblib.dump(xgb_pipeline, f'{model_dir}/model.joblib')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LD28QoIw8sdf"
   },
   "source": [
    "创建 requirements.txt 文件\n",
    "\n",
    "你需要编写 requirements.txt 文件来构建训练容器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CZfnfDAH8yos"
   },
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "pip==22.0.4\n",
    "PyYAML==5.3.1\n",
    "joblib==0.15.1\n",
    "numpy==1.18.5\n",
    "pandas==1.0.4\n",
    "scipy==1.4.1\n",
    "scikit-learn==0.23.1\n",
    "xgboost==1.1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F3KLInhPwc_7"
   },
   "source": [
    "### 创建训练配置\n",
    "\n",
    "您可以使用数据和模型参数来创建训练配置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_E7dPiChwc_7"
   },
   "outputs": [],
   "source": [
    "%%writefile config/config.yaml\n",
    "data_model_params:\n",
    "  target: churned\n",
    "  categorical_features:\n",
    "    - country\n",
    "    - operating_system\n",
    "    - language\n",
    "  numerical_features:\n",
    "    - cnt_user_engagement\n",
    "    - cnt_level_start_quickplay\n",
    "    - cnt_level_end_quickplay\n",
    "    - cnt_level_complete_quickplay\n",
    "    - cnt_level_reset_quickplay\n",
    "    - cnt_post_score\n",
    "    - cnt_spend_virtual_currency\n",
    "    - cnt_ad_reward\n",
    "    - cnt_challenge_a_friend\n",
    "    - cnt_completed_5_levels\n",
    "    - cnt_use_extra_steps\n",
    "  weight_feature: class_weights\n",
    "  train_test_split:\n",
    "    test_size: 0.2\n",
    "    random_state: 8\n",
    "model_params:\n",
    "  booster: gbtree\n",
    "  objective: binary:logistic\n",
    "  max_depth: 80\n",
    "  n_estimators: 100\n",
    "  random_state: 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55WduJ3M9WuH"
   },
   "source": [
    "使用`local-run`在本地测试模型。您可以利用 Vertex AI SDK 中的`local-run`命令来运行脚本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TKMtFOTM9R63"
   },
   "outputs": [],
   "source": [
    "test_job_script = f\"\"\"\n",
    "gcloud ai custom-jobs local-run \\\n",
    "--executor-image-uri={BASE_CPU_IMAGE} \\\n",
    "--python-module=trainer.task \\\n",
    "--extra-dirs=config,data,model \\\n",
    "-- \\\n",
    "--data_path data/raw/sample.csv \\\n",
    "--model_dir model \\\n",
    "--config_path config/config.yaml\n",
    "\"\"\"\n",
    "\n",
    "with open(\"local_train_job_run.sh\", \"w+\") as s:\n",
    "    s.write(test_job_script)\n",
    "s.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9teD2VCAwc_7"
   },
   "outputs": [],
   "source": [
    "!chmod +x ./local_train_job_run.sh && ./local_train_job_run.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Z1Kwg6pe0Jx"
   },
   "source": [
    "### 创建并启动自定义训练流程，使用 `autopackaging` 训练模型。\n",
    "\n",
    "您可以使用 Vertex AI SDK 中的 `autopackaging` 来：\n",
    "\n",
    "1. 构建自定义的 Docker 训练镜像。\n",
    "2. 将镜像推送到容器注册表。\n",
    "3. 启动一个 Vertex AI CustomJob。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xcnEOZKywc_8"
   },
   "outputs": [],
   "source": [
    "!mkdir -m 777 -p {MODEL_PACKAGE_PATH} && mv -t {MODEL_PACKAGE_PATH} trainer requirements.txt config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XQ4koRVCwc_8"
   },
   "outputs": [],
   "source": [
    "train_job_script = f\"\"\"\n",
    "gcloud ai custom-jobs create \\\n",
    "--region={REGION} \\\n",
    "--display-name={TRAIN_JOB_NAME} \\\n",
    "--worker-pool-spec=machine-type={TRAINING_MACHINE_TYPE},replica-count={TRAINING_REPLICA_COUNT},executor-image-uri={BASE_CPU_IMAGE},local-package-path={MODEL_PACKAGE_PATH},python-module=trainer.task,extra-dirs=config \\\n",
    "--args=--data_path={DATA_PATH},--model_dir={MODEL_DIR},--config_path=config/config.yaml \\\n",
    "--verbosity='info'\n",
    "\"\"\"\n",
    "\n",
    "with open(\"train_job_run.sh\", \"w+\") as s:\n",
    "    s.write(train_job_script)\n",
    "s.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dwq1Jyklwc_8"
   },
   "outputs": [],
   "source": [
    "!chmod +x ./train_job_run.sh && ./train_job_run.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3olHyo5MpPaC"
   },
   "source": [
    "### 检查培训任务的状态和结果。\n",
    "\n",
    "您可以使用以下命令来监视作业的状态，并在成功运行培训后检查存储桶中的工件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yh6-mWPhpfD_"
   },
   "outputs": [],
   "source": [
    "TRAIN_JOB_RESOURCE_NAME = \"[your-train-job-resource-name]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jpOA6aw5wc_8"
   },
   "outputs": [],
   "source": [
    "!gcloud ai custom-jobs describe $TRAIN_JOB_RESOURCE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lyborm0Myut5"
   },
   "outputs": [],
   "source": [
    "!gsutil ls $DESTINATION_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tC9fkIMBwc_8"
   },
   "source": [
    "### 在 Vertex AI 端点上上传和部署模型\n",
    "\n",
    "您可以使用自定义函数将您的模型上传到 Vertex AI 模型注册表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W17gaIjtwc_8"
   },
   "outputs": [],
   "source": [
    "xgb_model = upload_model(\n",
    "    display_name=MODEL_NAME,\n",
    "    serving_container_image_uri=SERVING_CONTAINER_IMAGE_URI,\n",
    "    artifact_uri=DESTINATION_URI,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nWZmWjnye4N5"
   },
   "source": [
    "### 将模型部署到具有流量分割的同一端点\n",
    "\n",
    "现在您已经在模型注册表中注册，您可以将其部署到一个端点中。因此，您首先创建端点，然后部署您的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZWpDZVtmwc_9"
   },
   "outputs": [],
   "source": [
    "endpoint = create_endpoint(display_name=ENDPOINT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Nw4o2fJwc_9"
   },
   "outputs": [],
   "source": [
    "deployed_model = deploy_model(\n",
    "    model=xgb_model,\n",
    "    machine_type=SERVING_MACHINE_TYPE,\n",
    "    endpoint=endpoint,\n",
    "    deployed_model_display_name=DEPLOYED_MODEL_NAME,\n",
    "    min_replica_count=1,\n",
    "    max_replica_count=1,\n",
    "    sync=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1TNzL_EGrVUm"
   },
   "source": [
    "# 使用低延迟在规模上提供机器学习特性\n",
    " \n",
    "那时候，你已经准备好**部署我们的简单模型，该模型需要在实时中提取预处理属性作为输入特性**。\n",
    " \n",
    "以下是它是如何运作的\n",
    " \n",
    "但是想一想那些特性。\n",
    " \n",
    "用于训练模型的行为特性，在在线服务模型时无法计算。\n",
    " \n",
    "你如何在现场计算用户在过去24小时内挑战朋友的次数？\n",
    " \n",
    "你需要在服务器端计算这种特性并以低延迟提供。并且因为 Bigquery 不是针对这些读操作进行了优化，我们需要另一种允许单例查找的服务，其中结果是一个具有许多列的单行。\n",
    " \n",
    "此外，即使不是这种情况，当你部署一个需要预处理数据的模型时，你需要确保在训练时采用相同的预处理步骤。如果你无法做到这一点，训练和服务数据之间会发生偏移，这将严重影响你的模型性能（并在最糟糕的情况下破坏你的服务系统）。\n",
    " \n",
    "你需要一种方法来减轻这种情况，你无需在线实施这些预处理步骤，只需提供用于训练的相同汇总特性，以生成在线预测。\n",
    " \n",
    "这些是引入 Vertex AI 特性存储的其他有价值的理由。有了它，你可以通过一种帮助你以与训练时相同的方式在规模上低延迟提供特性的服务，从而减轻可能的训练-服务偏移。\n",
    " \n",
    "现在你知道**为什么你需要一个特性存储**，让我们通过使用特性存储部署你的模型来结束这次旅程，在线检索特性，将它们传递到端点并生成预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1e600b031a50"
   },
   "source": [
    "## 开始模拟在线预测\n",
    "\n",
    "一旦模型准备好接收预测请求，您可以使用 `simulate_prediction` 函数来生成预测。\n",
    "\n",
    "具体来说，该函数会：\n",
    "\n",
    "- 为预测格式化实体\n",
    "- 通过在 Vertex AI 特征存储中执行单例查找操作检索静态特征\n",
    "- 运行预测请求并获取结果\n",
    "\n",
    "根据您定义的请求数和一些延迟。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k42QhcjcG5pe"
   },
   "outputs": [],
   "source": [
    "simulate_prediction(endpoint=endpoint, n_requests=10, latency=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8d3S1d1urZOy"
   },
   "source": [
    "清理工作\n",
    "\n",
    "要清理此项目中使用的所有Google Cloud资源，您可以删除用于本教程的[Google Cloud项目](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects)。\n",
    "\n",
    "否则，您可以删除在本教程中创建的各个资源。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NESfOgK5nkNu"
   },
   "outputs": [],
   "source": [
    "# delete feature store\n",
    "mobile_gaming_feature_store.delete(sync=True, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4W28-97kfPDb"
   },
   "outputs": [],
   "source": [
    "# delete Vertex AI resources\n",
    "endpoint.undeploy_all()\n",
    "xgb_model.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FMXT2akXrZOy"
   },
   "outputs": [],
   "source": [
    "# Delete bucket\n",
    "delete_bucket = False\n",
    "if (delete_bucket or os.getenv(\"IS_TESTING\")) and \"BUCKET_URI\" in globals():\n",
    "    ! gsutil -m rm -r $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oLVhx7bQeJ6H"
   },
   "outputs": [],
   "source": [
    "# Delete the BigQuery Dataset\n",
    "!bq rm -r -f -d $PROJECT_ID:$BQ_DATASET"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "mobile_gaming_feature_store.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
