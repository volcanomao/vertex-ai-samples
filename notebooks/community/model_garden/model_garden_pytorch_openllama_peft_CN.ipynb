{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7d9bbf86da5e"
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99c1c3fc2ca5"
   },
   "source": [
    "# Vertex AI 模型花园 - OpenLLaMA (PEFT)\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_openllama_peft.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> 在 Colab 中运行\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_openllama_peft.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      在 GitHub 上查看\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/notebooks/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/model_garden/model_garden_pytorch_openllama_peft.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "在 Vertex AI Workbench 中打开\n",
    "    </a> (建议使用 Python-3 GPU 笔记本)\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3de7470326a2"
   },
   "source": [
    "## 概述\n",
    "\n",
    "本笔记本演示了如何使用预构建的OpenLLaMA进行本地推理，部署预构建的OpenLLaMA，使用[vLLM](https://github.com/vllm-project/vllm)部署预构建的OpenLLaMA，使用性能高效的微调库（[PEFT](https://github.com/huggingface/peft)）对OpenLLaMA进行微调和部署，使用AWQ或GPTQ对OpenLLaMA进行量化和部署，并在Vertex AI中评估PEFT微调的OpenLLaMA。\n",
    "\n",
    "### 目标\n",
    "\n",
    "- 使用预构建的OpenLLaMA进行本地推理\n",
    "- 部署预构建的OpenLLaMA\n",
    "- 使用[vLLM](https://github.com/vllm-project/vllm)部署预构建的OpenLLaMA以提高服务吞吐量\n",
    "- 使用PEFT对OpenLLaMA进行微调和部署\n",
    "- 使用AWQ或GPTQ对OpenLLaMA模型进行量化和部署\n",
    "- 评估经过PEFT微调的OpenLLaMA\n",
    "\n",
    "| 模型 | LoRA |\n",
    "| :- | :- |\n",
    "| [openlm-research/open_llama_3b](https://huggingface.co/openlm-research/open_llama_3b) | 是 |\n",
    "| [openlm-research/open_llama_7b](https://huggingface.co/openlm-research/open_llama_7b) | 是 |\n",
    "| [openlm-research/open_llama_13b](https://huggingface.co/openlm-research/open_llama_13b) | 是 |\n",
    "\n",
    "### 成本\n",
    "\n",
    "此教程使用Google Cloud的收费组件：\n",
    "\n",
    "* Vertex AI\n",
    "* 云存储\n",
    "\n",
    "了解[Vertex AI定价](https://cloud.google.com/vertex-ai/pricing)和[云存储定价](https://cloud.google.com/storage/pricing)，并使用[定价计算器](https://cloud.google.com/products/calculator/)根据您的预期使用情况生成成本估算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "264c07757582"
   },
   "source": [
    "在开始之前\n",
    "\n",
    "注意：Jupyter将带有“!”前缀的行视为shell命令，并将带有“$”前缀的Python变量插入这些命令中。\n",
    "\n",
    "在OpenLLaMA中运行本地推理需要GPU。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ioensNKM8ned"
   },
   "source": [
    "仅限于Colab\n",
    "如果您正在使用Workbench，请运行以下命令并跳过此部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2707b02ef5df"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    ! pip3 install --upgrade google-cloud-aiplatform\n",
    "    from google.colab import auth as google_auth\n",
    "\n",
    "    google_auth.authenticate_user()\n",
    "\n",
    "    # Restart the notebook kernel after installs.\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)\n",
    "! pip3 install transformers==4.31.0\n",
    "! pip3 install sentencepiece==0.1.99\n",
    "! pip3 install accelerate==0.21.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bb7adab99e41"
   },
   "source": [
    "### 设置谷歌云项目\n",
    "\n",
    "1. [选择或创建一个谷歌云项目](https://console.cloud.google.com/cloud-resource-manager)。当您首次创建帐户时，您将获得300美元的免费信用，用于计算/存储成本。\n",
    "\n",
    "2. [确保您的项目已启用计费](https://cloud.google.com/billing/docs/how-to/modify-project)。\n",
    "\n",
    "3. [启用Vertex AI API和Compute Engine API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,compute_component)。\n",
    "\n",
    "4. [创建一个云存储桶](https://cloud.google.com/storage/docs/creating-buckets) 用于存储实验输出。\n",
    "\n",
    "5. [创建一个服务帐号](https://cloud.google.com/iam/docs/service-accounts-create#iam-service-accounts-create-console) ，并为部署经过微调的模型到 Vertex AI 端点分配 `Vertex AI User` 和 `Storage Object Admin` 角色。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6c460088b873"
   },
   "source": [
    "为实验环境设置以下变量。指定的云存储桶（`BUCKET_URI`）应位于指定的区域（`REGION`）中。请注意，多区域存储桶（例如“美国”）不被视为与多区域范围覆盖的单个区域相匹配（例如“us-central1”）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "855d6b96f291"
   },
   "outputs": [],
   "source": [
    "# Cloud project id.\n",
    "PROJECT_ID = \"\"  # @param {type:\"string\"}\n",
    "\n",
    "# The region you want to launch jobs in.\n",
    "REGION = \"\"  # @param {type:\"string\"}\n",
    "\n",
    "# The Cloud Storage bucket for storing experiments output.\n",
    "# Start with gs:// prefix, e.g. gs://foo_bucket.\n",
    "BUCKET_URI = \"gs://\"  # @param {type:\"string\"}\n",
    "\n",
    "! gcloud config set project $PROJECT_ID\n",
    "\n",
    "import os\n",
    "\n",
    "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
    "EXPERIMENT_BUCKET = os.path.join(BUCKET_URI, \"peft\")\n",
    "DATA_BUCKET = os.path.join(EXPERIMENT_BUCKET, \"data\")\n",
    "MODEL_BUCKET = os.path.join(EXPERIMENT_BUCKET, \"model\")\n",
    "\n",
    "# The service account looks like:\n",
    "# '@.iam.gserviceaccount.com'\n",
    "# Please go to https://cloud.google.com/iam/docs/service-accounts-create#iam-service-accounts-create-console\n",
    "# and create service account with `Vertex AI User` and `Storage Object Admin` roles.\n",
    "# The service account for deploying fine tuned model.\n",
    "SERVICE_ACCOUNT = \"\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e828eb320337"
   },
   "source": [
    "### 初始化Vertex AI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "12cd25839741"
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cc825514deb"
   },
   "source": [
    "定义常数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b42bd4fa2b2d"
   },
   "outputs": [],
   "source": [
    "# The pre-built training and serving docker images.\n",
    "TRAIN_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-peft-train:20231130_0936_RC00\"\n",
    "PREDICTION_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-peft-serve:20231130_0948_RC00\"\n",
    "VLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20231127_0916_RC00\"\n",
    "VLLM_GPTQ_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:gptq\"\n",
    "EVAL_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-lm-evaluation-harness:20231011_0934_RC00\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0c250872074f"
   },
   "source": [
    "### 定义常见函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "354da31189dc"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from typing import Tuple\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "\n",
    "def create_name_with_datetime(prefix: str) -> str:\n",
    "    \"\"\"Creates a name with date time when triggering training or deployment\n",
    "    jobs in Vertex AI.\n",
    "    \"\"\"\n",
    "    return prefix + datetime.now().strftime(\"_%Y%m%d_%H%M%S\")\n",
    "\n",
    "\n",
    "def deploy_model(\n",
    "    model_name: str,\n",
    "    model_id: str,\n",
    "    finetuned_lora_model_path: str,\n",
    "    service_account: str,\n",
    "    task: str,\n",
    "    precision_loading_mode: str = \"float16\",\n",
    "    machine_type: str = \"n1-standard-8\",\n",
    "    accelerator_type: str = \"NVIDIA_TESLA_V100\",\n",
    "    accelerator_count: int = 1,\n",
    ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
    "    \"\"\"Deploys trained models into Vertex AI.\"\"\"\n",
    "    endpoint = aiplatform.Endpoint.create(display_name=f\"{model_name}-endpoint\")\n",
    "    serving_env = {\n",
    "        \"MODEL_ID\": model_id,\n",
    "        \"PRECISION_LOADING_MODE\": precision_loading_mode,\n",
    "        \"TASK\": task,\n",
    "        \"DEPLOY_SOURCE\": \"notebook\",\n",
    "    }\n",
    "    if finetuned_lora_model_path:\n",
    "        serving_env[\"FINETUNED_LORA_MODEL_PATH\"] = finetuned_lora_model_path\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=model_name,\n",
    "        serving_container_image_uri=PREDICTION_DOCKER_URI,\n",
    "        serving_container_ports=[7080],\n",
    "        serving_container_predict_route=\"/predictions/peft_serving\",\n",
    "        serving_container_health_route=\"/ping\",\n",
    "        serving_container_environment_variables=serving_env,\n",
    "    )\n",
    "    model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        machine_type=machine_type,\n",
    "        accelerator_type=accelerator_type,\n",
    "        accelerator_count=accelerator_count,\n",
    "        deploy_request_timeout=1800,\n",
    "        service_account=service_account,\n",
    "    )\n",
    "    return model, endpoint\n",
    "\n",
    "\n",
    "def deploy_model_vllm(\n",
    "    model_name: str,\n",
    "    model_id: str,\n",
    "    service_account: str,\n",
    "    machine_type: str = \"n1-standard-8\",\n",
    "    accelerator_type: str = \"NVIDIA_TESLA_V100\",\n",
    "    accelerator_count: int = 1,\n",
    "    quantization_method: str = \"\",\n",
    ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
    "    \"\"\"Deploys trained models with vLLM into Vertex AI.\"\"\"\n",
    "    endpoint = aiplatform.Endpoint.create(display_name=f\"{model_name}-endpoint\")\n",
    "\n",
    "    vllm_args = [\n",
    "        \"--host=0.0.0.0\",\n",
    "        \"--port=7080\",\n",
    "        f\"--model={model_id}\",\n",
    "        f\"--tensor-parallel-size={accelerator_count}\",\n",
    "        \"--swap-space=16\",\n",
    "        \"--gpu-memory-utilization=0.9\",\n",
    "        \"--max-num-batched-tokens=4096\",\n",
    "        \"--disable-log-stats\",\n",
    "    ]\n",
    "    if quantization_method:\n",
    "        vllm_args.append(f\"--quantization={quantization_method}\")\n",
    "    if quantization_method == \"gptq\":\n",
    "        vllm_docker_uri = VLLM_GPTQ_DOCKER_URI\n",
    "    else:\n",
    "        vllm_docker_uri = VLLM_DOCKER_URI\n",
    "\n",
    "    serving_env = {\n",
    "        \"MODEL_ID\": \"openlm-research/open_llama\",\n",
    "        \"DEPLOY_SOURCE\": \"notebook\"\n",
    "    }\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=model_name,\n",
    "        serving_container_image_uri=vllm_docker_uri,\n",
    "        serving_container_command=[\"python\", \"-m\", \"vllm.entrypoints.api_server\"],\n",
    "        serving_container_args=vllm_args,\n",
    "        serving_container_ports=[7080],\n",
    "        serving_container_predict_route=\"/generate\",\n",
    "        serving_container_health_route=\"/ping\",\n",
    "        serving_container_environment_variables=serving_env,\n",
    "    )\n",
    "\n",
    "    model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        machine_type=machine_type,\n",
    "        accelerator_type=accelerator_type,\n",
    "        accelerator_count=accelerator_count,\n",
    "        deploy_request_timeout=1800,\n",
    "        service_account=service_account,\n",
    "    )\n",
    "    return model, endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "65eaa62632d1"
   },
   "source": [
    "使用预先构建的OpenLLaMA在本地运行推断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "339601a9500b"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "model_path = \"openlm-research/open_llama_3b\"\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "prompt = \"Q: What is the largest animal?\\nA:\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "input_ids = input_ids.to(\"cuda\")\n",
    "generation_output = model.generate(input_ids=input_ids, max_new_tokens=32)\n",
    "print(tokenizer.decode(generation_output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V7VOhhHGpUrj"
   },
   "source": [
    "## 使用vLLM部署预构建的OpenLLaMA\n",
    "\n",
    "本部分在终端上使用[vLLM](https://github.com/vllm-project/vllm)部署预构建的OpenLLaMA模型。模型部署步骤需要大约15分钟才能完成。\n",
    "\n",
    "vLLM是一个高度优化的LLM服务框架，可以显著提高服务吞吐量。您拥有的QPS越高，使用vLLM可以获得的性能优势就越多。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4GTNnnuYqrW_"
   },
   "source": [
    "设置预先构建的模型id。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kLsRoc4Kqrkx"
   },
   "outputs": [],
   "source": [
    "prebuilt_model_id = \"openlm-research/open_llama_7b\"  # @param [\"openlm-research/open_llama_3b\", \"openlm-research/open_llama_7b\", \"openlm-research/open_llama_13b\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YI0vaDi6p2fi"
   },
   "outputs": [],
   "source": [
    "# Finds Vertex AI prediction supported accelerators and regions in\n",
    "# https://cloud.google.com/vertex-ai/docs/predictions/configure-compute.\n",
    "\n",
    "# Sets V100 to deploy open_llama_3b and open_llama_7b.\n",
    "# V100 serving has better throughput and latency performance than L4 serving.\n",
    "machine_type = \"n1-standard-8\"\n",
    "accelerator_type = \"NVIDIA_TESLA_V100\"\n",
    "accelerator_count = 1\n",
    "\n",
    "# Sets L4 to deploy open_llama_3b and open_llama_7b.\n",
    "# L4 serving is more cost efficient than V100 serving.\n",
    "# machine_type = \"g2-standard-8\"\n",
    "# accelerator_type = \"NVIDIA_L4\"\n",
    "# accelerator_count = 1\n",
    "\n",
    "# Sets 2 V100 to deploy open_llama_13b.\n",
    "# V100 serving has better throughput and latency performance than L4 serving.\n",
    "# machine_type = \"n1-standard-16\"\n",
    "# accelerator_type = \"NVIDIA_TESLA_V100\"\n",
    "# accelerator_count = 2\n",
    "\n",
    "# Sets 2 L4 to deploy open_llama_13b.\n",
    "# L4 serving is more cost efficient than V100 serving.\n",
    "# machine_type = \"g2-standard-24\"\n",
    "# accelerator_type = \"NVIDIA_L4\"\n",
    "# accelerator_count = 2\n",
    "\n",
    "if prebuilt_model_id == \"openlm-research/open_llama_3b\":\n",
    "    # vLLM currently does not support OpenLLaMA 3B.\n",
    "    precision_loading_mode = \"float16\"\n",
    "    model_without_peft, endpoint_without_peft = deploy_model(\n",
    "        model_name=get_job_name_with_datetime(prefix=\"openllama-serve\"),\n",
    "        model_id=model_id,\n",
    "        finetuned_lora_model_path=\"\",  # This will avoid override finetuning models.\n",
    "        service_account=SERVICE_ACCOUNT,\n",
    "        task=\"causal-language-modeling-lora\",\n",
    "        precision_loading_mode=precision_loading_mode,\n",
    "        machine_type=machine_type,\n",
    "        accelerator_type=accelerator_type,\n",
    "        accelerator_count=accelerator_count,\n",
    "    )\n",
    "else:\n",
    "    model_without_peft, endpoint_without_peft = deploy_model_vllm(\n",
    "        model_name=create_name_with_datetime(prefix=\"openllama-serve-vllm\"),\n",
    "        model_id=prebuilt_model_id,\n",
    "        service_account=SERVICE_ACCOUNT,\n",
    "        machine_type=machine_type,\n",
    "        accelerator_type=accelerator_type,\n",
    "        accelerator_count=accelerator_count,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWYmYWoqqBuZ"
   },
   "source": [
    "注意：在部署成功后，预建模型权重将从原始位置实时下载。因此，在上述模型部署步骤成功后，需要额外等待5分钟，在可以运行下面的下一个步骤之前。否则，当向端点发送请求时，可能会出现`ServiceUnavailable: 503 502:Bad Gateway`错误。\n",
    "\n",
    "一旦部署成功，您可以使用文本提示向端点发送请求。如果您对额外的服务参数感兴趣，请参考vLLM GitHub的[examples/api_client.py](https://github.com/vllm-project/vllm/blob/main/examples/api_client.py)获取更多详细信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fjO4z3qAp3pK"
   },
   "outputs": [],
   "source": [
    "instance = {\n",
    "    \"prompt\": \"Hi, Google. How are you doing?\",\n",
    "    \"n\": 1,\n",
    "    \"max_tokens\": 32,\n",
    "    \"temperature\": 1.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"top_k\": 10,\n",
    "}\n",
    "response = endpoint_without_peft.predict(instances=[instance])\n",
    "print(response.predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e70e3519ff8b"
   },
   "source": [
    "## 使用PEFT对OpenLLaMA进行微调并部署\n",
    "\n",
    "本部分演示了如何对OpenLLaMA-7b模型进行微调，将微调后的LoRA适配器与基本模型合并，并使用vLLM进行服务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5qCrm_kJH5cz"
   },
   "source": [
    "设置基本模型ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N3UBLiYrM3sU"
   },
   "outputs": [],
   "source": [
    "model_id = \"openlm-research/open_llama_7b\"  # @param [\"openlm-research/open_llama_3b\", \"openlm-research/open_llama_7b\", \"openlm-research/open_llama_13b\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iWGwJHqI7LMs"
   },
   "source": [
    "###调优"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KKEYoRfiHDVv"
   },
   "source": [
    "使用Vertex AI SDK来创建和运行带有Vertex AI Model Garden训练图像的自定义训练任务。\n",
    "\n",
    "此示例使用数据集[Abirate/english_quotes](https://huggingface.co/datasets/Abirate/english_quotes)。您可以使用[huggingface的数据集](https://huggingface.co/datasets)，或者存储在Cloud Storage中的[Vertex文本模型数据集格式](https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-text-models-supervised#dataset-format)中的自定义JSONL数据集。 `template`参数为可选。\n",
    "\n",
    "为了有效地进行微调，我们启用了量化，以加载用于微调LoRA模型的预训练模型。精度选项包括`\"4bit\"`、`\"8bit\"`、`\"float16\"`(默认)和`\"float32\"`，精度可以通过`\"--precision_mode\"`进行设置。针对[openlm-research/open_llama_3b](https://huggingface.co/openlm-research/open_llama_3b)、[openlm-research/open_llama_7b](https://huggingface.co/openlm-research/open_llama_7b)和[openlm-research/open_llama_13b](https://huggingface.co/openlm-research/open_llama_13b)，使用默认训练参数和示例数据集进行微调LoRA模型时，峰值GPU内存使用量分别为~7G、~10G和~16G。`open_llama_3b`和`open_llama_7b`可以在**1个V100（16G）**和**1个L4（24G）**上进行微调，而`open_llama_13b`可以在**1个L4（24G）**上进行微调。\n",
    "\n",
    "在本节中，微调后的LoRA适配器将保存在下面的`lora_adapter_dir`变量指定的GCS存储桶中；我们将LoRa适配器与基础模型合并，并保存在下面的`merged_model_output_dir`变量指定的另一个GCS存储桶中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0810ef72dd9f"
   },
   "source": [
    "#### [可选] 使用自定义数据集进行微调\n",
    "\n",
    "要使用自定义数据集，您应该在下面的`dataset_name`中提供一个指向[Vertex文本模型数据集格式](https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-text-models-supervised#dataset-format)中的JSONL文件的`gs://` URI。\n",
    "\n",
    "例如，这是来自示例数据集`gs://cloud-samples-data/vertex-ai/model-evaluation/peft_train_sample.jsonl`中的一个数据点：\n",
    "\n",
    "```json\n",
    "{\"input_text\":\"TRANSCRIPT: \\nREASON FOR EVALUATION:,\\n\\n LABEL:\",\"output_text\":\"Chiropractic\"}\n",
    "```\n",
    "\n",
    "要使用包含`input_text`和`output_text`字段的这个示例数据集，将`dataset_name`设置为`gs://cloud-samples-data/vertex-ai/model-evaluation/peft_train_sample.jsonl`，并将`template`设置为`vertex_sample`。对于使用自定义数据集字段的高级用法，请参阅[模板示例](https://github.com/tloen/alpaca-lora/blob/main/templates/alpaca.json)，并提供自己的JSON模板作为`gs://` URI。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "65467b361315"
   },
   "outputs": [],
   "source": [
    "# Huggingface dataset name or gs:// URI to a custom JSONL dataset.\n",
    "dataset_name = \"Abirate/english_quotes\"  # @param {type:\"string\"}\n",
    "# Optional. Template name or gs:// URI to a custom template.\n",
    "template = \"\"  # @param {type:\"string\"}\n",
    "\n",
    "# Worker pool spec.\n",
    "# Finetunes open_llama_3b and open_llama_7b with 1 V100 (16G).\n",
    "machine_type = \"n1-standard-8\"\n",
    "accelerator_type = \"NVIDIA_TESLA_V100\"\n",
    "accelerator_count = 1\n",
    "\n",
    "# Finetunes open_llama_3b and open_llama_7b with 1 L4 (24G).\n",
    "# machine_type = \"g2-standard-8\"\n",
    "# accelerator_type = \"NVIDIA_L4\"\n",
    "# accelerator_count = 1\n",
    "\n",
    "# Finetunes open_llama_13b with 1 L4 (24G).\n",
    "# machine_type = \"g2-standard-8\"\n",
    "# accelerator_type = \"NVIDIA_L4\"\n",
    "# accelerator_count = 1\n",
    "\n",
    "# Finetunes open_llama_13b with 1 A100 (40G).\n",
    "# machine_type = \"a2-highgpu-1g\"\n",
    "# accelerator_type = \"NVIDIA_TESLA_A100\"\n",
    "# accelerator_count = 1\n",
    "\n",
    "replica_count = 1\n",
    "\n",
    "\n",
    "# Setup training job.\n",
    "job_name = create_name_with_datetime(\"openllama-lora-train\")\n",
    "train_job = aiplatform.CustomContainerTrainingJob(\n",
    "    display_name=job_name,\n",
    "    container_uri=TRAIN_DOCKER_URI,\n",
    ")\n",
    "\n",
    "# Create a GCS folder to store the LORA adapter.\n",
    "lora_adapter_dir = create_name_with_datetime(\"openllama-lora-adapter\")\n",
    "lora_output_dir = os.path.join(MODEL_BUCKET, lora_adapter_dir)\n",
    "lora_output_dir_gcsfuse = lora_output_dir.replace(\"gs://\", \"/gcs/\")\n",
    "\n",
    "# Create a GCS folder to store the merged model with the base model and the\n",
    "# finetuned LORA adapter.\n",
    "merged_model_dir = create_name_with_datetime(\"openllama-merged-model\")\n",
    "merged_model_output_dir = os.path.join(MODEL_BUCKET, merged_model_dir)\n",
    "merged_model_output_dir_gcsfuse = merged_model_output_dir.replace(\"gs://\", \"/gcs/\")\n",
    "\n",
    "# Pass training arguments and launch job.\n",
    "train_job.run(\n",
    "    args=[\n",
    "        \"--task=causal-language-modeling-lora\",\n",
    "        f\"--pretrained_model_id={model_id}\",\n",
    "        f\"--dataset_name={dataset_name}\",\n",
    "        f\"--output_dir={lora_output_dir_gcsfuse}\",\n",
    "        f\"--merge_base_and_lora_output_dir={merged_model_output_dir_gcsfuse}\",\n",
    "        \"--lora_rank=16\",\n",
    "        \"--lora_alpha=32\",\n",
    "        \"--lora_dropout=0.05\",\n",
    "        \"--warmup_steps=10\",\n",
    "        \"--max_steps=10\",\n",
    "        \"--learning_rate=2e-4\",\n",
    "        f\"--template={template}\",\n",
    "    ],\n",
    "    replica_count=replica_count,\n",
    "    machine_type=machine_type,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    "    boot_disk_size_gb=500,\n",
    ")\n",
    "\n",
    "print(\"The finetuned Lora adapter can be found at: \", lora_output_dir)\n",
    "print(\n",
    "    \"The finetuned Lora adapter merged with the base model can be found at: \",\n",
    "    merged_model_output_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53b8a1ad6def"
   },
   "source": [
    "### [可选] 超参数调优\n",
    "\n",
    "您可以使用Vertex AI SDK 创建和运行[超参数调优作业](https://cloud.google.com/vertex-ai/docs/training/hyperparameter-tuning-overview)，通过尝试不同的超参数（例如学习率）来获得更好的性能。\n",
    "\n",
    "定义以下规范：\n",
    "\n",
    "- `worker_pool_specs`：指定机器类型和Docker镜像的字典。\n",
    "\n",
    "- `parameter_spec`：指定要优化的参数的字典。字典键是您的训练应用程序代码中为每个超参数分配的命令行参数的字符串，字典值是参数规范。参数规范包括超参数的类型、最小/最大值和规模。\n",
    "\n",
    "- `metric_spec`：指定要优化的指标的字典。字典键是您在训练应用程序代码中设置的超参数指标标签，值是优化目标。\n",
    "\n",
    "以下是针对ARC Challenge数据集评估的4位QLoRA实验结果，用于参考超参数调整的有效性：\n",
    "\n",
    "| 模型         | 训练时间    | Trials | 并行 Trials | GPU  | ∆arc 挑战 | ∆hellaswag | ∆truthfulqa_mc | 成本       |\n",
    "|---------------|------------|--------|-------------|------|------------|------------|----------------|------------|\n",
    "| Openllama-3b  | 2天10小时   | 8      | 1           | L4x1 | +1.62      | +7.32      | +3.34          | \\$29.0232 |\n",
    "| Openllama-7b  | 1天4小时    | 8      | 2           | L4x1 | +2.82      | +3.55      | +6.68          | \\$47.8016 |\n",
    "| Openllama-13b | 6天10小时   | 8      | 2           | L4x1 | +1.01      | +3.67      | +6.19          | \\$87.9208 |\n",
    "\n",
    "以下示例在`timdettmers/openassistant-guanaco`上运行8个试验，使用不同的学习率，并在`arc_challenge`数据集上评估模型。您可以通过扩展学习率范围、添加LoRA等参数来定制搜索空间。请参阅[超参数调优文档](https://cloud.google.com/vertex-ai/docs/training/hyperparameter-tuning-overview)获取更多信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a81402f70641"
   },
   "outputs": [],
   "source": [
    "# Huggingface dataset name or gs:// URI to a custom JSONL dataset.\n",
    "dataset_name = \"timdettmers/openassistant-guanaco\"  # @param {type:\"string\"}\n",
    "# Optional. Template name or gs:// URI to a custom template.\n",
    "template = \"\"  # @param {type:\"string\"}\n",
    "\n",
    "hpt_precision_mode = \"4bit\"\n",
    "\n",
    "# Worker pool spec for 4bit finetuning.\n",
    "\n",
    "# Finetunes Openllama 3B / 7B / 13B with 1 L4 (24G).\n",
    "machine_type = \"g2-standard-8\"\n",
    "accelerator_type = \"NVIDIA_L4\"\n",
    "accelerator_count = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55ed2a2a9d54"
   },
   "source": [
    "### [可选] 自定义评估数据集\n",
    "\n",
    "为了获得在某些特定任务上性能更好的模型，您可能希望使用自定义评估数据集运行超参数调整。超参数调整服务将根据评估数据集和您选择的指标选择模型。您可以在下面的代码单元格中将任何以下任务用作`eval_task`：\n",
    "\n",
    "1. [lm-evaluation-harness 任务](https://github.com/EleutherAI/lm-evaluation-harness/tree/big-refactor/lm_eval/tasks)的名称。\n",
    "\n",
    "2. `custom_likelihood`。然后，添加一个标志 `--eval_dataset_path=<您的 JSONL 数据集的云存储 URI>`。JSONL 文件必须采用 Vertex AI 语言模型的[准备评估数据集](https://cloud.google.com/vertex-ai/docs/generative-ai/models/evaluate-models#classification)页面中的格式。\n",
    "\n",
    "3. `builtin_eval`。将使用训练器的内置评估循环来评估模型，而不是使用[lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness)库。您可以通过指定 `--eval_dataset_path`、`--eval_split`、`--eval_template` 和 `--eval_column` 提供与训练数据集相同格式的任何评估数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7864cff27197"
   },
   "outputs": [],
   "source": [
    "from google.cloud.aiplatform import hyperparameter_tuning as hpt\n",
    "\n",
    "eval_task = \"arc_challenge\"  # @param {type:\"string\"}\n",
    "eval_metric_name = \"acc_norm\"  # @param {type:\"string\"}\n",
    "\n",
    "# Runs 10 training steps as a minimal example. Use 1000 to reproduce the experiment results.\n",
    "max_steps = 10  # @param {type:\"integer\"}\n",
    "# Evaluates the model on 10 examples. Use 10000 to reproduce the experiment results.\n",
    "eval_limit = 10  # @param {type:\"integer\"}\n",
    "\n",
    "flags = {\n",
    "    \"learning_rate\": 1e-5,\n",
    "    \"precision_mode\": hpt_precision_mode,\n",
    "    \"task\": \"instruct-lora\",\n",
    "    \"pretrained_model_id\": model_id,\n",
    "    \"output_dir\": lora_output_dir_gcsfuse,\n",
    "    \"warmup_steps\": 10,\n",
    "    \"max_steps\": max_steps,\n",
    "    \"lora_rank\": 32,\n",
    "    \"lora_alpha\": 64,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"dataset_name\": dataset_name,\n",
    "    \"eval_steps\": max_steps + 1,  # Only evaluates in the end.\n",
    "    \"eval_tasks\": eval_task,\n",
    "    \"eval_limit\": eval_limit,\n",
    "    \"eval_metric_name\": eval_metric_name,\n",
    "    \"merge_base_and_lora_output_dir\": merged_model_output_dir_gcsfuse,\n",
    "}\n",
    "\n",
    "worker_pool_specs = [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": machine_type,\n",
    "            \"accelerator_type\": accelerator_type,\n",
    "            \"accelerator_count\": accelerator_count,\n",
    "        },\n",
    "        \"replica_count\": replica_count,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": TRAIN_DOCKER_URI,\n",
    "            \"args\": [\"--{}={}\".format(k, v) for k, v in flags.items()],\n",
    "        },\n",
    "    }\n",
    "]\n",
    "metric_spec = {\"model_performance\": \"maximize\"}\n",
    "parameter_spec = {\n",
    "    \"learning_rate\": hpt.DoubleParameterSpec(min=1e-5, max=1e-4, scale=\"linear\"),\n",
    "}\n",
    "train_job = aiplatform.CustomJob(\n",
    "    display_name=job_name,\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    "    staging_bucket=STAGING_BUCKET,\n",
    ")\n",
    "\n",
    "train_hpt_job = aiplatform.HyperparameterTuningJob(\n",
    "    display_name=f\"{job_name}_hpt\",\n",
    "    custom_job=train_job,\n",
    "    metric_spec=metric_spec,\n",
    "    parameter_spec=parameter_spec,\n",
    "    max_trial_count=8,\n",
    "    parallel_trial_count=2,\n",
    ")\n",
    "\n",
    "train_hpt_job.run()\n",
    "\n",
    "print(\"Trained models were saved in: \", lora_output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afad2431f37f"
   },
   "source": [
    "接着，从超参数调整工作中找到最佳试验。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "138b09ee6313"
   },
   "outputs": [],
   "source": [
    "best_trial_id = max(\n",
    "    train_hpt_job.trials, key=lambda trial: trial.final_measurement.metrics[0].value\n",
    ").id\n",
    "lora_output_dir = os.path.join(lora_output_dir, f\"trial_{best_trial_id}\")\n",
    "lora_output_dir_gcsfuse = lora_output_dir.replace(\"gs://\", \"/gcs/\")\n",
    "print(f\"Best trial {best_trial_id} saved model in:\", lora_output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jqmCtkGnhDmp"
   },
   "source": [
    "### 使用vLLM部署\n",
    "该部分将模型上传到模型注册表，并将其部署在端点上。vLLM目前不支持为经过微调的[openlm-research/open_llama_3b](https://huggingface.co/openlm-research/open_llama_3b)提供服务。\n",
    "\n",
    "模型部署步骤将需要大约15分钟的时间才能完成。\n",
    "\n",
    "使用LoRA权重的[openlm-research/open_llama_3b](https://huggingface.co/openlm-research/open_llama_3b)，[openlm-research/open_llama_7b](https://huggingface.co/openlm-research/open_llama_7b)，和[openlm-research/open_llama_13b](https://huggingface.co/openlm-research/open_llama_13b)的峰值GPU内存使用量分别为~5.3G，~8.7G和~15.2G（使用默认设置）。\n",
    "\n",
    "注意：vLLM需要一个合并模型，其中包含基础模型和经过微调的LoRA适配器。根据您的业务需求，如果您需要基础模型和经过微调的LoRA权重分开提供服务，请考虑改用正常的Vertex服务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bf55e38815dc"
   },
   "outputs": [],
   "source": [
    "# Finds Vertex AI prediction supported accelerators and regions in\n",
    "# https://cloud.google.com/vertex-ai/docs/predictions/configure-compute.\n",
    "\n",
    "# Sets V100 to deploy open_llama_3b and open_llama_7b.\n",
    "# V100 serving has better throughput and latency performance than L4 serving.\n",
    "machine_type = \"n1-standard-8\"\n",
    "accelerator_type = \"NVIDIA_TESLA_V100\"\n",
    "accelerator_count = 1\n",
    "\n",
    "# Sets L4 to deploy open_llama_3b and open_llama_7b.\n",
    "# L4 serving is more cost efficient than V100 serving.\n",
    "# machine_type = \"g2-standard-8\"\n",
    "# accelerator_type = \"NVIDIA_L4\"\n",
    "# accelerator_count = 1\n",
    "\n",
    "# Sets 2 V100 to deploy open_llama_13b.\n",
    "# V100 serving has better throughput and latency performance than L4 serving.\n",
    "# machine_type = \"n1-standard-16\"\n",
    "# accelerator_type = \"NVIDIA_TESLA_V100\"\n",
    "# accelerator_count = 2\n",
    "\n",
    "# Sets 2 L4 to deploy open_llama_13b.\n",
    "# L4 serving is more cost efficient than V100 serving.\n",
    "# machine_type = \"g2-standard-24\"\n",
    "# accelerator_type = \"NVIDIA_L4\"\n",
    "# accelerator_count = 2\n",
    "\n",
    "if prebuilt_model_id == \"openlm-research/open_llama_3b\":\n",
    "    # vLLM currently does not support OpenLLaMA 3B.\n",
    "    precision_loading_mode = \"float16\"\n",
    "    model_with_peft, endpoint_with_peft = deploy_model(\n",
    "        model_name=get_job_name_with_datetime(prefix=\"openllama-peft-serve\"),\n",
    "        model_id=model_id,\n",
    "        finetuned_lora_model_path=lora_output_dir,  # This will avoid override finetuning models.\n",
    "        service_account=SERVICE_ACCOUNT,\n",
    "        task=\"causal-language-modeling-lora\",\n",
    "        precision_loading_mode=precision_loading_mode,\n",
    "        machine_type=machine_type,\n",
    "        accelerator_type=accelerator_type,\n",
    "        accelerator_count=accelerator_count,\n",
    "    )\n",
    "else:\n",
    "    model_with_peft, endpoint_with_peft = deploy_model_vllm(\n",
    "        model_name=create_name_with_datetime(prefix=\"openllama-peft-serve-vllm\"),\n",
    "        model_id=merged_model_output_dir,\n",
    "        service_account=SERVICE_ACCOUNT,\n",
    "        machine_type=machine_type,\n",
    "        accelerator_type=accelerator_type,\n",
    "        accelerator_count=accelerator_count,\n",
    "    )\n",
    "\n",
    "print(\"endpoint_name:\", endpoint_with_peft.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80b3fd2ace09"
   },
   "source": [
    "注意：部署成功后，基本模型权重将从原始位置上的基础上动态下载，LoRA模型权重将从训练中使用的GCS存储桶中下载。因此，在上述模型部署步骤成功之后，需要额外等待5分钟，在进行下面的下一步之前。否则，当您发送请求到端点时，您可能会看到`ServiceUnavailable: 503 502:Bad Gateway`错误。\n",
    "\n",
    "一旦部署成功，您可以使用文本提示向端点发送请求。vLLM支持的参数可以在[这里](https://github.com/vllm-project/vllm/blob/2e8e49fce3775e7704d413b2f02da6d7c99525c9/vllm/sampling_params.py#L23-L64)找到。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ab04da3ec9a"
   },
   "outputs": [],
   "source": [
    "instance = {\n",
    "    \"prompt\": \"Hi, Google. How are you doing?\",\n",
    "    \"n\": 1,\n",
    "    \"max_tokens\": 32,\n",
    "    \"temperature\": 1.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"top_k\": 10,\n",
    "}\n",
    "response = endpoint_with_peft.predict(instances=[instance])\n",
    "print(response.predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vhDf9dNNn4bP"
   },
   "source": [
    "### [可选] 将之前训练过的LoRA适配器与基础模型合并\n",
    "\n",
    "本节介绍如何将之前训练过的LoRA适配器与基础模型合并，并将合并后的模型保存到GCS存储桶中。请注意，LoRA适配器应该在相同的基础模型上进行训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YHdru1aRqRFF"
   },
   "outputs": [],
   "source": [
    "merge_job_name = create_name_with_datetime(prefix=\"openllama-peft-merge\")\n",
    "\n",
    "# The base model to be merged upon. It can be a huggingface model id, or a GCS\n",
    "# path where the base model was stored.\n",
    "base_model_dir = \"gs://\"  # @param {type:\"string\"}\n",
    "# The previously trained LoRA adapter. It needs to be stored in a GCS path.\n",
    "finetuned_lora_adapter_dir = \"\"  # @param {type:\"string\"}\n",
    "\n",
    "# The GCS path to save the merged model\n",
    "merged_model_output_dir = os.path.join(MODEL_BUCKET, merge_job_name)\n",
    "merged_model_output_dir_gcsfuse = merged_model_output_dir.replace(\"gs://\", \"/gcs/\")\n",
    "\n",
    "# Worker pool spec.\n",
    "# Merges open_llama_3b and open_llama_7b with 1 V100 (16G).\n",
    "machine_type = \"n1-standard-8\"\n",
    "accelerator_type = \"NVIDIA_TESLA_V100\"\n",
    "\n",
    "# Merges open_llama_3b and open_llama_7b with 1 L4 (24G).\n",
    "# machine_type = \"g2-standard-8\"\n",
    "# accelerator_type = \"NVIDIA_L4\"\n",
    "\n",
    "# Merges open_llama_13b with 1 L4 (24G).\n",
    "# machine_type = \"g2-standard-8\"\n",
    "# accelerator_type = \"NVIDIA_L4\"\n",
    "\n",
    "# Merges open_llama_13b with 1 A100 (40G).\n",
    "# machine_type = \"a2-highgpu-1g\"\n",
    "# accelerator_type = \"NVIDIA_TESLA_A100\"\n",
    "\n",
    "worker_pool_specs = [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": machine_type,\n",
    "            \"accelerator_type\": accelerator_type,\n",
    "            \"accelerator_count\": 1,\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": TRAIN_DOCKER_URI,\n",
    "            \"command\": [],\n",
    "            \"args\": [\n",
    "                \"--task=merge-causal-language-model-lora\",\n",
    "                \"--merge_model_precision_mode=float16\",\n",
    "                \"--pretrained_model_id=%s\" % base_model_dir,\n",
    "                \"--finetuned_lora_model_dir=%s\" % finetuned_lora_adapter_dir,\n",
    "                \"--merge_base_and_lora_output_dir=%s\" % merged_model_output_dir_gcsfuse,\n",
    "            ],\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "merge_custom_job = aiplatform.CustomJob(\n",
    "    display_name=merge_job_name,\n",
    "    project=PROJECT_ID,\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    "    staging_bucket=STAGING_BUCKET,\n",
    ")\n",
    "\n",
    "merge_custom_job.run()\n",
    "\n",
    "print(\"The merged model is stored at: \", merged_model_output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wi6UZnldXpdi"
   },
   "source": [
    "## 量化并部署OpenLLaMA 2 模型\n",
    "\n",
    "本节展示了使用Vertex Custom Job 对OpenLLaMA模型进行训练后的量化。量化可以减少模型所需的内存，同时尽量保持相同的性能。其中两种算法为AWQ和GPTQ。想了解更多关于AWQ的信息，请阅读以下出版物：[AWQ: 激活感知权重量化用于LLM压缩和加速](https://arxiv.org/abs/2306.00978)。想了解更多关于GPTQ的信息，请阅读以下出版物：[GPTQ: 用于生成式预训练变压器的准确后训练量化](https://arxiv.org/abs/2210.17323)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HTUo8dWSo8t9"
   },
   "source": [
    "### 对OpenLLaMA模型进行量化\n",
    "\n",
    "量化通过减少权重的位精度来减少提供模型所需的GPU数量，同时最小化性能下降。在VLLM上提供量化模型需要将模型量化为4位。建议首先搜索是否已经有模型被量化并公开可用：[AWQ](https://huggingface.co/TheBloke?search_models=-awq) 和 [GPTQ](https://huggingface.co/TheBloke?search_models=-gptq)。\n",
    "\n",
    "使用1个NVIDIA_L4 GPU对AWQ量化模型需要大约\n",
    "20分钟针对OpenLLaMA 3B，30分钟针对OpenLLaMA 7B，1小时针对OpenLLaMA 13B。\n",
    "\n",
    "使用1个NVIDIA_L4 GPU对GPTQ量化模型需要大约30分钟针对OpenLLaMA 3B，45分钟针对OpenLLaMA 7B，1.5小时针对OpenLLaMA 13B。经过微调的模型也可以被量化，只要LoRA权重与基本模型合并即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_YB31uAEpusN"
   },
   "outputs": [],
   "source": [
    "# Setup quantization job.\n",
    "\n",
    "# Set `finetuned_model_path` to `merged_model_output_dir` from the previous\n",
    "# section above to quantize the finetuned model, if not set the base model will\n",
    "# be quantized.\n",
    "finetuned_model_path = \"\"  # @param {type:\"string\"}\n",
    "if finetuned_model_path:\n",
    "    prequantized_model_path = finetuned_model_path\n",
    "else:\n",
    "    prequantized_model_path = model_id\n",
    "\n",
    "quantization_method = \"awq\"  # @param [\"awq\", \"gptq\"]\n",
    "quantization_job_name = get_job_name_with_datetime(\n",
    "    f\"openllama-{quantization_method}-quantize\"\n",
    ")\n",
    "\n",
    "quantization_output_dir = os.path.join(MODEL_BUCKET, quantization_job_name)\n",
    "quantization_output_dir_gcsfuse = quantization_output_dir.replace(\"gs://\", \"/gcs/\")\n",
    "\n",
    "# Worker pool spec.\n",
    "\n",
    "# Sets 1 L4 (24G) to quantize OpenLLaMA model.\n",
    "machine_type = \"g2-standard-16\"\n",
    "accelerator_type = \"NVIDIA_L4\"\n",
    "accelerator_count = 1\n",
    "\n",
    "\n",
    "# Quantization parameters.\n",
    "quantization_precision_mode = \"4bit\"\n",
    "if quantization_method == \"awq\":\n",
    "    awq_dataset_name = \"pileval\"\n",
    "    group_size = 64\n",
    "    quantization_args = [\n",
    "        \"--task=quantize-model\",\n",
    "        f\"--quantization_method={quantization_method}\",\n",
    "        f\"--pretrained_model_id={model_id}\",\n",
    "        f\"--quantization_precision_mode={quantization_precision_mode}\",\n",
    "        f\"--quantization_output_dir={quantization_output_dir_gcsfuse}\",\n",
    "        f\"--quantization_dataset_name={awq_dataset_name}\",\n",
    "        f\"--group_size={group_size}\",\n",
    "    ]\n",
    "else:\n",
    "    # The original datasets used in GPTQ paper [\"wikitext2\",\"c4\",\"c4-new\",\"ptb\",\"ptb-new\"].\n",
    "    gptq_dataset_name = \"c4\"  # @param {type:\"string\"}\n",
    "    gptq_precision_mode = \"4bit\"\n",
    "    group_size = -1\n",
    "    damp_percent = 0.1\n",
    "    desc_act = True\n",
    "    quantization_args = [\n",
    "        \"--task=quantize-model\",\n",
    "        f\"--quantization_method={quantization_method}\",\n",
    "        f\"--pretrained_model_id={model_id}\",\n",
    "        f\"--quantization_precision_mode={quantization_precision_mode}\",\n",
    "        f\"--quantization_output_dir={quantization_output_dir_gcsfuse}\",\n",
    "        f\"--quantization_dataset_name={gptq_dataset_name}\",\n",
    "        f\"--group_size={group_size}\",\n",
    "        f\"--damp_percent={damp_percent}\",\n",
    "        f\"--desc_act={desc_act}\",\n",
    "    ]\n",
    "\n",
    "# Pass quantization arguments and launch job.\n",
    "worker_pool_specs = [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": machine_type,\n",
    "            \"accelerator_type\": accelerator_type,\n",
    "            \"accelerator_count\": accelerator_count,\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"disk_spec\": {\n",
    "            \"boot_disk_type\": \"pd-ssd\",\n",
    "            \"boot_disk_size_gb\": 500,\n",
    "        },\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": TRAIN_DOCKER_URI,\n",
    "            \"env\": [\n",
    "                {\n",
    "                    \"name\": \"PYTORCH_CUDA_ALLOC_CONF\",\n",
    "                    \"value\": \"max_split_size_mb:32\",\n",
    "                },\n",
    "            ],\n",
    "            \"command\": [],\n",
    "            \"args\": quantization_args,\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Quantizing {prequantized_model_path}.\")\n",
    "quantize_job = aiplatform.CustomJob(\n",
    "    display_name=quantization_job_name,\n",
    "    project=PROJECT_ID,\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    "    staging_bucket=STAGING_BUCKET,\n",
    ")\n",
    "quantize_job.run()\n",
    "\n",
    "print(\"Quantized models were saved in: \", quantization_output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ocm3m2Hzr4ln"
   },
   "source": [
    "使用Google Cloud Text Moderation部署量化模型\n",
    "本部分将模型上传至模型注册表，并在端点上部署它。\n",
    "\n",
    "模型部署步骤将需要15分钟到1小时的时间来完成，具体取决于模型大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DdINEB5Ur9FX"
   },
   "outputs": [],
   "source": [
    "# Finds Vertex AI prediction supported accelerators and regions in\n",
    "# https://cloud.google.com/vertex-ai/docs/predictions/configure-compute.\n",
    "\n",
    "# Sets 1 L4 (24G) to deploy OpenLLaMA models.\n",
    "machine_type = \"g2-standard-8\"\n",
    "accelerator_type = \"NVIDIA_L4\"\n",
    "accelerator_count = 1\n",
    "\n",
    "\n",
    "if prebuilt_model_id == \"openlm-research/open_llama_3b\":\n",
    "    # vLLM currently does not support OpenLLaMA 3B.\n",
    "    precision_loading_mode = \"float16\"\n",
    "    model_quantized_vllm, endpoint_quantized_vllm = deploy_model(\n",
    "        model_name=get_job_name_with_datetime(prefix=\"openllama-quantized-serve\"),\n",
    "        model_id=quantization_output_dir,\n",
    "        finetuned_lora_model_path=\"\",  # This will avoid override finetuning models.\n",
    "        service_account=SERVICE_ACCOUNT,\n",
    "        task=\"causal-language-modeling-lora\",\n",
    "        precision_loading_mode=precision_loading_mode,\n",
    "        machine_type=machine_type,\n",
    "        accelerator_type=accelerator_type,\n",
    "        accelerator_count=accelerator_count,\n",
    "    )\n",
    "else:\n",
    "    model_quantized_vllm, endpoint_quantized_vllm = deploy_model_vllm(\n",
    "        model_name=create_name_with_datetime(prefix=\"openllama-quantized-serve-vllm\"),\n",
    "        model_id=quantization_output_dir,\n",
    "        service_account=SERVICE_ACCOUNT,\n",
    "        machine_type=machine_type,\n",
    "        accelerator_type=accelerator_type,\n",
    "        accelerator_count=accelerator_count,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y3Iy3q7CsEGe"
   },
   "source": [
    "注意：部署成功后，模型权重将会动态下载。因此，在上述模型部署步骤成功之后、下一步之前，需要额外等待 10 到 40 分钟（取决于模型大小）。否则，在向端点发送请求时，可能会出现 `ServiceUnavailable: 503 502: Bad Gateway` 错误。\n",
    "\n",
    "一旦部署成功，您可以使用文本提示向端点发送请求。\n",
    "\n",
    "例如：\n",
    "\n",
    "```\n",
    "人类: 什么是汽车？\n",
    "助手: 汽车，或称摩托车，是一种与道路相连的人类交通系统，用于将人或货物从一地移到另一地。这个词还包括一系列车辆，包括摩托艇、火车和飞机。汽车通常有四个轮子，一个载客舱以及一个引擎或发动机。它们自19世纪初以来就存在，现在是最受欢迎的交通方式之一，用于日常通勤、购物和其他目的。\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eGP9sCq9sF0V"
   },
   "outputs": [],
   "source": [
    "# Loads an existing endpoint instance using the endpoint name:\n",
    "# - Using `endpoint_name = endpoint_quantized_vllm.name` allows us to get the\n",
    "#   endpoint name of the endpoint `endpoint_quantized_vllm` created in the cell\n",
    "#   above.\n",
    "# - Alternatively, you can set `endpoint_name = \"1234567890123456789\"` to load\n",
    "#   an existing endpoint with the ID 1234567890123456789.\n",
    "# You may uncomment the code below to load an existing endpoint.\n",
    "\n",
    "# endpoint_name = endpoint_quantized_vllm.name\n",
    "# # endpoint_name = \"\"  # @param {type:\"string\"}\n",
    "# aip_endpoint_name = (\n",
    "#     f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
    "# )\n",
    "# endpoint_quantized_vllm = aiplatform.Endpoint(aip_endpoint_name)\n",
    "\n",
    "\n",
    "# Overides max_length and top_k parameters during inferences.\n",
    "# If you encounter the issue like `ServiceUnavailable: 503 Took too long to respond when processing`,\n",
    "# you can reduce the max length, such as set max_length as 20.\n",
    "instances = [\n",
    "    {\n",
    "        \"prompt\": \"What is a car?\",\n",
    "        \"max_tokens\": 50,\n",
    "        \"temperature\": 1.0,\n",
    "        \"top_p\": 1.0,\n",
    "        \"top_k\": 10,\n",
    "    },\n",
    "]\n",
    "response = endpoint_quantized_vllm.predict(instances=instances)\n",
    "\n",
    "for prediction in response.predictions:\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JmuUk3l1DoEo"
   },
   "source": [
    "评估使用PEFT LoRA微调的OpenLLaMA\n",
    "\n",
    "本节展示如何使用EleutherAI的[语言模型评估工具（lm-evaluation-harness）](https://github.com/EleutherAI/lm-evaluation-harness)和Vertex CustomJob评估微调后的OpenLLaMA模型。\n",
    "\n",
    "此示例使用数据集[TruthfulQA](https://arxiv.org/abs/2109.07958)。所有支持的任务都列在[此任务表](https://github.com/EleutherAI/lm-evaluation-harness/blob/master/docs/task_table.md)中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gM4SXaquDoEo"
   },
   "outputs": [],
   "source": [
    "eval_dataset = \"truthfulqa_mc\"  # @param {type:\"string\"}\n",
    "\n",
    "# Worker pool spec.\n",
    "# Sets L4 to evaluate open_llama_3b and open_llama_7b.\n",
    "machine_type = \"n1-standard-8\"\n",
    "accelerator_type = \"NVIDIA_TESLA_V100\"\n",
    "accelerator_count = 1\n",
    "\n",
    "# Sets L4 to evaluate open_llama_3b and open_llama_7b.\n",
    "# machine_type = \"g2-standard-8\"\n",
    "# accelerator_type = \"NVIDIA_L4\"\n",
    "# accelerator_count = 1\n",
    "\n",
    "# Sets 2 V100 to evaluate open_llama_13b.\n",
    "# machine_type = \"n1-standard-8\"\n",
    "# accelerator_type = \"NVIDIA_TESLA_V100\"\n",
    "# accelerator_count = 2\n",
    "\n",
    "# Sets 2 L4 to evaluate open_llama_13b.\n",
    "# machine_type = \"g2-standard-24\"\n",
    "# accelerator_type = \"NVIDIA_L4\"\n",
    "# accelerator_count = 2\n",
    "\n",
    "replica_count = 1\n",
    "\n",
    "# Setup evaluation job.\n",
    "job_name = create_name_with_datetime(prefix=\"openllama-peft-eval\")\n",
    "eval_output_dir = os.path.join(MODEL_BUCKET, job_name)\n",
    "eval_output_dir_gcsfuse = eval_output_dir.replace(\"gs://\", \"/gcs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g0t0RBixIw0P"
   },
   "outputs": [],
   "source": [
    "# Prepare evaluation command that runs the evaluation harness.\n",
    "# Set `use_accelerate = True` to enable evaluation across multiple GPUs.\n",
    "eval_command = [\n",
    "    \"python\",\n",
    "    \"main.py\",\n",
    "    \"--model\",\n",
    "    \"hf-causal-experimental\",\n",
    "    \"--model_args\",\n",
    "    f\"pretrained={merged_model_output_dir_gcsfuse},use_accelerate=True,device_map_option=auto\",\n",
    "    \"--tasks\",\n",
    "    f\"{eval_dataset}\",\n",
    "    \"--output_path\",\n",
    "    f\"{eval_output_dir_gcsfuse}\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ItWB0WS__CX-"
   },
   "source": [
    "### 提交评估 CustomJob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BbyIk99bDoEo"
   },
   "outputs": [],
   "source": [
    "# Pass evaluation arguments and launch job.\n",
    "worker_pool_specs = [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": machine_type,\n",
    "            \"accelerator_type\": accelerator_type,\n",
    "            \"accelerator_count\": accelerator_count,\n",
    "        },\n",
    "        \"replica_count\": replica_count,\n",
    "        \"disk_spec\": {\n",
    "            \"boot_disk_size_gb\": 500,\n",
    "        },\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": EVAL_DOCKER_URI,\n",
    "            \"command\": eval_command,\n",
    "            \"args\": [],\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "eval_job = aiplatform.CustomJob(\n",
    "    display_name=job_name,\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    "    base_output_dir=eval_output_dir,\n",
    ")\n",
    "\n",
    "eval_job.run()\n",
    "\n",
    "print(\"Evaluation results were saved in:\", eval_output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kN0lE2iu_NXN"
   },
   "source": [
    "获取并打印评估结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "927oRxoADoEp"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "# Fetch evaluation results.\n",
    "storage_client = storage.Client()\n",
    "BUCKET_NAME = BUCKET_URI.split(\"gs://\")[1]\n",
    "bucket = storage_client.get_bucket(BUCKET_NAME)\n",
    "RESULT_FILE_PATH = eval_output_dir[len(BUCKET_URI) + 1 :]\n",
    "blob = bucket.blob(RESULT_FILE_PATH)\n",
    "raw_result = blob.download_as_string()\n",
    "\n",
    "# Print evaluation results.\n",
    "result = json.loads(raw_result)\n",
    "result_formatted = json.dumps(result, indent=2)\n",
    "print(f\"Evaluation result:\\n{result_formatted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "af21a3cff1e0"
   },
   "source": [
    "清理资源"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "911406c1561e"
   },
   "outputs": [],
   "source": [
    "# Delete custom train and evaluation jobs.\n",
    "train_job.delete()\n",
    "eval_job.delete()\n",
    "quantize_job.delete()\n",
    "\n",
    "# Undeploy models and delete endpoints.\n",
    "endpoint_without_peft.delete(force=True)\n",
    "endpoint_with_peft.delete(force=True)\n",
    "endpoint_quantized_vllm.delete(force=True)\n",
    "\n",
    "# Delete models.\n",
    "model_without_peft.delete()\n",
    "model_with_peft.delete()\n",
    "model_quantized_vllm.delete()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "model_garden_pytorch_openllama_peft.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
