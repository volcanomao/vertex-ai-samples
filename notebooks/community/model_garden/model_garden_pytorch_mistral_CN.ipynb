{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7d9bbf86da5e"
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99c1c3fc2ca5"
   },
   "source": [
    "# Vertex AI模型花园- Mistral和Mixtral 8x7B模型\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_mistral.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> 在Colab中运行\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_mistral.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"> 查看在GitHub上\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/notebooks/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/model_garden/model_garden_pytorch_mistral.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"> 在Vertex AI Workbench中打开\n",
    "    </a>（推荐使用Python-3 CPU笔记本）\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3de7470326a2"
   },
   "source": [
    "## 概述\n",
    "\n",
    "本笔记本演示了在Vertex AI中部署预构建的[Mistral](https://mistral.ai/)和Mixtral 8x7B模型。\n",
    "\n",
    "### 目标\n",
    "\n",
    "- 使用[vLLM](https://github.com/vllm-project/vllm)容器部署预构建的[Mistral模型](https://huggingface.co/mistralai)\n",
    "    - [mistralai/Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1)：具有70亿参数的预训练生成文本模型\n",
    "    - [mistralai/Mistral-7B-Instruct-v0.1](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1)：Mistral-7B-v0.1生成文本模型的指导微调版本\n",
    "    - [mistralai/Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)：支持32k上下文长度的Mistral-7B-Instruct-v0.1改进的指导微调版本\n",
    "- 使用[vLLM](https://github.com/vllm-project/vllm)容器部署预构建的[Mixtral 8x7B模型](https://huggingface.co/mistralai)\n",
    "    - [mistralai/Mixtral-8x7B-v0.1](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)：具有8个分支的预训练专家混合（MoE）模型\n",
    "    - [mistralai/Mixtral-8x7B-Instruct-v0.1](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1)：具有8个分支的Mixture of Experts（MoE）模型的指导微调版本\n",
    "\n",
    "### 成本\n",
    "\n",
    "本教程使用Google Cloud的计费组件：\n",
    "\n",
    "* Vertex AI\n",
    "* 云存储\n",
    "\n",
    "了解[Vertex AI定价](https://cloud.google.com/vertex-ai/pricing)、[云存储定价](https://cloud.google.com/storage/pricing)、[云NL API定价](https://cloud.google.com/natural-language/pricing)以及使用[定价计算器](https://cloud.google.com/products/calculator/)根据您的预期使用量生成成本估算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "264c07757582"
   },
   "source": [
    "在你开始之前"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ioensNKM8ned"
   },
   "source": [
    "### 仅限 Colab\n",
    "对于 Colab 运行以下命令，如果您正在使用 Workbench 则跳过此部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2707b02ef5df"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    ! pip3 install --upgrade google-cloud-aiplatform\n",
    "    from google.colab import auth as google_auth\n",
    "\n",
    "    google_auth.authenticate_user()\n",
    "    # Install gdown for downloading example training images.\n",
    "    ! pip3 install gdown\n",
    "\n",
    "    # Restart the notebook kernel after installs.\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46d25fe73955"
   },
   "source": [
    "### 安装依赖项"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c75c2c1fa6e0"
   },
   "outputs": [],
   "source": [
    "! pip3 install transformers==4.36.0\n",
    "! pip3 install accelerate==0.23.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bb7adab99e41"
   },
   "source": [
    "### 设置谷歌云项目\n",
    "\n",
    "1. [选择或创建一个谷歌云项目](https://console.cloud.google.com/cloud-resource-manager)。当您第一次创建帐户时，您将获得300美元的免费信用用于计算/存储成本。\n",
    "\n",
    "2. [确保您的项目启用了计费](https://cloud.google.com/billing/docs/how-to/modify-project)。\n",
    "\n",
    "3. [启用Vertex AI API，Compute Engine API和Cloud Natural Language API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,compute_component,language.googleapis.com)。\n",
    "\n",
    "4. [创建一个Cloud Storage存储桶](https://cloud.google.com/storage/docs/creating-buckets) 用于存储实验输出。\n",
    "\n",
    "5. [创建一个服务账号](https://cloud.google.com/iam/docs/service-accounts-create#iam-service-accounts-create-console) 并分配 `Vertex AI User` 和 `Storage Object Admin` 角色，用于部署微调模型到Vertex AI终端。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6c460088b873"
   },
   "source": [
    "定义环境变量\n",
    "\n",
    "为实验环境设置以下变量。指定的云存储桶（`BUCKET_URI`）应位于指定的区域（`REGION`）。请注意，多区域存储桶（例如“us”）不被认为与由多区域范围（例如“us-central1”）覆盖的单个区域匹配。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "855d6b96f291"
   },
   "outputs": [],
   "source": [
    "# Cloud project id.\n",
    "PROJECT_ID = \"\"  # @param {type:\"string\"}\n",
    "\n",
    "# The region you want to launch jobs in.\n",
    "# Select region based on the accelerators and regions supported by Vertex AI Prediction\n",
    "# https://cloud.google.com/vertex-ai/docs/predictions/configure-compute.\n",
    "REGION = \"\"  # @param {type:\"string\"}\n",
    "\n",
    "# The Cloud Storage bucket for storing experiments output.\n",
    "# Start with gs:// prefix, e.g. gs://foo_bucket.\n",
    "BUCKET_URI = \"gs://\"  # @param {type:\"string\"}\n",
    "\n",
    "! gcloud config set project $PROJECT_ID\n",
    "\n",
    "import os\n",
    "\n",
    "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
    "\n",
    "# The service account looks like:\n",
    "# '@.iam.gserviceaccount.com'\n",
    "# Please go to https://cloud.google.com/iam/docs/service-accounts-create#iam-service-accounts-create-console\n",
    "# and create service account with `Vertex AI User` and `Storage Object Admin` roles.\n",
    "# The service account for deploying fine tuned model.\n",
    "SERVICE_ACCOUNT = \"\"  # @param {type:\"string\"}# HuggingFace access token.\n",
    "# Create a token at https://huggingface.co/settings/tokens.\n",
    "# Accept the model terms of service for the model you wish to use.\n",
    "HF_TOKEN = \"\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e828eb320337"
   },
   "source": [
    "初始化 Vertex AI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "12cd25839741"
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cc825514deb"
   },
   "source": [
    "### 定义常数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b42bd4fa2b2d"
   },
   "outputs": [],
   "source": [
    "# The pre-built serving docker images with vLLM\n",
    "VLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20240313_0916_RC00\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0c250872074f"
   },
   "source": [
    "### 定义常见函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "354da31189dc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from typing import Tuple\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "\n",
    "def get_job_name_with_datetime(prefix: str) -> str:\n",
    "    \"\"\"Gets the job name with date time when triggering training or deployment\n",
    "    jobs in Vertex AI.\n",
    "    \"\"\"\n",
    "    return prefix + datetime.now().strftime(\"_%Y%m%d_%H%M%S\")\n",
    "\n",
    "\n",
    "def deploy_model_vllm(\n",
    "    model_name: str,\n",
    "    model_id: str,\n",
    "    service_account: str,\n",
    "    machine_type: str = \"g2-standard-8\",\n",
    "    accelerator_type: str = \"NVIDIA_L4\",\n",
    "    accelerator_count: int = 1,\n",
    "    max_model_len: int = 4096,\n",
    "    gpu_memory_utilization: float = 0.9,\n",
    "    use_openai_server: bool = False,\n",
    "    use_chat_completions_if_openai_server: bool = False,\n",
    "    huggingface_token: str = \"\",\n",
    ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
    "    \"\"\"Deploys Mistral models with vLLM on Vertex AI.\n",
    "\n",
    "    Args:\n",
    "        model_name: Display name of the model.\n",
    "        model_id: Model ID or path to model weights.\n",
    "        service_account: Service account for model uploading and deployment.\n",
    "        machine_type: Deployment machine type.\n",
    "        accelerator_type: Deployment accelerator type.\n",
    "        accelerator_count: Number of accelerators to use.\n",
    "        max_model_len: Maximum model length.\n",
    "        gpu_memory_utilization: Fraction of GPU memory to be used for the model\n",
    "            executor.\n",
    "        use_openai_server: Whether to use the OpenAI-format vLLM model server.\n",
    "        use_chat_completions_if_openai_server: If the OpenAI model server is\n",
    "            used, whether to use the chat completion API as opposed to the text\n",
    "            completion API. The vLLM text completion API mimics the OpenAI text\n",
    "            completion API:\n",
    "            https://platform.openai.com/docs/api-reference/completions/create.\n",
    "            It has two required parameters: the model ID to direct requests to\n",
    "            and the prompt. The response includes a \"choices\" field that\n",
    "            contains the generated text and a \"usage\" field that contains token\n",
    "            counts. The vLLM chat completion API mimics the OpenAI chat\n",
    "            completion API:\n",
    "            https://platform.openai.com/docs/api-reference/chat/create. It has\n",
    "            two required parameters: the model ID to direct requests to and\n",
    "            \"messages\" which is a sequence of system/user/assistant/tool\n",
    "            messages that can represent a multi-turn chat conversation. The\n",
    "            response includes a \"choices\" field that contains the generated\n",
    "            message from a role and a \"usage\" field that contains token counts.\n",
    "        huggingface_token: Huggingface token for accessing the model.\n",
    "\n",
    "    Returns:\n",
    "        Model instance and endpoint instance.\n",
    "    \"\"\"\n",
    "    endpoint = aiplatform.Endpoint.create(display_name=f\"{model_name}-endpoint\")\n",
    "\n",
    "    dtype = \"bfloat16\"\n",
    "    if accelerator_type in [\"NVIDIA_TESLA_T4\", \"NVIDIA_TESLA_V100\"]:\n",
    "        dtype = \"float16\"\n",
    "\n",
    "    vllm_args = [\n",
    "        \"--host=0.0.0.0\",\n",
    "        \"--port=7080\",\n",
    "        f\"--model={model_id}\",\n",
    "        f\"--tensor-parallel-size={accelerator_count}\",\n",
    "        \"--swap-space=16\",\n",
    "        f\"--dtype={dtype}\",\n",
    "        f\"--gpu-memory-utilization={gpu_memory_utilization}\",\n",
    "        f\"--max-model-len={max_model_len}\",\n",
    "        \"--disable-log-stats\",\n",
    "    ]\n",
    "    serving_env = {\n",
    "        \"MODEL_ID\": model_id,\n",
    "        \"DEPLOY_SOURCE\": \"notebook\",\n",
    "        \"HF_TOKEN\": huggingface_token,\n",
    "    }\n",
    "    if use_openai_server:\n",
    "        if use_chat_completions_if_openai_server:\n",
    "            serving_container_predict_route = \"/v1/chat/completions\"\n",
    "        else:\n",
    "            serving_container_predict_route = \"/v1/completions\"\n",
    "    else:\n",
    "        serving_container_predict_route = \"/generate\"\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=model_name,\n",
    "        serving_container_image_uri=VLLM_DOCKER_URI,\n",
    "        serving_container_command=[\n",
    "            \"python\",\n",
    "            \"-m\",\n",
    "            (\n",
    "                \"vllm.entrypoints.api_server\"\n",
    "                if not use_openai_server\n",
    "                else \"vllm.entrypoints.openai.api_server\"\n",
    "            ),\n",
    "        ],\n",
    "        serving_container_args=vllm_args,\n",
    "        serving_container_ports=[7080],\n",
    "        serving_container_predict_route=serving_container_predict_route,\n",
    "        serving_container_health_route=\"/health\",\n",
    "        serving_container_environment_variables=serving_env,\n",
    "    )\n",
    "\n",
    "    model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        machine_type=machine_type,\n",
    "        accelerator_type=accelerator_type,\n",
    "        accelerator_count=accelerator_count,\n",
    "        deploy_request_timeout=1800,\n",
    "        service_account=service_account,\n",
    "    )\n",
    "    return model, endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e057b5edcf81"
   },
   "source": [
    "使用预构建的Mistral和Mixtral模型在本地进行推理\n",
    "\n",
    "您至少需要24GB的内存才能运行Mistral-7B的推理。您可以在本地运行，或者在Vertex AI预测端点上使用以下任何规格：\n",
    "- g2-standard-8带有1个L4 GPU\n",
    "- n1-standard-16带有2个V100 GPU\n",
    "- n1-standard-16带有2个T4 GPU\n",
    "- a2-highgpu-1g带有1个A100 GPU\n",
    "\n",
    "您至少需要96GB的内存才能运行Mixtral 8x7B的推理。您可以在本地运行，或者在Vertex AI预测端点上使用以下任何规格：\n",
    "- g2-standard-96带有8个L4 GPU\n",
    "- n1-standard-32带有8个V100 GPU\n",
    "- n1-standard-32带有8个T4 GPU\n",
    "- a2-highgpu-4g带有4个A100 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "31f6cc84efdd"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device = \"cuda\"  # the device to load the model onto\n",
    "model_name = \"mistralai/Mistral-7B-v0.1\"  # @param [\"mistralai/Mistral-7B-v0.1\", \"mistralai/Mistral-7B-Instruct-v0.1\", \"mistralai/Mistral-7B-Instruct-v0.2\", \"mistralai/Mixtral-8x7B-v0.1\", \"mistralai/Mixtral-8x7B-Instruct-v0.1\"]\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, device_map=\"auto\", return_dict=True, torch_dtype=torch.float16\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "pipeline = transformers.pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "prompt = \"My favourite condiment is\"\n",
    "\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    max_length=200,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YKZ4CBJ2kYaW"
   },
   "source": [
    "## 使用vLLM部署预构建的Mistral模型\n",
    "\n",
    "本部分在Vertex端点上部署了预构建的Mistral模型，使用[vLLM](https://github.com/vllm-project/vllm)。模型部署步骤将需要大约15分钟完成。\n",
    "\n",
    "vLLM是一个高度优化的LLM服务框架，可以显著提高服务吞吐量。您拥有的QPS越高，使用vLLM获得的好处就越多。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25b5b3a44cf8"
   },
   "source": [
    "设置预先构建的模型ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "10547af949fc"
   },
   "outputs": [],
   "source": [
    "prebuilt_model_id = \"mistralai/Mistral-7B-v0.1\"  # @param [\"mistralai/Mistral-7B-v0.1\", \"mistralai/Mistral-7B-Instruct-v0.1\", \"mistralai/Mistral-7B-Instruct-v0.2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "03d504bcd60b"
   },
   "outputs": [],
   "source": [
    "# Find Vertex AI prediction supported accelerators and regions in\n",
    "# https://cloud.google.com/vertex-ai/docs/predictions/configure-compute.\n",
    "\n",
    "# Sets 1 L4 to deploy Mistral 7B.\n",
    "machine_type = \"g2-standard-8\"\n",
    "accelerator_type = \"NVIDIA_L4\"\n",
    "accelerator_count = 1\n",
    "\n",
    "# Sets 2 V100s to deploy Mistral 7B.\n",
    "# machine_type = \"n1-standard-16\"\n",
    "# accelerator_type = \"NVIDIA_TESLA_V100\"\n",
    "# accelerator_count = 2\n",
    "\n",
    "# Sets 2 T4s to deploy Mistral 7B.\n",
    "# machine_type = \"n1-standard-16\"\n",
    "# accelerator_type = \"NVIDIA_TESLA_T4\"\n",
    "# accelerator_count = 2\n",
    "\n",
    "# Sets 1 A100 (40G) to deploy Mistral 7B.\n",
    "# machine_type = \"a2-highgpu-1g\"\n",
    "# accelerator_type = \"NVIDIA_TESLA_A100\"\n",
    "# accelerator_count = 1\n",
    "\n",
    "# Larger setting of `max-model-len` can lead to higher requirements on\n",
    "# `gpu-memory-utilization` and GPU configuration. Larger setting of\n",
    "# `gpu-memory-utilization` increases the risk of running out of GPU memory with\n",
    "# long prompts.\n",
    "max_model_len = 4096\n",
    "gpu_memory_utilization = 0.9\n",
    "\n",
    "model, endpoint = deploy_model_vllm(\n",
    "    model_name=get_job_name_with_datetime(prefix=\"mistral-serve-vllm\"),\n",
    "    model_id=prebuilt_model_id,\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    machine_type=machine_type,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    "    max_model_len=max_model_len,\n",
    "    gpu_memory_utilization=gpu_memory_utilization,\n",
    "    use_openai_server=False,\n",
    "    use_chat_completions_if_openai_server=False,\n",
    "    huggingface_token=HF_TOKEN,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RRR11SWykYaX"
   },
   "source": [
    "请注意：如果在向端点发送请求时出现`ServiceUnavailable: 503 502:Bad Gateway`错误，那么模型服务器很可能仍在初始化中。请稍后重试。\n",
    "\n",
    "请注意：如果在部署过程中收到`InternalServerError: 500 System error`，很可能操作失败是因为资源不可用。请重试或使用不同的加速器类型。\n",
    "\n",
    "一旦部署成功，您可以使用文本提示向端点发送请求。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3a7948c56e3d"
   },
   "source": [
    "### 运行示例提示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3f5a1e1de60d"
   },
   "outputs": [],
   "source": [
    "# Loads an existing endpoint instance using the endpoint name:\n",
    "# - Using `endpoint_name = endpoint.name` allows us to get the endpoint name of\n",
    "#   the endpoint `endpoint` created in the cell above.\n",
    "# - Alternatively, you can set `endpoint_name = \"1234567890123456789\"` to load\n",
    "#   an existing endpoint with the ID 1234567890123456789.\n",
    "# You may uncomment the code below to load an existing endpoint.\n",
    "\n",
    "# endpoint_name = endpoint_without_peft.name\n",
    "# # endpoint_name = \"\"  # @param {type:\"string\"}\n",
    "# aip_endpoint_name = (\n",
    "#     f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
    "# )\n",
    "# endpoint = aiplatform.Endpoint(aip_endpoint_name)\n",
    "\n",
    "instances = [\n",
    "    {\n",
    "        \"prompt\": \"My favourite condiment is\",\n",
    "        \"n\": 1,\n",
    "        \"max_tokens\": 200,\n",
    "        \"temperature\": 1.0,\n",
    "        \"top_p\": 1.0,\n",
    "        \"top_k\": 10,\n",
    "    },\n",
    "]\n",
    "response = endpoint.predict(instances=instances)\n",
    "\n",
    "for prediction in response.predictions:\n",
    "    print(prediction)\n",
    "\n",
    "# Reference the following code for using the OpenAI vLLM server.\n",
    "# import json\n",
    "# response = endpoint.raw_predict(\n",
    "#     body=json.dumps({\n",
    "#         \"model\": prebuilt_model_id,\n",
    "#         \"prompt\": \"My favourite condiment is\",\n",
    "#         \"n\": 1,\n",
    "#         \"max_tokens\": 200,\n",
    "#         \"temperature\": 1.0,\n",
    "#         \"top_p\": 1.0,\n",
    "#         \"top_k\": 10,\n",
    "#     }),\n",
    "#     headers={\"Content-Type\": \"application/json\"},\n",
    "# )\n",
    "# print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wOh9irbqJ-MM"
   },
   "source": [
    "## 使用vLLM部署预构建的Mixtral 8x7B模型\n",
    "\n",
    "本部分将在Vertex端点上部署预构建的Mixtral 8x7B模型，使用[vLLM](https://github.com/vllm-project/vllm)。模型部署步骤将需要大约40分钟完成。\n",
    "\n",
    "vLLM是一个高度优化的LLM服务框架，可以显著提高服务吞吐量。您拥有的QPS越高，使用vLLM带来的好处就越多。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h2uCSnoaJ-MM"
   },
   "source": [
    "设置预建模型ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-X42gkGYJ-MM"
   },
   "outputs": [],
   "source": [
    "prebuilt_model_id = \"mistralai/Mixtral-8x7B-v0.1\"  # @param [\"mistralai/Mixtral-8x7B-v0.1\", \"mistralai/Mixtral-8x7B-Instruct-v0.1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M-YiJXT3J-MM"
   },
   "outputs": [],
   "source": [
    "# Find Vertex AI prediction supported accelerators and regions in\n",
    "# https://cloud.google.com/vertex-ai/docs/predictions/configure-compute.\n",
    "\n",
    "# Sets 8 L4s to deploy Mixtral 8x7B.\n",
    "machine_type = \"g2-standard-96\"\n",
    "accelerator_type = \"NVIDIA_L4\"\n",
    "accelerator_count = 8\n",
    "\n",
    "# Sets 4 A100s (40G) to deploy Mixtral 8x7B.\n",
    "# machine_type = \"a2-highgpu-4g\"\n",
    "# accelerator_type = \"NVIDIA_TESLA_A100\"\n",
    "# accelerator_count = 4\n",
    "\n",
    "# Larger setting of `max-model-len` can lead to higher requirements on\n",
    "# `gpu-memory-utilization` and GPU configuration. Larger setting of\n",
    "# `gpu-memory-utilization` increases the risk of running out of GPU memory with\n",
    "# long prompts.\n",
    "max_model_len = 4096\n",
    "gpu_memory_utilization = 0.85\n",
    "\n",
    "model, endpoint = deploy_model_vllm(\n",
    "    model_name=get_job_name_with_datetime(prefix=\"mixtral-serve-vllm\"),\n",
    "    model_id=prebuilt_model_id,\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    machine_type=machine_type,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    "    max_model_len=max_model_len,\n",
    "    gpu_memory_utilization=gpu_memory_utilization,\n",
    "    use_openai_server=False,\n",
    "    use_chat_completions_if_openai_server=False,\n",
    "    huggingface_token=HF_TOKEN,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "agDw0_7JJ-MM"
   },
   "source": [
    "注意：如果在向端点发送请求时看到`ServiceUnavailable: 503 502:Bad Gateway`错误，那么模型服务器可能仍在初始化中。请稍后重试。\n",
    "\n",
    "注意：如果在部署过程中收到`InternalServerError: 500 System error`，很可能是由于资源不足导致操作失败。请重试或使用不同的加速器类型。\n",
    "\n",
    "一旦部署成功，您可以通过文本提示向端点发送请求。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zLyWJK5aJ-MM"
   },
   "source": [
    "### 运行示例提示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NYN1Z49SJ-MM"
   },
   "outputs": [],
   "source": [
    "# Loads an existing endpoint instance using the endpoint name:\n",
    "# - Using `endpoint_name = endpoint.name` allows us to get the endpoint name of\n",
    "#   the endpoint `endpoint` created in the cell above.\n",
    "# - Alternatively, you can set `endpoint_name = \"1234567890123456789\"` to load\n",
    "#   an existing endpoint with the ID 1234567890123456789.\n",
    "# You may uncomment the code below to load an existing endpoint.\n",
    "\n",
    "# endpoint_name = endpoint_without_peft.name\n",
    "# # endpoint_name = \"\"  # @param {type:\"string\"}\n",
    "# aip_endpoint_name = (\n",
    "#     f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
    "# )\n",
    "# endpoint = aiplatform.Endpoint(aip_endpoint_name)\n",
    "\n",
    "instances = [\n",
    "    {\n",
    "        \"prompt\": \"What is a car?\",\n",
    "        \"max_tokens\": 50,\n",
    "        \"temperature\": 1.0,\n",
    "        \"top_p\": 1.0,\n",
    "        \"top_k\": 10,\n",
    "    },\n",
    "]\n",
    "response = endpoint.predict(instances=instances)\n",
    "\n",
    "for prediction in response.predictions:\n",
    "    print(prediction)\n",
    "\n",
    "# Reference the following code for using the OpenAI vLLM server.\n",
    "# import json\n",
    "# response = endpoint.raw_predict(\n",
    "#     body=json.dumps({\n",
    "#         \"model\": prebuilt_model_id,\n",
    "#         \"prompt\": \"My favourite condiment is\",\n",
    "#         \"n\": 1,\n",
    "#         \"max_tokens\": 200,\n",
    "#         \"temperature\": 1.0,\n",
    "#         \"top_p\": 1.0,\n",
    "#         \"top_k\": 10,\n",
    "#     }),\n",
    "#     headers={\"Content-Type\": \"application/json\"},\n",
    "# )\n",
    "# print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NRaBADRM4JEn"
   },
   "source": [
    "清理\n",
    "\n",
    "要清理此项目中使用的所有Google Cloud资源，您可以[删除用于本教程的Google Cloud项目](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects)。\n",
    "\n",
    "否则，您可以删除本教程中创建的各个资源。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0a9da70d4abc"
   },
   "source": [
    "### 下线模型并删除端点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e53749ae6b2c"
   },
   "outputs": [],
   "source": [
    "# Set this flag to delete endpoint including undeploying models\n",
    "delete_endpoint = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "65078f3ec44e"
   },
   "outputs": [],
   "source": [
    "def list_endpoints():\n",
    "    return [\n",
    "        (r.name, r.display_name)\n",
    "        for r in aiplatform.Endpoint.list()\n",
    "        if r.display_name.startswith(\"mistral-serve-vllm\")\n",
    "        or r.display_name.startswith(\"mixtral-serve-vllm\")\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cf56ac4cc73b"
   },
   "outputs": [],
   "source": [
    "# Delete the endpoint using the Vertex AI fully qualified identifier for the endpoint\n",
    "try:\n",
    "    if delete_endpoint:\n",
    "        endpoints = list_endpoints()\n",
    "        for endpoint_id, endpoint_name in endpoints:\n",
    "            endpoint = aiplatform.Endpoint(endpoint_id)\n",
    "            print(\n",
    "                f\"Undeploying all deployed models and deleting endpoint {endpoint_id} [{endpoint_name}]\"\n",
    "            )\n",
    "            endpoint.delete(force=True)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d25a89a34b5e"
   },
   "source": [
    "### 删除云存储存储桶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PAr4UWWx4JEo"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "delete_bucket = False\n",
    "\n",
    "job.delete()\n",
    "\n",
    "if delete_bucket or os.getenv(\"ID_TESTING\"):\n",
    "    ! gsutil rm -rf {BUCKET_URI}"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "model_garden_pytorch_mistral.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
