{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7d9bbf86da5e"
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99c1c3fc2ca5"
   },
   "source": [
    "# Vertex AI模型花园-猎鹰指导（PEFT）\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_falcon_instruct_peft.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> 在Colab中运行\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_falcon_instruct_peft.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      在GitHub上查看\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/notebooks/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/model_garden/model_garden_pytorch_falcon_instruct_peft.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "在Vertex AI Workbench中打开\n",
    "    </a> （建议使用Python-3 GPU笔记本）\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3de7470326a2"
   },
   "source": [
    "## 概述\n",
    "\n",
    "本笔记本演示了在本地运行预先构建的猎鹰指导模型进行推理，部署预建猎鹰指导模型，使用性能高效的微调库（[PEFT](https://github.com/huggingface/peft)）对猎鹰指导模型进行微调和部署，使用[GPTQ](https://arxiv.org/abs/2210.17323)对猎鹰指导模型进行量化和部署，以及在Vertex AI中评估PEFT微调的猎鹰指导模型。\n",
    "\n",
    "### 目标\n",
    "\n",
    "- 在预建猎鹰指导模型上在本地运行推理\n",
    "- 部署预建猎鹰指导模型\n",
    "- 使用PEFT微调和部署猎鹰指导模型\n",
    "- 使用GPTQ量化和部署猎鹰指导模型\n",
    "- 评估PEFT微调的猎鹰指导模型\n",
    "\n",
    "| 模型 | LoRA |\n",
    "| :- | :- |\n",
    "| [tiiuae/falcon-7b-instruct](https://huggingface.co/tiiuae/falcon-7b-instruct) | 是 |\n",
    "| [tiiuae/falcon-40b-instruct](https://huggingface.co/tiiuae/falcon-40b-instruct) | 是 |\n",
    "\n",
    "### 成本\n",
    "\n",
    "本教程使用谷歌云的收费组件：\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "了解[Vertex AI定价](https://cloud.google.com/vertex-ai/pricing)和[Cloud Storage定价](https://cloud.google.com/storage/pricing)，并使用[定价计算器](https://cloud.google.com/products/calculator/)根据您的预期使用量生成成本估算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "264c07757582"
   },
   "source": [
    "在开始之前\n",
    "\n",
    "**注意**：Jupyter以带有`!`前缀的行作为shell命令运行，并将以`$`前缀的Python变量插入这些命令中。\n",
    "\n",
    "使用Falcon Instruct模型在本地进行推理需要GPU。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ioensNKM8ned"
   },
   "source": [
    "仅适用于 Colab\n",
    "如果您使用 Workbench，请跳过此部分并运行以下命令。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2707b02ef5df"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    ! pip3 install --upgrade google-cloud-aiplatform\n",
    "    ! pip3 install google-cloud-language==2.10.0\n",
    "    from google.colab import auth as google_auth\n",
    "\n",
    "    google_auth.authenticate_user()\n",
    "\n",
    "    # Restart the notebook kernel after installs.\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)\n",
    "! pip3 install transformers==4.31.0\n",
    "! pip3 install einops==0.6.1\n",
    "! pip3 install accelerate==0.21.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bb7adab99e41"
   },
   "source": [
    "### 设置Google Cloud项目\n",
    "\n",
    "1. [选择或创建一个Google Cloud项目](https://console.cloud.google.com/cloud-resource-manager)。当您首次创建帐户时，您将获得300美元的免费信用额度，用于您的计算/存储成本。\n",
    "\n",
    "2. [确保您的项目已启用计费](https://cloud.google.com/billing/docs/how-to/modify-project)。\n",
    "\n",
    "3. [启用Vertex AI API，Compute Engine API和Cloud Natural Language API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,compute_component,language.googleapis.com)。\n",
    "\n",
    "4. [创建一个Cloud Storage存储桶](https://cloud.google.com/storage/docs/creating-buckets)来存储实验输出。\n",
    "\n",
    "5. [创建一个服务账号](https://cloud.google.com/iam/docs/service-accounts-create#iam-service-accounts-create-console)，具有`Vertex AI User`和`Storage Object Admin`角色，用于将经过微调的模型部署到Vertex AI端点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6c460088b873"
   },
   "source": [
    "为实验环境设置以下变量。指定的云存储桶（`BUCKET_URI`）应位于指定的地区（`REGION`）中。请注意，多区域存储桶（例如“us”）不被视为与多区域范围（例如“us-central1”）中覆盖的单一地区匹配。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "855d6b96f291"
   },
   "outputs": [],
   "source": [
    "# Cloud project id.\n",
    "PROJECT_ID = \"\"  # @param {type:\"string\"}\n",
    "\n",
    "# The region you want to launch jobs in.\n",
    "REGION = \"\"  # @param {type:\"string\"}\n",
    "\n",
    "# The Cloud Storage bucket for storing experiments output with gs:// prefix.\n",
    "BUCKET_URI = \"gs://\"  # @param {type:\"string\"}\n",
    "\n",
    "! gcloud config set project $PROJECT_ID\n",
    "\n",
    "import os\n",
    "\n",
    "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
    "EXPERIMENT_BUCKET = os.path.join(BUCKET_URI, \"peft\")\n",
    "DATA_BUCKET = os.path.join(EXPERIMENT_BUCKET, \"data\")\n",
    "MODEL_BUCKET = os.path.join(EXPERIMENT_BUCKET, \"model\")\n",
    "\n",
    "# The service account looks like:\n",
    "# '@.iam.gserviceaccount.com'\n",
    "# Please go to https://cloud.google.com/iam/docs/service-accounts-create#iam-service-accounts-create-console\n",
    "# and create service account with `Vertex AI User` and `Storage Object Admin` roles.\n",
    "# The service account for deploying fine tuned model.\n",
    "SERVICE_ACCOUNT = \"\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e828eb320337"
   },
   "source": [
    "### 初始化 Vertex AI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "12cd25839741"
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cc825514deb"
   },
   "source": [
    "### 定义常量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b42bd4fa2b2d"
   },
   "outputs": [],
   "source": [
    "# The pre-built training, serving and evaluation docker images.\n",
    "TRAIN_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-peft-train:20231207_0936_RC00\"\n",
    "PREDICTION_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-peft-serve:20231129_0948_RC00\"\n",
    "VLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20231127_0916_RC00\"\n",
    "VLLM_GPTQ_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:gptq\"\n",
    "EVAL_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-lm-evaluation-harness:20231011_0934_RC00\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0c250872074f"
   },
   "source": [
    "定义常用函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "354da31189dc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from typing import Tuple\n",
    "\n",
    "from google.cloud import aiplatform, language\n",
    "\n",
    "\n",
    "def get_job_name_with_datetime(prefix: str) -> str:\n",
    "    \"\"\"Gets the job name with date time when triggering training or deployment\n",
    "    jobs in Vertex AI.\n",
    "    \"\"\"\n",
    "    return prefix + datetime.now().strftime(\"_%Y%m%d_%H%M%S\")\n",
    "\n",
    "\n",
    "def deploy_model(\n",
    "    model_name: str,\n",
    "    model_id: str,\n",
    "    finetuned_lora_model_path: str,\n",
    "    service_account: str,\n",
    "    task: str,\n",
    "    machine_type: str = \"n1-standard-8\",\n",
    "    accelerator_type: str = \"NVIDIA_TESLA_V100\",\n",
    "    accelerator_count: int = 1,\n",
    ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
    "    \"\"\"Deploys trained models into Vertex AI.\"\"\"\n",
    "    endpoint = aiplatform.Endpoint.create(display_name=f\"{model_name}-endpoint\")\n",
    "    serving_env = {\n",
    "        \"MODEL_ID\": model_id,\n",
    "        \"TASK\": task,\n",
    "        \"DEPLOY_SOURCE\": \"notebook\",\n",
    "    }\n",
    "    if finetuned_lora_model_path:\n",
    "        serving_env[\"FINETUNED_LORA_MODEL_PATH\"] = finetuned_lora_model_path\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=model_name,\n",
    "        serving_container_image_uri=PREDICTION_DOCKER_URI,\n",
    "        serving_container_ports=[7080],\n",
    "        serving_container_predict_route=\"/predictions/peft_serving\",\n",
    "        serving_container_health_route=\"/ping\",\n",
    "        serving_container_environment_variables=serving_env,\n",
    "    )\n",
    "    model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        machine_type=machine_type,\n",
    "        accelerator_type=accelerator_type,\n",
    "        accelerator_count=accelerator_count,\n",
    "        deploy_request_timeout=1800,\n",
    "        service_account=service_account,\n",
    "    )\n",
    "    return model, endpoint\n",
    "\n",
    "\n",
    "def deploy_model_vllm(\n",
    "    model_name: str,\n",
    "    model_id: str,\n",
    "    service_account: str,\n",
    "    machine_type: str = \"n1-standard-8\",\n",
    "    accelerator_type: str = \"NVIDIA_TESLA_V100\",\n",
    "    accelerator_count: int = 1,\n",
    "    quantization_method: str = \"\",\n",
    ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
    "    \"\"\"Deploys trained models with vLLM into Vertex AI.\"\"\"\n",
    "    endpoint = aiplatform.Endpoint.create(display_name=f\"{model_name}-endpoint\")\n",
    "\n",
    "    vllm_args = [\n",
    "        \"--host=0.0.0.0\",\n",
    "        \"--port=7080\",\n",
    "        f\"--model={model_id}\",\n",
    "        f\"--tensor-parallel-size={accelerator_count}\",\n",
    "        \"--swap-space=16\",\n",
    "        \"--gpu-memory-utilization=0.9\",\n",
    "        \"--disable-log-stats\",\n",
    "        \"--dtype=float16\",\n",
    "        \"--trust-remote-code\",\n",
    "    ]\n",
    "    if quantization_method:\n",
    "        vllm_args.append(f\"--quantization={quantization_method}\")\n",
    "    if quantization_method == \"gptq\":\n",
    "        vllm_docker_uri = VLLM_GPTQ_DOCKER_URI\n",
    "    else:\n",
    "        vllm_docker_uri = VLLM_DOCKER_URI\n",
    "\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=model_name,\n",
    "        serving_container_image_uri=vllm_docker_uri,\n",
    "        serving_container_command=[\"python\", \"-m\", \"vllm.entrypoints.api_server\"],\n",
    "        serving_container_args=vllm_args,\n",
    "        serving_container_ports=[7080],\n",
    "        serving_container_predict_route=\"/generate\",\n",
    "        serving_container_health_route=\"/ping\",\n",
    "    )\n",
    "\n",
    "    model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        machine_type=machine_type,\n",
    "        accelerator_type=accelerator_type,\n",
    "        accelerator_count=accelerator_count,\n",
    "        deploy_request_timeout=1800,\n",
    "        service_account=service_account,\n",
    "    )\n",
    "    return model, endpoint\n",
    "\n",
    "\n",
    "def moderate_text(text: str) -> language.ModerateTextResponse:\n",
    "    \"\"\"Calls Vertex AI APIs to analyze text moderations.\"\"\"\n",
    "    client = language.LanguageServiceClient()\n",
    "    document = language.Document(\n",
    "        content=text,\n",
    "        type_=language.Document.Type.PLAIN_TEXT,\n",
    "    )\n",
    "    return client.moderate_text(document=document)\n",
    "\n",
    "\n",
    "def show_text_moderation(text: str, response: language.ModerateTextResponse) -> None:\n",
    "    \"\"\"Shows text moderation results.\"\"\"\n",
    "    import pandas as pd\n",
    "\n",
    "    def confidence(category: language.ClassificationCategory) -> float:\n",
    "        return category.confidence\n",
    "\n",
    "    columns = [\"category\", \"confidence\"]\n",
    "    categories = sorted(response.moderation_categories, key=confidence, reverse=True)\n",
    "    data = ((category.name, category.confidence) for category in categories)\n",
    "    df = pd.DataFrame(columns=columns, data=data)\n",
    "\n",
    "    print(f\"Text analyzed:\\n{text}\")\n",
    "    print(df.to_markdown(index=False, tablefmt=\"presto\", floatfmt=\".0%\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06e73cb3f412"
   },
   "source": [
    "使用预先构建的Falcon Instruct模型在本地运行推理\n",
    "\n",
    "您需要至少16GB的内存才能快速运行Falcon-7B-Instruct的推理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ea64305957f"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model = \"tiiuae/falcon-7b-instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "sequences = pipeline(\n",
    "    \"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Girafatron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\n",
    "    max_length=200,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8neJc8CnDDpu"
   },
   "source": [
    "## 部署预先构建的Falcon Instruct模型\n",
    "\n",
    "本节在端点上部署预构建的Falcon Instruct模型。模型部署步骤需要15到40分钟才能完成。\n",
    "\n",
    "[tiiuae/falcon-7b-instruct](https://huggingface.co/tiiuae/falcon-7b-instruct)和[tiiuae/falcon-40b-instruct](https://huggingface.co/tiiuae/falcon-40b-instruct)的GPU内存峰值使用分别为约15.5G和约84G，具有默认设置。请根据需要调整机器类型、加速器类型和加速器数量。我们在部署中使用V100作为示例。请注意，V100服务通常比L4服务提供更好的吞吐量和延迟性能，而L4服务通常比V100服务更具成本效益。V100和L4 GPU的服务效率不及A100 GPU，但如果没有A100配额，V100和L4 GPU仍然是良好的服务解决方案。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2MjaORIIFDVu"
   },
   "source": [
    "设置预构建模型ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E8OiHHNNE_wj"
   },
   "outputs": [],
   "source": [
    "prebuilt_model_id = \"tiiuae/falcon-7b-instruct\"  # @param [\"tiiuae/falcon-7b-instruct\", \"tiiuae/falcon-40b-instruct\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dHFW7yvjaVFV"
   },
   "source": [
    "我们使用PEFT服务的图像来部署预构建的猎鹰指导模型，将微调LoRA模型路径设置为空。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uak1pyEeExYM"
   },
   "outputs": [],
   "source": [
    "# Find Vertex AI supported accelerators and regions in:\n",
    "#  https://cloud.google.com/vertex-ai/docs/predictions/configure-compute\n",
    "\n",
    "# Sets V100 (16G) to deploy tiiuae/falcon-7b-instruct or tiiuae/falcon-40b-instruct.\n",
    "# If A100 is not available, you may deploy tiiuae/falcon-40b-instruct with\n",
    "#  multiple V100s. Please keep in mind that the efficiency of serving with\n",
    "#  multiple V100s is inferior to that of serving with A100s.\n",
    "# Compared with L4, V100 serving can have better throughput and latency.\n",
    "machine_type = \"n1-standard-8\"\n",
    "accelerator_type = \"NVIDIA_TESLA_V100\"\n",
    "accelerator_count = 1  # for tiiuae/falcon-7b-instruct\n",
    "# accelerator_count = 8  # for tiiuae/falcon-40b-instruct\n",
    "\n",
    "# Sets L4 (24G) to deploy tiiuae/falcon-7b-instruct or tiiuae/falcon-40b-instruct.\n",
    "# If A100 is not available, you may deploy tiiuae/falcon-40b-instruct with\n",
    "#  multiple L4s. Please keep in mind that the efficiency of serving with\n",
    "#  multiple L4s is inferior to that of serving with A100s.\n",
    "# Compared with V100, L4 serving can be more cost efficient.\n",
    "\n",
    "# For tiiuae/falcon-7b-instruct.\n",
    "# machine_type = \"g2-standard-8\"\n",
    "# accelerator_type = \"NVIDIA_L4\"\n",
    "# accelerator_count = 1\n",
    "\n",
    "# For tiiuae/falcon-40b-instruct.\n",
    "# machine_type = \"g2-standard-48\"\n",
    "# accelerator_type = \"NVIDIA_L4\"\n",
    "# accelerator_count = 4\n",
    "\n",
    "# Sets A100 (40G) to deploy tiiuae/falcon-7b-instruct or tiiuae/falcon-40b-instruct.\n",
    "# machine_type = \"a2-highgpu-1g\"\n",
    "# accelerator_type = \"NVIDIA_TESLA_A100\"\n",
    "# accelerator_count = 1  # for tiiuae/falcon-7b-instruct\n",
    "# accelerator_count = 4  # for tiiuae/falcon-40b-instruct\n",
    "\n",
    "# Sets A100 (80G) to deploy falcon-40b-instruct models for faster inferences.\n",
    "# machine_type = \"a2-ultragpu-1g\"\n",
    "# accelerator_type = \"NVIDIA_TESLA_A100\"\n",
    "# accelerator_count = 2\n",
    "\n",
    "model_without_peft, endpoint_without_peft = deploy_model(\n",
    "    model_name=get_job_name_with_datetime(prefix=\"falcon-instruct-serve\"),\n",
    "    model_id=prebuilt_model_id,\n",
    "    finetuned_lora_model_path=\"\",  # This will avoid override finetuning models.\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    task=\"instruct-lora\",\n",
    "    machine_type=machine_type,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    ")\n",
    "print(\"endpoint_name:\", endpoint_without_peft.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sGKIjgmDFRW2"
   },
   "source": [
    "注意：部署成功后，预先构建的模型权重将从原始位置即时下载。因此，在上述模型部署步骤成功后，需要额外等待10-30分钟，然后才能运行下面的下一步。否则，在发送请求到端点时，您可能会看到“ServiceUnavailable: 503 502:Bad Gateway”错误。\n",
    "\n",
    "一旦部署成功，您可以通过文本提示向端点发送请求。\n",
    "\n",
    "示例：\n",
    "\n",
    "```\n",
    "人类：汽车是什么？\n",
    "助手：汽车，或者叫机动车，是一种用于将人或货物从一个地方运送到另一个地方的与道路相连的人类交通系统。该术语还包括各种车辆，包括摩托艇、火车和飞机。汽车通常有四个轮子、一个乘客舱和一个发动机或马达。它们自19世纪初以来就存在，并且现在是最受欢迎的交通方式之一，用于日常通勤、购物和其他目的。\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rDHsCOqvFYBi"
   },
   "outputs": [],
   "source": [
    "# Loads an existing endpoint instance using the endpoint name:\n",
    "# - Using `endpoint_name = endpoint_without_peft.name` allows us to get the\n",
    "#   endpoint name of the endpoint `endpoint_without_peft` created in the cell\n",
    "#   above.\n",
    "# - Alternatively, you can set `endpoint_name = \"1234567890123456789\"` to load\n",
    "#   an existing endpoint with the ID 1234567890123456789.\n",
    "# You may uncomment the code below to load an existing endpoint.\n",
    "\n",
    "# endpoint_name = endpoint_without_peft.name\n",
    "# # endpoint_name = \"\"  # @param {type:\"string\"}\n",
    "# aip_endpoint_name = (\n",
    "#     f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
    "# )\n",
    "# endpoint_without_peft = aiplatform.Endpoint(aip_endpoint_name)\n",
    "\n",
    "instances = [\n",
    "    {\n",
    "        \"prompt\": \"What is a car?\",\n",
    "        \"max_tokens\": 50,\n",
    "        \"temperature\": 1.0,\n",
    "        \"top_p\": 1.0,\n",
    "        \"top_k\": 10,\n",
    "    },\n",
    "]\n",
    "response = endpoint_without_peft.predict(instances=instances)\n",
    "\n",
    "for prediction in response.predictions:\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e70e3519ff8b"
   },
   "source": [
    "使用PEFT微调和部署Falcon Instruct模型的方法示例在此部分展示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5qCrm_kJH5cz"
   },
   "source": [
    "设置基本模型ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N3UBLiYrM3sU"
   },
   "outputs": [],
   "source": [
    "model_id = \"tiiuae/falcon-7b-instruct\"  # @param [\"tiiuae/falcon-7b-instruct\", \"tiiuae/falcon-40b-instruct\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iWGwJHqI7LMs"
   },
   "source": [
    "### 调整优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KKEYoRfiHDVv"
   },
   "source": [
    "使用Vertex AI SDK来使用Vertex AI Model Garden训练图像，创建和运行自定义训练作业。\n",
    "\n",
    "这个示例使用数据集[timdettmers/openassistant-guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco)。您可以使用[huggingface的数据集](https://huggingface.co/datasets)或存储在Cloud Storage中的[Vertex文本模型数据集格式](https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-text-models-supervised#dataset-format)中的自定义JSONL数据集。`template`参数是可选的。\n",
    "\n",
    "LoRA模型在[tiiuae/falcon-7b-instruct](https://huggingface.co/tiiuae/falcon-7b-instruct)和[tiiuae/falcon-40b-instruct](https://huggingface.co/tiiuae/falcon-40b-instruct)的微调过程中，默认训练参数和示例数据集下，使用的显存峰值分别为~11G和~34G。Falcon-7b-instruct可以在1个P100/V100上进行微调，而falcon-40b-instruct可以在1个A100（40G）上进行微调。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d272b9f416a9"
   },
   "source": [
    "#### [可选] 使用自定义数据集进行微调\n",
    "\n",
    "要使用自定义数据集，您应该在下面的`dataset_name`中提供`gs://` URI指向一个符合[Vertex文本模型数据集格式](https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-text-models-supervised#dataset-format)的JSONL文件。\n",
    "\n",
    "例如，这是样本数据集`gs://cloud-samples-data/vertex-ai/model-evaluation/peft_train_sample.jsonl`中的一个数据点：\n",
    "\n",
    "```json\n",
    "{\"input_text\":\"TRANSCRIPT: \\nREASON FOR EVALUATION:,\\n\\n LABEL:\",\"output_text\":\"Chiropractic\"}\n",
    "```\n",
    "\n",
    "要使用包含`input_text`和`output_text`字段的样本数据集，请将`dataset_name`设置为`gs://cloud-samples-data/vertex-ai/model-evaluation/peft_train_sample.jsonl`，并将`template`设置为`vertex_sample`。要进一步使用自定义数据集字段，请参阅[模板示例](https://github.com/tloen/alpaca-lora/blob/main/templates/alpaca.json)，并提供您自己的JSON模板作为`gs://` URIs。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "65467b361315"
   },
   "outputs": [],
   "source": [
    "# Huggingface dataset name or gs:// URI to a custom JSONL dataset.\n",
    "dataset_name = \"timdettmers/openassistant-guanaco\"  # @param {type:\"string\"}\n",
    "# Optional. Template name or gs:// URI to a custom template.\n",
    "template = \"\"  # @param {type:\"string\"}\n",
    "\n",
    "# Worker pool spec.\n",
    "# Find Vertex AI supported accelerators and regions in:\n",
    "#  https://cloud.google.com/vertex-ai/docs/training/configure-compute\n",
    "\n",
    "# Uses V100 (16G) to finetune falcon-7b-instruct.\n",
    "machine_type = \"n1-standard-8\"\n",
    "accelerator_type = \"NVIDIA_TESLA_V100\"\n",
    "accelerator_count = 1\n",
    "\n",
    "# Uses L4 (24G) to finetune falcon-7b-instruct.\n",
    "# machine_type = \"g2-standard-8\"\n",
    "# accelerator_type = \"NVIDIA_L4\"\n",
    "# accelerator_count = 1\n",
    "\n",
    "# Uses V100 (16G) to finetune falcon-40b-instruct.\n",
    "# machine_type = \"n1-standard-16\"\n",
    "# accelerator_type = \"NVIDIA_TESLA_V100\"\n",
    "# accelerator_count = 4\n",
    "\n",
    "# Uses L4 (24G) to finetune falcon-40b-instruct.\n",
    "# machine_type = \"g2-standard-24\"\n",
    "# accelerator_type = \"NVIDIA_L4\"\n",
    "# accelerator_count = 2\n",
    "\n",
    "# Uses A100 (40G) to finetune falcon-40b-instruct.\n",
    "# machine_type = \"a2-highgpu-1g\"\n",
    "# accelerator_type = \"NVIDIA_TESLA_A100\"\n",
    "# accelerator_count = 1\n",
    "\n",
    "replica_count = 1\n",
    "\n",
    "# Setup training job.\n",
    "job_name = get_job_name_with_datetime(\"falcon-instruct-lora-train\")\n",
    "train_job = aiplatform.CustomContainerTrainingJob(\n",
    "    display_name=job_name,\n",
    "    container_uri=TRAIN_DOCKER_URI,\n",
    ")\n",
    "output_dir = os.path.join(MODEL_BUCKET, job_name)\n",
    "output_dir_gcsfuse = output_dir.replace(\"gs://\", \"/gcs/\")\n",
    "\n",
    "# Pass training arguments and launch job.\n",
    "max_steps = 10\n",
    "train_job.run(\n",
    "    args=[\n",
    "        \"--task=instruct-lora\",\n",
    "        f\"--pretrained_model_id={model_id}\",\n",
    "        f\"--dataset_name={dataset_name}\",\n",
    "        f\"--output_dir={output_dir_gcsfuse}\",\n",
    "        \"--lora_rank=64\",\n",
    "        \"--lora_alpha=16\",\n",
    "        \"--lora_dropout=0.1\",\n",
    "        \"--warmup_ratio=0.03\",\n",
    "        f\"--max_steps={max_steps}\",\n",
    "        \"--max_seq_length=512\",\n",
    "        \"--learning_rate=2e-4\",\n",
    "        f\"--template={template}\",\n",
    "    ],\n",
    "    replica_count=replica_count,\n",
    "    machine_type=machine_type,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    "    boot_disk_size_gb=500,\n",
    ")\n",
    "\n",
    "print(\"Trained models were saved in: \", output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jqmCtkGnhDmp"
   },
   "source": [
    "### 部署\n",
    "该部分将模型上传至模型注册表并在端点上部署。\n",
    "\n",
    "模型部署步骤将需要15分钟至40分钟完成。\n",
    "\n",
    "[tiiuae/falcon-7b-instruct](https://huggingface.co/tiiuae/falcon-7b-instruct)和[tiiuae/falcon-40b-instruct](https://huggingface.co/tiiuae/falcon-40b-instruct)在默认设置下使用LoRA权重的峰值GPU内存使用分别为约15.5G和约84G。请根据需要调整机器类型、加速器类型和加速器数量。我们在部署中使用V100作为示例。请注意，V100服务通常比L4服务提供更好的吞吐量和延迟性能，而L4服务通常比V100服务更具成本效益。V100和L4 GPU的服务效率不及A100 GPU，但如果您没有A100配额，V100和L4 GPU仍然是不错的服务解决方案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bf55e38815dc"
   },
   "outputs": [],
   "source": [
    "# Find Vertex AI supported accelerators and regions in:\n",
    "#  https://cloud.google.com/vertex-ai/docs/predictions/configure-compute\n",
    "\n",
    "# Sets V100 (16G) to deploy tiiuae/falcon-7b-instruct or tiiuae/falcon-40b-instruct.\n",
    "# If A100 is not available, you may deploy tiiuae/falcon-40b-instruct with\n",
    "#  multiple V100s. Please keep in mind that the efficiency of serving with\n",
    "#  multiple V100s is inferior to that of serving with A100s.\n",
    "# Compared with L4, V100 serving can have better throughput and latency.\n",
    "machine_type = \"n1-standard-8\"\n",
    "accelerator_type = \"NVIDIA_TESLA_V100\"\n",
    "accelerator_count = 1  # for tiiuae/falcon-7b-instruct\n",
    "# accelerator_count = 8  # for tiiuae/falcon-40b-instruct\n",
    "\n",
    "# Sets L4 (24G) to deploy tiiuae/falcon-7b-instruct or tiiuae/falcon-40b-instruct.\n",
    "# If A100 is not available, you may deploy tiiuae/falcon-40b-instruct with\n",
    "#  multiple L4s. Please keep in mind that the efficiency of serving with\n",
    "#  multiple L4s is inferior to that of serving with A100s.\n",
    "# Compared with V100, L4 serving can be more cost efficient.\n",
    "\n",
    "# For tiiuae/falcon-7b-instruct.\n",
    "# machine_type = \"g2-standard-8\"\n",
    "# accelerator_type = \"NVIDIA_L4\"\n",
    "# accelerator_count = 1\n",
    "\n",
    "# For tiiuae/falcon-40b-instruct.\n",
    "# machine_type = \"g2-standard-48\"\n",
    "# accelerator_type = \"NVIDIA_L4\"\n",
    "# accelerator_count = 4\n",
    "\n",
    "# Sets A100 (40G) to deploy tiiuae/falcon-7b-instruct or tiiuae/falcon-40b-instruct.\n",
    "# machine_type = \"a2-highgpu-1g\"\n",
    "# accelerator_type = \"NVIDIA_TESLA_A100\"\n",
    "# accelerator_count = 1  # for tiiuae/falcon-7b-instruct\n",
    "# accelerator_count = 4  # for tiiuae/falcon-40b-instruct\n",
    "\n",
    "# Sets A100 (80G) to deploy falcon-40b-instruct models for faster inferences.\n",
    "# machine_type = \"a2-ultragpu-1g\"\n",
    "# accelerator_type = \"NVIDIA_TESLA_A100_80GB\"\n",
    "# accelerator_count = 2\n",
    "\n",
    "model_with_peft, endpoint_with_peft = deploy_model(\n",
    "    model_name=get_job_name_with_datetime(prefix=\"falcon-instruct-peft-serve\"),\n",
    "    model_id=model_id,\n",
    "    finetuned_lora_model_path=os.path.join(output_dir, \"checkpoint-\" + str(max_steps)),\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    task=\"instruct-lora\",\n",
    "    machine_type=machine_type,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    ")\n",
    "print(\"endpoint_name:\", endpoint_with_peft.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80b3fd2ace09"
   },
   "source": [
    "注意：部署成功后，基本模型权重将会从原始位置实时下载，并且 LoRA 模型权重将从在训练中使用的 GCS 存储桶中下载。因此，在上述模型部署步骤成功之后，需要额外等待 10-30 分钟，**然后**才能运行下面的下一步。否则，在向端点发送请求时可能会看到“ServiceUnavailable: 503 502: Bad Gateway”错误。\n",
    "\n",
    "一旦部署成功，您就可以发送文本提示请求到端点。\n",
    "\n",
    "例如：\n",
    "\n",
    "```\n",
    "人类：汽车是什么？\n",
    "助手：汽车，或者叫汽车，是一种用于将人或货物从一处运到另一处的与道路连接的人类交通系统。这个名词还包括各种车辆，如摩托艇、火车和飞机。汽车通常有四个车轮、一个乘客舱和一个发动机。从19世纪初就存在，现在是最流行的交通工具之一，常用于日常通勤、购物等各种目的。\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ab04da3ec9a"
   },
   "outputs": [],
   "source": [
    "# Loads an existing endpoint instance using the endpoint name:\n",
    "# - Using `endpoint_name = endpoint_with_peft.name` allows us to get the\n",
    "#   endpoint name of the endpoint `endpoint_with_peft` created in the cell\n",
    "#   above.\n",
    "# - Alternatively, you can set `endpoint_name = \"1234567890123456789\"` to load\n",
    "#   an existing endpoint with the ID 1234567890123456789.\n",
    "# You may uncomment the code below to load an existing endpoint.\n",
    "\n",
    "# endpoint_name = endpoint_with_peft.name\n",
    "# # endpoint_name = \"\"  # @param {type:\"string\"}\n",
    "# aip_endpoint_name = (\n",
    "#     f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
    "# )\n",
    "# endpoint_with_peft = aiplatform.Endpoint(aip_endpoint_name)\n",
    "\n",
    "instances = [\n",
    "    {\n",
    "        \"prompt\": \"What is a car?\",\n",
    "        \"max_tokens\": 50,\n",
    "        \"temperature\": 1.0,\n",
    "        \"top_p\": 1.0,\n",
    "        \"top_k\": 10,\n",
    "    },\n",
    "]\n",
    "response = endpoint_with_peft.predict(instances=instances)\n",
    "\n",
    "for prediction in response.predictions:\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YcbApnK4iyyu"
   },
   "source": [
    "## 对Falcon Instruct模型进行量化并部署\n",
    "\n",
    "本节展示了使用Vertex Custom Job对Falcon Instruct模型进行后训练量化。量化可以减少模型所需的内存，同时尽量保持相同的性能。更多关于GPTQ的信息请阅读以下文章：[GPTQ: 准确的生成式预训练Transformer后训练量化](https://arxiv.org/abs/2210.17323)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rt9pFQV-jG1v"
   },
   "source": [
    "### 使用Google Cloud文本审查部署预量化模型\n",
    "这里提供了许多GPTQ-量化模型（链接：https://huggingface.co/TheBloke?search_models=-gptq）。\n",
    "\n",
    "本节将模型上传至模型注册表并在端点上部署。\n",
    "\n",
    "模型部署步骤将花费15分钟到1小时的时间，取决于模型大小。\n",
    "\n",
    "请注意，部署一个量化模型所需的GPU资源要少得多。我们只需使用两个L4s即可部署一个量化的40B模型，而不是四个。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_SFje2ifjNIx"
   },
   "outputs": [],
   "source": [
    "quantized_model_id = \"TheBloke/Falcon-7B-Instruct-GPTQ\"  # @param [\"TheBloke/Falcon-7B-Instruct-GPTQ\", \"TheBloke/falcon-40b-instruct-GPTQ\"]\n",
    "\n",
    "quantization_method = \"gptq\"\n",
    "\n",
    "# Finds Vertex AI prediction supported accelerators and regions in\n",
    "# https://cloud.google.com/vertex-ai/docs/predictions/configure-compute.\n",
    "\n",
    "# Sets 1 L4 (24G) to deploy Falcon Instruct 7B model.\n",
    "machine_type = \"g2-standard-8\"\n",
    "accelerator_type = \"NVIDIA_L4\"\n",
    "accelerator_count = 1\n",
    "\n",
    "# Sets 2 L4's (24G) to deploy Falcon Instruct 40B model.\n",
    "# machine_type = \"g2-standard-24\"\n",
    "# accelerator_type = \"NVIDIA_L4\"\n",
    "# accelerator_count = 2\n",
    "\n",
    "model_prequantized_vllm, endpoint_prequantized_vllm = deploy_model_vllm(\n",
    "    model_name=get_job_name_with_datetime(\n",
    "        prefix=\"falcon-instruct-serve-vllm-prequantized\"\n",
    "    ),\n",
    "    model_id=quantized_model_id,\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    machine_type=machine_type,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    "    quantization_method=quantization_method,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7vpslc04szHW"
   },
   "source": [
    "注意：在部署成功后，模型权重将会即时下载。因此在上述模型部署步骤成功之后，以及在您运行下面的下一步之前，需要额外10~40分钟的等待时间（取决于模型大小）。否则，当您向端点发送请求时，可能会看到`ServiceUnavailable: 503 502: Bad Gateway`错误。\n",
    "\n",
    "一旦部署成功，您就可以使用文本提示向端点发送请求。\n",
    "\n",
    "示例：\n",
    "\n",
    "```\n",
    "人类：什么是汽车？\n",
    "助手：汽车，或称为机动车，是一种用于将人们或货物从一处运送到另一处的道路交通系统。这个术语还包括了各种车辆，如摩托艇、火车和飞机。汽车通常有四个轮子、一个乘客舱和一个引擎或马达。它们自19世纪初以来就存在了，并且现在已经成为最受欢迎的交通方式之一，用于日常通勤、购物和其他用途。\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HPFMjyMEs0qt"
   },
   "outputs": [],
   "source": [
    "# Loads an existing endpoint instance using the endpoint name:\n",
    "# - Using `endpoint_name = endpoint_prequantized_vllm.name` allows us to get the\n",
    "#   endpoint name of the endpoint `endpoint_prequantized_vllm` created in the cell\n",
    "#   above.\n",
    "# - Alternatively, you can set `endpoint_name = \"1234567890123456789\"` to load\n",
    "#   an existing endpoint with the ID 1234567890123456789.\n",
    "# You may uncomment the code below to load an existing endpoint.\n",
    "\n",
    "# endpoint_name = endpoint_prequantized_vllm.name\n",
    "# # endpoint_name = \"\"  # @param {type:\"string\"}\n",
    "# aip_endpoint_name = (\n",
    "#     f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
    "# )\n",
    "# endpoint_prequantized_vllm = aiplatform.Endpoint(aip_endpoint_name)\n",
    "\n",
    "\n",
    "# Overides max_length and top_k parameters during inferences.\n",
    "# If you encounter the issue like `ServiceUnavailable: 503 Took too long to respond when processing`,\n",
    "# you can reduce the max length, such as set max_length as 20.\n",
    "# If you are using L4 GPUs to serve Falcon Instruct 40B models, you should set\n",
    "# max_length to around 1,000 tokens or fewer. If you need longer generated\n",
    "# sequences, please file a request with Vertex to allowlist your project for a\n",
    "# longer timeout threshold with Vertex endpoints.\n",
    "instances = [\n",
    "    {\n",
    "        \"prompt\": \"What is a car?\",\n",
    "        \"max_tokens\": 50,\n",
    "        \"temperature\": 1.0,\n",
    "        \"top_p\": 1.0,\n",
    "        \"top_k\": 10,\n",
    "    },\n",
    "]\n",
    "response = endpoint_prequantized_vllm.predict(instances=instances)\n",
    "\n",
    "for prediction in response.predictions:\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-0Xjw3Gxs_Lv"
   },
   "source": [
    "文本审核会分析文档与安全属性列表进行对比，其中包括“有害类别”和可能被视为敏感的主题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GR2a8rzvs_kk"
   },
   "outputs": [],
   "source": [
    "for generated_text in response.predictions:\n",
    "    # Send a request to the API.\n",
    "    response = moderate_text(generated_text)\n",
    "    # Show the results.\n",
    "    show_text_moderation(generated_text, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ApFQMgVKmUbD"
   },
   "source": [
    "### 量化法鹰指导模型\n",
    "\n",
    "量化通过减少权重的比特精度来减少为模型提供服务所需的GPU数量，同时最大限度地减少性能下降。在VLLM上为模型提供量化模型需要将模型量化为4位。建议首先搜索模型是否已经被量化并公开可用：[GPTQ](https://huggingface.co/TheBloke?search_models=-gptq)。\n",
    "\n",
    "使用GPTQ量化Falcon Instruct 7B模型需大约40分钟，使用1个NVIDIA_L4 GPU；使用4个NVIDIA_L4 GPU量化Falcon Instruct 40B模型需大约4小时。\n",
    "\n",
    "经过微调的模型也可以被量化，只要LoRA权重与基础模型合并。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wz8V8oNkxdEZ"
   },
   "source": [
    "设置基本模型ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oeyvnj8BxX8K"
   },
   "outputs": [],
   "source": [
    "model_id = \"tiiuae/falcon-7b-instruct\"  # @param [\"tiiuae/falcon-7b-instruct\", \"tiiuae/falcon-40b-instruct\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MEquWSCkmrJ6"
   },
   "outputs": [],
   "source": [
    "# Set up quantization job.\n",
    "\n",
    "# Set `finetuned_model_path` to the GCS path of the merged finetuned model\n",
    "# from the section above, if not set, the base model will be quantized.\n",
    "finetuned_model_path = \"\"  # @param {type:\"string\"}\n",
    "if finetuned_model_path:\n",
    "    prequantized_model_path = finetuned_model_path\n",
    "else:\n",
    "    prequantized_model_path = model_id\n",
    "\n",
    "quantization_method = \"gptq\"\n",
    "quantization_job_name = get_job_name_with_datetime(\n",
    "    f\"falcon-instruct-{quantization_method}-quantize\"\n",
    ")\n",
    "\n",
    "quantization_output_dir = os.path.join(MODEL_BUCKET, quantization_job_name)\n",
    "quantization_output_dir_gcsfuse = quantization_output_dir.replace(\"gs://\", \"/gcs/\")\n",
    "\n",
    "# Worker pool spec.\n",
    "\n",
    "# Set 1 L4 (24G) for quantizing 7b model.\n",
    "machine_type = \"g2-standard-32\"\n",
    "accelerator_type = \"NVIDIA_L4\"\n",
    "accelerator_count = 1\n",
    "\n",
    "# Set 4 L4 (24G) for quantizing 40b model.\n",
    "# machine_type = \"g2-standard-48\"\n",
    "# accelerator_type = \"NVIDIA_L4\"\n",
    "# accelerator_count = 4\n",
    "\n",
    "# Quantization parameters.\n",
    "quantization_precision_mode = \"4bit\"\n",
    "\n",
    "# The original datasets used in GPTQ paper.\n",
    "gptq_dataset_name = \"c4\"  # @param [\"wikitext2\",\"c4\",\"c4-new\",\"ptb\",\"ptb-new\"]\n",
    "group_size = -1\n",
    "damp_percent = 0.1\n",
    "desc_act = True\n",
    "quantization_args = [\n",
    "    \"--task=quantize-model\",\n",
    "    f\"--quantization_method={quantization_method}\",\n",
    "    f\"--pretrained_model_id={model_id}\",\n",
    "    f\"--quantization_precision_mode={quantization_precision_mode}\",\n",
    "    f\"--quantization_output_dir={quantization_output_dir_gcsfuse}\",\n",
    "    f\"--quantization_dataset_name={gptq_dataset_name}\",\n",
    "    f\"--group_size={group_size}\",\n",
    "    f\"--damp_percent={damp_percent}\",\n",
    "    f\"--desc_act={desc_act}\",\n",
    "    \"--cache_examples_on_gpu=False\",\n",
    "]\n",
    "\n",
    "# Pass quantization arguments and launch job.\n",
    "worker_pool_specs = [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": machine_type,\n",
    "            \"accelerator_type\": accelerator_type,\n",
    "            \"accelerator_count\": accelerator_count,\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"disk_spec\": {\n",
    "            \"boot_disk_type\": \"pd-ssd\",\n",
    "            \"boot_disk_size_gb\": 500,\n",
    "        },\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": TRAIN_DOCKER_URI,\n",
    "            \"env\": [\n",
    "                {\n",
    "                    \"name\": \"PYTORCH_CUDA_ALLOC_CONF\",\n",
    "                    \"value\": \"max_split_size_mb:32\",\n",
    "                },\n",
    "            ],\n",
    "            \"command\": [],\n",
    "            \"args\": quantization_args,\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Quantizing {prequantized_model_path}.\")\n",
    "quantize_job = aiplatform.CustomJob(\n",
    "    display_name=quantization_job_name,\n",
    "    project=PROJECT_ID,\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    "    staging_bucket=STAGING_BUCKET,\n",
    ")\n",
    "quantize_job.run()\n",
    "\n",
    "print(\"Quantized models were saved in: \", quantization_output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H7GIC4AYrG0E"
   },
   "source": [
    "### 使用Google Cloud Text Moderation部署量化模型\n",
    "本节将模型上传至模型注册表并部署到终端。\n",
    "\n",
    "模型部署步骤需要15分钟至1小时不等的时间才能完成，取决于模型大小。\n",
    "\n",
    "注意，部署一个量化模型需要更少的GPU。我们可以只用两个L4 GPU来部署一个量化的40B模型，而不是四个。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0SpJ_lbatIf7"
   },
   "outputs": [],
   "source": [
    "# Finds Vertex AI prediction supported accelerators and regions in\n",
    "# https://cloud.google.com/vertex-ai/docs/predictions/configure-compute.\n",
    "\n",
    "# Sets 1 L4 (24G) to deploy Falcon Instruct 7B.\n",
    "machine_type = \"g2-standard-8\"\n",
    "accelerator_type = \"NVIDIA_L4\"\n",
    "accelerator_count = 1\n",
    "\n",
    "# Sets 2 L4's (24G) to deploy Falcon Instruct 70B models.\n",
    "# machine_type = \"g2-standard-24\"\n",
    "# accelerator_type = \"NVIDIA_L4\"\n",
    "# accelerator_count = 2\n",
    "\n",
    "model_quantized_vllm, endpoint_quantized_vllm = deploy_model_vllm(\n",
    "    model_name=get_job_name_with_datetime(\n",
    "        prefix=\"falcon-instruct-serve-vllm-quantized\"\n",
    "    ),\n",
    "    model_id=quantization_output_dir,\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    machine_type=machine_type,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    "    quantization_method=quantization_method,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dzq7aM6ctPD9"
   },
   "source": [
    "注意：部署成功后，模型权重将会立即下载。因此，在上述模型部署步骤成功之后，在您运行下面的下一步之前，需要额外等待10到40分钟的时间（取决于模型大小）。否则，当您向端点发送请求时，可能会看到“ 服务不可用：503 502：网关错误”。\n",
    "\n",
    "一旦部署成功，您就可以使用文本提示向端点发送请求。\n",
    "\n",
    "例：\n",
    "```\n",
    "人类：什么是汽车？\n",
    "助手： 汽车，或者摩托车，是一种道路连接的人类交通系统，用于将人或货物从一地运送到另一地。这个术语还包括各种车辆，包括摩托艇，火车和飞机。汽车通常有四个轮子，一个供乘客的机舱，以及一个发动机或电动机。汽车自19世纪初以来一直存在，现在是最受欢迎的交通方式之一，用于日常通勤，购物和其他用途。\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "twm_awnMtQa2"
   },
   "outputs": [],
   "source": [
    "# Loads an existing endpoint instance using the endpoint name:\n",
    "# - Using `endpoint_name = endpoint_quantized_vllm.name` allows us to get the\n",
    "#   endpoint name of the endpoint `endpoint_quantized_vllm` created in the cell\n",
    "#   above.\n",
    "# - Alternatively, you can set `endpoint_name = \"1234567890123456789\"` to load\n",
    "#   an existing endpoint with the ID 1234567890123456789.\n",
    "# You may uncomment the code below to load an existing endpoint.\n",
    "\n",
    "# endpoint_name = endpoint_quantized_vllm.name\n",
    "# # endpoint_name = \"\"  # @param {type:\"string\"}\n",
    "# aip_endpoint_name = (\n",
    "#     f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
    "# )\n",
    "# endpoint_quantized_vllm = aiplatform.Endpoint(aip_endpoint_name)\n",
    "\n",
    "\n",
    "# Overides max_length and top_k parameters during inferences.\n",
    "# If you encounter the issue like `ServiceUnavailable: 503 Took too long to respond when processing`,\n",
    "# you can reduce the max length, such as set max_length as 20.\n",
    "# If you are using L4 GPUs to serve Falcon Instruct 40B models, you should set\n",
    "# max_length to around 1,000 tokens or fewer. If you need longer generated\n",
    "# sequences, please file a request with Vertex to allowlist your project for a\n",
    "# longer timeout threshold with Vertex endpoints.\n",
    "instances = [\n",
    "    {\n",
    "        \"prompt\": \"What is a car?\",\n",
    "        \"max_tokens\": 50,\n",
    "        \"temperature\": 1.0,\n",
    "        \"top_p\": 1.0,\n",
    "        \"top_k\": 10,\n",
    "    },\n",
    "]\n",
    "response = endpoint_quantized_vllm.predict(instances=instances)\n",
    "\n",
    "for prediction in response.predictions:\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AeUE4K_CteAj"
   },
   "source": [
    "文本审核会分析文档中的安全属性列表，其中包括“有害类别”和可能被视为敏感的主题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fAoVzA69teui"
   },
   "outputs": [],
   "source": [
    "for generated_text in response.predictions:\n",
    "    # Send a request to the API.\n",
    "    response = moderate_text(generated_text)\n",
    "    # Show the results.\n",
    "    show_text_moderation(generated_text, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ffffac5b20a"
   },
   "source": [
    "## 评估使用PEFT微调的Falcon Instruct模型\n",
    "\n",
    "本节展示如何使用EleutherAI的[语言模型评估工具（lm-evaluation-harness)](https://github.com/EleutherAI/lm-evaluation-harness) 和 Vertex CustomJob 评估使用PEFT LoRA微调的Falcon Instruct模型。请参考用于服务的峰值GPU内存使用情况，并相应调整机器类型、加速器类型和加速器数量。\n",
    "\n",
    "本示例使用数据集[TruthfulQA](https://arxiv.org/abs/2109.07958)。所有支持的任务均列在[任务表格](https://github.com/EleutherAI/lm-evaluation-harness/blob/master/docs/task_table.md)中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "078b4d178624"
   },
   "outputs": [],
   "source": [
    "eval_dataset = \"truthfulqa_mc\"  # @param {type:\"string\"}\n",
    "\n",
    "# Worker pool spec.\n",
    "# Find Vertex AI supported accelerators and regions in:\n",
    "#  https://cloud.google.com/vertex-ai/docs/training/configure-compute\n",
    "\n",
    "# Sets V100 (16G) to evaluate tiiuae/falcon-7b-instruct or tiiuae/falcon-40b-instruct.\n",
    "# If A100 is not available, you may evaluate tiiuae/falcon-40b-instruct with\n",
    "#  multiple V100s. Please keep in mind that the efficiency of evaluating with\n",
    "#  multiple V100s is inferior to that of evaluating with A100s.\n",
    "# Compared with L4, V100 inference can have better throughput and latency.\n",
    "machine_type = \"n1-standard-8\"\n",
    "accelerator_type = \"NVIDIA_TESLA_V100\"\n",
    "accelerator_count = 1  # for tiiuae/falcon-7b-instruct\n",
    "# accelerator_count = 8  # for tiiuae/falcon-40b-instruct\n",
    "\n",
    "# Sets L4 (24G) to evaluate tiiuae/falcon-7b-instruct or tiiuae/falcon-40b-instruct.\n",
    "# If A100 is not available, you may evaluate tiiuae/falcon-40b-instruct with\n",
    "#  multiple L4s. Please keep in mind that the efficiency of evaluating with\n",
    "#  multiple L4s is inferior to that of evaluating with A100s.\n",
    "# Compared with V100, L4 inference can be more cost efficient.\n",
    "\n",
    "# For tiiuae/falcon-7b-instruct.\n",
    "# machine_type = \"g2-standard-8\"\n",
    "# accelerator_type = \"NVIDIA_L4\"\n",
    "# accelerator_count = 1\n",
    "\n",
    "# For tiiuae/falcon-40b-instruct.\n",
    "# machine_type = \"g2-standard-48\"\n",
    "# accelerator_type = \"NVIDIA_L4\"\n",
    "# accelerator_count = 4\n",
    "\n",
    "# Sets A100 (40G) to evaluate tiiuae/falcon-7b-instruct or tiiuae/falcon-40b-instruct.\n",
    "# machine_type = \"a2-highgpu-1g\"\n",
    "# accelerator_type = \"NVIDIA_TESLA_A100\"\n",
    "# accelerator_count = 1  # for tiiuae/falcon-7b-instruct\n",
    "# accelerator_count = 4  # for tiiuae/falcon-40b-instruct\n",
    "\n",
    "# Sets A100 (80G) to evaluate falcon-40b-instruct models for faster inferences.\n",
    "# machine_type = \"a2-ultragpu-1g\"\n",
    "# accelerator_type = \"NVIDIA_TESLA_A100_80GB\"\n",
    "# accelerator_count = 2\n",
    "\n",
    "replica_count = 1\n",
    "\n",
    "# Setup evaluation job.\n",
    "job_name = get_job_name_with_datetime(prefix=\"falcon-instruct-peft-eval\")\n",
    "eval_output_dir = os.path.join(MODEL_BUCKET, job_name)\n",
    "eval_output_dir_gcsfuse = eval_output_dir.replace(\"gs://\", \"/gcs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g0t0RBixIw0P"
   },
   "outputs": [],
   "source": [
    "# Prepare evaluation command that runs the evaluation harness.\n",
    "# Set `trust_remote_code = True` because evaluating the model requires\n",
    "# executing code from the model repository.\n",
    "# Set `use_accelerate = True` to enable evaluation across multiple GPUs.\n",
    "eval_command = [\n",
    "    \"python\",\n",
    "    \"main.py\",\n",
    "    \"--model\",\n",
    "    \"hf-causal-experimental\",\n",
    "    \"--model_args\",\n",
    "    f\"pretrained={model_id},peft={output_dir_gcsfuse},trust_remote_code=True,use_accelerate=True,device_map_option=auto\",\n",
    "    \"--tasks\",\n",
    "    f\"{eval_dataset}\",\n",
    "    \"--output_path\",\n",
    "    f\"{eval_output_dir_gcsfuse}\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afd9305771ba"
   },
   "source": [
    "提交评价 CustomJob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "602413896b58"
   },
   "outputs": [],
   "source": [
    "# Pass evaluation arguments and launch job.\n",
    "worker_pool_specs = [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": machine_type,\n",
    "            \"accelerator_type\": accelerator_type,\n",
    "            \"accelerator_count\": accelerator_count,\n",
    "        },\n",
    "        \"replica_count\": replica_count,\n",
    "        \"disk_spec\": {\n",
    "            \"boot_disk_size_gb\": 500,\n",
    "        },\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": EVAL_DOCKER_URI,\n",
    "            \"command\": eval_command,\n",
    "            \"args\": [],\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "eval_job = aiplatform.CustomJob(\n",
    "    display_name=job_name,\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    "    base_output_dir=eval_output_dir,\n",
    ")\n",
    "\n",
    "eval_job.run()\n",
    "\n",
    "print(\"Evaluation results were saved in:\", eval_output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "de47e182a37e"
   },
   "source": [
    "### 获取并打印评估结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1f15ed6d375a"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "# Fetch evaluation results.\n",
    "storage_client = storage.Client()\n",
    "BUCKET_NAME = BUCKET_URI.split(\"gs://\")[1]\n",
    "bucket = storage_client.get_bucket(BUCKET_NAME)\n",
    "RESULT_FILE_PATH = eval_output_dir[len(BUCKET_URI) + 1 :]\n",
    "blob = bucket.blob(RESULT_FILE_PATH)\n",
    "raw_result = blob.download_as_string()\n",
    "\n",
    "# Print evaluation results.\n",
    "result = json.loads(raw_result)\n",
    "result_formatted = json.dumps(result, indent=2)\n",
    "print(f\"Evaluation result:\\n{result_formatted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "af21a3cff1e0"
   },
   "source": [
    "清理资源"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "911406c1561e"
   },
   "outputs": [],
   "source": [
    "# Delete custom train, quantization, and evaluation jobs.\n",
    "train_job.delete()\n",
    "quantize_job.delete()\n",
    "eval_job.delete()\n",
    "\n",
    "# Undeploy models and delete endpoints.\n",
    "endpoint_without_peft.delete(force=True)\n",
    "endpoint_with_peft.delete(force=True)\n",
    "endpoint_prequantized_vllm.delete(force=True)\n",
    "endpoint_quantized_vllm.delete(force=True)\n",
    "\n",
    "# Delete models.\n",
    "model_without_peft.delete()\n",
    "model_with_peft.delete()\n",
    "model_prequantized_vllm.delete()\n",
    "model_quantized_vllm.delete()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "model_garden_pytorch_falcon_instruct_peft.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
