{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "TirJ-SGQseby"
   },
   "source": [
    "# 顶点 AI 模型花园 MoViNet 视频动作识别\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_movinet_action_recognition.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> 在 Colab 中运行\n",
    "    </a>\n",
    "  </td>\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_movinet_action_recognition.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      查看 GitHub 上的内容\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>                                                                                               <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/notebooks/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/model_garden/model_garden_movinet_action_recognition.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "在 Vertex AI Workbench 中打开\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "dwGLvtIeECLK"
   },
   "source": [
    "**_注意_**: 这个笔记本已在以下环境中进行测试：\n",
    "\n",
    "- Python 版本 = 3.9"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## 概观\n",
    "\n",
    "本文档演示了如何在 Vertex AI 模型花园中使用 [MoViNet](https://github.com/tensorflow/models/tree/master/official/projects/movinet) 进行视频动作识别。\n",
    "\n",
    "### 目标\n",
    "\n",
    "* 训练新模型\n",
    "  * 将输入数据转换为训练格式\n",
    "  * 创建 [超参数调整作业](https://cloud.google.com/vertex-ai/docs/training/hyperparameter-tuning-overview) 来训练新模型\n",
    "  * 找到并导出最佳模型\n",
    "\n",
    "* 测试训练好的模型\n",
    "  * 将模型上传到 [Vertex AI 模型注册表](https://cloud.google.com/vertex-ai/docs/model-registry/introduction)\n",
    "  * 运行批量预测\n",
    "\n",
    "* 清理资源\n",
    "\n",
    "### 成本\n",
    "\n",
    "本教程使用 Google Cloud 的可计费组件：\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "了解有关 [Vertex AI 价格](https://cloud.google.com/vertex-ai/pricing) 和 [Cloud Storage 价格](https://cloud.google.com/storage/pricing)，并使用 [定价计算器](https://cloud.google.com/products/calculator/) 来根据您的预期使用情况生成成本估算。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "KEukV6uRk_S3"
   },
   "source": [
    "## 在你开始之前"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "z__i0w0lCAsW"
   },
   "source": [
    "仅适用于 Colab\n",
    "在 Colab 上运行以下命令，如果您使用 Workbench 则跳过此部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jvqs-ehKlaYh"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    ! pip3 install --upgrade google-cloud-aiplatform\n",
    "\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)\n",
    "\n",
    "    from google.colab import auth as google_auth\n",
    "\n",
    "    google_auth.authenticate_user()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "BF1j6f9HApxa"
   },
   "source": [
    "### 设置您的Google Cloud项目\n",
    "\n",
    "**无论您使用的是哪种笔记本环境，以下步骤都是必需的。**\n",
    "\n",
    "1. [选择或创建一个Google Cloud项目](https://console.cloud.google.com/cloud-resource-manager)。当您首次创建帐户时，您将获得$300的免费信用额度用于计算/存储成本。\n",
    "\n",
    "1. [确保为您的项目启用计费](https://cloud.google.com/billing/docs/how-to/modify-project)。\n",
    "\n",
    "1. [启用Vertex AI API和Compute Engine API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,compute_component)。\n",
    "\n",
    "1. 如果您在本地运行此笔记本，您需要安装[Cloud SDK](https://cloud.google.com/sdk)。\n",
    "\n",
    "1. 在下面的单元格中输入您的项目ID。然后运行该单元格，以确保Cloud SDK为本笔记本中的所有命令使用正确的项目。\n",
    "\n",
    "1. [创建一个服务账号](https://cloud.google.com/iam/docs/service-accounts-create#iam-service-accounts-create-console)，该账号需具备`Vertex AI User`和`Storage Object Admin`角色，用于运行对调整模型进行批量预测的操作。\n",
    "\n",
    "**注意**：Jupyter运行使用`!`前缀的行为shell命令，并将以`$`前缀的Python变量插入这些命令中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9wExiMUxFk91"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "# The GCP project ID for experiments.\n",
    "PROJECT_ID = \"\"  # @param {type:\"string\"}\n",
    "\n",
    "# Bucket URI with gs:// prefix.\n",
    "BUCKET_URI = \"\"  # @param {type:\"string\"}\n",
    "\n",
    "# You can choose a region from https://cloud.google.com/about/locations.\n",
    "# Only regions prefixed by \"us\", \"asia\", or \"europe\" are supported.\n",
    "REGION = \"us-central1\"  # @param {type:\"string\"}\n",
    "REGION_PREFIX = REGION.split(\"-\")[0]\n",
    "assert REGION_PREFIX in (\n",
    "    \"us\",\n",
    "    \"europe\",\n",
    "    \"asia\",\n",
    "), f'{REGION} is not supported. It must be prefixed by \"us\", \"asia\", or \"europe\".'\n",
    "\n",
    "! gcloud config set project $PROJECT_ID\n",
    "\n",
    "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
    "CHECKPOINT_BUCKET = os.path.join(BUCKET_URI, \"ckpt\")\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)\n",
    "\n",
    "# Download config files.\n",
    "CONFIG_DIR = os.path.join(BUCKET_URI, \"config\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "n6IFz75WGCam"
   },
   "source": [
    "### 定义常量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "riG_qUokg0XZ"
   },
   "outputs": [],
   "source": [
    "OBJECTIVE = \"var\"\n",
    "\n",
    "# Data converter constants.\n",
    "DATA_CONVERTER_JOB_PREFIX = \"data_converter\"\n",
    "DATA_CONVERTER_CONTAINER = f\"{REGION_PREFIX}-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/data-converter\"\n",
    "DATA_CONVERTER_MACHINE_TYPE = \"n1-highmem-8\"\n",
    "IMAGE_SIZES = {\n",
    "    \"a0\": 172,\n",
    "    \"a1\": 172,\n",
    "    \"a2\": 224,\n",
    "    \"a3\": 256,\n",
    "    \"a4\": 290,\n",
    "    \"a5\": 320,\n",
    "}\n",
    "\n",
    "# Training constants.\n",
    "TRAINING_JOB_PREFIX = \"train\"\n",
    "TRAIN_CONTAINER_URI = f\"{REGION_PREFIX}-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/movinet-train\"\n",
    "TRAIN_MACHINE_TYPE = \"n1-highmem-32\"\n",
    "TRAIN_ACCELERATOR_TYPE = \"NVIDIA_TESLA_V100\"\n",
    "TRAIN_NUM_GPU = 8\n",
    "\n",
    "# Evaluation constants.\n",
    "EVALUATION_METRIC = \"accuracy\"\n",
    "\n",
    "# Export constants.\n",
    "EXPORT_JOB_PREFIX = \"export\"\n",
    "EXPORT_CONTAINER_URI = f\"{REGION_PREFIX}-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/movinet-model-export\"\n",
    "EXPORT_MACHINE_TYPE = \"n1-highmem-8\"\n",
    "\n",
    "# Prediction constants.\n",
    "# You can adjust accelerator types and machine types to get faster predictions.\n",
    "PREDICTION_CONTAINER_URI = f\"{REGION_PREFIX}-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/movinet-serve\"\n",
    "PREDICTION_PORT = 8080\n",
    "PREDICTION_ACCELERATOR_COUNT = 1\n",
    "PREDICTION_ACCELERATOR_TYPE = \"NVIDIA_TESLA_T4\"\n",
    "PREDICTION_MACHINE_TYPE = \"n1-standard-4\"\n",
    "PREDICTION_JOB_PREFIX = \"predict\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZFPe_GezXg8"
   },
   "source": [
    "### 定义常见的辅助函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XcYUGwr-AJGY"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from typing import Any\n",
    "\n",
    "import tensorflow as tf\n",
    "import yaml\n",
    "\n",
    "\n",
    "def get_job_name_with_datetime(prefix: str) -> str:\n",
    "    \"\"\"Returns a timestamped job name with the given prefix.\"\"\"\n",
    "    return prefix + datetime.now().strftime(\"_%Y%m%d_%H%M%S\")\n",
    "\n",
    "\n",
    "def get_label_map(label_map_yaml_filepath: str) -> tuple[dict[int, str], int]:\n",
    "    \"\"\"Reads label map from a YAML file and returns the label map with the number of classes.\"\"\"\n",
    "    with tf.io.gfile.GFile(label_map_yaml_filepath, \"rb\") as input_file:\n",
    "        label_map = yaml.safe_load(input_file.read())[\"label_map\"]\n",
    "    num_classes = max(label_map.keys()) + 1\n",
    "    return label_map, num_classes\n",
    "\n",
    "\n",
    "def get_best_trial(\n",
    "    model_di: str, max_trial_count: int, evaluation_metric: str\n",
    ") -> tuple[str, Any]:\n",
    "    \"\"\"Finds the best trial directory and eval results from a hyperparameter tuning job.\"\"\"\n",
    "    best_trial_dir = \"\"\n",
    "    best_trial_evaluation_results = {}\n",
    "    best_performance = -1\n",
    "\n",
    "    for i in range(max_trial_count):\n",
    "        current_trial = i + 1\n",
    "        current_trial_dir = os.path.join(model_dir, \"trial_\" + str(current_trial))\n",
    "        current_trial_best_ckpt_dir = os.path.join(current_trial_dir, \"best_ckpt\")\n",
    "        current_trial_best_ckpt_evaluation_filepath = os.path.join(\n",
    "            current_trial_best_ckpt_dir, \"info.json\"\n",
    "        )\n",
    "        with tf.io.gfile.GFile(current_trial_best_ckpt_evaluation_filepath, \"rb\") as f:\n",
    "            eval_metric_results = json.load(f)\n",
    "            current_performance = eval_metric_results[evaluation_metric]\n",
    "            if current_performance > best_performance:\n",
    "                best_performance = current_performance\n",
    "                best_trial_dir = current_trial_dir\n",
    "                best_trial_evaluation_results = eval_metric_results\n",
    "    return best_trial_dir, best_trial_evaluation_results\n",
    "\n",
    "\n",
    "def print_response_instance(json_str: str, label_map: dict[int, str]):\n",
    "    \"\"\"Prints summary of a prediction JSON result from the model response.\"\"\"\n",
    "    json_obj = json.loads(json_str)\n",
    "    if \"prediction\" not in json_obj:\n",
    "        print(\"Error:\", json_str)\n",
    "        return\n",
    "    instance = json_obj[\"instance\"]\n",
    "    prediction = json_obj[\"prediction\"]\n",
    "    gcs_uri = instance[\"content\"]\n",
    "    time_start = instance.get(\"timeSegmentStart\", \"0.0s\")\n",
    "    time_end = instance.get(\"timeSegmentEnd\", \"Infinity\")\n",
    "    print(f\"---------- Predict {gcs_uri}, {time_start} to {time_end}:\")\n",
    "    for predicted in prediction:\n",
    "        time = predicted[\"timeSegmentStart\"]\n",
    "        label = label_map[predicted[\"label\"]]\n",
    "        confidence = predicted[\"confidence\"]\n",
    "        print(f\"At {time}, detected {label} with {confidence} confidence.\")\n",
    "\n",
    "\n",
    "def find_checkpoint_in_dir(checkpoint_dir: str) -> str:\n",
    "    \"\"\"Finds a checkpoint path relative to the directory.\"\"\"\n",
    "    for root, dirs, files in tf.io.gfile.walk(checkpoint_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".index\"):\n",
    "                return os.path.join(root, os.path.splitext(file)[0])\n",
    "\n",
    "\n",
    "def upload_checkpoint_to_gcs(checkpoint_url: str) -> str:\n",
    "    \"\"\"Uploads a compressed .tar.gz checkpoint at the given URL to Cloud Storage.\"\"\"\n",
    "    filename = os.path.basename(checkpoint_url)\n",
    "    checkpoint_name = filename.replace(\".tar.gz\", \"\")\n",
    "    print(\"Download checkpoint from\", checkpoint_url, \"and store to\", CHECKPOINT_BUCKET)\n",
    "    ! wget $checkpoint_url -O $filename\n",
    "    ! mkdir -p $checkpoint_name\n",
    "    ! tar -xvzf $filename -C $checkpoint_name\n",
    "\n",
    "    checkpoint_path = find_checkpoint_in_dir(checkpoint_name)\n",
    "    checkpoint_path = os.path.relpath(checkpoint_path, checkpoint_name)\n",
    "\n",
    "    ! gsutil cp -r $checkpoint_name $CHECKPOINT_BUCKET/\n",
    "    checkpoint_uri = os.path.join(CHECKPOINT_BUCKET, checkpoint_name, checkpoint_path)\n",
    "    print(\"Checkpoint uploaded to\", checkpoint_uri)\n",
    "    return checkpoint_uri\n",
    "\n",
    "\n",
    "def upload_config_to_gcs(url: str) -> str:\n",
    "    \"\"\"Uploads a config file at the given URL to Cloud Storage.\"\"\"\n",
    "    filename = os.path.basename(url)\n",
    "    destination = os.path.join(CONFIG_DIR, filename)\n",
    "    print(\"Copy\", url, \"to\", destination)\n",
    "    ! wget \"$url\" -O \"$filename\"\n",
    "    ! gsutil cp \"$filename\" \"$destination\"\n",
    "    return destination"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "RB_xY9ipr7ZU"
   },
   "source": [
    "## 训练新模型\n",
    "本节展示如何训练新模型。\n",
    "1. 将输入数据转换为训练格式\n",
    "2. 创建超参数调整作业以训练新模型\n",
    "3. 找到并导出最佳模型\n",
    "\n",
    "如果您已经训练过模型，请转至`测试训练过的模型`部分。\n",
    "\n",
    "请选择一个模型：\n",
    "- `model_id`：MoViNet模型变体ID，可以选择`a0`、`a1`、`a2`、`a3`、`a4`、`a5`中的一个。数字较大的模型需要更多资源来训练，并且预计具有更高的准确性和延迟。这里我们选择`a3`作为演示用途。**目前不建议使用`a0`、`a1`和`a2`，因为我们正在调查一些与它们的推理问题。**\n",
    "- `model_mode`：MoViNet模型类型，可以是`base`或`stream`。基础模型具有略高的准确性，而流模型则针对流媒体和更快的CPU推理进行了优化。有关更多信息，请参阅[官方MoViNet文档](https://github.com/tensorflow/models/tree/master/official/projects/movinet)。\n",
    "\n",
    "**注意**：目前预测容器仅支持基础模型（非流式）。如果您训练了一个流式模型，您需要下载模型，并参考[MoViNet官方指南](https://github.com/tensorflow/models/blob/master/official/projects/movinet/movinet_streaming_model_training_and_inference.ipynb)以在本地运行预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Ry1mw6AHLTy"
   },
   "outputs": [],
   "source": [
    "model_id = \"a3\"  # @param [\"a0\", \"a1\", \"a2\", \"a3\", \"a4\", \"a5\"]\n",
    "model_mode = \"base\"  # @param [\"base\", \"stream\"]\n",
    "is_stream = model_mode == \"stream\"\n",
    "model_name = f\"movinet_{model_id}_{model_mode}\"\n",
    "image_size = IMAGE_SIZES[model_id]\n",
    "\n",
    "if is_stream:\n",
    "    export_container_args = {\n",
    "        \"conv_type\": \"2plus1d\",\n",
    "        \"se_type\": \"2plus3d\",\n",
    "        \"activation\": \"hard_swish\",\n",
    "        \"gating_activation\": \"hard_sigmoid\",\n",
    "        \"use_positional_encoding\": model_id in {\"a3\", \"a4\", \"a5\"},\n",
    "    }\n",
    "else:\n",
    "    export_container_args = {\n",
    "        \"conv_type\": \"3d\",\n",
    "        \"se_type\": \"3d\",\n",
    "        \"activation\": \"swish\",\n",
    "        \"gating_activation\": \"sigmoid\",\n",
    "        \"use_positional_encoding\": False,\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "### 为训练准备输入数据\n",
    "\n",
    "按照[此处](https://cloud.google.com/vertex-ai/docs/video-data/action-recognition/prepare-data)描述的格式准备数据，然后通过运行下面的单元格将其转换为训练格式：\n",
    "\n",
    "* `input_file_path`：准备数据的输入文件路径。\n",
    "* `input_file_type`：输入文件类型，如`csv`或`jsonl`。\n",
    "* `output_fps`：视频的采样率；每秒帧数。\n",
    "* `num_frames`：围绕关键帧输入进行采样的帧数。\n",
    "* `min_duration_sec`：围绕关键帧输入采样视频片段的最小持续时间（以秒为单位）。这是为了验证目的-如果关键帧周围没有足够的上下文，将抛出错误。\n",
    "* `pos_neg_ratio`：正负段之间的采样比例。例如，pos_neg_ratio为0.5表示每2个正实例采样1个负实例。\n",
    "* `split_ratio`：三个逗号分隔的浮点数，表示要分割为训练/验证/测试集的数据比例。它们必须加起来等于1。\n",
    "* `num_shard`：三个逗号分隔的整数，表示用于训练/验证/测试的分片数。\n",
    "\n",
    "**注意**：对于JSONL输入，请在`dataItemResourceLabels`中使用`aiplatform.googleapis.com/ml_use`而不是`ml_use`作为ML用途的JSON键。这是为了与其他目标保持一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IndQ_m6ddUEM"
   },
   "outputs": [],
   "source": [
    "# This job will convert input data as training format, with given split ratios\n",
    "# and number of shards on train/test/validation.\n",
    "\n",
    "data_converter_job_name = get_job_name_with_datetime(\n",
    "    DATA_CONVERTER_JOB_PREFIX + \"_\" + OBJECTIVE\n",
    ")\n",
    "\n",
    "input_file_path = \"\"  # @param {type:\"string\"}\n",
    "input_file_type = \"csv\"  # @param [\"csv\", \"jsonl\"]\n",
    "output_fps = 10  # @param {type:\"integer\"}\n",
    "num_frames = 32  # @param {type:\"integer\"}\n",
    "min_duration_sec = 1.0  # @param {type:\"number\"}\n",
    "pos_neg_ratio = 1.0  # @param {type:\"number\"}\n",
    "split_ratio = \"0.8,0.1,0.1\"\n",
    "num_shard = \"10,10,10\"\n",
    "data_converter_output_dir = os.path.join(BUCKET_URI, data_converter_job_name)\n",
    "\n",
    "worker_pool_specs = [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": DATA_CONVERTER_MACHINE_TYPE,\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": DATA_CONVERTER_CONTAINER,\n",
    "            \"command\": [],\n",
    "            \"args\": [\n",
    "                \"--input_file_path=%s\" % input_file_path,\n",
    "                \"--input_file_type=%s\" % input_file_type,\n",
    "                \"--objective=%s\" % OBJECTIVE,\n",
    "                \"--num_shard=%s\" % num_shard,\n",
    "                \"--split_ratio=%s\" % split_ratio,\n",
    "                \"--output_dir=%s\" % data_converter_output_dir,\n",
    "                \"--output_fps=%d\" % output_fps,\n",
    "                \"--num_frames=%d\" % num_frames,\n",
    "                \"--min_duration_sec=%f\" % min_duration_sec,\n",
    "                \"--pos_neg_ratio=%f\" % pos_neg_ratio,\n",
    "                \"--output_shape=%d,%d\" % (image_size, image_size),\n",
    "            ],\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "data_converter_custom_job = aiplatform.CustomJob(\n",
    "    display_name=data_converter_job_name,\n",
    "    project=PROJECT_ID,\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    "    staging_bucket=STAGING_BUCKET,\n",
    ")\n",
    "\n",
    "data_converter_custom_job.run()\n",
    "\n",
    "input_train_data_path = os.path.join(data_converter_output_dir, \"train.tfrecord*\")\n",
    "input_validation_data_path = os.path.join(data_converter_output_dir, \"val.tfrecord*\")\n",
    "label_map_path = os.path.join(data_converter_output_dir, \"label_map.yaml\")\n",
    "print(\"input_train_data_path for training: \", input_train_data_path)\n",
    "print(\"input_validation_data_path for training: \", input_validation_data_path)\n",
    "print(\"label_map_path for prediction: \", label_map_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "aaff6f5be7f6"
   },
   "source": [
    "使用Vertex AI SDK来创建和运行具有Vertex AI Model Garden训练Docker镜像的超参数调整作业。\n",
    "\n",
    "#### 定义以下规格\n",
    "\n",
    "* `worker_pool_specs`：指定机器类型和Docker镜像的字典列表。此示例定义了一个单节点集群，其中有一个`n1-highmem-32`机器，配备8个`NVIDIA_TESLA_V100` GPU。\n",
    "\n",
    "  **注意**：我们建议对MoViNet-A2和更大的模型使用8个GPU。由于加载视频数据需要大量的GPU内存，建议先尝试小批量的大小进行实验。\n",
    "* `parameter_spec`：指定要优化的参数的字典。字典键是分配给训练应用程序代码中每个超参数的命令行参数的字符串，字典值是参数规范。参数规范包括超参数的类型、最小/最大值和规模。\n",
    "* `metric_spec`：指定要优化的度量的字典。字典键是您在训练应用程序代码中设置的`hyperparameter_metric_tag`，值是优化目标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "um_XKbmpTaHx"
   },
   "outputs": [],
   "source": [
    "from google.cloud.aiplatform import hyperparameter_tuning as hpt\n",
    "\n",
    "# Input train and validation datasets can be found from the section above\n",
    "# `Prepare input data for training`.\n",
    "# Or, set prepared datasets paths if already exist.\n",
    "# input_train_data_path = \"\"\n",
    "# input_validation_data_path = \"\"\n",
    "# label_map_path = \"\"\n",
    "\n",
    "train_job_name = get_job_name_with_datetime(f\"{TRAINING_JOB_PREFIX}_{model_name}\")\n",
    "model_dir = os.path.join(BUCKET_URI, train_job_name)\n",
    "label_map, num_classes = get_label_map(label_map_path)\n",
    "\n",
    "# Uploads pretained checkpoint to GCS bucket.\n",
    "init_checkpoint = f\"https://storage.googleapis.com/tf_model_garden/vision/movinet/{model_name}_with_backbone.tar.gz\"\n",
    "init_checkpoint = upload_checkpoint_to_gcs(init_checkpoint)\n",
    "\n",
    "# Uploads config file according to model_id and streaming options.\n",
    "config_file = f\"{model_id}_stream\" if is_stream else model_id\n",
    "config_file = f\"https://raw.githubusercontent.com/tensorflow/models/master/official/projects/movinet/configs/yaml/movinet_{config_file}_gpu.yaml\"\n",
    "config_file = upload_config_to_gcs(config_file)\n",
    "\n",
    "# The parameters here are mainly for demonstration purpose. Please update them\n",
    "# for better performance.\n",
    "trainer_args = {\n",
    "    \"experiment\": \"movinet_kinetics600\",\n",
    "    \"config_file\": config_file,\n",
    "    \"input_train_data_path\": input_train_data_path,\n",
    "    \"input_validation_data_path\": input_validation_data_path,\n",
    "    \"init_checkpoint\": init_checkpoint,\n",
    "    \"model_dir\": model_dir,\n",
    "    \"num_classes\": num_classes,\n",
    "    \"global_batch_size\": 16,\n",
    "    \"prefetch_buffer_size\": 16,\n",
    "    \"shuffle_buffer_size\": 32,\n",
    "    \"train_steps\": 2000,\n",
    "}\n",
    "\n",
    "worker_pool_specs = [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": TRAIN_MACHINE_TYPE,\n",
    "            \"accelerator_type\": TRAIN_ACCELERATOR_TYPE,\n",
    "            # Each training job uses TRAIN_NUM_GPU GPUs.\n",
    "            \"accelerator_count\": TRAIN_NUM_GPU,\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": TRAIN_CONTAINER_URI,\n",
    "            \"args\": [\n",
    "                \"--mode=train_and_eval\",\n",
    "                \"--params_override=runtime.num_gpus=%d\" % TRAIN_NUM_GPU,\n",
    "            ]\n",
    "            + [\"--{}={}\".format(k, v) for k, v in trainer_args.items()],\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "metric_spec = {\"model_performance\": \"maximize\"}\n",
    "\n",
    "# These learning rates might not be optimal for your selected model type; To\n",
    "# tune learning rates, try hpt.DoubleParameterSpec with more trials.\n",
    "LEARNING_RATES = [1e-3, 3e-3]\n",
    "MAX_TRIAL_COUNT = len(LEARNING_RATES)\n",
    "parameter_spec = {\n",
    "    \"learning_rate\": hpt.DiscreteParameterSpec(values=LEARNING_RATES, scale=\"linear\"),\n",
    "}\n",
    "\n",
    "print(worker_pool_specs, metric_spec, parameter_spec)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "HwcCjwlBTQIz"
   },
   "source": [
    "#### 运行超参数调整任务\n",
    "* `max_trial_count`: 设置服务将运行的试验次数的上限。推荐的做法是开始时使用较少的试验次数，了解您选择的超参数的影响程度，然后再逐渐增加。\n",
    "* `parallel_trial_count`: 如果使用并行试验，服务会提供多个训练处理集群。在创建作业时指定的工作池规范将用于每个单独的训练集群。增加并行试验的数量会减少超参数调整任务运行的时间；但是，这可能会降低作业的总体有效性。这是因为默认调整策略使用之前试验的结果来指导后续试验中的数值分配。\n",
    "* `search_algorithm`: 可用的搜索算法包括grid、random或默认（无）。默认选项将应用贝叶斯优化来搜索可能超参数值的空间，这是推荐的算法。\n",
    "点击输出中生成的链接，查看在云控制台中的运行情况。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aec22792ee84"
   },
   "outputs": [],
   "source": [
    "train_custom_job = aiplatform.CustomJob(\n",
    "    display_name=train_job_name,\n",
    "    project=PROJECT_ID,\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    "    staging_bucket=STAGING_BUCKET,\n",
    ")\n",
    "\n",
    "train_hpt_job = aiplatform.HyperparameterTuningJob(\n",
    "    display_name=train_job_name,\n",
    "    custom_job=train_custom_job,\n",
    "    metric_spec=metric_spec,\n",
    "    parameter_spec=parameter_spec,\n",
    "    max_trial_count=MAX_TRIAL_COUNT,\n",
    "    parallel_trial_count=1,\n",
    "    project=PROJECT_ID,\n",
    "    search_algorithm=None,\n",
    ")\n",
    "\n",
    "train_hpt_job.run()\n",
    "\n",
    "print(\"model_dir is:\", model_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "vugUfJEC2HrK"
   },
   "source": [
    "### 在Tensorflow SavedModel格式中导出模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "09Rz1AYspK19"
   },
   "outputs": [],
   "source": [
    "# This job will export models from TF checkpoints to TF saved model format.\n",
    "# model_dir is from the section above.\n",
    "best_trial_dir, best_trial_evaluation_results = get_best_trial(\n",
    "    model_dir, MAX_TRIAL_COUNT, EVALUATION_METRIC\n",
    ")\n",
    "best_checkpoint_path = find_checkpoint_in_dir(f\"{best_trial_dir}/best_ckpt/\")\n",
    "print(\"best_trial_dir: \", best_trial_dir)\n",
    "print(\"best_trial_evaluation_results: \", best_trial_evaluation_results)\n",
    "print(\"best_checkpoint: \", best_checkpoint_path)\n",
    "\n",
    "container_args = {\n",
    "    \"export_path\": f\"{model_dir}/best_model\",\n",
    "    \"model_id\": model_id,\n",
    "    \"num_classes\": num_classes,\n",
    "    \"causal\": is_stream,\n",
    "    \"checkpoint_path\": best_checkpoint_path,\n",
    "    \"assert_checkpoint_objects_matched\": False,\n",
    "    **export_container_args,\n",
    "}\n",
    "\n",
    "worker_pool_specs = [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": EXPORT_MACHINE_TYPE,\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": EXPORT_CONTAINER_URI,\n",
    "            \"args\": [\"--{}={}\".format(k, v) for k, v in container_args.items()],\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "model_export_job_name = get_job_name_with_datetime(EXPORT_JOB_PREFIX + \"_\" + OBJECTIVE)\n",
    "model_export_custom_job = aiplatform.CustomJob(\n",
    "    display_name=model_export_job_name,\n",
    "    project=PROJECT_ID,\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    "    staging_bucket=STAGING_BUCKET,\n",
    ")\n",
    "\n",
    "model_export_custom_job.run()\n",
    "\n",
    "print(\"best model is saved to: \", container_args[\"export_path\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "g0BGaofgsMsy"
   },
   "source": [
    "## 测试训练好的模型\n",
    "这一部分展示了如何使用训练好的模型进行测试。\n",
    "1. 上传并部署模型到[Vertex AI模型注册表](https://cloud.google.com/vertex-ai/docs/model-registry/introduction)\n",
    "2. 运行批量预测\n",
    "\n",
    "**注意：** 预测容器仅适用于基础模型。如果您训练了一个流模型，请从导出路径下载模型，并参考[MoViNet官方指南](https://github.com/tensorflow/models/blob/master/official/projects/movinet/movinet_streaming_model_training_and_inference.ipynb)进行本地预测。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "gdlca3BOypXU"
   },
   "source": [
    "### 将模型上传至 Vertex AI 模型注册表\n",
    "\n",
    "以下单元格将训练好的模型上传至 Vertex AI 模型注册表。如果您想要在已经上传的模型上运行批量预测，请跳过此步骤。\n",
    "\n",
    "#### 可配置的环境变量\n",
    "\n",
    "* `MODEL_PATH`：MoViNet 模型在云存储中的 URI。\n",
    "* `BATCH_SIZE`：推断的批次大小。使用更大的值可以加快 GPU 预测速度。\n",
    "* `NUM_FRAMES`：使用该模型进行单次预测所需的帧数。\n",
    "* `FPS`：视频抽帧率。\n",
    "* `OVERLAP_FRAMES`：允许连续预测窗口之间的重叠帧数。设置较小的值可以加快推断速度，但准确性会降低。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NYuQowyZEtxK"
   },
   "outputs": [],
   "source": [
    "serving_env = {\n",
    "    \"MODEL_ID\": \"tfvision-movinet-var\",\n",
    "    \"MODEL_PATH\": container_args[\"export_path\"],\n",
    "    \"BATCH_SIZE\": 1,\n",
    "    \"NUM_FRAMES\": 32,\n",
    "    \"FPS\": output_fps,\n",
    "    \"OVERLAP_FRAMES\": 24,\n",
    "    \"OBJECTIVE\": OBJECTIVE,\n",
    "    \"IMAGE_WIDTH\": image_size,\n",
    "    \"IMAGE_HEIGHT\": image_size,\n",
    "    \"DEPLOY_SOURCE\": \"notebook\",\n",
    "}\n",
    "\n",
    "model = aiplatform.Model.upload(\n",
    "    display_name=model_name,\n",
    "    serving_container_image_uri=PREDICTION_CONTAINER_URI,\n",
    "    serving_container_ports=[PREDICTION_PORT],\n",
    "    serving_container_predict_route=\"/predict\",\n",
    "    serving_container_health_route=\"/ping\",\n",
    "    serving_container_environment_variables=serving_env,\n",
    ")\n",
    "\n",
    "model.wait()\n",
    "\n",
    "print(\"The uploaded model name is: \", model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2b47e629a01"
   },
   "source": [
    "或者，取消下面的单元格的注释，使用已经上传的模型。用现有模型的模型名称字符串替换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aa503a565b8f"
   },
   "outputs": [],
   "source": [
    "# model = aiplatform.Model(\"projects/123456789/locations/us-central1/models/12345678901234567890\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "9SZsKGeS3x6S"
   },
   "source": [
    "### 运行批量预测\n",
    "\n",
    "我们现在将使用经过训练的MoViNet动作识别模型通过[Vertex AI Batch Prediction](https://cloud.google.com/vertex-ai/docs/predictions/get-batch-predictions)来运行批量预测。\n",
    "\n",
    "请准备一个输入JSONL文件，每行都遵循[这个格式](https://cloud.google.com/vertex-ai/docs/video-data/action-recognition/get-predictions?hl=en#input_data_requirements)，并将其存储在一个Cloud Storage存储桶中。服务账号应具有对包含训练模型和输入数据的存储桶的读取权限。请参阅[Service accounts overview](https://cloud.google.com/iam/docs/service-account-overview)获取更多信息。\n",
    "\n",
    "[Vertex AI Batch Prediction](https://cloud.google.com/vertex-ai/docs/predictions/get-batch-predictions)的默认超时时间为10分钟。因此，请确保输入视频片段大约在5分钟长，帧率在5~10之间，否则可能会遇到超时错误。要在此笔记本演示之外更大规模地使用此模型，您可以尝试以下方法之一：\n",
    "\n",
    "- 将serving Docker镜像拉取到虚拟机或本地机器，并直接发送预测请求。\n",
    "- 为了同时处理更多的数据，请编写一个自定义[DataFlow](https://cloud.google.com/dataflow)管道，将预测请求发送到movinet服务容器。\n",
    "- 将视频分割成5分钟片段，使用较小的批量大小运行批量预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vbIW9me1F2RY"
   },
   "outputs": [],
   "source": [
    "# Path to the prediction input JSONL file.\n",
    "test_jsonl_path = \"\"  # @param {type:\"string\"}\n",
    "# Full service account name with the suffix `gserviceaccount.com`.\n",
    "batch_predict_service_account = \"\"  # @param {type:\"string\"}\n",
    "\n",
    "predict_job_name = get_job_name_with_datetime(f\"{PREDICTION_JOB_PREFIX}_{model_name}\")\n",
    "predict_destination_prefix = os.path.join(STAGING_BUCKET, predict_job_name)\n",
    "\n",
    "batch_prediction_job = model.batch_predict(\n",
    "    job_display_name=predict_job_name,\n",
    "    gcs_source=test_jsonl_path,\n",
    "    gcs_destination_prefix=predict_destination_prefix,\n",
    "    machine_type=PREDICTION_MACHINE_TYPE,\n",
    "    accelerator_count=PREDICTION_ACCELERATOR_COUNT,\n",
    "    accelerator_type=PREDICTION_ACCELERATOR_TYPE,\n",
    "    max_replica_count=1,\n",
    "    service_account=batch_predict_service_account,\n",
    ")\n",
    "\n",
    "batch_prediction_job.wait()\n",
    "\n",
    "print(batch_prediction_job.display_name)\n",
    "print(batch_prediction_job.resource_name)\n",
    "print(batch_prediction_job.state)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ik-XPjfx9OCE"
   },
   "source": [
    "您可以在输出目录中阅读预测响应的JSONL文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tdkW9e5B9OU1"
   },
   "outputs": [],
   "source": [
    "# The label map file was generated from the section above (`Prepare input data for training`).\n",
    "for file in tf.io.gfile.glob(os.path.join(predict_destination_prefix, \"*/*\")):\n",
    "    with tf.io.gfile.GFile(file, \"r\") as f:\n",
    "        for line in f:\n",
    "            print_response_instance(line, label_map)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "kkH2nrpdp4sp"
   },
   "source": [
    "清理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ax6vQVZhp9pR"
   },
   "outputs": [],
   "source": [
    "# Delete the trained model.\n",
    "model.delete()\n",
    "# Delete custom and hpt jobs.\n",
    "if data_converter_custom_job.list(filter=f'display_name=\"{data_converter_job_name}\"'):\n",
    "    data_converter_custom_job.delete()\n",
    "if train_hpt_job.list(filter=f'display_name=\"{train_job_name}\"'):\n",
    "    train_hpt_job.delete()\n",
    "if model_export_custom_job.list(filter=f'display_name=\"{model_export_job_name}\"'):\n",
    "    model_export_custom_job.delete()\n",
    "if batch_prediction_job.list(filter=f'display_name=\"{predict_job_name}\"'):\n",
    "    batch_prediction_job.delete()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "model_garden_movinet_action_recognition.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
