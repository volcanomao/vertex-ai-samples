{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "使用KerasNLP微调Gemma并部署到Vertex AI\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fcommunity%2Fmodel_garden%2Fmodel_garden_gemma_kerasnlp_to_vertexai.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> 在Colab Enterprise中运行\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_gemma_kerasnlp_to_vertexai.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg\" alt=\"GitHub logo\"><br> 在GitHub上查看\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iYQE9Mza89tj"
   },
   "source": [
    "这本笔记本是在以下环境中测试的：\n",
    "- Python 3.10\n",
    "- 使用`g2-standard-8`运行时的Colab Enterprise:\n",
    "  - 32GB系统内存\n",
    "  - 24GB GPU内存（NVIDIA L4）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eIVltG2O2qgF"
   },
   "source": [
    "##概览"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "Gemma是一系列轻量级、先进的开放模型家族，使用了与Gemini模型相同的研究和技术来创建。\n",
    "\n",
    "这本笔记本演示了加载、微调、转换和部署Gemma到Vertex AI。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d975e698c9a4"
   },
   "source": [
    "### 目标\n",
    "\n",
    "- 使用KerasNLP加载Gemma\n",
    "- 使用KerasNLP微调Gemma\n",
    "- 将Gemma转换为Hugging Face Transformers\n",
    "- 将Gemma部署到Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aed92deeb4a0"
   },
   "source": [
    "成本\n",
    "\n",
    "本教程使用Google Cloud的计费组件：\n",
    "\n",
    "- Vertex AI\n",
    "- 云存储\n",
    "\n",
    "了解关于[Vertex AI](https://cloud.google.com/vertex-ai/pricing)和[Cloud Storage](https://cloud.google.com/storage/pricing)的定价，\n",
    "并使用[Pricing Calculator](https://cloud.google.com/products/calculator/)来根据您的预期使用量生成成本估算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OrDeaJ_3vL0-"
   },
   "source": [
    "安装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "安装以下所需的软件包来执行这个笔记本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SpeKcQcw5E5T"
   },
   "outputs": [],
   "source": [
    "# Keras & KerasNLP\n",
    "# Install Keras 3 last, see https://keras.io/getting_started\n",
    "%pip install --upgrade --quiet keras-nlp\n",
    "%pip install --upgrade --quiet keras\n",
    "\n",
    "# Hugging Face Transformers\n",
    "%pip install --upgrade --quiet accelerate sentencepiece transformers\n",
    "\n",
    "# Vertex AI SDK\n",
    "%pip install --upgrade --quiet google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JaKPnjQsWiLn"
   },
   "source": [
    "在你开始之前"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9xWLhlsd09zY"
   },
   "source": [
    "### Kaggle 凭证"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2y1Zf6OcvDxU"
   },
   "source": [
    "Gemma 模型由 Kaggle 托管。要使用 Gemma，请在 Kaggle 上请求访问：\n",
    "\n",
    "- 在 [kaggle.com](https://www.kaggle.com) 登录或注册\n",
    "- 打开 [Gemma 模型卡片](https://www.kaggle.com/models/google/gemma) 并选择 _\"请求访问\"_\n",
    "- 填写同意书并接受条款和条件\n",
    "\n",
    "然后，要使用 Kaggle API，创建一个 API 令牌：\n",
    "\n",
    "- 打开 [Kaggle 设置](https://www.kaggle.com/settings)\n",
    "- 选择 _\"创建新令牌\"_\n",
    "- 将下载一个 `kaggle.json` 文件。它包含您的 Kaggle 凭据\n",
    "\n",
    "运行以下单元格并输入您的 Kaggle 凭据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TxmBScR4vjkf"
   },
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "kagglehub.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8hNHhFkjMaf"
   },
   "source": [
    "注意：如果`kagglehub.login()` 对你不起作用，另一种方法是设置`KAGGLE_USERNAME`和`KAGGLE_KEY`环境变量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_WqZXDDQqrgW"
   },
   "source": [
    "### 谷歌云设置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF1j6f9HApxa",
    "tags": []
   },
   "source": [
    "1. [选择或创建Google Cloud项目](https://console.cloud.google.com/cloud-resource-manager)。当您第一次创建帐户时，您将获得$300免费信用额度用于支付计算/存储费用。\n",
    "\n",
    "2. [确保为您的项目启用了计费](https://cloud.google.com/billing/docs/how-to/modify-project)。\n",
    "\n",
    "3. [启用Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com)。\n",
    "\n",
    "4. 如果您在本地运行此笔记本，则需要安装[Cloud SDK](https://cloud.google.com/sdk)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PRKmPKBnqjaY"
   },
   "source": [
    "Google Cloud身份验证"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBCra4QMA2wR"
   },
   "source": [
    "如果您从Colab Enterprise运行此笔记本，Cloud SDK、代码和其他库已经在使用您的Google Cloud账户运行。\n",
    "\n",
    "请检查您的活动账户："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "40e71J5Go2b7"
   },
   "outputs": [],
   "source": [
    "!gcloud config get core/account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "espvMVsIph-p"
   },
   "source": [
    "如果您的帳戶未定義，您需要進行身份驗證："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tqNbW2pmphbG"
   },
   "outputs": [],
   "source": [
    "# Authenticate the Cloud SDK with your credentials\n",
    "# !gcloud auth login\n",
    "\n",
    "# Authenticate code and libraries with your credentials\n",
    "# !gcloud auth application-default login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6TNLOa5JqmVI"
   },
   "source": [
    "### 谷歌云项目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WReHDGG5g0XY"
   },
   "source": [
    "如果您在 Colab Enterprise 中运行此笔记本，则默认项目会自动定义:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oM1iC_MfAts1"
   },
   "outputs": [],
   "source": [
    "res = !gcloud config get core/project\n",
    "PROJECT_ID = res[0]\n",
    "\n",
    "print(f\"{PROJECT_ID=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1onIbGdaclHa"
   },
   "source": [
    "否则，列出您的项目并手动定义默认项目："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O6ymkb2dZjFN"
   },
   "outputs": [],
   "source": [
    "# List your projects\n",
    "# !gcloud projects list\n",
    "\n",
    "# Define the default project\n",
    "# PROJECT_ID = \"\"  # @param {type:\"string\"}\n",
    "# !gcloud config set core/project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6CGQ1lxEr_WQ"
   },
   "source": [
    "### Vertex AI 区域"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "定义您的默认 Vertex AI 区域。请参考可用的 [Vertex AI 区域](https://cloud.google.com/vertex-ai/docs/general/locations)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kfe_T6dv5E5Y"
   },
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"  # @param {type: \"string\"}\n",
    "\n",
    "!gcloud config set ai/region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8dEmh_icsNQI"
   },
   "source": [
    "注意：此笔记本将 Gemma 模型部署到单个区域。在生产环境中，您可以部署到多个区域，以提供最佳延迟服务于您的全球用户。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ETnL6RT1sGKM"
   },
   "source": [
    "云存储桶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "创建一个存储桶（或使用现有的存储桶）来存储模型权重或数据集等工件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "# Define a bucket related to your project\n",
    "BUCKET_URI = f\"gs://gemma-{PROJECT_ID}-unique\"\n",
    "# Or use an existing one\n",
    "# BUCKET_URI = \"gs://\"  # @param {type:\"string\"}\n",
    "\n",
    "res = !gcloud storage buckets describe $BUCKET_URI --format \"value(name)\"\n",
    "if len(res) == 1 and \"ERROR\" not in res[0]:\n",
    "    print(\"✔️ The bucket exists\")\n",
    "else:\n",
    "    print(\"⚙️ Creating the bucket…\")\n",
    "    !gcloud storage buckets create $BUCKET_URI --project $PROJECT_ID --location $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4OqNY6-Xwj2W"
   },
   "source": [
    "服务账户"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vRAvXQlxubf7"
   },
   "source": [
    "将Gemma部署到Vertex AI端点时，模型服务将需要具有“存储对象管理员”和“Vertex AI用户”角色的服务账户。\n",
    "\n",
    "创建服务账户（或使用现有账户）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FuFyhcsSri-O"
   },
   "outputs": [],
   "source": [
    "# Create the service account for the Vertex AI endpoint\n",
    "SERVICE_ACCOUNT_NAME = \"gemma-vertexai\"\n",
    "SERVICE_ACCOUNT_DISPLAY_NAME = \"Gemma Vertex AI endpoint\"\n",
    "SERVICE_ACCOUNT = f\"{SERVICE_ACCOUNT_NAME}@{PROJECT_ID}.iam.gserviceaccount.com\"\n",
    "# Or use an existing one\n",
    "# SERVICE_ACCOUNT = \"\"  # @param {type:\"string\"}\n",
    "assert SERVICE_ACCOUNT.endswith(f\"@{PROJECT_ID}.iam.gserviceaccount.com\")\n",
    "\n",
    "res = !gcloud iam service-accounts describe $SERVICE_ACCOUNT --format \"value(email)\"\n",
    "if len(res) == 1 and \"ERROR\" not in res[0]:\n",
    "    print(\"✔️ The service account exists\")\n",
    "else:\n",
    "    print(\"⚙️ Creating the service account…\")\n",
    "    !gcloud iam service-accounts create $SERVICE_ACCOUNT_NAME --display-name \"$SERVICE_ACCOUNT_DISPLAY_NAME\"\n",
    "    # Grant \"Storage Object Admin\" role\n",
    "    !gcloud projects add-iam-policy-binding $PROJECT_ID --member \"serviceAccount:$SERVICE_ACCOUNT\" --role \"roles/storage.objectAdmin\"\n",
    "    # Grant \"Vertex AI User\" role\n",
    "    !gcloud projects add-iam-policy-binding $PROJECT_ID --member \"serviceAccount:$SERVICE_ACCOUNT\" --role \"roles/aiplatform.user\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQDEhMB_3kbE"
   },
   "source": [
    "### 依赖关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DVQhjWte3s7M"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import locale\n",
    "\n",
    "import keras\n",
    "import keras_nlp\n",
    "import torch\n",
    "import transformers\n",
    "from google.cloud import aiplatform\n",
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zd9-P50rzNNW"
   },
   "source": [
    "### 模型常量\n",
    "\n",
    "Gemma 模型有多种大小和变体可供选择。本笔记本使用 `gemma_2b_en` 版本，该版本的资源需求较低。要了解有关 Gemma 的更多信息，请参阅[Gemma 模型花园卡片](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/gemma)。\n",
    "\n",
    "定义模型和相关常量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M6xCNXc8ROnU"
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"gemma_2b_en\"\n",
    "# MODEL_NAME = \"gemma_instruct_2b_en\"\n",
    "# MODEL_NAME = \"gemma_7b_en\"\n",
    "# MODEL_NAME = \"gemma_instruct_7b_en\"\n",
    "\n",
    "# Deduce model size from name format: \"gemma[_instruct]_{2b,7b}_en\"\n",
    "MODEL_SIZE = MODEL_NAME.split(\"_\")[-2]\n",
    "assert MODEL_SIZE in (\"2b\", \"7b\")\n",
    "\n",
    "# Dataset\n",
    "DATASET_NAME = \"databricks-dolly-15k\"\n",
    "DATASET_PATH = f\"{DATASET_NAME}.jsonl\"\n",
    "DATASET_URL = f\"https://huggingface.co/datasets/databricks/{DATASET_NAME}/resolve/main/{DATASET_PATH}\"\n",
    "\n",
    "# Finetuned model\n",
    "FINETUNED_MODEL_DIR = f\"./{MODEL_NAME}_{DATASET_NAME}\"\n",
    "FINETUNED_WEIGHTS_PATH = f\"{FINETUNED_MODEL_DIR}/model.weights.h5\"\n",
    "FINETUNED_VOCAB_PATH = f\"{FINETUNED_MODEL_DIR}/vocabulary.spm\"\n",
    "\n",
    "# Converted model\n",
    "HUGGINGFACE_MODEL_DIR = f\"./{MODEL_NAME}_huggingface\"\n",
    "\n",
    "# Deployed model\n",
    "DEPLOYED_MODEL_URI = f\"{BUCKET_URI}/{MODEL_NAME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OaIE0BoZyzgE"
   },
   "source": [
    "数据集\n",
    "\n",
    "为了对Gemma进行微调，此笔记本使用[databricks-dolly-15k](https://huggingface.co/datasets/databricks/databricks-dolly-15k) 测试数据集。\n",
    "\n",
    "下载数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iJSUhANIyqf0"
   },
   "outputs": [],
   "source": [
    "!wget -nv -nc -O $DATASET_PATH $DATASET_URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BuOc318GVlB9"
   },
   "source": [
    "加载Gemma\n",
    "\n",
    "在这一步中，您将配置Keras的精度设置并加载Gemma与KerasNLP。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k54KOGzSNrGz"
   },
   "source": [
    "### Keras 精度设置\n",
    "\n",
    "当在NVIDIA GPU上训练时，可以使用混合精度 (`keras.mixed_precision.set_global_policy(\"mixed_bfloat16\")`) 来加快训练速度而对训练质量影响最小。在大多数情况下，建议打开混合精度，因为它既节省内存又节省时间。然而，请注意，在小批量大小时，它可能会使内存使用量增加1.5倍（权重将以半精确度和全精度加载两次）。\n",
    "\n",
    "对于推理，半精度 (`keras.config.set_floatx(\"bfloat16\")`) 将有效并节省内存（而混合精度则不适用）。\n",
    "\n",
    "配置您的精度设置："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JodwwVgROfuZ"
   },
   "outputs": [],
   "source": [
    "# Run inferences at half precision\n",
    "keras.config.set_floatx(\"bfloat16\")\n",
    "\n",
    "# Train at mixed precision (enable for large batch sizes)\n",
    "# keras.mixed_precision.set_global_policy(\"mixed_bfloat16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lj7bzTamp7_f"
   },
   "source": [
    "### 模型概要\n",
    "\n",
    "使用`GemmaCausalLM.from_preset()`方法加载Gemma模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EzHhGJgG5E5T",
    "outputId": "06b4403f-bfdb-4430-8baa-7a525a4bbf64"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/models/keras/gemma/keras/gemma_2b_en/1/download/config.json...\n",
      "100%|██████████| 555/555 [00:00<00:00, 634kB/s]\n",
      "Downloading from https://www.kaggle.com/api/v1/models/keras/gemma/keras/gemma_2b_en/1/download/model.weights.h5...\n",
      "100%|██████████| 4.67G/4.67G [02:28<00:00, 33.7MB/s]\n",
      "Downloading from https://www.kaggle.com/api/v1/models/keras/gemma/keras/gemma_2b_en/1/download/tokenizer.json...\n",
      "100%|██████████| 401/401 [00:00<00:00, 554kB/s]\n",
      "Downloading from https://www.kaggle.com/api/v1/models/keras/gemma/keras/gemma_2b_en/1/download/assets/tokenizer/vocabulary.spm...\n",
      "100%|██████████| 4.04M/4.04M [00:00<00:00, 5.27MB/s]\n"
     ]
    }
   ],
   "source": [
    "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GbXgDqKZ5I8_"
   },
   "source": [
    "显示模型总结:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 385
    },
    "id": "kc77gyOR4xNh",
    "outputId": "8c132d49-feb3-473c-e850-97ebc74e6a71"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   │                                             \u001b[38;5;34m256,000\u001b[0m │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        │   \u001b[38;5;34m2,506,172,416\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m524,288,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (4.67 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (4.67 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (4.67 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (4.67 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gemma_lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LicAd7yjyRiQ"
   },
   "source": [
    "### 测试案例\n",
    "\n",
    "在微调模型之前和之后定义测试案例和函数，用于测试模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LybWJAJeylhB"
   },
   "outputs": [],
   "source": [
    "TEST_EXAMPLES = [\n",
    "    \"What are good activities for a toddler?\",\n",
    "    \"What can we hope to see after rain and sun?\",\n",
    "    \"What's the most famous painting by Monet?\",\n",
    "    \"Who engineered the Statue of Liberty?\",\n",
    "    'Who were \"The Lumières\"?',\n",
    "]\n",
    "\n",
    "# Prompt template for the training data and the finetuning tests\n",
    "PROMPT_TEMPLATE = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n",
    "\n",
    "TEST_PROMPTS = [\n",
    "    PROMPT_TEMPLATE.format(instruction=example, response=\"\")\n",
    "    for example in TEST_EXAMPLES\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hm8bHp160oh6"
   },
   "source": [
    "### 采样器\n",
    "\n",
    "您可以通过调用`compile()`方法并使用`sampler`参数来控制`GemmaCausalLM`生成令牌的方式。\n",
    "\n",
    "例如：\n",
    "\n",
    "- `greedy`: 选择具有最大概率的下一个令牌\n",
    "- `top_k`: 从前K个概率最高的令牌中随机选择下一个令牌\n",
    "\n",
    "为了在此笔记本中获得确定性的输出，请确保您使用`greedy`采样器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Gi17ZmD0-WY"
   },
   "outputs": [],
   "source": [
    "gemma_lm.compile(sampler=\"greedy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9m5EAkkAsbK5"
   },
   "source": [
    "要了解更多有关可用取样器的信息，请查看[取样器](https://keras.io/api/keras_nlp/samplers)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Akqc9HoM8e9X"
   },
   "source": [
    "### 微调之前的推断\n",
    "\n",
    "检查模型对测试示例的响应情况："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ue-LRnJ_9Mv1",
    "outputId": "e547de93-caec-4b07-8706-ec2385cd7abf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are good activities for a toddler?\n",
      "'\\n\\nWhat are the best activities for a toddler?\\n\\nWhat are the best activities for a toddler?\\n\\nWhat are the best activities for a toddler?\\n\\nWhat are the best activities for a toddler'\n",
      "\n",
      "What can we hope to see after rain and sun?\n",
      "'\\n\\nThe answer is: a lot.\\n\\nThe rain and sun are the two most important elements in the world of photography.\\n\\nThe rain is the most important element because it creates'\n",
      "\n",
      "What's the most famous painting by Monet?\n",
      "\"\\n\\nWhat's the most famous painting by Van Gogh?\\n\\nWhat's the most famous painting by Picasso?\\n\\nWhat's the most famous painting by Dali?\\n\\nWhat'\"\n",
      "\n",
      "Who engineered the Statue of Liberty?\n",
      "'\\n\\nA. George Washington\\nB. Napoleon Bonaparte\\nC. Robert Fulton\\nD. Gustave Eiffel\\n\\nIn the following sentence, underline the correct modifier from the pair given in parentheses. Example 1'\n",
      "\n",
      "Who were \"The Lumières\"?\n",
      "' What did they invent?\\n\\nIn the following sentence, underline the correct modifier from the pair given in parentheses. Example 1. He is (real, $\\\\underline{\\\\text{really}}$) talented'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for test_example in TEST_EXAMPLES:\n",
    "    response = gemma_lm.generate(test_example, max_length=48)\n",
    "    output = response[len(test_example) :]\n",
    "    print(f\"{test_example}\\n{output!r}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1zkSfGlQ6ftN"
   },
   "source": [
    "一个预训练模型可能会生成与您期望的输出偏离的文本。以下是一些例子：\n",
    "\n",
    "- 输出不符合您的输出要求。\n",
    "- 输出过于普遍或不够一致。\n",
    "- 输出事实上不正确或已过时。\n",
    "- 输出必须符合您特定的安全政策。\n",
    "\n",
    "更具体的输入（提示工程）可以解决其中一些问题，但会在复杂性和提示长度上付出代价。如果期望的输出不在模型训练数据中，LLMs 仍会生成可信的文本，并产生所谓的幻觉。\n",
    "\n",
    "您可以进行模型微调以提高模型性能并保持更简单的提示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oVJ9b5tWOlyn"
   },
   "source": [
    "细调您的Gemma模型，以提高其在回答问题方面的性能，使其更加一贯和准确。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tk7LUHcCqj1N"
   },
   "source": [
    "### 训练数据\n",
    "\n",
    "使用数据集生成训练示例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q9YLaaPpvODd",
    "outputId": "e7a3c2c1-f946-432f-b417-b975dac798cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples: 1054/10544\n"
     ]
    }
   ],
   "source": [
    "def generate_training_data(training_ratio: int = 100) -> list[str]:\n",
    "    assert 0 < training_ratio <= 100\n",
    "    data = []\n",
    "    with open(DATASET_PATH) as file:\n",
    "        for line in file.readlines():\n",
    "            features = json.loads(line)\n",
    "            # Skip examples with context, for simplicity\n",
    "            if features[\"context\"]:\n",
    "                continue\n",
    "            data.append(PROMPT_TEMPLATE.format(**features))\n",
    "    total_data_count = len(data)\n",
    "    training_data_count = total_data_count * training_ratio // 100\n",
    "    print(f\"Training examples: {training_data_count}/{total_data_count}\")\n",
    "\n",
    "    return data[:training_data_count]\n",
    "\n",
    "\n",
    "# Limit to 10% for test purposes\n",
    "training_data = generate_training_data(training_ratio=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qm89dWZU52q0"
   },
   "source": [
    "### 低秩适应（LoRA）\n",
    "\n",
    "[低秩适应](https://arxiv.org/abs/2106.09685)（LoRA）是一种微调技术，通过冻结模型的所有权重并在模型中插入少量新的可训练权重，极大地减少了下游任务的可训练参数数量。这种技术使训练速度更快，更节省内存。\n",
    "\n",
    "启用LoRA，将LoRA秩设置为4："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iN-I5RhJC1fh"
   },
   "outputs": [],
   "source": [
    "gemma_lm.backbone.enable_lora(rank=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DGVreHDoDOkO"
   },
   "source": [
    "检查可训练参数的数量是否显著减少："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 385
    },
    "id": "YCcKAxodDN0S",
    "outputId": "964ab75e-d5da-4d62-b25b-0f38f109e470"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   │                                             \u001b[38;5;34m256,000\u001b[0m │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,507,536,384</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        │   \u001b[38;5;34m2,507,536,384\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m524,288,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,507,536,384</span> (4.67 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,507,536,384\u001b[0m (4.67 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,363,968</span> (2.60 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,363,968\u001b[0m (2.60 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (4.67 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (4.67 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gemma_lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8dyzB5MXfo0"
   },
   "source": [
    "可训练参数的数量从25亿减少到140万（少了1800倍），使得可以通过合理的GPU内存要求来微调模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bsw_-qIGCZnv"
   },
   "source": [
    "### 微调\n",
    "\n",
    "使用训练数据对模型进行微调。这一步可能需要几分钟时间："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mfk9X11tvPWy",
    "outputId": "cd17f18d-e3ad-422a-df6b-bb2e50cdea97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1054/1054\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 77ms/step - loss: 19.3561 - sparse_categorical_accuracy: 0.5872\n"
     ]
    }
   ],
   "source": [
    "def finetune_gemma(model: keras_nlp.models.GemmaCausalLM, data: list[str]):\n",
    "    # Reduce the input sequence length to limit memory usage\n",
    "    model.preprocessor.sequence_length = 128\n",
    "\n",
    "    # Use AdamW (a common optimizer for transformer models)\n",
    "    optimizer = keras.optimizers.AdamW(\n",
    "        learning_rate=5e-5,\n",
    "        weight_decay=0.01,\n",
    "    )\n",
    "\n",
    "    # Exclude layernorm and bias terms from decay\n",
    "    optimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n",
    "\n",
    "    model.compile(\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        optimizer=optimizer,\n",
    "        weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "        sampler=\"greedy\",\n",
    "    )\n",
    "    model.fit(data, epochs=1, batch_size=1)\n",
    "\n",
    "\n",
    "finetune_gemma(gemma_lm, training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twNI1SKd-4Sp"
   },
   "source": [
    "### 微调后的推理\n",
    "\n",
    "测试微调后的模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z4zf-UtTVXuG",
    "outputId": "b7a323d2-3863-42ec-965a-c534d4cc1157"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction:\n",
      "What are good activities for a toddler?\n",
      "\n",
      "Response:\n",
      "The best activities for a toddler are those that are fun and engaging.\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Instruction:\n",
      "What can we hope to see after rain and sun?\n",
      "\n",
      "Response:\n",
      "After rain and sun, we can see the rainbow.\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Instruction:\n",
      "What's the most famous painting by Monet?\n",
      "\n",
      "Response:\n",
      "The most famous painting by Monet is \"Impression, Sunrise\".\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Instruction:\n",
      "Who engineered the Statue of Liberty?\n",
      "\n",
      "Response:\n",
      "The Statue of Liberty was designed by a French sculptor, Frederic Auguste Bartholdi\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Instruction:\n",
      "Who were \"The Lumières\"?\n",
      "\n",
      "Response:\n",
      "The Lumières were the inventors of the first motion picture camera. They were\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n"
     ]
    }
   ],
   "source": [
    "for prompt in TEST_PROMPTS:\n",
    "    output = gemma_lm.generate(prompt, max_length=30)\n",
    "    print(f\"{output}\\n{'- '*40}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XyraRakQrz3i"
   },
   "source": [
    "你应该注意到输出现在更有结构、更一致和更具事实性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BqBUxdtO2HLs"
   },
   "source": [
    "## 将 Gemma 转换为 Hugging Face Transformers\n",
    "\n",
    "在下一步中，该模型将部署到Vertex AI，并由[vLLM](https://docs.vllm.ai)容器映像提供服务。vLLM是一个优化的LLM服务库，支持Hugging Face的[Transformers](https://huggingface.co/docs/transformers)。要被vLLM服务加载，微调的模型需要转换为Hugging Face架构。KerasNLP提供了一个转换脚本用于这个过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZLmYQmKhZql2"
   },
   "source": [
    "### 检查点\n",
    "\n",
    "保存微调过的模型资源："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4NloTkakaIRi"
   },
   "outputs": [],
   "source": [
    "# Make sure the directory exists\n",
    "%mkdir -p $FINETUNED_MODEL_DIR\n",
    "\n",
    "gemma_lm.save_weights(FINETUNED_WEIGHTS_PATH)\n",
    "\n",
    "gemma_lm.preprocessor.tokenizer.save_assets(FINETUNED_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_3124GtDAvJf"
   },
   "source": [
    "列出检查点文件:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8kJV2NStkclG",
    "outputId": "83d3bb76-4432-4c92-b34a-3da1f7705745"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.7G\t./gemma_2b_en_databricks-dolly-15k/model.weights.h5\n",
      "4.1M\t./gemma_2b_en_databricks-dolly-15k/vocabulary.spm\n",
      "4.7G\ttotal\n"
     ]
    }
   ],
   "source": [
    "!du -shc $FINETUNED_MODEL_DIR/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1D9h3CUaA5am"
   },
   "source": [
    "释放资源，确保GPU可用于下一步操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I6Uoo52i_QZV"
   },
   "outputs": [],
   "source": [
    "del gemma_lm\n",
    "\n",
    "device = cuda.get_current_device()\n",
    "cuda.select_device(device.id)\n",
    "cuda.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSaYS78aKCGM"
   },
   "source": [
    "### 模型转换\n",
    "\n",
    "运行KerasNLP转换脚本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7K4yDhZH2OnQ"
   },
   "outputs": [],
   "source": [
    "# Download the conversion script from KerasNLP tools\n",
    "!wget -nv -nc https://raw.githubusercontent.com/keras-team/keras-nlp/master/tools/gemma/export_gemma_to_hf.py\n",
    "\n",
    "# Run the conversion script\n",
    "# Note: it uses the PyTorch backend of Keras (hence the KERAS_BACKEND env variable)\n",
    "!KERAS_BACKEND=torch python export_gemma_to_hf.py \\\n",
    "    --weights_file $FINETUNED_WEIGHTS_PATH \\\n",
    "    --size $MODEL_SIZE \\\n",
    "    --vocab_path $FINETUNED_VOCAB_PATH \\\n",
    "    --output_dir $HUGGINGFACE_MODEL_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ma56cdjKH4KW"
   },
   "source": [
    "### 使用Transformers进行推理\n",
    "\n",
    "在部署转换后的模型之前，使用`transformers`库进行测试。\n",
    "\n",
    "加载模型和分词器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "bfd1144bd7ad48e6b3686c5fdf6f1563",
      "fd1dfcfd0cbf4923823736e1789b5112",
      "67a173aa66e94699a157c5f4d7449d56",
      "7413e1164e074cbea6091644a4580e3c",
      "1f3ed8c6c9294b60b13440d435d4d280",
      "6919c81e65e548e09db80694d48d6523",
      "651f75d7f7e54c959364fae5ff6d707b",
      "0a8545ddc700438d8c915fc64a694787",
      "2cea77d31a1a41b5a983693ddbe55fb9",
      "5fb6c6fc55d846d5a8ad639b91ad32ef",
      "5ac0d9be26774111b61a543037e3c7fd"
     ]
    },
    "id": "bMNFa-Tx5E5U",
    "outputId": "91ddd005-46d4-4532-d108-2772426377ba",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfd1144bd7ad48e6b3686c5fdf6f1563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = transformers.GemmaForCausalLM.from_pretrained(\n",
    "    HUGGINGFACE_MODEL_DIR,\n",
    "    local_files_only=True,\n",
    "    device_map=\"auto\",  # Library \"accelerate\" to auto-select GPU\n",
    ")\n",
    "tokenizer = transformers.GemmaTokenizer.from_pretrained(\n",
    "    HUGGINGFACE_MODEL_DIR,\n",
    "    local_files_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OuiKR6BlzbME"
   },
   "source": [
    "测试这个模型:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "24196WWF5E5U",
    "outputId": "0f041e93-4308-47fe-a41e-df22f62c555b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction:\n",
      "What are good activities for a toddler?\n",
      "\n",
      "Response:\n",
      "Toddlers are very active and curious. They love to explore and learn\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Instruction:\n",
      "What can we hope to see after rain and sun?\n",
      "\n",
      "Response:\n",
      "After rain and sun, we can see the rainbow.\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Instruction:\n",
      "What's the most famous painting by Monet?\n",
      "\n",
      "Response:\n",
      "The most famous painting by Monet is \"Impression, Sunrise\".\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Instruction:\n",
      "Who engineered the Statue of Liberty?\n",
      "\n",
      "Response:\n",
      "The Statue of Liberty was designed by a French sculptor, Frederic Auguste Bartholdi\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Instruction:\n",
      "Who were \"The Lumières\"?\n",
      "\n",
      "Response:\n",
      "The Lumières were the inventors of the first motion picture camera. They were\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n"
     ]
    }
   ],
   "source": [
    "def test_transformers_model(\n",
    "    model: transformers.GemmaForCausalLM,\n",
    "    tokenizer: transformers.GemmaTokenizer,\n",
    ") -> None:\n",
    "    for prompt in TEST_PROMPTS:\n",
    "        inputs = tokenizer([prompt], return_tensors=\"pt\").to(model.device)\n",
    "        outputs = model.generate(**inputs, max_length=30)\n",
    "\n",
    "        output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(f\"{output}\\n{'- '*40}\")\n",
    "\n",
    "\n",
    "test_transformers_model(model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RpR6uXVQ-9Gc"
   },
   "source": [
    "释放资源。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1xuCTtQp88lX"
   },
   "outputs": [],
   "source": [
    "# Release resources\n",
    "del model, tokenizer\n",
    "\n",
    "# Free GPU RAM\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Restore the default encoding (current issue with the transformers library)\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KD0HiIPE4tpI"
   },
   "source": [
    "您已准备好将您的微调模型部署到Vertex AI！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "source": [
    "## 将 Gemma 部署到 Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RtMu4m_tPQ0u"
   },
   "source": [
    "初始化Vertex AI："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tks_NPrJLrWZ"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GlREfDROQFEE"
   },
   "source": [
    "### 模型上传\n",
    "\n",
    "将模型上传到云存储桶："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7aSWCofpP_5m"
   },
   "outputs": [],
   "source": [
    "!gcloud storage rsync --recursive --verbosity error $HUGGINGFACE_MODEL_DIR $DEPLOYED_MODEL_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "647rA5KoQ4y6"
   },
   "source": [
    "检查桶内物品:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d3NSSf7TQ4Vz"
   },
   "outputs": [],
   "source": [
    "!gcloud storage du $DEPLOYED_MODEL_URI --readable-sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umrDdBZGRfTK"
   },
   "source": [
    "### 辅助函数\n",
    "\n",
    "定义辅助函数以部署模型到vLLM容器中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dZEyVeuf5E5Z",
    "tags": []
   },
   "outputs": [],
   "source": [
    "VLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20240220_0936_RC01\"\n",
    "\n",
    "\n",
    "def get_job_name_with_datetime(prefix: str) -> str:\n",
    "    suffix = datetime.datetime.now().strftime(\"_%Y%m%d_%H%M%S\")\n",
    "    return f\"{prefix}{suffix}\"\n",
    "\n",
    "\n",
    "def deploy_model_vllm(\n",
    "    model_name: str,\n",
    "    model_uri: str,\n",
    "    service_account: str,\n",
    "    machine_type: str = \"g2-standard-8\",\n",
    "    accelerator_type: str = \"NVIDIA_L4\",\n",
    "    accelerator_count: int = 1,\n",
    "    max_model_len: int = 8192,\n",
    "    dtype: str = \"bfloat16\",\n",
    ") -> tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
    "    # Upload the model to \"Model Registry\"\n",
    "    job_name = get_job_name_with_datetime(model_name)\n",
    "    vllm_args = [\n",
    "        \"--host=0.0.0.0\",\n",
    "        \"--port=7080\",\n",
    "        f\"--tensor-parallel-size={accelerator_count}\",\n",
    "        \"--swap-space=16\",\n",
    "        \"--gpu-memory-utilization=0.95\",\n",
    "        f\"--max-model-len={max_model_len}\",\n",
    "        f\"--dtype={dtype}\",\n",
    "        \"--disable-log-stats\",\n",
    "    ]\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=job_name,\n",
    "        artifact_uri=model_uri,\n",
    "        serving_container_image_uri=VLLM_DOCKER_URI,\n",
    "        serving_container_command=[\"python\", \"-m\", \"vllm.entrypoints.api_server\"],\n",
    "        serving_container_args=vllm_args,\n",
    "        serving_container_ports=[7080],\n",
    "        serving_container_predict_route=\"/generate\",\n",
    "        serving_container_health_route=\"/ping\",\n",
    "    )\n",
    "\n",
    "    # Deploy the model to an endpoint to serve \"Online predictions\"\n",
    "    endpoint = aiplatform.Endpoint.create(display_name=f\"{model_name}-endpoint\")\n",
    "    model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        machine_type=machine_type,\n",
    "        accelerator_type=accelerator_type,\n",
    "        accelerator_count=accelerator_count,\n",
    "        deploy_request_timeout=1800,\n",
    "        service_account=service_account,\n",
    "    )\n",
    "\n",
    "    return model, endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OfTTbrx2SwDf"
   },
   "source": [
    "模型部署\n",
    "\n",
    "部署模型。这一步可能需要10分钟以上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7FK3F_eu5E5Z",
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_NAME_VLLM = f\"{MODEL_NAME}-vllm\"\n",
    "\n",
    "# Start with a G2 Series cost-effective configuration\n",
    "match MODEL_SIZE:\n",
    "    case \"2b\":\n",
    "        machine_type = \"g2-standard-8\"\n",
    "        accelerator_type = \"NVIDIA_L4\"\n",
    "        accelerator_count = 1\n",
    "    case \"7b\":\n",
    "        machine_type = \"g2-standard-12\"\n",
    "        accelerator_type = \"NVIDIA_L4\"\n",
    "        accelerator_count = 1\n",
    "    case _:\n",
    "        assert MODEL_SIZE in (\"2b\", \"7b\")\n",
    "\n",
    "# See supported machine/GPU configurations in chosen region:\n",
    "# https://cloud.google.com/vertex-ai/docs/predictions/configure-compute\n",
    "\n",
    "# For even more performance, consider V100 and A100 GPUs\n",
    "# > Nvidia Tesla V100\n",
    "# machine_type = \"n1-standard-8\"\n",
    "# accelerator_type = \"NVIDIA_TESLA_V100\"\n",
    "# > Nvidia Tesla A100\n",
    "# machine_type = \"a2-highgpu-1g\"\n",
    "# accelerator_type = \"NVIDIA_TESLA_A100\"\n",
    "\n",
    "# Larger `max_model_len` values will require more GPU memory\n",
    "max_model_len = 2048\n",
    "\n",
    "model, endpoint = deploy_model_vllm(\n",
    "    MODEL_NAME_VLLM,\n",
    "    DEPLOYED_MODEL_URI,\n",
    "    SERVICE_ACCOUNT,\n",
    "    machine_type=machine_type,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    "    max_model_len=max_model_len,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eg-gVbwnTpYD"
   },
   "source": [
    "### 在线推理\n",
    "\n",
    "模型已部署！测试端点："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "assuAGh7DEUz",
    "outputId": "802c483e-b2ef-4fd7-f1ae-b930f9980e11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are good activities for a toddler?\n",
      "The best activities for a toddler are those that are\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "What can we hope to see after rain and sun?\n",
      "After rain and sun, we can see the rainbow\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "What's the most famous painting by Monet?\n",
      "The most famous painting by Monet is \"Impression,\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Who engineered the Statue of Liberty?\n",
      "The Statue of Liberty was designed by a French sculptor\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Who were \"The Lumières\"?\n",
      "The Lumières were the inventors of the first motion\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n"
     ]
    }
   ],
   "source": [
    "def test_vertexai_endpoint(endpoint: aiplatform.Endpoint):\n",
    "    for question, prompt in zip(TEST_EXAMPLES, TEST_PROMPTS):\n",
    "        instance = {\n",
    "            \"prompt\": prompt,\n",
    "            \"max_tokens\": 10,\n",
    "            \"temperature\": 0.0,\n",
    "            \"top_p\": 1.0,\n",
    "            \"top_k\": 1,\n",
    "            \"raw_response\": True,\n",
    "        }\n",
    "        response = endpoint.predict(instances=[instance])\n",
    "        output = response.predictions[0]\n",
    "        print(f\"{question}\\n{output}\\n{'- '*40}\")\n",
    "\n",
    "\n",
    "test_vertexai_endpoint(endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cH9qQRqyDrkS"
   },
   "source": [
    "请查看[vLLM `SamplingParams`](https://github.com/vllm-project/vllm/blob/main/vllm/sampling_params.py)了解vLLM支持的采样参数的更多详情。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "清理工作\n",
    "\n",
    "要清理本项目中使用的所有Google Cloud资源，您可以删除用于本教程的[Google Cloud项目](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects)。\n",
    "\n",
    "否则，您可以删除在本教程中创建的各个资源："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sx_vKniMq9ZX"
   },
   "outputs": [],
   "source": [
    "delete_model = False\n",
    "delete_objects = False\n",
    "delete_bucket = False\n",
    "\n",
    "if delete_model:\n",
    "    endpoint.delete(force=True)\n",
    "    model.delete()\n",
    "if delete_objects:\n",
    "    !gcloud storage rm --recursive $BUCKET_URI/**\n",
    "if delete_bucket:\n",
    "    !gcloud storage buckets delete $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dzESMIONd6IO"
   },
   "source": [
    "接下来做什么呢\n",
    "\n",
    "- 探索[Vertex AI模型花园](https://console.cloud.google.com/vertex-ai/model-garden)\n",
    "- 还可以看看如何使用GKE上的GPU为Gemma开放模型提供服务，使用vLLM(https://cloud.google.com/kubernetes-engine/docs/tutorials/serve-gemma-gpu-vllm)\n",
    "- 了解更多关于[KerasLP](https://keras.io/keras_nlp)\n",
    "- 了解更多关于[vLLM](https://github.com/vllm-project/vllm)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m115",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m115"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0a8545ddc700438d8c915fc64a694787": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1f3ed8c6c9294b60b13440d435d4d280": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2cea77d31a1a41b5a983693ddbe55fb9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5ac0d9be26774111b61a543037e3c7fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5fb6c6fc55d846d5a8ad639b91ad32ef": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "651f75d7f7e54c959364fae5ff6d707b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "67a173aa66e94699a157c5f4d7449d56": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0a8545ddc700438d8c915fc64a694787",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2cea77d31a1a41b5a983693ddbe55fb9",
      "value": 3
     }
    },
    "6919c81e65e548e09db80694d48d6523": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7413e1164e074cbea6091644a4580e3c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5fb6c6fc55d846d5a8ad639b91ad32ef",
      "placeholder": "​",
      "style": "IPY_MODEL_5ac0d9be26774111b61a543037e3c7fd",
      "value": " 3/3 [00:46&lt;00:00, 11.05s/it]"
     }
    },
    "bfd1144bd7ad48e6b3686c5fdf6f1563": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fd1dfcfd0cbf4923823736e1789b5112",
       "IPY_MODEL_67a173aa66e94699a157c5f4d7449d56",
       "IPY_MODEL_7413e1164e074cbea6091644a4580e3c"
      ],
      "layout": "IPY_MODEL_1f3ed8c6c9294b60b13440d435d4d280"
     }
    },
    "fd1dfcfd0cbf4923823736e1789b5112": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6919c81e65e548e09db80694d48d6523",
      "placeholder": "​",
      "style": "IPY_MODEL_651f75d7f7e54c959364fae5ff6d707b",
      "value": "Loading checkpoint shards: 100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
