{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7d9bbf86da5e"
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99c1c3fc2ca5"
   },
   "source": [
    "# Vertex AI模型花园 - PEFT\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_peft.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> 在Colab中运行\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_peft.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      在GitHub上查看\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/notebooks/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/model_garden/model_garden_pytorch_peft.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "在Vertex AI工作台中打开\n",
    "    </a>（推荐使用Python-3 CPU笔记本）\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3de7470326a2"
   },
   "source": [
    "## 概述\n",
    "\n",
    "本笔记本展示了如何使用性能高效的微调库（[PEFT](https://github.com/huggingface/peft)）微调模型，并在 Vertex AI 中使用容器进行推理。[PEFT](https://github.com/huggingface/peft)支持各种模型。本笔记本展示了一些模型的示例，如 OpenLLaMA、Falcon-instruct、BERT、RoBERTa-large 和 XLM-RoBERTa-large 等。\n",
    "\n",
    "### 目标\n",
    "\n",
    "- 使用 PEFT 对因果语言模型进行微调，并在 Vertex AI 中进行推理，支持的模型如下\n",
    "\n",
    "| 模型 | LoRA |\n",
    "| :- | :- |\n",
    "| [openlm-research/open_llama_3b](https://huggingface.co/openlm-research/open_llama_3b) | 是 |\n",
    "| [openlm-research/open_llama_7b](https://huggingface.co/openlm-research/open_llama_7b) | 是 |\n",
    "| [openlm-research/open_llama_13b](https://huggingface.co/openlm-research/open_llama_13b) | 是 |\n",
    "\n",
    "- 使用 PEFT 对 instruct 模型进行微调，并在 Vertex AI 中进行推理，支持的模型如下\n",
    "\n",
    "| 模型 | LoRA |\n",
    "| :- | :- |\n",
    "| [tiiuae/falcon-7b-instruct](https://huggingface.co/tiiuae/falcon-7b-instruct) | 是 |\n",
    "| [tiiuae/falcon-40b-instruct](https://huggingface.co/tiiuae/falcon-40b-instruct) | 是 |\n",
    "\n",
    "\n",
    "- 使用 PEFT 对序列分类模型进行微调，并在 Vertex AI 中进行推理，支持的模型如下\n",
    "\n",
    "| 模型 | LoRA |\n",
    "| :- | :- |\n",
    "| [bert-base-uncase](https://huggingface.co/bert-base-uncased) | 是 |\n",
    "| [RoBERTa-large](https://huggingface.co/roberta-large) | 是 |\n",
    "| [XLM-RoBERTa-large](https://huggingface.co/xlm-roberta-large) | 是 |\n",
    "\n",
    "### 成本\n",
    "\n",
    "本教程使用 Google Cloud 的计费组件：\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "了解 [Vertex AI 定价](https://cloud.google.com/vertex-ai/pricing) 和 [Cloud Storage 定价](https://cloud.google.com/storage/pricing)，并使用 [定价计算器](https://cloud.google.com/products/calculator/) 根据您的预期使用情况生成成本估算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "264c07757582"
   },
   "source": [
    "在开始之前\n",
    "\n",
    "**注意**: Jupyter运行以`!`为前缀的行作为shell命令，并且将以`$`为前缀的Python变量插入到这些命令中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ioensNKM8ned"
   },
   "source": [
    "只在 Colab 上运行以下命令，如果您使用的是 Workbench，则跳过此部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2707b02ef5df"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    ! pip3 install --upgrade google-cloud-aiplatform\n",
    "    from google.colab import auth as google_auth\n",
    "\n",
    "    google_auth.authenticate_user()\n",
    "    # Install gdown for downloading example training images.\n",
    "    ! pip3 install gdown\n",
    "\n",
    "    # Restart the notebook kernel after installs.\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bb7adab99e41"
   },
   "source": [
    "### 设置Google云项目\n",
    "\n",
    "1. [选择或创建一个Google云项目](https://console.cloud.google.com/cloud-resource-manager)。当您第一次创建帐户时，您将获得$300的免费信用额度，用于计算/存储成本。\n",
    "\n",
    "2. [确保您的项目已启用计费](https://cloud.google.com/billing/docs/how-to/modify-project)。\n",
    "\n",
    "3. [启用Vertex AI API和Compute Engine API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,compute_component)。\n",
    "\n",
    "4. [创建一个云存储存储桶](https://cloud.google.com/storage/docs/creating-buckets) 用于存储实验输出。\n",
    "\n",
    "5. [创建一个服务帐号](https://cloud.google.com/iam/docs/service-accounts-create#iam-service-accounts-create-console)，并授予`Vertex AI User`和`Storage Object Admin`角色，用于将经过微调的模型部署到Vertex AI端点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6c460088b873"
   },
   "source": [
    "为实验环境设置以下变量。指定的云存储桶（`BUCKET_URI`）应位于指定的区域（`REGION`）。请注意，多区域桶（例如“us”）不被认为与由多区域范围（例如“us-central1”）覆盖的单个区域匹配。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "855d6b96f291"
   },
   "outputs": [],
   "source": [
    "# Cloud project id.\n",
    "PROJECT_ID = \"\"  # @param {type:\"string\"}\n",
    "\n",
    "# The region you want to launch jobs in.\n",
    "REGION = \"\"  # @param {type:\"string\"}\n",
    "\n",
    "# The Cloud Storage bucket for storing experiments output.\n",
    "BUCKET_URI = \"\"  # @param {type:\"string\"}\n",
    "\n",
    "! gcloud config set project $PROJECT_ID\n",
    "\n",
    "import os\n",
    "\n",
    "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
    "EXPERIMENT_BUCKET = os.path.join(BUCKET_URI, \"peft\")\n",
    "DATA_BUCKET = os.path.join(EXPERIMENT_BUCKET, \"data\")\n",
    "MODEL_BUCKET = os.path.join(EXPERIMENT_BUCKET, \"model\")\n",
    "\n",
    "# The service account looks like:\n",
    "# '@.iam.gserviceaccount.com'\n",
    "# Please go to https://cloud.google.com/iam/docs/service-accounts-create#iam-service-accounts-create-console\n",
    "# and create service account with `Vertex AI User` and `Storage Object Admin` roles.\n",
    "# The service account for deploying fine tuned model.\n",
    "SERVICE_ACCOUNT = \"\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e828eb320337"
   },
   "source": [
    "### 初始化Vertex AI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "12cd25839741"
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cc825514deb"
   },
   "source": [
    "### 定义常量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b42bd4fa2b2d"
   },
   "outputs": [],
   "source": [
    "# The pre-built training and serving docker images.\n",
    "TRAIN_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-peft-train:20231020_0936_RC00\"\n",
    "PREDICTION_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-peft-serve:20231108_1540_RC00\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0c250872074f"
   },
   "source": [
    "### 定义常见功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "354da31189dc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "\n",
    "def get_job_name_with_datetime(prefix: str):\n",
    "    \"\"\"Gets the job name with date time when triggering training or deployment\n",
    "    jobs in Vertex AI.\n",
    "    \"\"\"\n",
    "    return prefix + datetime.now().strftime(\"_%Y%m%d_%H%M%S\")\n",
    "\n",
    "\n",
    "def deploy_model(\n",
    "    model_name,\n",
    "    model_id,\n",
    "    finetuned_lora_model_path,\n",
    "    service_account,\n",
    "    task,\n",
    "    machine_type=\"n1-standard-8\",\n",
    "    accelerator_type=\"NVIDIA_TESLA_V100\",\n",
    "):\n",
    "    \"\"\"Deploys trained models into Vertex AI.\"\"\"\n",
    "    endpoint = aiplatform.Endpoint.create(display_name=f\"{model_name}-endpoint\")\n",
    "    serving_env = {\n",
    "        \"MODEL_ID\": model_id,\n",
    "        \"FINETUNED_LORA_MODEL_PATH\": finetuned_lora_model_path,\n",
    "        \"TASK\": task,\n",
    "        \"DEPLOY_SOURCE\": \"notebook\",\n",
    "    }\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=model_name,\n",
    "        serving_container_image_uri=PREDICTION_DOCKER_URI,\n",
    "        serving_container_ports=[7080],\n",
    "        serving_container_predict_route=\"/predictions/peft_serving\",\n",
    "        serving_container_health_route=\"/ping\",\n",
    "        serving_container_environment_variables=serving_env,\n",
    "    )\n",
    "    model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        machine_type=machine_type,\n",
    "        accelerator_type=accelerator_type,\n",
    "        accelerator_count=1,\n",
    "        deploy_request_timeout=1800,\n",
    "        service_account=service_account,\n",
    "    )\n",
    "    return model, endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e70e3519ff8b"
   },
   "source": [
    "本节展示如何使用PEFT LoRA对OpenLLaMA进行微调，包括[openlm-research/open_llama_3b](https://huggingface.co/openlm-research/open_llama_3b)，[openlm-research/open_llama_7b](https://huggingface.co/openlm-research/open_llama_7b)，以及[openlm-research/open_llama_13b](https://huggingface.co/openlm-research/open_llama_13b)。## casual语言建模+ PEFT## casual语言建模+ PEFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iWGwJHqI7LMs"
   },
   "source": [
    "### 用LoRA进行微调"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5qCrm_kJH5cz"
   },
   "source": [
    "设置基本模型ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N3UBLiYrM3sU"
   },
   "outputs": [],
   "source": [
    "model_id = \"openlm-research/open_llama_3b\"  # @param [\"openlm-research/open_llama_3b\", \"openlm-research/open_llama_7b\", \"openlm-research/open_llama_13b\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KKEYoRfiHDVv"
   },
   "source": [
    "使用Vertex AI SDK创建和运行自定义训练作业，使用Vertex AI Model Garden的训练图像。\n",
    "\n",
    "这个例子使用数据集[Abirate/english_quotes](https://huggingface.co/datasets/Abirate/english_quotes)。您可以使用来自[huggingface的数据集](https://huggingface.co/datasets)，或者在Cloud Storage中存储的[Vertex文本模型数据集格式](https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-text-models-supervised#dataset-format)中的自定义JSONL数据集。\n",
    "\n",
    "为了使微调更加高效，我们在加载用于微调LoRA模型的预训练模型时启用了量化(8位)。微调LoRA模型所需的GPU内存峰值使用量大约为~7G、~10G和~16G，分别用于[openlm-research/open_llama_3b](https://huggingface.co/openlm-research/open_llama_3b)、[openlm-research/open_llama_7b](https://huggingface.co/openlm-research/open_llama_7b)和[openlm-research/open_llama_13b](https://huggingface.co/openlm-research/open_llama_13b)，使用默认训练参数和示例数据集。理论上，open_llama_3b和open_llama_7b可以在1个V100上进行微调，而open_llama_13b可以在1个A100(40G)上进行微调。为了简单起见，我们选择默认使用1个A100(40G)来支持这些模型在本笔记本中的使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bd995e7aa529"
   },
   "source": [
    "#### 【可选】使用自定义数据集进行微调\n",
    "\n",
    "要使用自定义数据集，您应该在下面的 `dataset_name` 提供一个指向遵循[Vertex文本模型数据集格式](https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-text-models-supervised#dataset-format)的JSONL文件的 `gs://` URI。`template` 参数是可选的。\n",
    "\n",
    "例如，这是来自示例数据集 `gs://cloud-samples-data/vertex-ai/model-evaluation/peft_train_sample.jsonl` 的一个数据点：\n",
    "\n",
    "```json\n",
    "{\"input_text\":\"TRANSCRIPT: \\nREASON FOR EVALUATION:,\\n\\n LABEL:\",\"output_text\":\"Chiropractic\"}\n",
    "```\n",
    "\n",
    "要使用包含 `input_text` 和 `output_text` 字段的这个示例数据集，将 `dataset_name` 设置为 `gs://cloud-samples-data/vertex-ai/model-evaluation/peft_train_sample.jsonl`，将 `template` 设置为 `vertex_sample`。要使用自定义数据集字段的高级用法，请参阅[模板示例](https://github.com/tloen/alpaca-lora/blob/main/templates/alpaca.json)，并提供您自己的JSON模板作为 `gs://` URI。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "65467b361315"
   },
   "outputs": [],
   "source": [
    "# Huggingface dataset name or gs:// URI to a custom JSONL dataset.\n",
    "dataset_name = \"Abirate/english_quotes\"  # @param {type:\"string\"}\n",
    "# Optional. Template name or gs:// URI to a custom template.\n",
    "template = \"\"  # @param {type:\"string\"}\n",
    "\n",
    "# Worker pool spec.\n",
    "# machine_type = \"n1-standard-8\"\n",
    "# accelerator_type = \"NVIDIA_TESLA_V100\"\n",
    "machine_type = \"a2-highgpu-1g\"\n",
    "accelerator_type = \"NVIDIA_TESLA_A100\"\n",
    "replica_count = 1\n",
    "accelerator_count = 1\n",
    "\n",
    "# Setup training job.\n",
    "job_name = get_job_name_with_datetime(\"openllama-lora-train\")\n",
    "train_job = aiplatform.CustomContainerTrainingJob(\n",
    "    display_name=job_name,\n",
    "    container_uri=TRAIN_DOCKER_URI,\n",
    ")\n",
    "output_dir = os.path.join(MODEL_BUCKET, job_name)\n",
    "output_dir_gcsfuse = output_dir.replace(\"gs://\", \"/gcs/\")\n",
    "\n",
    "# Pass training arguments and launch job.\n",
    "train_job.run(\n",
    "    args=[\n",
    "        \"--task=causal-language-modeling-lora\",\n",
    "        f\"--pretrained_model_id={model_id}\",\n",
    "        f\"--dataset_name={dataset_name}\",\n",
    "        f\"--output_dir={output_dir_gcsfuse}\",\n",
    "        \"--lora_rank=16\",\n",
    "        \"--lora_alpha=32\",\n",
    "        \"--lora_dropout=0.05\",\n",
    "        \"--warmup_steps=10\",\n",
    "        \"--max_steps=10\",\n",
    "        \"--learning_rate=2e-4\",\n",
    "        f\"--template={template}\",\n",
    "    ],\n",
    "    replica_count=replica_count,\n",
    "    machine_type=machine_type,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    "    boot_disk_size_gb=500,\n",
    ")\n",
    "\n",
    "print(\"Trained models were saved in: \", output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jqmCtkGnhDmp"
   },
   "source": [
    "使用提供图片进行推断\n",
    "\n",
    "该部分将模型上传到模型注册表并在端点上部署。\n",
    "\n",
    "模型部署步骤大约需要15分钟才能完成。\n",
    "\n",
    "使用LoRA权重的[openlm-research/open_llama_3b](https://huggingface.co/openlm-research/open_llama_3b)、[openlm-research/open_llama_7b](https://huggingface.co/openlm-research/open_llama_7b)和[openlm-research/open_llama_13b](https://huggingface.co/openlm-research/open_llama_13b)的峰值GPU内存使用情况分别为约5.3G、8.7G和15.2G，默认设置下。我们在部署中使用V100以保持简单。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bf55e38815dc"
   },
   "outputs": [],
   "source": [
    "model, endpoint = deploy_model(\n",
    "    model_name=get_job_name_with_datetime(prefix=\"openllama-peft-serve\"),\n",
    "    model_id=model_id,\n",
    "    finetuned_lora_model_path=output_dir,\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    task=\"causal-language-modeling-lora\",\n",
    ")\n",
    "print(\"endpoint_name:\", endpoint.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80b3fd2ace09"
   },
   "source": [
    "请注意：模型权重将在部署成功后下载。因此，在上述模型部署步骤成功之后，您需要额外等待5分钟，然后再运行下面的下一个步骤。否则，在发送请求到端点时可能会看到`ServiceUnavailable：503 502：Bad Gateway`错误。\n",
    "\n",
    "一旦部署成功，您可以使用文本提示向端点发送请求。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ab04da3ec9a"
   },
   "outputs": [],
   "source": [
    "# # Loads an existing endpoint as below.\n",
    "# endpoint_name = endpoint.name\n",
    "# aip_endpoint_name = (\n",
    "#     f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
    "# )\n",
    "# endpoint = aiplatform.Endpoint(aip_endpoint_name)\n",
    "instances = [\n",
    "    {\n",
    "        \"prompt\": \"Hi Google.\",\n",
    "        \"max_tokens\": 50,\n",
    "        \"temperature\": 1.0,\n",
    "        \"top_p\": 1.0,\n",
    "        \"top_k\": 10,\n",
    "    },\n",
    "]\n",
    "response = endpoint.predict(instances=instances)\n",
    "\n",
    "for prediction in response.predictions:\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "af21a3cff1e0"
   },
   "source": [
    "整理资源"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "911406c1561e"
   },
   "outputs": [],
   "source": [
    "# Delete custom train jobs.\n",
    "train_job.delete()\n",
    "\n",
    "# Undeploy model and delete endpoint.\n",
    "endpoint.delete(force=True)\n",
    "\n",
    "# Delete models.\n",
    "model.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W2USaHtQbE-l"
   },
   "source": [
    "教学 + PEFT\n",
    "\n",
    "本节展示了如何使用PEFT LoRA对猎隼指令进行微调，包括[tiiuae/falcon-7b-instruct](https://huggingface.co/tiiuae/falcon-7b-instruct)和[tiiuae/falcon-40b-instruct](https://huggingface.co/tiiuae/falcon-40b-instruct)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PwluMqvYbLu9"
   },
   "source": [
    "### 使用LoRA进行微调"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ap1pQd-ckAAb"
   },
   "source": [
    "设置基础模型ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vVTfG2bpbR90"
   },
   "outputs": [],
   "source": [
    "model_id = \"tiiuae/falcon-7b-instruct\"  # @param [\"tiiuae/falcon-7b-instruct\", \"tiiuae/falcon-40b-instruct\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iJe_us8mkZEV"
   },
   "source": [
    "使用Vertex AI SDK创建和运行自定义训练作业，并使用Vertex AI Model Garden训练图像。\n",
    "\n",
    "此示例使用数据集 [timdettmers/openassistant-guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco)。您可以使用来自 huggingface 的数据集 或者存储在 Cloud Storage 中的 Vertex 文本模型数据集格式 自定义的 JSONL 数据集。`template` 参数是可选的。\n",
    "\n",
    "对于使用默认训练参数和示例数据集分别对 [tiiuae/falcon-7b-instruct](https://huggingface.co/tiiuae/falcon-7b-instruct) 和 [tiiuae/falcon-40b-instruct](https://huggingface.co/tiiuae/falcon-40b-instruct) 进行微调 LoRA 模型时，GPU 的最大内存使用量分别为 ~11G 和 ~34G。理论上，falcon-7b-instruct 可以在 1 个 P100/V100 上进行微调，而 falcon-40b-instruct 可以在 1 个 A100（40G）上进行微调。为了简单起见，我们选择默认使用 1 个 A100（40G）来支持这些模型在本笔记本中使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4c128f63be2a"
   },
   "source": [
    "#### [可选] 使用自定义数据集进行微调\n",
    "\n",
    "要使用自定义数据集，您应该在下面的`dataset_name`中提供一个指向符合 [Vertex 文本模型数据集格式](https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-text-models-supervised#dataset-format)的 JSONL 文件的 `gs://` URI。\n",
    "\n",
    "例如，以下是来自示例数据集 `gs://cloud-samples-data/vertex-ai/model-evaluation/peft_train_sample.jsonl` 的一个数据点：\n",
    "\n",
    "```json\n",
    "{\"input_text\":\"TRANSCRIPT: \\nREASON FOR EVALUATION:,\\n\\n LABEL:\",\"output_text\":\"Chiropractic\"}\n",
    "```\n",
    "\n",
    "要使用包含 `input_text` 和 `output_text` 字段的此示例数据集，请将`dataset_name`设置为`gs://cloud-samples-data/vertex-ai/model-evaluation/peft_train_sample.jsonl`，并将`template`设置为`vertex_sample`。如需使用自定义数据集字段进行高级用法，请参阅[模板示例](https://github.com/tloen/alpaca-lora/blob/main/templates/alpaca.json)，并提供您自己的 JSON 模板作为 `gs://` URI。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-i4M7mWPbV8s"
   },
   "outputs": [],
   "source": [
    "# Huggingface dataset name or gs:// URI to a custom JSONL dataset.\n",
    "dataset_name = \"timdettmers/openassistant-guanaco\"  # @param {type:\"string\"}\n",
    "# Optional. Template name or gs:// URI to a custom template.\n",
    "template = \"\"  # @param {type:\"string\"}\n",
    "\n",
    "# Worker pool spec.\n",
    "machine_type = \"a2-highgpu-1g\"\n",
    "accelerator_type = \"NVIDIA_TESLA_A100\"\n",
    "replica_count = 1\n",
    "accelerator_count = 1\n",
    "\n",
    "# Setup training job.\n",
    "job_name = get_job_name_with_datetime(\"falcon-instruct-lora-train\")\n",
    "train_job = aiplatform.CustomContainerTrainingJob(\n",
    "    display_name=job_name,\n",
    "    container_uri=TRAIN_DOCKER_URI,\n",
    ")\n",
    "output_dir = os.path.join(MODEL_BUCKET, job_name)\n",
    "output_dir_gcsfuse = output_dir.replace(\"gs://\", \"/gcs/\")\n",
    "\n",
    "# Pass training arguments and launch job.\n",
    "max_steps = 10\n",
    "train_job.run(\n",
    "    args=[\n",
    "        \"--task=instruct-lora\",\n",
    "        f\"--pretrained_model_id={model_id}\",\n",
    "        f\"--dataset_name={dataset_name}\",\n",
    "        f\"--output_dir={output_dir_gcsfuse}\",\n",
    "        \"--lora_rank=64\",\n",
    "        \"--lora_alpha=16\",\n",
    "        \"--lora_dropout=0.1\",\n",
    "        \"--warmup_ratio=0.03\",\n",
    "        f\"--max_steps={max_steps}\",\n",
    "        \"--max_seq_length=512\",\n",
    "        \"--learning_rate=2e-4\",\n",
    "        f\"--template={template}\",\n",
    "    ],\n",
    "    replica_count=replica_count,\n",
    "    machine_type=machine_type,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    "    boot_disk_size_gb=500,\n",
    ")\n",
    "\n",
    "print(\"Trained models were saved in: \", output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oCgivsdllZ1x"
   },
   "source": [
    "### 使用图像进行推理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_lNIFvielcyV"
   },
   "source": [
    "这一部分将模型上传到模型注册表中，并部署到端点。\n",
    "\n",
    "模型部署步骤大约需要15分钟才能完成。\n",
    "\n",
    "使用LoRA权重的[tiiuae/falcon-7b-instruct](https://huggingface.co/tiiuae/falcon-7b-instruct)和[tiiuae/falcon-40b-instruct](https://huggingface.co/tiiuae/falcon-40b-instruct)的峰值GPU内存使用量分别为约15.5G和38.2G，使用默认设置。我们在部署中使用V100以简化操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "inrBXzoxk53t"
   },
   "outputs": [],
   "source": [
    "# # If deploy finetuned falcon-40b-instruct models, please set\n",
    "# machine_type = \"a2-highgpu-1g\",\n",
    "# accelerator_type = \"NVIDIA_TESLA_A100\"\n",
    "machine_type = \"n1-standard-8\"\n",
    "accelerator_type = \"NVIDIA_TESLA_V100\"\n",
    "\n",
    "model, endpoint = deploy_model(\n",
    "    model_name=get_job_name_with_datetime(prefix=\"falcon-peft-serve\"),\n",
    "    model_id=model_id,\n",
    "    finetuned_lora_model_path=os.path.join(output_dir, \"checkpoint-\" + str(max_steps)),\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    task=\"instruct-lora\",\n",
    "    machine_type=machine_type,\n",
    "    accelerator_type=accelerator_type,\n",
    ")\n",
    "print(\"endpoint_name: \", endpoint.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_dmO3XooliAs"
   },
   "source": [
    "注意：模型权重将在部署成功后下载。因此，在上述模型部署步骤成功之后，需要额外等待5分钟，然后再运行下面的下一步。否则，当您向端点发送请求时，可能会看到ServiceUnavailable: 503 502:Bad Gateway错误。\n",
    "\n",
    "一旦部署成功，您可以使用文本提示向端点发送请求。\n",
    "\n",
    "例子：\n",
    "\n",
    "```\n",
    "人类：什么是汽车？\n",
    "助手：一辆汽车，或称为汽车，是一种与道路相连的人类运输系统，用于将人们或货物从一个地方移动到另一个地方。这个术语还涵盖了各种车辆，包括摩托艇、火车和飞机。汽车通常有四个轮子，为乘客设有驾驶舱，配备发动机或电动机。汽车自19世纪初就已存在，现在是最流行的交通方式之一，用于日常通勤、购物和其他目的。\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JrKsrffGl14T"
   },
   "outputs": [],
   "source": [
    "# # Loads an existing endpoint as below.\n",
    "# endpoint_name = endpoint.name\n",
    "# aip_endpoint_name = (\n",
    "#     f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
    "# )\n",
    "# endpoint = aiplatform.Endpoint(aip_endpoint_name)\n",
    "instances = [\n",
    "    {\n",
    "        \"prompt\": \"What is a car?\",\n",
    "        \"max_tokens\": 50,\n",
    "        \"temperature\": 1.0,\n",
    "        \"top_p\": 1.0,\n",
    "        \"top_k\": 10,\n",
    "    },\n",
    "]\n",
    "response = endpoint.predict(instances=instances)\n",
    "\n",
    "for prediction in response.predictions:\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uk-t69nFl9rr"
   },
   "source": [
    "清理资源"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CcIX2WAJmAI7"
   },
   "outputs": [],
   "source": [
    "# Delete custom train jobs.\n",
    "train_job.delete()\n",
    "\n",
    "# Undeploy model and delete endpoint.\n",
    "endpoint.delete(force=True)\n",
    "\n",
    "# Delete models.\n",
    "model.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "og_s64QVmJDb"
   },
   "source": [
    "序列分类 + PEFT\n",
    "\n",
    "本节演示了如何使用PEFT LoRA对序列分类模型进行微调，包括[bert-base-uncased](https://huggingface.co/bert-base-uncased)，[RoBERTa-large](https://huggingface.co/roberta-large)和[XLM-RoBERTa-large](https://huggingface.co/xlm-roberta-large)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QqJcvCb2mn8l"
   },
   "source": [
    "### 使用PEFT进行微调"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0E7dfqloeBB"
   },
   "source": [
    "设置基本模型ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eru79ot6mtNG"
   },
   "outputs": [],
   "source": [
    "model_id = \"xlm-roberta-large\"  # @param [\"bert-base-uncased\", \"roberta-large\", \"xlm-roberta-large\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y9n-qheaotS9"
   },
   "source": [
    "使用Vertex AI SDK 来创建和运行使用Vertex AI Model Garden 训练图像的自定义训练作业。\n",
    "\n",
    "此示例使用数据集[glue](https://huggingface.co/datasets/glue)。\n",
    "\n",
    "用于微调 [RoBERTa-large](https://huggingface.co/roberta-large) 和 [XLM-RoBERTa-large](https://huggingface.co/xlm-roberta-large) 模型的最大 GPU 内存使用量分别是 ~8.8G 和 ~15G，使用默认训练参数和示例数据集。理论上，RoBERTa-large 可以在 1 个P100/V100 上进行微调，而 XLM-RoBERTa-large 可以在 1 个A100 (40G) 上进行微调。我们选择默认使用 1 个 A100 (40G) 以简化此笔记本中所有这些模型的支持。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OHeSYEKinfP3"
   },
   "outputs": [],
   "source": [
    "dataset_name = \"glue\"  # @param {type:\"string\"}\n",
    "\n",
    "# Worker pool spec.\n",
    "# # Please switch to A100 or other powerful machines if you run out of memory.\n",
    "# machine_type = \"a2-highgpu-1g\"\n",
    "# accelerator_type = \"NVIDIA_TESLA_A100\"\n",
    "machine_type = \"n1-standard-8\"\n",
    "accelerator_type = \"NVIDIA_TESLA_V100\"\n",
    "replica_count = 1\n",
    "accelerator_count = 1\n",
    "\n",
    "# Setup training job.\n",
    "job_name = get_job_name_with_datetime(os.path.basename(model_id) + \"-lora-train\")\n",
    "train_job = aiplatform.CustomContainerTrainingJob(\n",
    "    display_name=job_name,\n",
    "    container_uri=TRAIN_DOCKER_URI,\n",
    ")\n",
    "output_dir = os.path.join(MODEL_BUCKET, job_name)\n",
    "output_dir_gcsfuse = output_dir.replace(\"gs://\", \"/gcs/\")\n",
    "\n",
    "# Pass training arguments and launch job.\n",
    "train_job.run(\n",
    "    args=[\n",
    "        \"--task=sequence-classification-lora\",\n",
    "        f\"--pretrained_model_id={model_id}\",\n",
    "        f\"--dataset_name={dataset_name}\",\n",
    "        f\"--output_dir={output_dir_gcsfuse}\",\n",
    "        \"--lora_rank=8\",\n",
    "        \"--lora_alpha=16\",\n",
    "        \"--lora_dropout=0.1\",\n",
    "        \"--num_epochs=20\",\n",
    "        \"--batch_size=32\",\n",
    "        \"--learning_rate=3e-4\",\n",
    "    ],\n",
    "    replica_count=replica_count,\n",
    "    machine_type=machine_type,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    "    boot_disk_size_gb=500,\n",
    ")\n",
    "\n",
    "print(\"Trained models were saved in: \", output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJuHaBCqoacg"
   },
   "source": [
    "### 使用图像服务进行推理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jw1GFXydoft0"
   },
   "source": [
    "这部分将模型上传到模型注册表并部署到终端点。\n",
    "\n",
    "模型部署步骤大约需要15分钟才能完成。\n",
    "\n",
    "使用默认设置，[RoBERTa-large](https://huggingface.co/roberta-large) 和[XLM-RoBERTa-large](https://huggingface.co/xlm-roberta-large) 的峰值GPU内存使用量分别为约2G和约2.4G，我们在部署中使用V100以便简便。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A5j0a3AZoj3h"
   },
   "outputs": [],
   "source": [
    "model, endpoint = deploy_model(\n",
    "    model_name=get_job_name_with_datetime(prefix=\"sequence-classification-peft-serve\"),\n",
    "    model_id=model_id,\n",
    "    finetuned_lora_model_path=output_dir,\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    task=\"sequence-classification-lora\",\n",
    ")\n",
    "print(\"endpoint_name:\", endpoint.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6dR3-Hc7oqu-"
   },
   "source": [
    "注意：模型权重将在部署成功后下载。因此，在上述模型部署步骤成功之后和您运行下面的下一个步骤之前，需要额外等待5分钟的时间。否则，当您向端点发送请求时，您可能会看到“ServiceUnavailable: 503 502:Bad Gateway”错误。\n",
    "\n",
    "一旦部署成功，您就可以使用文本提示向端点发送请求。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xAmULPegoyXl"
   },
   "outputs": [],
   "source": [
    "# # Loads an existing endpoint as below.\n",
    "# endpoint_name = endpoint.name\n",
    "# aip_endpoint_name = (\n",
    "#     f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
    "# )\n",
    "# endpoint = aiplatform.Endpoint(aip_endpoint_name)\n",
    "instances = [\n",
    "    {\"prompt\": \"The cat sat on the mat.\"},\n",
    "]\n",
    "response = endpoint.predict(instances=instances)\n",
    "labels = [int(item) for item in response.predictions]\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Vg_b46Jo6C1"
   },
   "source": [
    "清理资源"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vNiTDL_VpAlp"
   },
   "outputs": [],
   "source": [
    "# Delete custom train jobs.\n",
    "train_job.delete()\n",
    "\n",
    "# Undeploy model and delete endpoint.\n",
    "endpoint.delete(force=True)\n",
    "\n",
    "# Delete models.\n",
    "model.delete()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "model_garden_pytorch_peft.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
