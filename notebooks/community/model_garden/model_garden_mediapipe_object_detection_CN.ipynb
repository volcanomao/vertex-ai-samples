{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TirJ-SGQseby"
   },
   "source": [
    "# 使用对象检测的Vertex AI Model Garden MediaPipe\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_mediapipe_object_detection.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> 在Colab中运行\n",
    "    </a>\n",
    "  </td>\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_mediapipe_object_detection.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      在GitHub上查看\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/notebooks/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/model_garden/model_garden_mediapipe_object_detection.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "在Vertex AI工作台中打开\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dwGLvtIeECLK"
   },
   "source": [
    "**_注意_**: 该笔记本已在以下环境中进行测试：\n",
    "\n",
    "* Python版本 = 3.9\n",
    "\n",
    "**注意**：此Colab中链接的检查点和数据集不是由谷歌拥有或分发的，而是由第三方提供。在使用检查点和数据之前，请先阅读第三方提供的条款和条件。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## 概述\n",
    "\n",
    "本笔记本演示了如何在Vertex AI模型花园中使用[MediaPipe Model Maker](https://developers.google.com/mediapipe/solutions/model_maker)。\n",
    "\n",
    "### 目标\n",
    "\n",
    "* 训练新模型\n",
    "  * 将输入数据转换为训练格式\n",
    "  * 创建[自定义作业](https://cloud.google.com/vertex-ai/docs/training/create-custom-job)来训练新模型\n",
    "  * 导出模型\n",
    "\n",
    "* 清理资源\n",
    "\n",
    "### 成本\n",
    "\n",
    "此教程使用Google Cloud的可计费组件：\n",
    "\n",
    "* Vertex AI\n",
    "* 云存储\n",
    "\n",
    "了解[Vertex AI定价](https://cloud.google.com/vertex-ai/pricing)和[云存储定价](https://cloud.google.com/storage/pricing)，并使用[Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "根据您的预期使用情况生成成本估算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KEukV6uRk_S3"
   },
   "source": [
    "在你开始之前"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z__i0w0lCAsW"
   },
   "source": [
    "只有Colab才能运行\n",
    "请运行以下命令来安装依赖项并在Colab上进行Google Cloud身份验证。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jvqs-ehKlaYh"
   },
   "outputs": [],
   "source": [
    "! pip3 install --upgrade pip\n",
    "\n",
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    ! pip3 install --upgrade google-cloud-aiplatform\n",
    "\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)\n",
    "\n",
    "    from google.colab import auth as google_auth\n",
    "\n",
    "    google_auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WReHDGG5g0XY"
   },
   "source": [
    "#### 设置您的项目ID\n",
    "\n",
    "**如果您不知道您的项目ID**，请参阅支持页面：[找到项目ID](https://support.google.com/googleapi/answer/7014113)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oM1iC_MfAts1"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
    "\n",
    "# Set the project id\n",
    "! gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "### 区域\n",
    "\n",
    "您还可以更改 Vertex AI 使用的 `REGION` 变量。了解更多关于[Vertex AI 区域](https://cloud.google.com/vertex-ai/docs/general/locations)的信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tTy1gX11kCJY"
   },
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"  # @param {type: \"string\"}\n",
    "REGION_PREFIX = REGION.split(\"-\")[0]\n",
    "assert REGION_PREFIX in (\n",
    "    \"us\",\n",
    "    \"europe\",\n",
    "    \"asia\",\n",
    "), f'{REGION} is not supported. It must be prefixed by \"us\", \"asia\", or \"europe\".'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "创建一个云存储桶\n",
    "\n",
    "创建一个存储桶来存储中间产物，比如数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "BUCKET_URI = f\"gs://your-bucket-name-{PROJECT_ID}-unique\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EcIXiGsCePi"
   },
   "source": [
    "只有在您的存储桶不存在时才运行以下单元格来创建您的云存储存储桶。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIq7R4HZCfIc"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l {REGION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "960505627ddf"
   },
   "source": [
    "### 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PyQmSRbKA8r-"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow\n",
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "source": [
    "### 初始化用于 Python 的 Vertex AI SDK\n",
    "\n",
    "为您的项目初始化用于 Python 的 Vertex AI SDK。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9wExiMUxFk91"
   },
   "outputs": [],
   "source": [
    "now = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temp/%s\" % now)\n",
    "\n",
    "EVALUATION_RESULT_OUTPUT_DIRECTORY = os.path.join(STAGING_BUCKET, \"evaluation\")\n",
    "EVALUATION_RESULT_OUTPUT_FILE = os.path.join(\n",
    "    EVALUATION_RESULT_OUTPUT_DIRECTORY, \"evaluation.json\"\n",
    ")\n",
    "\n",
    "EXPORTED_MODEL_OUTPUT_DIRECTORY = os.path.join(STAGING_BUCKET, \"model\")\n",
    "EXPORTED_MODEL_OUTPUT_FILE = os.path.join(\n",
    "    EXPORTED_MODEL_OUTPUT_DIRECTORY, \"model.tflite\"\n",
    ")\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n6IFz75WGCam"
   },
   "source": [
    "### 定义培训机器规格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "riG_qUokg0XZ"
   },
   "outputs": [],
   "source": [
    "TRAINING_JOB_DISPLAY_NAME = \"mediapipe_object_detector_%s\" % now\n",
    "TRAINING_CONTAINER = f\"{REGION_PREFIX}-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/mediapipe-train\"\n",
    "TRAINING_MACHINE_TYPE = \"n1-highmem-16\"\n",
    "TRAINING_ACCELERATOR_TYPE = \"NVIDIA_TESLA_V100\"\n",
    "TRAINING_ACCELERATOR_COUNT = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XDq9TiRUc7dV"
   },
   "source": [
    "训练您自定义的模型##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "### 为训练准备输入数据\n",
    "\n",
    "对目标检测模型进行微调需要一个包含你希望完成模型能够识别的项目或类别的数据集。你可以通过从公共数据集中筛选出与你的用例相关的类别、编制自己的数据集，或者两者结合的方式来实现。数据集可以显著小于从头开始训练新模型所需的数据量。例如，用于训练许多参考模型的 [COCO](https://cocodataset.org/) 数据集包含数十万张图像，涵盖了 91 类对象。使用 Model Maker 进行迁移学习可以通过较小的数据集对现有模型进行微调，根据你的推断精度目标，模型仍然可以表现良好。本指南使用包含 2 种安卓小人玩偶（或者 2 类）的较小数据集，共有 62 张训练图像。\n",
    "\n",
    "你可以重复使用现有数据集，如 `gs://mediapipe-tasks/object_detector/android_figurine` 来微调模型。该目录包含两个子目录，分别为训练集和验证集，分别位于 android_figurine/train 和 android_figurine/validation 目录中。每个训练集和验证集都遵循下面描述的 COCO 数据集的格式。如果你使用自己的数据集，请确保符合格式规范后再上传到 Google Cloud Storage。\n",
    "\n",
    "### 支持的数据集格式\n",
    "Model Maker 目标检测 API 支持读取以下数据集格式：\n",
    "\n",
    "#### COCO 格式\n",
    "COCO 数据集格式有一个 `data` 目录用于存储所有图像，还有一个包含所有图像对象注释的 `labels.json` 文件。\n",
    "```\n",
    "<dataset_dir>/\n",
    "  data/\n",
    "    <img0>.<jpg/jpeg>\n",
    "    <img1>.<jpg/jpeg>\n",
    "    ...\n",
    "  labels.json\n",
    "```\n",
    "其中 `labels.json` 的格式如下：\n",
    "```\n",
    "{\n",
    "  \"categories\":[\n",
    "    {\"id\":1, \"name\":<cat1_name>},\n",
    "    ...\n",
    "  ],\n",
    "  \"images\":[\n",
    "    {\"id\":0, \"file_name\":\"<img0>.<jpg/jpeg>\"},\n",
    "    ...\n",
    "  ],\n",
    "  \"annotations\":[\n",
    "    {\"id\":0, \"image_id\":0, \"category_id\":1, \"bbox\":[x-左上, y-左上, 宽度, 高度]},\n",
    "    ...\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "#### PASCAL VOC 格式\n",
    "PASCAL VOC 数据集格式也有一个 `data` 目录用于存储所有图像，不过每个图像的注释被拆分为相应的 xml 文件存在 `Annotations` 目录中。\n",
    "```\n",
    "<dataset_dir>/\n",
    "  data/\n",
    "    <file0>.<jpg/jpeg>\n",
    "    ...\n",
    "  Annotations/\n",
    "    <file0>.xml\n",
    "    ...\n",
    "```\n",
    "其中 xml 文件格式如下：\n",
    "```\n",
    "<annotation>\n",
    "  <filename>file0.jpg</filename>\n",
    "  <object>\n",
    "    <name>kangaroo</name>\n",
    "    <bndbox>\n",
    "      <xmin>233</xmin>\n",
    "      <ymin>89</ymin>\n",
    "      <xmax>386</xmax>\n",
    "      <ymax>262</ymax>\n",
    "    </bndbox>\n",
    "  </object>\n",
    "  <object>\n",
    "    ...\n",
    "  </object>\n",
    "  ...\n",
    "</annotation>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O32DU5RRGhdV"
   },
   "source": [
    "### 配置训练数据集\n",
    "\n",
    "完成数据准备后，您可以开始微调模型以识别由训练数据定义的新对象或类。以下说明将使用前一部分准备的数据来微调一个目标检测模型，以识别两种类型的安卓人偶。\n",
    "\n",
    "如果您没有单独的测试数据集，可以将测试数据的路径留空。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IndQ_m6ddUEM"
   },
   "outputs": [],
   "source": [
    "training_data_path = \"gs://mediapipe-tasks/object_detector/android_figurine/train\"  # @param {type:\"string\"}\n",
    "validation_data_path = \"gs://mediapipe-tasks/object_detector/android_figurine/validation\"  # @param {type:\"string\"}\n",
    "test_data_path = \"\"  # @param {type:\"string\"}\n",
    "data_format = \"coco\"  # @param [\"coco\", \"pascal_voc\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aaff6f5be7f6"
   },
   "source": [
    "### 设置微调选项\n",
    "\n",
    "您可以在不同的模型架构之间进行选择，以进一步定制您的训练：\n",
    "\n",
    "- MobileNet-V2\n",
    "- MobileNet-MultiHW-AVG\n",
    "\n",
    "要设置模型架构和其他训练参数，请调整以下数值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "um_XKbmpTaHx"
   },
   "outputs": [],
   "source": [
    "model_architecture = \"mobilenet_v2\"  # @param [\"mobilenet_v2\", \"mobilenet_multihw_avg\"]\n",
    "\n",
    "# The learning rate to use for gradient descent training.\n",
    "learning_rate: float = 0.01  # @param {type:\"number\"}\n",
    "# Batch size for training.\n",
    "batch_size: int = 2  # @param {type:\"number\"}\n",
    "# Number of training iterations over the dataset.\n",
    "epochs: int = 10  # @param {type:\"slider\", min:0, max:100, step:1}\n",
    "# If true, the base module is trained together with the classification layer on\n",
    "# top.\n",
    "do_fine_tuning: bool = False  # @param {type:\"boolean\"}\n",
    "# A regularizer that applies a L1 regularization penalty.\n",
    "l1_regularizer: float = 0.0  # @param {type:\"number\"}\n",
    "# A regularizer that applies a L2 regularization penalty.\n",
    "l2_regularizer: float = 0.0001  # @param {type:\"number\"}\n",
    "# A boolean controlling whether the training dataset is augmented by randomly\n",
    "# distorting input images, including random cropping, flipping, etc. See\n",
    "# utils.image_preprocessing documentation for details.\n",
    "do_data_augmentation: bool = True  # @param {type:\"boolean\"}\n",
    "# Number of training samples used to calculate the decay steps\n",
    "# and create the training optimizer.\n",
    "decay_samples: int = 2560000  # @param {type:\"number\"}\n",
    "# Number of warmup steps for a linear increasing warmup schedule on learning\n",
    "# rate. Used to set up warmup schedule by model_util.WarmUp.\n",
    "warmup_epochs: int = 2  # @param {type:\"number\"}\n",
    "# The number of epochs for cosine decay learning rate.\n",
    "cosine_decay_epochs: int = 5  # @param {type:\"number\"}\n",
    "# The alpha value for cosine decay learning rate.\n",
    "cosine_decay_alpha: float = 5  # @param {type:\"number\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwcCjwlBTQIz"
   },
   "source": [
    "### 运行微调\n",
    "准备好您的训练数据集和微调选项后，您就可以开始微调过程。这个过程消耗资源，根据您的可用计算资源可能需要几分钟到几个小时。在使用GPU处理的Vertex AI上，下面的示例微调大约需要3到4分钟。\n",
    "\n",
    "要开始微调过程，请使用以下代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aec22792ee84"
   },
   "outputs": [],
   "source": [
    "model_export_path = EXPORTED_MODEL_OUTPUT_DIRECTORY\n",
    "evaluation_result_path = EVALUATION_RESULT_OUTPUT_DIRECTORY\n",
    "\n",
    "worker_pool_specs = [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": TRAINING_MACHINE_TYPE,\n",
    "            \"accelerator_type\": TRAINING_ACCELERATOR_TYPE,\n",
    "            \"accelerator_count\": TRAINING_ACCELERATOR_COUNT,\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": TRAINING_CONTAINER,\n",
    "            \"command\": [],\n",
    "            \"args\": [\n",
    "                \"--task_name=object_detector\",\n",
    "                \"--training_data_path=%s\" % training_data_path,\n",
    "                \"--validation_data_path=%s\" % validation_data_path,\n",
    "                \"--test_data_path=%s\" % test_data_path,\n",
    "                \"--data_format=%s\" % data_format,\n",
    "                \"--model_export_path=%s\" % model_export_path,\n",
    "                \"--evaluation_result_path=%s\" % evaluation_result_path,\n",
    "                \"--model_architecture=%s\" % model_architecture,\n",
    "                \"--hparams=%s\"\n",
    "                % json.dumps(\n",
    "                    {\n",
    "                        \"learning_rate\": learning_rate,\n",
    "                        \"batch_size\": batch_size,\n",
    "                        \"epochs\": epochs,\n",
    "                        \"do_fine_tuning\": do_fine_tuning,\n",
    "                        \"l1_regularizer\": l1_regularizer,\n",
    "                        \"l2_regularizer\": l2_regularizer,\n",
    "                        \"do_data_augmentation\": do_data_augmentation,\n",
    "                        \"decay_samples\": decay_samples,\n",
    "                        \"warmup_epochs\": warmup_epochs,\n",
    "                        \"cosine_decay_epochs\": cosine_decay_epochs,\n",
    "                        \"cosine_decay_alpha\": cosine_decay_alpha,\n",
    "                    }\n",
    "                ),\n",
    "            ],\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "training_job = aiplatform.CustomJob(\n",
    "    display_name=TRAINING_JOB_DISPLAY_NAME,\n",
    "    project=PROJECT_ID,\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    "    staging_bucket=STAGING_BUCKET,\n",
    ")\n",
    "\n",
    "training_job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zcKzIa5QeIIU"
   },
   "source": [
    "## 评估并导出模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mV-Djz-frBni"
   },
   "source": [
    "### 评估性能\n",
    "\n",
    "如果您已经指定了测试数据，您可以在测试数据集上进行评估，并打印出损失和coco指标。评估模型性能最重要的指标通常是“AP” coco指标，即平均精度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "09Rz1AYspK19"
   },
   "outputs": [],
   "source": [
    "def get_evaluation_result(evaluation_result_path):\n",
    "    try:\n",
    "        with tensorflow.io.gfile.GFile(evaluation_result_path, \"r\") as input_file:\n",
    "            evalutation_result = json.loads(input_file.read())\n",
    "        return evalutation_result[\"loss\"], evalutation_result[\"coco_metrics\"]\n",
    "    except:\n",
    "        print(\"Evaluation result not found. Did you provide a test dataset?\")\n",
    "        return None\n",
    "\n",
    "\n",
    "evaluation_result = get_evaluation_result(EVALUATION_RESULT_OUTPUT_FILE)\n",
    "\n",
    "if evaluation_result is not None:\n",
    "    print(f\"Validation loss: {evaluation_result[0]}\")\n",
    "    print(f\"Validation coco metrics: {evaluation_result[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0BGaofgsMsy"
   },
   "source": [
    "### 导出模型\n",
    "在对模型进行微调和评估之后，您可以将其保存为Tensorflow Lite模型，尝试在MediaPipe Studio中的[Object Detector](https://mediapipe-studio.webapps.google.com/demo/object_detector)演示中使用，或者根据[Object detection task guide](https://developers.google.com/mediapipe/solutions/vision/object_detector)将其集成到您的应用程序中。导出的模型还包括元数据和标签映射。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NYuQowyZEtxK"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "def copy_model(model_source, model_dest):\n",
    "    ! gsutil cp {model_source} {model_dest}\n",
    "\n",
    "copy_model(EXPORTED_MODEL_OUTPUT_FILE, \"object_detection_model.tflite\")\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import files\n",
    "\n",
    "    files.download(\"object_detection_model.tflite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kkH2nrpdp4sp"
   },
   "source": [
    "整理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ax6vQVZhp9pR"
   },
   "outputs": [],
   "source": [
    "# Delete training data and jobs.\n",
    "if training_job.list(filter=f'display_name=\"{TRAINING_JOB_DISPLAY_NAME}\"'):\n",
    "    training_job.delete()\n",
    "\n",
    "!gsutil rm -r {STAGING_BUCKET}"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "model_garden_mediapipe_object_detection.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
