{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TirJ-SGQseby"
   },
   "source": [
    "# 使用人脸风格化器的顶点 AI 模型花园 MediaPipe\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_mediapipe_face_stylizer.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> 在 Colab 中运行\n",
    "    </a>\n",
    "  </td>\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_mediapipe_face_stylizer.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      在 GitHub 上查看\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/notebooks/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/model_garden/model_garden_mediapipe_face_stylizer.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "在 Vertex AI Workbench 中打开\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dwGLvtIeECLK"
   },
   "source": [
    "**_注意_**：此笔记本已在以下环境中进行测试：\n",
    "\n",
    "* Python 版本 = 3.9\n",
    "\n",
    "**_注意_**：在该 Colab 中链接的检查点和数据集不是由谷歌拥有或分发的，是由第三方提供的。在使用检查点和数据之前，请先查看第三方提供的条款和条件。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## 概述\n",
    "\n",
    "本手册演示如何使用[MediaPipe模型制作器](https://developers.google.com/mediapipe/solutions/model_maker)在Vertex AI模型花园中自定义一个设备上的面部风格化器模型。\n",
    "\n",
    "MediaPipe面部风格化解决方案提供了几个可以立即使用的模型，您可以在您的应用程序中将脸部转换为包括卡通、油画等风格。但是，如果您需要将面部转换为提供的模型未涵盖的未见风格，您可以使用自己的数据和MediaPipe模型制作器自定义预训练模型。这个模型修改工具会使用您提供的数据微调模型的一部分。这种方法比从头开始训练一个新模型更快，可以生成一个适用于您特定应用程序的模型。\n",
    "\n",
    "以下部分将向您展示如何使用模型制作器在Vertex AI上使用您自己的数据重新训练预构建的面部风格化模型，然后您可以将其用于MediaPipe面部风格化器。\n",
    "\n",
    "### 目标\n",
    "\n",
    "* 自定义一个面部风格化器模型\n",
    "  * 将输入数据转换为训练格式\n",
    "  * 创建[自定义工作](https://cloud.google.com/vertex-ai/docs/training/create-custom-job)以自定义新模型\n",
    "  * 导出定制模型\n",
    "\n",
    "* 清理资源\n",
    "\n",
    "### 成本\n",
    "\n",
    "本教程使用谷歌云的计费组件：\n",
    "\n",
    "* Vertex AI\n",
    "* 云存储\n",
    "\n",
    "了解[Vertex AI价格](https://cloud.google.com/vertex-ai/pricing)和[云存储价格](https://cloud.google.com/storage/pricing)，并使用[定价计算器](https://cloud.google.com/products/calculator/)根据您的预计使用量生成成本估算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KEukV6uRk_S3"
   },
   "source": [
    "在你开始之前"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z__i0w0lCAsW"
   },
   "source": [
    "只能在Colab上运行\n",
    "运行以下命令安装依赖项并与Google Cloud进行身份验证。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jvqs-ehKlaYh"
   },
   "outputs": [],
   "source": [
    "! pip3 install --upgrade pip\n",
    "\n",
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    ! pip3 install --upgrade google-cloud-aiplatform\n",
    "\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)\n",
    "\n",
    "    from google.colab import auth as google_auth\n",
    "\n",
    "    google_auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WReHDGG5g0XY"
   },
   "source": [
    "#### 设置您的项目ID\n",
    "\n",
    "**如果您不知道您的项目ID**，请参考支持页面：[查找项目ID](https://support.google.com/googleapi/answer/7014113)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oM1iC_MfAts1"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
    "\n",
    "# Set the project id\n",
    "! gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "区域\n",
    "\n",
    "您还可以更改 Vertex AI 使用的“REGION” 变量。了解有关 [Vertex AI 区域](https://cloud.google.com/vertex-ai/docs/general/locations) 的更多信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tTy1gX11kCJY"
   },
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"  # @param {type: \"string\"}\n",
    "REGION_PREFIX = REGION.split(\"-\")[0]\n",
    "assert REGION_PREFIX in (\n",
    "    \"us\",\n",
    "    \"europe\",\n",
    "    \"asia\",\n",
    "), f'{REGION} is not supported. It must be prefixed by \"us\", \"asia\", or \"europe\".'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "创建一个云存储桶\n",
    "\n",
    "创建一个存储桶，用于存储中间产物，如数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "BUCKET_URI = f\"gs://your-bucket-name-{PROJECT_ID}-unique\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EcIXiGsCePi"
   },
   "source": [
    "只有在您的存储桶不存在时才运行以下单元格以创建您的云存储存储桶。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIq7R4HZCfIc"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l {REGION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "960505627ddf"
   },
   "source": [
    "### 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PyQmSRbKA8r-"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "source": [
    "### 初始化 Python 的 Vertex AI SDK\n",
    "\n",
    "为您的项目初始化 Python 的 Vertex AI SDK。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9wExiMUxFk91"
   },
   "outputs": [],
   "source": [
    "now = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temp/%s\" % now)\n",
    "\n",
    "\n",
    "EXPORTED_MODEL_OUTPUT_DIRECTORY = os.path.join(STAGING_BUCKET, \"model\")\n",
    "EXPORTED_MODEL_OUTPUT_FILE = os.path.join(\n",
    "    EXPORTED_MODEL_OUTPUT_DIRECTORY, \"model.tflite\"\n",
    ")\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n6IFz75WGCam"
   },
   "source": [
    "定义培训机器规格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "riG_qUokg0XZ"
   },
   "outputs": [],
   "source": [
    "TRAINING_JOB_DISPLAY_NAME = \"mediapipe_face_stylizer_%s\" % now\n",
    "TRAINING_CONTAINER = f\"{REGION_PREFIX}-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/mediapipe-train\"\n",
    "TRAINING_MACHINE_TYPE = \"n1-highmem-16\"\n",
    "TRAINING_ACCELERATOR_TYPE = \"NVIDIA_TESLA_V100\"\n",
    "TRAINING_ACCELERATOR_COUNT = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-rsdAcBV-vlf"
   },
   "source": [
    "训练您定制的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LmZ4efOd-sak"
   },
   "source": [
    "准备训练的输入数据\n",
    "\n",
    "重新训练面部风格化模型需要用户提供一张单独的风格化面部图片。期望风格化面部是正面朝向，左右眼睛和嘴巴可见。面部旋转应该很小，即偏航、俯仰和横滚轴的旋转小于30度。\n",
    "\n",
    "您可以将图片上传到Google云存储，或使用我们提供的示例图片链接。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IndQ_m6ddUEM"
   },
   "outputs": [],
   "source": [
    "training_data_path = \"gs://mediapipe-assets/face_stylizer_style_color_sketch.jpg\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aaff6f5be7f6"
   },
   "source": [
    "重新训练模型\n",
    "\n",
    "一旦您提供了输入图片，您可以开始重新训练人脸风格模型以适应新的风格。这种模型修改称为迁移学习。以下说明使用上一节准备好的数据来重新训练一个人脸风格模型，以将漫画风格应用到原始人脸上。\n",
    "\n",
    "**_注意_**：对于这种类型的模型，重新训练过程会使模型忘记它之前可以应用的任何风格。一旦重新训练完成，新模型只能应用由新风格图片定义的新风格。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kySYKb3-hnr6"
   },
   "source": [
    "## 设置重新训练选项\n",
    "除了您的训练数据集之外，运行重新训练还需要一些必需的设置：\n",
    "\n",
    "* **交换层次:** `swap_layers`参数用于确定如何在学习的风格和原始面部图像之间混合潜在代码层。潜在代码被表示为形状为[1, 12, 512]的张量。潜在代码张量的第二维称为层。面部风格化者通过在交换层上生成两个潜在代码的加权和来混合学习的风格和原始面部图像。因此，交换层是在[1, 12]之间的整数。设置更多层，将更多的风格应用于输出图像。尽管风格语义与层索引之间没有明确的映射，但是浅层，例如8、9，代表面部的全局特征，而深层，例如10、11，代表面部的细粒度特征。输出的风格化图像对交换层的设置敏感。默认情况下，设置为[8, 9, 10, 11]。\n",
    "* **学习率和迭代次数:** 使用`learning_rate`和`epochs`来指定这两个超参数。learning_rate默认设置为4e-4。epochs定义了微调BlazeStyleGAN模型的迭代次数，默认设置为100。学习率越低，预期重新训练模型收敛的迭代次数就越大。\n",
    "* **批大小:** `batch_size`用于定义我们围绕由编码器提取的潜在代码而采样的潜在代码样本的数量。潜在代码批用于微调解码器。通常批大小越大，性能就越好。它还受硬件内存的限制。对于A100 GPU，最大批大小为8。对于P100和T4 GPU，最大批大小为2。\n",
    "\n",
    "您可以配置更进一步的高级参数，如`alpha`、`perception_loss_weight`、`adv_loss_weight`、`beta_1`和`beta_2`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "um_XKbmpTaHx"
   },
   "outputs": [],
   "source": [
    "# The layers of feature to be interpolated between encoding features and\n",
    "# StyleGAN input features.\n",
    "swap_layers: str = \"[8, 9, 10, 11]\"  # @param {type:\"string\"}\n",
    "# The learning rate to use for gradient descent training.\n",
    "learning_rate: float = 0.0001  # @param {type:\"number\"}\n",
    "# Number of training iterations over the dataset.\n",
    "epochs: int = 100  # @param {type:\"slider\", min:0, max:100, step:1}\n",
    "# Batch size for training.\n",
    "batch_size: int = 2  # @param {type:\"number\"}\n",
    "\n",
    "\n",
    "# Other supported options\n",
    "\n",
    "# Weighting coefficient of style latent for swapping layer interpolation.\n",
    "# Its valid range is [0, 1]. The greater weight means stronger style is\n",
    "# applied to the output image. Expect to set it to a small value,\n",
    "# i.e. < 0.1.\n",
    "alpha: float = 0.1  # @param {type:\"number\"}\n",
    "\n",
    "# Weighting coefficients of image perception quality loss. It contains three\n",
    "# coefficients, l1, content, and style which control the difference between the\n",
    "# generated image and raw input image, the content difference between generated\n",
    "# face and raw input face, and the how similar the style between the generated\n",
    "# image and raw input image. Users can increase the style weight to enforce\n",
    "# stronger style or the content weight to reserve more raw input face details.\n",
    "# Weight for L1 loss.\n",
    "perception_loss_l1: float = 0.5  # @param {type:\"number\"}\n",
    "# Weight for content loss.\n",
    "perception_loss_content: float = 4.0  # @param {type:\"number\"}\n",
    "# Weight for stlye loss.\n",
    "perception_loss_style: float = 1.0  # @param {type:\"number\"}\n",
    "\n",
    "# Weighting coeffcieint of adversarial loss versus image perceptual quality loss.\n",
    "# This hyperparameter is used to control the realism of the generated image. It\n",
    "# expects a small value, i.e. < 0.2.\n",
    "adv_loss_weight: float = 0.2  # @param {type:\"number\"}\n",
    "# beta_1 used in tf.keras.optimizers.Adam.\n",
    "beta_1: float = 0.0  # @param {type:\"number\"}\n",
    "# beta_2 used in tf.keras.optimizers.Adam.\n",
    "beta_2: float = 0.99  # @param {type:\"number\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwcCjwlBTQIz"
   },
   "source": [
    "### 重新训练\n",
    "准备好您的训练数据集和重新训练选项后，您可以开始重新训练过程。该过程需要在 GPU 上运行，根据您的计算资源不同，可能需要几分钟到几个小时的时间。在 Vertex AI 上进行 GPU 处理，下面的示例重新训练大约需要 2 分钟。\n",
    "\n",
    "要开始微调过程，请使用以下代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aec22792ee84"
   },
   "outputs": [],
   "source": [
    "model_export_path = EXPORTED_MODEL_OUTPUT_DIRECTORY\n",
    "\n",
    "worker_pool_specs = [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": TRAINING_MACHINE_TYPE,\n",
    "            \"accelerator_type\": TRAINING_ACCELERATOR_TYPE,\n",
    "            \"accelerator_count\": TRAINING_ACCELERATOR_COUNT,\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": TRAINING_CONTAINER,\n",
    "            \"command\": [],\n",
    "            \"args\": [\n",
    "                \"--task_name=face_stylizer\",\n",
    "                \"--training_data_path=%s\" % training_data_path,\n",
    "                \"--model_export_path=%s\" % model_export_path,\n",
    "                \"--evaluation_result_path=%s\" % model_export_path,\n",
    "                \"--hparams=%s\"\n",
    "                % json.dumps(\n",
    "                    {\n",
    "                        \"learning_rate\": learning_rate,\n",
    "                        \"batch_size\": batch_size,\n",
    "                        \"epochs\": epochs,\n",
    "                        \"beta_1\": beta_1,\n",
    "                        \"beta_2\": beta_2,\n",
    "                    }\n",
    "                ),\n",
    "                \"--model_options=%s\"\n",
    "                % json.dumps(\n",
    "                    {\n",
    "                        \"swap_layers\": json.loads(swap_layers),\n",
    "                        \"alpha\": alpha,\n",
    "                        \"perception_loss_l1\": perception_loss_l1,\n",
    "                        \"perception_loss_content\": perception_loss_content,\n",
    "                        \"perception_loss_style\": perception_loss_style,\n",
    "                        \"adv_loss_weight\": adv_loss_weight,\n",
    "                    }\n",
    "                ),\n",
    "            ],\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "training_job = aiplatform.CustomJob(\n",
    "    display_name=TRAINING_JOB_DISPLAY_NAME,\n",
    "    project=PROJECT_ID,\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    "    staging_bucket=STAGING_BUCKET,\n",
    ")\n",
    "\n",
    "training_job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0BGaofgsMsy"
   },
   "source": [
    "## 导出模型\n",
    "重新训练模型后，您可以保存Tensorflow Lite模型，并按照[面部风格化任务指南](https://developers.google.com/mediapipe/solutions/vision/face_stylizer)将其集成到您的设备应用程序中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NYuQowyZEtxK"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def copy_model(model_source, model_dest):\n",
    "    ! gsutil cp {model_source} {model_dest}\n",
    "\n",
    "copy_model(EXPORTED_MODEL_OUTPUT_FILE, \"face_stylizer.task\")\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import files\n",
    "\n",
    "    files.download(\"face_stylizer.task\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kkH2nrpdp4sp"
   },
   "source": [
    "清理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ax6vQVZhp9pR"
   },
   "outputs": [],
   "source": [
    "# Delete training data and jobs.\n",
    "if training_job.list(filter=f'display_name=\"{TRAINING_JOB_DISPLAY_NAME}\"'):\n",
    "    training_job.delete()\n",
    "\n",
    "!gsutil rm -r {STAGING_BUCKET}"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "model_garden_mediapipe_face_stylizer.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
