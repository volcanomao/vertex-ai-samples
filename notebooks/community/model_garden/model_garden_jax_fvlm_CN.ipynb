{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99c1c3fc2ca5"
   },
   "source": [
    "# Vertex AI模型花园 - JAX F-VLM\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_jax_fvlm.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> 在Colab中运行\n",
    "    </a>\n",
    "  </td>\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_jax_fvlm.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      在GitHub上查看\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>                                                                                               <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/notebooks/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/model_garden/model_garden_jax_fvlm.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "在Vertex AI Workbench中打开\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24743cf4a1e1"
   },
   "source": [
    "**_注意_**：此笔记本已在以下环境中进行测试：\n",
    "\n",
    "- Python 版本 = 3.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## 概述\n",
    "\n",
    "本笔记本演示了训练和部署[JAX F-VLM模型](https://github.com/google-research/google-research/tree/master/fvlm)用于[开放词汇目标检测和实例分割](https://arxiv.org/abs/2209.15639)任务，并在Vertex AI上用于在线预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d975e698c9a4"
   },
   "source": [
    "### 目标\n",
    "\n",
    "在本教程中，您将学习如何：\n",
    "\n",
    "- 准备训练数据集。\n",
    "- 训练一个新的JAX F-VLM模型。\n",
    "- 将模型上传到[模型注册表](https://cloud.google.com/vertex-ai/docs/model-registry/introduction)。\n",
    "- 在[端点](https://cloud.google.com/vertex-ai/docs/predictions/using-private-endpoints)部署模型。\n",
    "- 运行用于开放词汇图像对象检测和实例分割的在线预测。\n",
    "\n",
    "本教程使用以下Google Cloud ML服务和资源：\n",
    "\n",
    "- Vertex AI模型注册表\n",
    "- Vertex AI在线预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08d289fa873f"
   },
   "source": [
    "数据集\n",
    "\n",
    "这份笔记本使用以下预测图像作为示例:\n",
    "\n",
    "图像: https://pixabay.com/nl/photos/het-fruit-eten-citroen-limoen-3134631/\n",
    "\n",
    "知识共享许可证: https://pixabay.com/nl/service/terms/\n",
    "\n",
    "您也可以使用自己的自定义预测图像，方法是修改本笔记本中`DEMO_IMAGE_PATH`变量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aed92deeb4a0"
   },
   "source": [
    "费用\n",
    "\n",
    "本教程使用Google Cloud的付费组件：\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "了解[Vertex AI定价](https://cloud.google.com/vertex-ai/pricing)和[Cloud Storage定价](https://cloud.google.com/storage/pricing)，并使用[Pricing Calculator](https://cloud.google.com/products/calculator/)根据您预计的使用情况生成费用估算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "## 安装\n",
    "\n",
    "安装以下所需的包以执行此笔记本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2b4ef9b72d43"
   },
   "outputs": [],
   "source": [
    "# Install the packages.\n",
    "! pip3 install --upgrade google-cloud-aiplatform\n",
    "! sudo apt-get install -y subversion protobuf-compiler python3-lxml\\\n",
    "  python3-pip python3-dev git unzip\n",
    "# Note: The following libraries are pinned down versions of:\n",
    "# https://github.com/google-research/google-research/blob/master/fvlm/requirements.txt\n",
    "! pip install tensorflow==2.12.0\n",
    "! pip install tensorflow-datasets==4.9.2\n",
    "! pip install numpy==1.23.5\n",
    "! pip install torch==2.0.1+cu118 --extra-index-url https://download.pytorch.org/whl/cu118\n",
    "! pip install torchvision==0.15.2+cu118 --extra-index-url https://download.pytorch.org/whl/cu118\n",
    "! pip install opencv-python==4.7.0.72\n",
    "! pip install tqdm==4.65.0\n",
    "! pip install git+https://github.com/openai/CLIP.git@a1d071733d7111c9c014f024669f959182114e33\n",
    "! pip install Pillow==9.1.1\n",
    "! pip install orbax-checkpoint==0.3.3\n",
    "! pip install gin-config==0.5.0\n",
    "! pip install pycocotools==2.0.6\n",
    "! pip install contextlib2==21.6.0\n",
    "! pip install ml-collections==0.1.1\n",
    "! pip install chex==0.1.7\n",
    "! pip install optax==0.1.5\n",
    "# Dependencies already included. Use no-deps to not update numpy.\n",
    "! pip install --no-deps flax==0.7.2\n",
    "! pip install --no-deps clu==0.0.9\n",
    "! pip install jax[cuda11_cudnn86]==0.4.9 \\\n",
    "  --find-links https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
    "! pip install Cython\n",
    "! pip install git+https://github.com/cocodataset/cocoapi#subdirectory=PythonAPI\n",
    "\n",
    "# Get F-VLM repository by using svn to avoid downloading entire google-research repository.\n",
    "%cd\n",
    "! rm -rf ./fvlm\n",
    "! svn export -r 60422 https://github.com/google-research/google-research/trunk/fvlm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9q83As4G2Yn"
   },
   "source": [
    "将F-VLM检查点下载到“fvlm/checkpoints”文件夹中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eXVI6s57FPyg"
   },
   "outputs": [],
   "source": [
    "%cd fvlm/checkpoints\n",
    "! ./download.sh\n",
    "%cd ../../"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7237befefbb2"
   },
   "source": [
    "下载 COCO嵌入并设置数据准备代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "324b37791d24"
   },
   "outputs": [],
   "source": [
    "%cd fvlm\n",
    "! bash ./scripts/download_precomputed_embeddings.sh\n",
    "! git clone https://github.com/tensorflow/tpu.git\n",
    "! git clone http://github.com/tensorflow/models tf-models\n",
    "%cd tf-models/research\n",
    "! protoc object_detection/protos/*.proto --python_out=.\n",
    "%cd ../../../"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58707a750154"
   },
   "source": [
    "重新启动内核"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f200f10a1da3"
   },
   "outputs": [],
   "source": [
    "# Restart kernel after installs so that your environment can access the new packages.\n",
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF1j6f9HApxa"
   },
   "source": [
    "## 开始之前\n",
    "\n",
    "### 设置你的Google Cloud项目\n",
    "\n",
    "**无论您使用什么笔记本环境，以下步骤都是必须的。**\n",
    "\n",
    "1. [选择或创建一个Google Cloud项目](https://console.cloud.google.com/cloud-resource-manager)。当您第一次创建帐户时，您将获得$300的免费信用额度用于您的计算/存储成本。\n",
    "\n",
    "1. [确保您的项目已启用计费](https://cloud.google.com/billing/docs/how-to/modify-project)。\n",
    "\n",
    "1. [启用Vertex AI API和Compute Engine API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,compute_component)。\n",
    "\n",
    "1. 如果您在本地运行此笔记本，您需要安装[Cloud SDK](https://cloud.google.com/sdk)。\n",
    "\n",
    "1. 检查您的加速器[配额](https://console.cloud.google.com/iam-admin/quotas)。此笔记本使用TPU V3 8核心，您可以在您的地区为`Vertex AI API`服务的8个核心申请`每个地区的自定义模型训练TPU V3核心`配额。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WReHDGG5g0XY"
   },
   "source": [
    "设置您的项目ID\n",
    "\n",
    "**如果您不知道您的项目ID**，请尝试以下步骤：\n",
    "- 运行 `gcloud config list`。\n",
    "- 运行 `gcloud projects list`。\n",
    "- 查看支持页面：[定位项目ID](https://support.google.com/googleapi/answer/7014113)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oM1iC_MfAts1"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"\"  # @param {type:\"string\"}\n",
    "\n",
    "# Set the project id\n",
    "! gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "### 区域\n",
    "\n",
    "您也可以更改 Vertex AI 使用的 `REGION` 变量。了解有关[Vertex AI 区域](https://cloud.google.com/vertex-ai/docs/general/locations)的更多信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "twgKk-LsLmX3"
   },
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBCra4QMA2wR"
   },
   "source": [
    "### 认证您的Google Cloud账户\n",
    "\n",
    "根据您的Jupyter环境，您可能需要手动认证。请按照以下相关说明操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74ccc9e52986"
   },
   "source": [
    "1. Vertex AI工作台\n",
    "* 无需操作，因为您已经通过身份验证。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "de775a3773ba"
   },
   "source": [
    "2. 本地JupyterLab实例，取消注释并运行:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "254614fa0c46"
   },
   "outputs": [],
   "source": [
    "# ! gcloud auth login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ef21552ccea8"
   },
   "source": [
    "3. 合作，取消注释并运行:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "603adbbf0532"
   },
   "outputs": [],
   "source": [
    "# from google.colab import auth\n",
    "# auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6b2ccc891ed"
   },
   "source": [
    "4. 服务账户或其他\n",
    "* 查看如何将云存储权限授予您的服务账户，请访问https://cloud.google.com/storage/docs/gsutil/commands/iam#ch-examples。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "创建一个云存储桶\n",
    "\n",
    "创建一个存储桶，用于存储中间产物，如数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "BUCKET_URI = f\"gs://your-bucket-name-{PROJECT_ID}-unique\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EcIXiGsCePi"
   },
   "source": [
    "只有在您的存储桶尚不存在时：运行以下单元格以创建您的云存储存储桶。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIq7R4HZCfIc"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l {REGION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "960505627ddf"
   },
   "source": [
    "导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PyQmSRbKA8r-"
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import datetime\n",
    "import functools\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from io import BytesIO\n",
    "\n",
    "import gin\n",
    "import jax\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "# Work in fvlm directory in order to create gin config later.\n",
    "os.chdir(os.path.join(os.path.expanduser(\"~\"), \"fvlm\"))\n",
    "sys.path.append(os.getcwd())\n",
    "from demo_utils import input_utils as inputs\n",
    "from demo_utils import vis_utils\n",
    "from google.cloud import aiplatform\n",
    "from google.protobuf import json_format\n",
    "from google.protobuf.struct_pb2 import Value\n",
    "from utils import clip_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "source": [
    "### 初始化用于 Python 的 Vertex AI SDK\n",
    "\n",
    "为您的项目初始化用于 Python 的 Vertex AI SDK。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vS1hQiGuLmX4"
   },
   "outputs": [],
   "source": [
    "staging_bucket = os.path.join(BUCKET_URI, \"jax_fvlm_staging\")\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=staging_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cc825514deb"
   },
   "source": [
    "### 定义常量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b42bd4fa2b2d"
   },
   "outputs": [],
   "source": [
    "REGION_PREFIX = REGION.split(\"-\")[0]\n",
    "\n",
    "# The F-VLM model to use. Choose between 'resnet_50', 'resnet_50x4', or 'resnet_50x16'.\n",
    "MODEL = \"resnet_50\"\n",
    "\n",
    "# COCO dataset constants. Only used for training with COCO dataset.\n",
    "BASE_ANNOTATION_URL = \"http://images.cocodataset.org/annotations\"\n",
    "INSTANCES_FILE = \"annotations_trainval2017.zip\"\n",
    "IMAGE_INFO_FILE = \"image_info_test2017.zip\"\n",
    "BASE_IMAGE_URL = \"http://images.cocodataset.org/zips\"\n",
    "TRAIN_IMAGE_FILE = \"train2017.zip\"\n",
    "VAL_IMAGE_FILE = \"val2017.zip\"\n",
    "TRAIN_ANNOTATION_PATH = \"./annotations/instances_train2017.json\"\n",
    "VAL_ANNOTATION_PATH = \"./annotations/instances_val2017.json\"\n",
    "LOCAL_CATEGORY_EMBEDDING_PATH = f\"./embeddings/{MODEL}/coco_embed.npy\"\n",
    "# Paths to save annotations for training example.\n",
    "TRAIN_TOY_ANNOTATION_PATH = \"./annotations/toy_instances_train2017.json\"\n",
    "VAL_TOY_ANNOTATION_PATH = \"./annotations/toy_instances_val2017.json\"\n",
    "TRAIN_IMAGE_DIR = \"./train2017\"\n",
    "VAL_IMAGE_DIR = \"./val2017\"\n",
    "TRAIN_DATA_PREFIX = \"train\"\n",
    "VAL_DATA_PREFIX = \"val\"\n",
    "# Local directory to save tfrecord dataset.\n",
    "LOCAL_TFRECORD_DIR = \"./tfrecord\"\n",
    "# GCS directory to upload tfrecord dataset.\n",
    "GCS_TFRECORD_PATH = f\"{BUCKET_URI}/dataset\"\n",
    "\n",
    "# GCS location of the category embeddings file.\n",
    "GCS_CATEGORY_EMBEDDING_PATH = f\"{BUCKET_URI}/embeddings/{MODEL}/embed.npy\"\n",
    "# An upper bound on the number of classes.\n",
    "MAX_NUM_CLS = 91\n",
    "# URL to download checkpoint.\n",
    "CHECKPOINT_URL = (\n",
    "    \"https://storage.googleapis.com/cloud-tpu-checkpoints/detection/projects/fvlm/\"\n",
    "    f\"jax_checkpoints/{MODEL.replace('resnet_', 'r')}_checkpoint_184000\"\n",
    ")\n",
    "\n",
    "# The local path to the F-VLM folder.\n",
    "F_VLM_FOLDER = \".\"  # Current directory.\n",
    "# Train config template path.\n",
    "LOCAL_TRAIN_CONFIG_TEMPLATE_PATH = f\"{F_VLM_FOLDER}/configs/fvlm_train_and_eval.gin\"\n",
    "# Final train config path.\n",
    "GCS_TRAIN_CONFIG_PATH = f\"{BUCKET_URI}/fvlm_train_and_eval.gin\"\n",
    "# Base output path for training artifacts.\n",
    "GCS_TRAIN_BASE_PATH = f\"{BUCKET_URI}/train\"\n",
    "# Final train config path.\n",
    "GCS_TRAIN_CONFIG_PATH = f\"{GCS_TRAIN_BASE_PATH}/fvlm_train_and_eval.gin\"\n",
    "# Training container image.\n",
    "TRAIN_DOCKER_URI = f\"{REGION_PREFIX}-docker.pkg.dev/vertex-ai-restricted/vertex-vision-model-garden-dockers/jax-f-vlm-train\"\n",
    "GCS_TRAIN_DATA_PATH = f\"{GCS_TFRECORD_PATH}/{TRAIN_DATA_PREFIX}*.tfrecord\"\n",
    "GCS_VAL_DATA_PATH = f\"{GCS_TFRECORD_PATH}/{VAL_DATA_PREFIX}*.tfrecord\"\n",
    "GCS_VAL_ANNOTATION_FILE = f\"{GCS_TRAIN_BASE_PATH}/val_annotation.json\"\n",
    "LOCAL_CHECKPOINT_FILE = (\n",
    "    f\"./checkpoints/{MODEL.replace('resnet_', 'r')}_checkpoint_184000\"\n",
    ")\n",
    "# GCS location to upload pretrained checkpoint to.\n",
    "GCS_CHECKPOINT_DIR = f\"{GCS_TRAIN_BASE_PATH}/pretrained_checkpoint\"\n",
    "\n",
    "# The train/eval batch sizes should be divisible by the number of GPUs used.\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "EVAL_BATCH_SIZE = 8\n",
    "# Whether to predict mask.\n",
    "# Set False for object detection task and True for segmentation task.\n",
    "INCLUDE_MASK = True\n",
    "# Number of TPU cores to be used.\n",
    "NUM_CORES = 8\n",
    "# Total train steps.\n",
    "TRAIN_STEPS = 1000  # 1 epoch is 3787 steps in this example.\n",
    "# Total evaluation steps.\n",
    "EVAL_STEPS = 156\n",
    "# Initial learning rate.\n",
    "INIT_LEARNING_RATE = 0.001\n",
    "# Training image size.\n",
    "OUTPUT_SIZE = 1024\n",
    "# Variable dtype. bfloat16 or float32 are supported.\n",
    "DTYPE = \"bfloat16\"\n",
    "\n",
    "# The pre-built TF SavedModel conversion docker image.\n",
    "MODEL_CONVERSION_DOCKER_URI = f\"{REGION_PREFIX}-docker.pkg.dev/vertex-ai-restricted/vertex-vision-model-garden-dockers/jax-f-vlm-model-conversion\"\n",
    "\n",
    "# The pre-built prediction docker image.\n",
    "OPTIMIZED_TF_RUNTIME_IMAGE_URI = (\n",
    "    \"us-docker.pkg.dev/vertex-ai-restricted/prediction/tf_opt-cpu.nightly:latest\"\n",
    ")\n",
    "# The max number of boxes to draw on the output image.\n",
    "MAX_BOXES_TO_DRAW = 25\n",
    "# The minimum score required to draw a detected object.\n",
    "MIN_SCORE_THRESH = 0.2  # @param {type:\"slider\", min:0, max:0.9, step:0.05}\n",
    "# The local path to the output image.\n",
    "OUTPUT_IMAGE_PATH = \"./output.jpg\"\n",
    "# The F-VLM SavedModel folder which takes image and text embeddings as inputs.\n",
    "GCS_SAVED_MODEL_DIR = f\"{BUCKET_URI}/saved_model\"\n",
    "# The converted SavedModel folder which takes jpeg bytes and text-embeddings bytes as inputs.\n",
    "LOCAL_CONVERTED_SAVED_MODEL_DIR = f\"{F_VLM_FOLDER}/converted_saved_model\"\n",
    "# The Cloud Storage location for the converted SavedModel.\n",
    "GCS_CONVERTED_SAVED_MODEL_DIR = f\"{BUCKET_URI}/fvlm_saved_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0c250872074f"
   },
   "source": [
    "### 定义常用函数\n",
    "\n",
    "本节定义以下函数：\n",
    "\n",
    "- 将输入图像加载并转换为所需的预测格式。\n",
    "- 可视化检测输出。\n",
    "- 获取GCS Fuse路径。\n",
    "- 获取带有当前时间的作业名称。\n",
    "- 保存 COCO 注释文件的子集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XcYUGwr-AJGY"
   },
   "outputs": [],
   "source": [
    "def convert_numpy_array_to_byte_string_via_tf_tensor(np_array):\n",
    "    \"\"\"Serializes a numpy array to tensor bytes.\"\"\"\n",
    "    tensor_array = tf.convert_to_tensor(np_array)\n",
    "    tensor_byte_string = tf.io.serialize_tensor(tensor_array)\n",
    "    return tensor_byte_string.numpy()\n",
    "\n",
    "\n",
    "def generate_text_embeddings(categories):\n",
    "    \"\"\"Generates text embeddings in numpy format from object categories.\"\"\"\n",
    "    clip_text_fn = clip_utils.get_clip_text_fn(MODEL)\n",
    "    class_clip_features = []\n",
    "    print(\"Computing custom category text embeddings.\")\n",
    "    for cls_name in tqdm.tqdm(categories, total=len(categories)):\n",
    "        cls_feat = clip_text_fn(cls_name)\n",
    "        class_clip_features.append(cls_feat)\n",
    "    text_embeddings = np.concatenate(class_clip_features, axis=0)\n",
    "    embed_path = (\n",
    "        f'{F_VLM_FOLDER}/data/{MODEL.replace(\"resnet_\", \"r\")}_bg_empty_embed.npy'\n",
    "    )\n",
    "    background_embedding, empty_embeddings = np.load(embed_path)\n",
    "    background_embedding = background_embedding[np.newaxis, Ellipsis]\n",
    "    empty_embeddings = empty_embeddings[np.newaxis, Ellipsis]\n",
    "    tile_empty_embeddings = np.tile(\n",
    "        empty_embeddings, (MAX_NUM_CLS - len(categories) - 1, 1)\n",
    "    )\n",
    "    # Concatenate 'background' and 'empty' embeddings.\n",
    "    text_embeddings = np.concatenate(\n",
    "        (background_embedding, text_embeddings, tile_empty_embeddings), axis=0\n",
    "    )\n",
    "    return text_embeddings\n",
    "\n",
    "\n",
    "def get_jpeg_bytes(local_image_path, new_width=-1):\n",
    "    \"\"\"Returns jpeg bytes given an image path and resizes if required.\"\"\"\n",
    "    image = Image.open(local_image_path)\n",
    "    if new_width <= 0:\n",
    "        new_image = image\n",
    "    else:\n",
    "        width, height = image.size\n",
    "        print(\"original input image size: \", width, \" , \", height)\n",
    "        new_height = int(height * new_width / width)\n",
    "        print(\"new input image size: \", new_width, \" , \", new_height)\n",
    "        new_image = image.resize((new_width, new_height))\n",
    "    buffered = BytesIO()\n",
    "    new_image.save(buffered, format=\"JPEG\")\n",
    "    return buffered.getvalue()\n",
    "\n",
    "\n",
    "def generate_prediction_output_image(\n",
    "    input_image_path, prediction_output, output_image_path, categories\n",
    "):\n",
    "    \"\"\"Generates prediction output image with detected objects and bounding boxes.\"\"\"\n",
    "    # Generate tensors from prediction outputs.\n",
    "    prediction_output_tensor = {}\n",
    "    for key, val in prediction_output.items():\n",
    "        prediction_output_tensor[key] = tf.expand_dims(\n",
    "            tf.convert_to_tensor(val), axis=0\n",
    "        )\n",
    "    prediction_output_tensor[\"num_detections\"] = tf.cast(\n",
    "        prediction_output_tensor[\"num_detections\"], tf.int32\n",
    "    )\n",
    "    # Generate image embeddings for the input image.\n",
    "    with open(input_image_path, \"rb\") as f:\n",
    "        np_image = np.array(Image.open(f))\n",
    "    parser_fn = inputs.get_maskrcnn_parser()\n",
    "    data = parser_fn({\"image\": np_image, \"source_id\": np.array([0])})\n",
    "    np_data = jax.tree_map(lambda x: x.numpy()[np.newaxis, Ellipsis], data)\n",
    "    image_embeddings = np_data.pop(\"images\")\n",
    "    labels = np_data.pop(\"labels\")\n",
    "    # Generate visualization.\n",
    "    print(\"Preparing visualization.\")\n",
    "    id_mapping = {(i + 1): c for i, c in enumerate(categories)}\n",
    "    id_mapping[0] = \"background\"\n",
    "    for k in range(len(categories) + 2, MAX_NUM_CLS):\n",
    "        id_mapping[k] = \"empty\"\n",
    "    category_index = inputs.get_category_index(id_mapping)\n",
    "    maskrcnn_visualizer_fn = functools.partial(\n",
    "        vis_utils.visualize_boxes_and_labels_on_image_array,\n",
    "        category_index=category_index,\n",
    "        use_normalized_coordinates=False,\n",
    "        max_boxes_to_draw=MAX_BOXES_TO_DRAW,\n",
    "        min_score_thresh=MIN_SCORE_THRESH,\n",
    "        skip_labels=False,\n",
    "    )\n",
    "    vis_image = vis_utils.visualize_instance_segmentations(\n",
    "        prediction_output_tensor,\n",
    "        image_embeddings,\n",
    "        labels[\"image_info\"],\n",
    "        maskrcnn_visualizer_fn,\n",
    "    )\n",
    "    pil_vis_image = Image.fromarray(vis_image, mode=\"RGB\")\n",
    "    pil_vis_image.save(output_image_path)\n",
    "    print(\"Completed saving the output image at: \", output_image_path)\n",
    "\n",
    "\n",
    "def gcs_fuse_path(path: str) -> str:\n",
    "    \"\"\"Try to convert path to gcsfuse path if it starts with gs:// else do not modify it.\"\"\"\n",
    "    path = path.strip()\n",
    "    if path.startswith(\"gs://\"):\n",
    "        return \"/gcs/\" + path[5:]\n",
    "    return path\n",
    "\n",
    "\n",
    "def get_job_name_with_datetime(prefix: str):\n",
    "    \"\"\"Gets a job name by adding current time to prefix.\"\"\"\n",
    "    return prefix + datetime.datetime.now().strftime(\"_%Y%m%d_%H%M%S\")\n",
    "\n",
    "\n",
    "def save_subset_annotation(input_annotation_path, output_annotation_path):\n",
    "    \"\"\"Saves a subset of COCO annotation json file with CCA 4.0 license.\"\"\"\n",
    "\n",
    "    with open(input_annotation_path) as f:\n",
    "        coco_json = json.load(f)\n",
    "\n",
    "    img_ids = set()\n",
    "    images = []\n",
    "    annotations = []\n",
    "\n",
    "    for img in coco_json[\"images\"]:\n",
    "        if img[\"license\"] in [4, 5]:  # CCA 4.0 license.\n",
    "            img_ids.add(img[\"id\"])\n",
    "            images.append(img)\n",
    "\n",
    "    for ann in coco_json[\"annotations\"]:\n",
    "        if ann[\"image_id\"] in img_ids:\n",
    "            annotations.append(ann)\n",
    "\n",
    "    new_json = {\n",
    "        \"info\": coco_json[\"info\"],\n",
    "        \"licenses\": coco_json[\"licenses\"],\n",
    "        \"images\": images,\n",
    "        \"annotations\": annotations,\n",
    "        \"categories\": coco_json[\"categories\"],\n",
    "    }\n",
    "\n",
    "    with open(output_annotation_path, \"w\") as f:\n",
    "        json.dump(new_json, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ayNrua2txk0B"
   },
   "source": [
    "## 训练新模型\n",
    "\n",
    "本部分显示如何训练新模型：\n",
    "\n",
    "1. 将输入数据转换为tfrecord格式。\n",
    "2. 上传tfrecord文件和类别嵌入到GCS位置。\n",
    "3. 创建自定义训练作业以训练新的JAX模型。\n",
    "\n",
    "如果您想使用提供的预训练保存的模型，您可以跳过本部分，直接转到“将F-VLM SavedModel转换为支持更小输入尺寸”的部分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2d45a55bf581"
   },
   "source": [
    "### 准备用于训练的输入数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98c56d2a5ba8"
   },
   "source": [
    "将F-VLM SavedModels和检查点下载到`fvlm/checkpoints`文件夹中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a3f2d8ec7473"
   },
   "outputs": [],
   "source": [
    "%cd checkpoints\n",
    "! ./download.sh\n",
    "! wget {CHECKPOINT_URL}\n",
    "%cd ../../"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9c947b28adec"
   },
   "source": [
    "下载并解压COCO标注和图像。这可能需要大约25分钟。只有当你在训练时使用COCO数据集时才需要这一部分。如果你想使用自己的数据集，那么跳过这一部分，并通过将你的训练/验证图像分别保存在`TRAIN_IMAGE_DIR`和`VAL_IMAGE_DIR`中，并分别在`TRAIN_TOY_ANNOTATION_PATH`和`VAL_TOY_ANNOTATION_PATH`中编写训练/验证COCO格式的标注。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5a1e3561df85"
   },
   "outputs": [],
   "source": [
    "# Download and unzip instances file.\n",
    "! wget -nd -c {BASE_ANNOTATION_URL}/{INSTANCES_FILE}\n",
    "! unzip -nq {INSTANCES_FILE}\n",
    "\n",
    "# Download and unzip image info file.\n",
    "! wget -nd -c {BASE_ANNOTATION_URL}/{IMAGE_INFO_FILE}\n",
    "! unzip -nq {IMAGE_INFO_FILE}\n",
    "\n",
    "# Download and unzip train images.\n",
    "! wget -nd -c {BASE_IMAGE_URL}/{TRAIN_IMAGE_FILE}\n",
    "! unzip -nq {TRAIN_IMAGE_FILE}\n",
    "\n",
    "# Download and unzip eval images.\n",
    "! wget -nd -c {BASE_IMAGE_URL}/{VAL_IMAGE_FILE}\n",
    "! unzip -nq {VAL_IMAGE_FILE}\n",
    "\n",
    "# Remove zip files.\n",
    "! rm {INSTANCES_FILE}\n",
    "! rm {IMAGE_INFO_FILE}\n",
    "! rm {TRAIN_IMAGE_FILE}\n",
    "! rm {VAL_IMAGE_FILE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfa1084bba1b"
   },
   "source": [
    "使用COCO注释的子集创建玩具注释文件。仅当您在训练时使用COCO数据集时才需要此部分。如果要使用自己的数据集，则跳过此部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "82ddec0cfd8d"
   },
   "outputs": [],
   "source": [
    "save_subset_annotation(\n",
    "    input_annotation_path=TRAIN_ANNOTATION_PATH,\n",
    "    output_annotation_path=TRAIN_TOY_ANNOTATION_PATH,\n",
    ")\n",
    "save_subset_annotation(\n",
    "    input_annotation_path=VAL_ANNOTATION_PATH,\n",
    "    output_annotation_path=VAL_TOY_ANNOTATION_PATH,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5f8d691d78c2"
   },
   "source": [
    "将数据集保存为TF Record。这可能需要大约10分钟。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "77be933a6ac9"
   },
   "outputs": [],
   "source": [
    "! PYTHONPATH=\"tf-models:tf-models/research\" python3 ./tpu/tools/datasets/create_coco_tf_record.py \\\n",
    "--logtostderr \\\n",
    "--include_masks \\\n",
    "--image_dir={TRAIN_IMAGE_DIR} \\\n",
    "--object_annotations_file={TRAIN_TOY_ANNOTATION_PATH} \\\n",
    "--output_file_prefix={LOCAL_TFRECORD_DIR}/{TRAIN_DATA_PREFIX} \\\n",
    "--num_shards=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8c8d56cc2085"
   },
   "outputs": [],
   "source": [
    "! PYTHONPATH=\"tf-models:tf-models/research\" python3 ./tpu/tools/datasets/create_coco_tf_record.py \\\n",
    "--include_masks \\\n",
    "--image_dir={VAL_IMAGE_DIR} \\\n",
    "--object_annotations_file={VAL_TOY_ANNOTATION_PATH} \\\n",
    "--output_file_prefix={LOCAL_TFRECORD_DIR}/{VAL_DATA_PREFIX} \\\n",
    "--num_shards=256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "990866f0a3cb"
   },
   "source": [
    "### 将tfrecord文件、预训练检查点和类别嵌入上传到GCS位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e1482d2bf054"
   },
   "outputs": [],
   "source": [
    "! gsutil -m rm -R -f {GCS_TFRECORD_PATH}\n",
    "! gsutil -m cp -R {LOCAL_TFRECORD_DIR} {GCS_TFRECORD_PATH}\n",
    "! gsutil -m cp {VAL_TOY_ANNOTATION_PATH} {GCS_VAL_ANNOTATION_FILE}\n",
    "! gsutil -m cp {LOCAL_CHECKPOINT_FILE} {GCS_CHECKPOINT_DIR}/checkpoint_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34c8961224f9"
   },
   "source": [
    "上传类别嵌入。如果你想使用自己的数据集，那么跳过这个单元格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b2514916b9cb"
   },
   "outputs": [],
   "source": [
    "! gsutil -m cp {LOCAL_CATEGORY_EMBEDDING_PATH} {GCS_CATEGORY_EMBEDDING_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "605a02f752e5"
   },
   "source": [
    "如果你想使用自己的数据集，取消下面的单元格注释以创建你自己的类别嵌入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8f442f2c25c1"
   },
   "outputs": [],
   "source": [
    "# CATEGORIES = [\n",
    "#     \"kiwi\",\n",
    "#     \"orange\",\n",
    "#     \"lemon\",\n",
    "#     \"blackberry\",\n",
    "#     \"pine cone\",\n",
    "#     \"red orange\",\n",
    "#     \"table\",\n",
    "#     \"spoon\",\n",
    "#     \"pine needles\",\n",
    "#     \"seed\",\n",
    "# ]\n",
    "# text_embeddings = generate_text_embeddings(categories=CATEGORIES)\n",
    "# with tf.io.gfile.GFile(GCS_CATEGORY_EMBEDDING_PATH, \"w\") as f:\n",
    "#     np.save(f, text_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dd1b3a5d5171"
   },
   "source": [
    "### 创建自定义训练作业并训练一个新模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8a30f2fa7d34"
   },
   "source": [
    "设置训练超参数。要查看所有可自定义超参数的完整列表，请查看[config template file](https://github.com/google-research/google-research/blob/e1f9fae637db06ba885217518cbbf7f4fa4b9d7b/fvlm/configs/fvlm_train_and_eval.gin)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b2f5dd9951ff"
   },
   "outputs": [],
   "source": [
    "assert TRAIN_BATCH_SIZE % NUM_CORES == 0, \"NUM_CORES must divide TRAIN_BATCH_SIZE.\"\n",
    "assert EVAL_BATCH_SIZE % NUM_CORES == 0, \"NUM_CORES must divide EVAL_BATCH_SIZE.\"\n",
    "config_overrides = [\n",
    "    \"evaluate.host_evaluator = None\",  # For evaluation without COCO annotation json file.\n",
    "    \"evaluate.eval_metrics = {'coco_metric': @COCODetectionMetric}\",  # For evaluation without COCO annotation json file.\n",
    "    f\"TRAIN_FILE_PATTERN = '{gcs_fuse_path(GCS_TRAIN_DATA_PATH)}'\",\n",
    "    f\"EVAL_FILE_PATTERN = '{gcs_fuse_path(GCS_VAL_DATA_PATH)}'\",\n",
    "    f\"TRAIN_BS = {TRAIN_BATCH_SIZE}\",\n",
    "    f\"EVAL_BS = {EVAL_BATCH_SIZE}\",\n",
    "    f\"TRAIN_STEPS = {TRAIN_STEPS}\",\n",
    "    f\"EVAL_STEPS = {EVAL_STEPS}\",\n",
    "    f\"EMBED_PATH = '{GCS_CATEGORY_EMBEDDING_PATH}'\",\n",
    "    f\"CATG_PAD_SIZE = {MAX_NUM_CLS}\",\n",
    "    f\"INCLUDE_MASK = {INCLUDE_MASK}\",\n",
    "    f\"step_learning_rate_with_linear_warmup.init_learning_rate = {INIT_LEARNING_RATE}\",\n",
    "    f\"OUTPUT_SIZE = {OUTPUT_SIZE}\",\n",
    "    f\"get_host_evaluator.annotation_file = '{GCS_VAL_ANNOTATION_FILE}'\",\n",
    "    f\"train.pretrain_dir = '{GCS_CHECKPOINT_DIR}'\",\n",
    "    f\"DTYPE = %jnp.{DTYPE}\",\n",
    "]\n",
    "print(config_overrides)\n",
    "\n",
    "gin.parse_config_files_and_bindings(\n",
    "    [LOCAL_TRAIN_CONFIG_TEMPLATE_PATH], config_overrides, finalize_config=False\n",
    ")\n",
    "config = gin.config_str()\n",
    "\n",
    "with tf.io.gfile.GFile(GCS_TRAIN_CONFIG_PATH, \"w\") as f:\n",
    "    f.write(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "290fd279e7f0"
   },
   "source": [
    "使用Vertex AI SDK 创建并运行训练作业，使用模型工具 JAX F-VLM 训练 Docker。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "003068435a36"
   },
   "outputs": [],
   "source": [
    "JOB_NAME = get_job_name_with_datetime(prefix=\"jax_fvlm\")\n",
    "\n",
    "docker_args_list = [\n",
    "    f\"--output_dir={gcs_fuse_path(GCS_TRAIN_BASE_PATH)}\",\n",
    "    f\"--config_path={gcs_fuse_path(GCS_TRAIN_CONFIG_PATH)}\",\n",
    "    \"--mode=train_and_eval\",\n",
    "]\n",
    "print(docker_args_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9de7e802b53c"
   },
   "outputs": [],
   "source": [
    "# Click on the generated link in the output under \"View backing custom job:\" to see your run in the Cloud Console.\n",
    "# The job will run for appoximately 5 minutes in the current settings.\n",
    "job = aiplatform.CustomContainerTrainingJob(\n",
    "    display_name=JOB_NAME,\n",
    "    container_uri=TRAIN_DOCKER_URI,\n",
    ")\n",
    "model = job.run(\n",
    "    args=docker_args_list,\n",
    "    base_output_dir=f\"{GCS_TRAIN_BASE_PATH}\",\n",
    "    replica_count=1,\n",
    "    machine_type=\"cloud-tpu\",\n",
    "    accelerator_type=\"TPU_V3\",\n",
    "    accelerator_count=NUM_CORES,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "798d2268f869"
   },
   "source": [
    "将之前细调的JAX F-VLM模型转换为TF SavedModel，用于在线预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f31f3e19c6ee"
   },
   "outputs": [],
   "source": [
    "JOB_NAME = get_job_name_with_datetime(prefix=\"jax_model_conversion\")\n",
    "\n",
    "docker_args_list = [\n",
    "    f\"--input_dir={gcs_fuse_path(GCS_TRAIN_BASE_PATH)}\",\n",
    "    f\"--output_dir={gcs_fuse_path(GCS_SAVED_MODEL_DIR)}\",\n",
    "    f\"--max_num_classes={MAX_NUM_CLS}\",\n",
    "    f\"--model_name={MODEL}\",\n",
    "    f\"--include_mask={INCLUDE_MASK}\",\n",
    "]\n",
    "print(docker_args_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d0a4457779b1"
   },
   "outputs": [],
   "source": [
    "# Create and run the model conversion job.\n",
    "# Click on the generated link in the output under \"View backing custom job:\" to see your run in the Cloud Console.\n",
    "container_uri = MODEL_CONVERSION_DOCKER_URI\n",
    "job = aiplatform.CustomContainerTrainingJob(\n",
    "    display_name=JOB_NAME,\n",
    "    container_uri=container_uri,\n",
    ")\n",
    "model_conversion_workdir = os.path.join(BUCKET_URI, JOB_NAME)\n",
    "model = job.run(\n",
    "    args=docker_args_list,\n",
    "    base_output_dir=f\"{model_conversion_workdir}\",\n",
    "    replica_count=1,\n",
    "    machine_type=\"n1-highmem-8\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ayNrua2txk0B"
   },
   "source": [
    "将F-VLM SavedModel转换为支持较小输入大小\n",
    "\n",
    "F-VLM SavedModel接受图像嵌入和文本嵌入作为输入。但是，您不能直接将这些输入发送到Vertex AI Online Prediction，因为预测请求大小有1.5 MB的限制。因此，您将首先将SavedModel格式转换为接受jpeg字节和文本嵌入字节作为输入的格式。这种修改后的输入格式将满足1.5 MB的限制要求。\n",
    "\n",
    "如果您没有训练新模型，请取消注释并运行下面的单元格以使用提供的保存的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "12e3d9c0221a"
   },
   "outputs": [],
   "source": [
    "# # Path to the downloaded pre-trained saved model.\n",
    "# GCS_SAVED_MODEL_DIR = f'{F_VLM_FOLDER}/checkpoints/{MODEL.replace(\"resnet_\",\"r\")}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ef-svu2Ix1OA"
   },
   "outputs": [],
   "source": [
    "def preprocess_jpeg_byte_string(tensor_byte_string):\n",
    "    \"\"\"Converts jpeg bytes to image embeddings as an input for the original F-VLM SavedModel.\"\"\"\n",
    "    decoded_image_tensor = tf.io.decode_jpeg(tensor_byte_string, channels=3)\n",
    "    parser_fn = inputs.get_maskrcnn_parser()\n",
    "    parser_output = parser_fn({\"image\": decoded_image_tensor})\n",
    "    image_embeddings_tensor = parser_output[\"images\"]\n",
    "    return image_embeddings_tensor\n",
    "\n",
    "\n",
    "def preprocess_text_embeddings_byte_string(tensor_byte_string):\n",
    "    \"\"\"Converts text-embeddings bytes to text-embeddings as an input for the original F-VLM SavedModel.\"\"\"\n",
    "    return tf.io.parse_tensor(tensor_byte_string, tf.float32)\n",
    "\n",
    "\n",
    "def get_serve_fn(model):\n",
    "    \"\"\"Creates a serving function for the modified SavedModel which takes jpeg bytes and text-embeddings bytes as an input.\"\"\"\n",
    "\n",
    "    @tf.function(\n",
    "        input_signature=[\n",
    "            tf.TensorSpec([None], tf.string),\n",
    "            tf.TensorSpec([None], tf.string),\n",
    "        ]\n",
    "    )\n",
    "    def serve_fn(image_jpeg_bytes_inputs, text_embeddings_bytes_inputs):\n",
    "        image_embeddings_tensor = tf.map_fn(\n",
    "            preprocess_jpeg_byte_string, image_jpeg_bytes_inputs, dtype=tf.bfloat16\n",
    "        )\n",
    "        text_embeddings_tensor = tf.map_fn(\n",
    "            preprocess_text_embeddings_byte_string,\n",
    "            text_embeddings_bytes_inputs,\n",
    "            dtype=tf.float32,\n",
    "        )\n",
    "        return model({\"image\": image_embeddings_tensor, \"text\": text_embeddings_tensor})\n",
    "\n",
    "    return serve_fn\n",
    "\n",
    "\n",
    "! rm -rf {LOCAL_CONVERTED_SAVED_MODEL_DIR}\n",
    "model = tf.saved_model.load(GCS_SAVED_MODEL_DIR)\n",
    "signatures = {\n",
    "    \"serving_default\": get_serve_fn(model=model).get_concrete_function(\n",
    "        tf.TensorSpec(shape=[None], dtype=tf.string), tf.TensorSpec([None], tf.string)\n",
    "    )\n",
    "}\n",
    "tf.saved_model.save(model, LOCAL_CONVERTED_SAVED_MODEL_DIR, signatures=signatures)\n",
    "print(\"Saved the converted SavedModel to directory: \", LOCAL_CONVERTED_SAVED_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1cJQEETi1jsg"
   },
   "source": [
    "将本地转换后的TF SavedModel 复制到云存储。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6hlTWKxh11dF"
   },
   "outputs": [],
   "source": [
    "! gsutil -m rm -R -f {GCS_CONVERTED_SAVED_MODEL_DIR}\n",
    "! gsutil -m cp -R {LOCAL_CONVERTED_SAVED_MODEL_DIR} {GCS_CONVERTED_SAVED_MODEL_DIR}\n",
    "! gsutil ls {GCS_CONVERTED_SAVED_MODEL_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iILhhP3TfO8B"
   },
   "source": [
    "## 执行在线预测\n",
    "使用转换后的TF SavedModel 运行在线预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExIyCnKf3a94"
   },
   "source": [
    "上传TF SavedModel，并部署到一个端点用于预测。这个步骤可能需要最多15分钟来完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t0xYDT0BxP0W"
   },
   "outputs": [],
   "source": [
    "serving_env = {\n",
    "    \"MODEL_ID\": \"F-VLM-JAX-\",\n",
    "    \"DEPLOY_SOURCE\": \"notebook\",\n",
    "}\n",
    "\n",
    "jax_fvlm_model = aiplatform.Model.upload(\n",
    "    display_name=\"jax_fvlm\",\n",
    "    artifact_uri=GCS_CONVERTED_SAVED_MODEL_DIR,\n",
    "    serving_container_image_uri=OPTIMIZED_TF_RUNTIME_IMAGE_URI,\n",
    "    serving_container_args=[],\n",
    "    location=REGION,\n",
    "    serving_container_environment_variables=serving_env,\n",
    ")\n",
    "\n",
    "jax_fvlm_endpoint = jax_fvlm_model.deploy(\n",
    "    deployed_model_display_name=\"jax_fvlm_deployed\",\n",
    "    traffic_split={\"0\": 100},\n",
    "    machine_type=\"n1-highmem-16\",\n",
    "    min_replica_count=1,\n",
    "    max_replica_count=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w99wNhz_3ruV"
   },
   "source": [
    "准备输入预测图像。\n",
    "\n",
    "注意：您可以根据需要修改输入图像。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Itg0k1s30t3"
   },
   "outputs": [],
   "source": [
    "# Local path to the prediction image.\n",
    "DEMO_IMAGE_PATH = \"./prediction_image.jpg\"\n",
    "# Download the prediction image.\n",
    "! wget -O {DEMO_IMAGE_PATH} https://cdn.pixabay.com/photo/2018/02/06/12/37/fruit-3134631_1280.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B1Q7AbmJ4QxZ"
   },
   "source": [
    "在下面的单元格中定义您自己的类别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "50109d914484"
   },
   "outputs": [],
   "source": [
    "CATEGORIES = [\n",
    "    \"kiwi\",\n",
    "    \"orange\",\n",
    "    \"lemon\",\n",
    "    \"blackberry\",\n",
    "    \"pine cone\",\n",
    "    \"red orange\",\n",
    "    \"table\",\n",
    "    \"spoon\",\n",
    "    \"pine needles\",\n",
    "    \"seed\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ad9b7e69dc9"
   },
   "source": [
    "准备jpeg字节和文本嵌入字节输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qxj4Xv_DhHXj"
   },
   "outputs": [],
   "source": [
    "image_jpeg_bytes_inputs = get_jpeg_bytes(\n",
    "    local_image_path=DEMO_IMAGE_PATH, new_width=1024\n",
    ")\n",
    "\n",
    "text_embeddings = generate_text_embeddings(categories=CATEGORIES)\n",
    "text_embeddings_bytes_inputs = convert_numpy_array_to_byte_string_via_tf_tensor(\n",
    "    text_embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ys88lEkK4XDp"
   },
   "source": [
    "使用base-64编码后跟UTF-8解码来封装字节输入，然后将它们发送到用于预测的端点。Vertex AI预测服务将根据`b64`关键字自动将这些输入字符串转换回字节。\n",
    "\n",
    "**注意：由于模型的一次性JIT编译，第一次预测可能需要最多2分钟。这可能会导致下面的超时错误。如果您收到超时错误，请等待2分钟后再次运行预测。之后不会再出现超时错误。**\n",
    "后续的预测需要4秒钟才能完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mj4sqTAG4sU5"
   },
   "outputs": [],
   "source": [
    "instances_list = [\n",
    "    {\n",
    "        \"image_jpeg_bytes_inputs\": {\n",
    "            \"b64\": base64.b64encode(image_jpeg_bytes_inputs).decode(\"utf-8\")\n",
    "        },\n",
    "        \"text_embeddings_bytes_inputs\": {\n",
    "            \"b64\": base64.b64encode(text_embeddings_bytes_inputs).decode(\"utf-8\")\n",
    "        },\n",
    "    }\n",
    "]\n",
    "instances = [json_format.ParseDict(s, Value()) for s in instances_list]\n",
    "prediction_output = jax_fvlm_endpoint.predict(instances=instances).predictions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S3qC-MrN6SEs"
   },
   "source": [
    "生成带有预测边界框、标签和概率的输出图像。输出图像将被保存到 `OUTPUT_IMAGE_PATH`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wnPY2MFN6fL6"
   },
   "outputs": [],
   "source": [
    "generate_prediction_output_image(\n",
    "    input_image_path=DEMO_IMAGE_PATH,\n",
    "    prediction_output=prediction_output,\n",
    "    output_image_path=OUTPUT_IMAGE_PATH,\n",
    "    categories=CATEGORIES,\n",
    ")\n",
    "\n",
    "img = Image.open(OUTPUT_IMAGE_PATH)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "清理\n",
    "\n",
    "要清理此项目中使用的所有 Google Cloud 资源，您可以[删除用于本教程的 Google Cloud 项目](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects)。\n",
    "\n",
    "否则，您可以删除在本教程中创建的各个资源。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sx_vKniMq9ZX"
   },
   "outputs": [],
   "source": [
    "# Delete endpoint resource.\n",
    "jax_fvlm_endpoint.delete(force=True)\n",
    "\n",
    "# Delete model resource.\n",
    "jax_fvlm_model.delete()\n",
    "\n",
    "# Delete Cloud Storage objects that were created.\n",
    "delete_bucket = False\n",
    "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
    "    ! gsutil -m rm -r $BUCKET_URI"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "model_garden_jax_fvlm.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
