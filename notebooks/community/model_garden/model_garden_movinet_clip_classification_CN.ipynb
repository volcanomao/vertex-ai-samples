{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TirJ-SGQseby"
   },
   "source": [
    "# Vertex AI 模型花园 MoViNet 视频剪辑分类\n",
    "\n",
    "请参考以下链接：[在Colab中运行](https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_movinet_clip_classification.ipynb)，[在GitHub上查看](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_movinet_clip_classification.ipynb)，[在Vertex AI Workbench中打开](https://console.cloud.google.com/vertex-ai/notebooks/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/model_garden/model_garden_movinet_clip_classification.ipynb)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dwGLvtIeECLK"
   },
   "source": [
    "**_注意_**: 本笔记本已在以下环境中进行测试：\n",
    "\n",
    "* Python版本 = 3.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## 概述\n",
    "\n",
    "本笔记本演示了如何在Vertex AI Model Garden中使用[MoViNet](https://github.com/tensorflow/models/tree/master/official/projects/movinet)。\n",
    "\n",
    "### 目标\n",
    "\n",
    "* 训练新模型\n",
    "  * 将输入数据转换为训练格式\n",
    "  * 创建[超参数调整作业](https://cloud.google.com/vertex-ai/docs/training/hyperparameter-tuning-overview)来训练新模型\n",
    "  * 查找并导出最佳模型\n",
    "\n",
    "* 测试已训练的模型\n",
    "  * 将模型上传到[Vertex AI模型注册表](https://cloud.google.com/vertex-ai/docs/model-registry/introduction)\n",
    "  * 运行批量预测\n",
    "\n",
    "* 清理资源\n",
    "\n",
    "### 成本\n",
    "\n",
    "此教程使用Google Cloud的可计费组件：\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "了解[Vertex AI定价](https://cloud.google.com/vertex-ai/pricing)和[Cloud Storage定价](https://cloud.google.com/storage/pricing)，并使用[Pricing计算器](https://cloud.google.com/products/calculator/)根据预期使用量生成成本估算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KEukV6uRk_S3"
   },
   "source": [
    "在你开始之前"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z__i0w0lCAsW"
   },
   "source": [
    "只有合作办公\n",
    "如果您使用工作台，请跳过此部分并运行以下命令。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jvqs-ehKlaYh"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    ! pip3 install --upgrade google-cloud-aiplatform\n",
    "\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)\n",
    "\n",
    "    from google.colab import auth as google_auth\n",
    "\n",
    "    google_auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF1j6f9HApxa"
   },
   "source": [
    "### 设置您的Google Cloud项目\n",
    "\n",
    "**无论您使用什么笔记本环境，以下步骤都是必需的。**\n",
    "\n",
    "1. [选择或创建一个Google Cloud项目](https://console.cloud.google.com/cloud-resource-manager)。当您第一次创建帐户时，您将获得300美元的免费信用额度，用于支付计算/存储成本。\n",
    "\n",
    "1. [确保为您的项目启用了计费](https://cloud.google.com/billing/docs/how-to/modify-project)。\n",
    "\n",
    "1. [启用Vertex AI API和Compute Engine API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,compute_component)。\n",
    "\n",
    "1. 如果您将此笔记本本地运行，则需要安装[Cloud SDK](https://cloud.google.com/sdk)。\n",
    "\n",
    "1. 在下面的单元格中输入您的项目ID。然后运行该单元格，以确保Cloud SDK在本笔记本中的所有命令中使用正确的项目。\n",
    "\n",
    "1. [创建一个服务帐号](https://cloud.google.com/iam/docs/service-accounts-create#iam-service-accounts-create-console)，并为其分配`Vertex AI用户`和`存储对象管理员`角色，以便使用经过优化的模型运行批量预测。\n",
    "\n",
    "**注意**：Jupyter会将以`!`前缀开头的行视为shell命令，并且会将以`$`前缀开头的Python变量插入这些命令中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9wExiMUxFk91"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "# The GCP project ID for experiments.\n",
    "PROJECT_ID = \"\"  # @param {type:\"string\"}\n",
    "\n",
    "# Bucket URI with gs:// prefix.\n",
    "BUCKET_URI = \"\"  # @param {type:\"string\"}\n",
    "\n",
    "# You can choose a region from https://cloud.google.com/about/locations.\n",
    "# Only regions prefixed by \"us\", \"asia\", or \"europe\" are supported.\n",
    "REGION = \"us-central1\"  # @param {type:\"string\"}\n",
    "REGION_PREFIX = REGION.split(\"-\")[0]\n",
    "assert REGION_PREFIX in (\n",
    "    \"us\",\n",
    "    \"europe\",\n",
    "    \"asia\",\n",
    "), f'{REGION} is not supported. It must be prefixed by \"us\", \"asia\", or \"europe\".'\n",
    "\n",
    "! gcloud config set project $PROJECT_ID\n",
    "\n",
    "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
    "CHECKPOINT_BUCKET = os.path.join(BUCKET_URI, \"ckpt\")\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)\n",
    "\n",
    "# Download config files.\n",
    "CONFIG_DIR = os.path.join(BUCKET_URI, \"config\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n6IFz75WGCam"
   },
   "source": [
    "### 定义常量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "riG_qUokg0XZ"
   },
   "outputs": [],
   "source": [
    "OBJECTIVE = \"vcn\"\n",
    "\n",
    "# Data converter constants.\n",
    "DATA_CONVERTER_JOB_PREFIX = \"data_converter\"\n",
    "DATA_CONVERTER_CONTAINER = f\"{REGION_PREFIX}-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/data-converter\"\n",
    "DATA_CONVERTER_MACHINE_TYPE = \"n1-highmem-8\"\n",
    "\n",
    "# Training constants.\n",
    "TRAINING_JOB_PREFIX = \"train\"\n",
    "TRAIN_CONTAINER_URI = f\"{REGION_PREFIX}-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/movinet-train\"\n",
    "TRAIN_MACHINE_TYPE = \"n1-highmem-16\"\n",
    "TRAIN_ACCELERATOR_TYPE = \"NVIDIA_TESLA_V100\"\n",
    "TRAIN_NUM_GPU = 2\n",
    "\n",
    "# Evaluation constants.\n",
    "EVALUATION_METRIC = \"accuracy\"\n",
    "\n",
    "# Export constants.\n",
    "EXPORT_JOB_PREFIX = \"export\"\n",
    "EXPORT_CONTAINER_URI = f\"{REGION_PREFIX}-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/movinet-model-export\"\n",
    "EXPORT_MACHINE_TYPE = \"n1-highmem-8\"\n",
    "\n",
    "# Prediction constants.\n",
    "# You can adjust accelerator types and machine types to get faster predictions.\n",
    "PREDICTION_CONTAINER_URI = f\"{REGION_PREFIX}-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/movinet-serve\"\n",
    "PREDICTION_PORT = 8080\n",
    "PREDICTION_ACCELERATOR_COUNT = 1\n",
    "PREDICTION_ACCELERATOR_TYPE = \"NVIDIA_TESLA_T4\"\n",
    "PREDICTION_MACHINE_TYPE = \"n1-standard-4\"\n",
    "PREDICTION_JOB_PREFIX = \"predict\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZFPe_GezXg8"
   },
   "source": [
    "### 定义常见的助手函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XcYUGwr-AJGY"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import yaml\n",
    "\n",
    "\n",
    "def get_job_name_with_datetime(prefix: str) -> str:\n",
    "    \"\"\"Returns a timestamped job name with the given prefix.\"\"\"\n",
    "    return prefix + datetime.now().strftime(\"_%Y%m%d_%H%M%S\")\n",
    "\n",
    "\n",
    "def print_response_instance(json_str: str, label_map: dict[int, str]):\n",
    "    \"\"\"Prints summary of a prediction JSON result from the model response.\"\"\"\n",
    "    json_obj = json.loads(json_str)\n",
    "    if \"prediction\" not in json_obj:\n",
    "        print(\"Error:\", json_str)\n",
    "        return\n",
    "    instance = json_obj[\"instance\"]\n",
    "    prediction = json_obj[\"prediction\"]\n",
    "    gcs_uri = instance[\"content\"]\n",
    "    time_start = instance.get(\"timeSegmentStart\", \"0.0s\")\n",
    "    time_end = instance.get(\"timeSegmentEnd\", \"Infinity\")\n",
    "    max_idx = np.argmax(prediction)\n",
    "    print(f\"{gcs_uri} {time_start}-{time_end}:\", label_map[max_idx])\n",
    "\n",
    "\n",
    "def get_label_map(label_map_yaml_filepath: str) -> tuple[dict[int, str], int]:\n",
    "    \"\"\"Reads label map from a YAML file and returns the label map with the number of classes.\"\"\"\n",
    "    with tf.io.gfile.GFile(label_map_yaml_filepath, \"rb\") as input_file:\n",
    "        label_map = yaml.safe_load(input_file.read())[\"label_map\"]\n",
    "    num_classes = max(label_map.keys()) + 1\n",
    "    return label_map, num_classes\n",
    "\n",
    "\n",
    "def get_best_trial(\n",
    "    model_di: str, max_trial_count: int, evaluation_metric: str\n",
    ") -> tuple[str, Any]:\n",
    "    \"\"\"Finds the best trial directory and eval results from a hyperparameter tuning job.\"\"\"\n",
    "    best_trial_dir = \"\"\n",
    "    best_trial_evaluation_results = {}\n",
    "    best_performance = -1\n",
    "\n",
    "    for i in range(max_trial_count):\n",
    "        current_trial = i + 1\n",
    "        current_trial_dir = os.path.join(model_dir, \"trial_\" + str(current_trial))\n",
    "        current_trial_best_ckpt_dir = os.path.join(current_trial_dir, \"best_ckpt\")\n",
    "        current_trial_best_ckpt_evaluation_filepath = os.path.join(\n",
    "            current_trial_best_ckpt_dir, \"info.json\"\n",
    "        )\n",
    "        with tf.io.gfile.GFile(current_trial_best_ckpt_evaluation_filepath, \"rb\") as f:\n",
    "            eval_metric_results = json.load(f)\n",
    "            current_performance = eval_metric_results[evaluation_metric]\n",
    "            if current_performance > best_performance:\n",
    "                best_performance = current_performance\n",
    "                best_trial_dir = current_trial_dir\n",
    "                best_trial_evaluation_results = eval_metric_results\n",
    "    return best_trial_dir, best_trial_evaluation_results\n",
    "\n",
    "\n",
    "def find_checkpoint_in_dir(checkpoint_dir: str) -> str:\n",
    "    \"\"\"Finds a checkpoint path relative to the directory.\"\"\"\n",
    "    for root, dirs, files in tf.io.gfile.walk(checkpoint_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".index\"):\n",
    "                return os.path.join(root, os.path.splitext(file)[0])\n",
    "\n",
    "\n",
    "def upload_checkpoint_to_gcs(checkpoint_url: str) -> str:\n",
    "    \"\"\"Uploads a compressed .tar.gz checkpoint at the given URL to Cloud Storage.\"\"\"\n",
    "    filename = os.path.basename(checkpoint_url)\n",
    "    checkpoint_name = filename.replace(\".tar.gz\", \"\")\n",
    "    print(\"Download checkpoint from\", checkpoint_url, \"and store to\", CHECKPOINT_BUCKET)\n",
    "    ! wget $checkpoint_url -O $filename\n",
    "    ! mkdir -p $checkpoint_name\n",
    "    ! tar -xvzf $filename -C $checkpoint_name\n",
    "\n",
    "    checkpoint_path = find_checkpoint_in_dir(checkpoint_name)\n",
    "    checkpoint_path = os.path.relpath(checkpoint_path, checkpoint_name)\n",
    "\n",
    "    ! gsutil cp -r $checkpoint_name $CHECKPOINT_BUCKET/\n",
    "    checkpoint_uri = os.path.join(CHECKPOINT_BUCKET, checkpoint_name, checkpoint_path)\n",
    "    print(\"Checkpoint uploaded to\", checkpoint_uri)\n",
    "    return checkpoint_uri\n",
    "\n",
    "\n",
    "def upload_config_to_gcs(url: str) -> str:\n",
    "    \"\"\"Uploads a config file at the given URL to Cloud Storage.\"\"\"\n",
    "    filename = os.path.basename(url)\n",
    "    destination = os.path.join(CONFIG_DIR, filename)\n",
    "    print(\"Copy\", url, \"to\", destination)\n",
    "    ! wget \"$url\" -O \"$filename\"\n",
    "    ! gsutil cp \"$filename\" \"$destination\"\n",
    "    return destination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RB_xY9ipr7ZU"
   },
   "source": [
    "## 训练新模型\n",
    "本节介绍如何训练新模型。\n",
    "1. 将输入数据转换为训练格式\n",
    "2. 创建超参数调整作业以训练新模型\n",
    "3. 寻找并导出最佳模型\n",
    "\n",
    "如果您已经训练过模型，请转到部分`测试训练过的模型`。\n",
    "\n",
    "请选择一个模型：\n",
    "* `model_id`：MoViNet模型变体ID之一，可选择`a0`，`a1`，`a2`，`a3`，`a4`，`a5`。数字较大的模型需要更多资源来训练，并且预计具有更高的准确性和延迟。在这里，我们使用`a0`作为演示目的。\n",
    "* `model_mode`：MoViNet模型类型，可以是`base`或者`stream`。基础模型具有稍高的准确性，而流式模型则经过优化，适用于流式传输和更快速的CPU推理。更多信息请参考[官方MoViNet文档](https://github.com/tensorflow/models/tree/master/official/projects/movinet)。\n",
    "\n",
    "**注意**：目前预测容器仅支持基础模型（非流式）。如果您训练了流式模型，需要下载模型并参考[MoViNet官方指南](https://github.com/tensorflow/models/blob/master/official/projects/movinet/movinet_streaming_model_training_and_inference.ipynb)来在本地运行预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Ry1mw6AHLTy"
   },
   "outputs": [],
   "source": [
    "model_id = \"a0\"  # @param [\"a0\", \"a1\", \"a2\", \"a3\", \"a4\", \"a5\"]\n",
    "model_mode = \"base\"  # @param [\"base\", \"stream\"]\n",
    "is_stream = model_mode == \"stream\"\n",
    "model_name = f\"movinet_{model_id}_{model_mode}\"\n",
    "\n",
    "if is_stream:\n",
    "    export_container_args = {\n",
    "        \"conv_type\": \"2plus1d\",\n",
    "        \"se_type\": \"2plus3d\",\n",
    "        \"activation\": \"hard_swish\",\n",
    "        \"gating_activation\": \"hard_sigmoid\",\n",
    "        \"use_positional_encoding\": model_id in {\"a3\", \"a4\", \"a5\"},\n",
    "    }\n",
    "else:\n",
    "    export_container_args = {\n",
    "        \"conv_type\": \"3d\",\n",
    "        \"se_type\": \"3d\",\n",
    "        \"activation\": \"swish\",\n",
    "        \"gating_activation\": \"sigmoid\",\n",
    "        \"use_positional_encoding\": False,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "### 为训练准备输入数据\n",
    "\n",
    "按照[此处](https://cloud.google.com/vertex-ai/docs/video-data/classification/prepare-data)描述的格式准备数据，然后通过运行下面的代码单元将其转换为训练格式：\n",
    "\n",
    "* `input_file_path`：准备数据的输入文件路径。\n",
    "* `input_file_type`：输入文件类型，如`csv`或`jsonl`。\n",
    "* `output_fps`：视频的采样率；每秒帧数。\n",
    "* `split_ratio`：三个逗号分隔的浮点数，表示要拆分为训练/验证/测试数据的比例。它们的总和必须为1。\n",
    "* `num_shard`：三个逗号分隔的整数，表示用于训练/验证/测试的分片数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IndQ_m6ddUEM"
   },
   "outputs": [],
   "source": [
    "# This job will convert input data as training format, with given split ratios\n",
    "# and number of shards on train/test/validation.\n",
    "\n",
    "data_converter_job_name = get_job_name_with_datetime(\n",
    "    DATA_CONVERTER_JOB_PREFIX + \"_\" + OBJECTIVE\n",
    ")\n",
    "\n",
    "input_file_path = \"\"  # @param {type:\"string\"}\n",
    "input_file_type = \"csv\"  # @param [\"csv\", \"jsonl\"]\n",
    "output_fps = 5  # @param {type:\"integer\"}\n",
    "split_ratio = \"0.8,0.1,0.1\"\n",
    "num_shard = \"10,10,10\"\n",
    "data_converter_output_dir = os.path.join(BUCKET_URI, data_converter_job_name)\n",
    "\n",
    "\n",
    "worker_pool_specs = [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": DATA_CONVERTER_MACHINE_TYPE,\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": DATA_CONVERTER_CONTAINER,\n",
    "            \"command\": [],\n",
    "            \"args\": [\n",
    "                \"--input_file_path=%s\" % input_file_path,\n",
    "                \"--input_file_type=%s\" % input_file_type,\n",
    "                \"--objective=%s\" % OBJECTIVE,\n",
    "                \"--num_shard=%s\" % num_shard,\n",
    "                \"--split_ratio=%s\" % split_ratio,\n",
    "                \"--output_dir=%s\" % data_converter_output_dir,\n",
    "                \"--output_fps=%d\" % output_fps,\n",
    "            ],\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "data_converter_custom_job = aiplatform.CustomJob(\n",
    "    display_name=data_converter_job_name,\n",
    "    project=PROJECT_ID,\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    "    staging_bucket=STAGING_BUCKET,\n",
    ")\n",
    "\n",
    "data_converter_custom_job.run()\n",
    "\n",
    "input_train_data_path = os.path.join(data_converter_output_dir, \"train.tfrecord*\")\n",
    "input_validation_data_path = os.path.join(data_converter_output_dir, \"val.tfrecord*\")\n",
    "label_map_path = os.path.join(data_converter_output_dir, \"label_map.yaml\")\n",
    "print(\"input_train_data_path for training: \", input_train_data_path)\n",
    "print(\"input_validation_data_path for training: \", input_validation_data_path)\n",
    "print(\"label_map_path for prediction: \", label_map_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aaff6f5be7f6"
   },
   "source": [
    "使用Vertex AI SDK创建并运行具有Vertex AI Model Garden训练docker镜像的[超参数调整作业](https://cloud.google.com/vertex-ai/docs/training/hyperparameter-tuning-overview)。\n",
    "\n",
    "#### 定义以下规格\n",
    "\n",
    "* `worker_pool_specs`：一系列指定机器类型和docker镜像的字典。此示例定义一个具有一个`n1-highmem-16`机器和2个`NVIDIA_TESLA_V100` GPU的单节点集群。\n",
    "\n",
    "  **注意**：我们建议为MoViNet-A2和更大模型使用8个GPU。由于加载视频数据需要大量的GPU内存，建议首先尝试小批量大小进行实验。\n",
    "* `parameter_spec`：指定要优化的参数的字典。字典键是您的训练应用代码中每个超参数的命令行参数分配的字符串，字典值是参数规范。参数规范包括超参数的类型、最小/最大值和比例。\n",
    "* `metric_spec`：指定要优化的指标的字典。字典键是您在训练应用代码中设置的`hyperparameter_metric_tag`，值是优化目标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "um_XKbmpTaHx"
   },
   "outputs": [],
   "source": [
    "from google.cloud.aiplatform import hyperparameter_tuning as hpt\n",
    "\n",
    "# Input train and validation datasets can be found from the section above\n",
    "# `Prepare input data for training`.\n",
    "# Or, set prepared datasets paths if already exist.\n",
    "# input_train_data_path = \"\"\n",
    "# input_validation_data_path = \"\"\n",
    "# label_map_path = \"\"\n",
    "\n",
    "train_job_name = get_job_name_with_datetime(f\"{TRAINING_JOB_PREFIX}_{model_name}\")\n",
    "model_dir = os.path.join(BUCKET_URI, train_job_name)\n",
    "label_map, num_classes = get_label_map(label_map_path)\n",
    "\n",
    "# Uploads pretained checkpoint to GCS bucket.\n",
    "init_checkpoint = f\"https://storage.googleapis.com/tf_model_garden/vision/movinet/{model_name}_with_backbone.tar.gz\"\n",
    "init_checkpoint = upload_checkpoint_to_gcs(init_checkpoint)\n",
    "\n",
    "# Uploads config file according to model_id and streaming options.\n",
    "config_file = f\"{model_id}_stream\" if is_stream else model_id\n",
    "config_file = f\"https://raw.githubusercontent.com/tensorflow/models/master/official/projects/movinet/configs/yaml/movinet_{config_file}_gpu.yaml\"\n",
    "config_file = upload_config_to_gcs(config_file)\n",
    "\n",
    "# The parameters here are mainly for demonstration purpose. Please update them\n",
    "# for better performance.\n",
    "trainer_args = {\n",
    "    \"experiment\": \"movinet_kinetics600\",\n",
    "    \"config_file\": config_file,\n",
    "    \"input_train_data_path\": input_train_data_path,\n",
    "    \"input_validation_data_path\": input_validation_data_path,\n",
    "    \"init_checkpoint\": init_checkpoint,\n",
    "    \"model_dir\": model_dir,\n",
    "    \"num_classes\": num_classes,\n",
    "    \"global_batch_size\": 4,\n",
    "    \"prefetch_buffer_size\": 8,\n",
    "    \"shuffle_buffer_size\": 32,\n",
    "    \"train_steps\": 2000,\n",
    "}\n",
    "\n",
    "worker_pool_specs = [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": TRAIN_MACHINE_TYPE,\n",
    "            \"accelerator_type\": TRAIN_ACCELERATOR_TYPE,\n",
    "            # Each training job uses TRAIN_NUM_GPU GPUs.\n",
    "            \"accelerator_count\": TRAIN_NUM_GPU,\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": TRAIN_CONTAINER_URI,\n",
    "            \"args\": [\n",
    "                \"--mode=train_and_eval\",\n",
    "                \"--params_override=runtime.num_gpus=%d\" % TRAIN_NUM_GPU,\n",
    "            ]\n",
    "            + [\"--{}={}\".format(k, v) for k, v in trainer_args.items()],\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "metric_spec = {\"model_performance\": \"maximize\"}\n",
    "\n",
    "# These learning rates might not be optimal for your selected model type; To\n",
    "# tune learning rates, try hpt.DoubleParameterSpec with more trials.\n",
    "LEARNING_RATES = [1e-3, 3e-3]\n",
    "MAX_TRIAL_COUNT = len(LEARNING_RATES)\n",
    "parameter_spec = {\n",
    "    \"learning_rate\": hpt.DiscreteParameterSpec(values=LEARNING_RATES, scale=\"linear\"),\n",
    "}\n",
    "\n",
    "print(worker_pool_specs, metric_spec, parameter_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwcCjwlBTQIz"
   },
   "source": [
    "#### 运行超参数调优作业\n",
    "* `max_trial_count`: 设置服务将运行的试验次数上限。建议的做法是从较小的试验次数开始，了解您选择的超参数对结果的影响程度，然后再逐步增加试验次数。\n",
    "\n",
    "* `parallel_trial_count`: 如果您使用并行试验，服务将为多个训练处理集群提供资源。在创建作业时指定的工作池规范将用于每个独立的训练集群。增加并行试验的数量会减少超参数调优作业运行的时间；但是，这可能会降低整体作业的效果。这是因为默认的调优策略使用先前试验的结果来指导后续试验中值的指定。\n",
    "\n",
    "* `search_algorithm`: 可用的搜索算法有网格算法、随机算法或默认算法（None）。默认选项应用贝叶斯优化来搜索可能超参数值的空间，并且是推荐的算法。\n",
    "\n",
    "单击输出中生成的链接以在Cloud Console中查看您的运行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aec22792ee84"
   },
   "outputs": [],
   "source": [
    "train_custom_job = aiplatform.CustomJob(\n",
    "    display_name=train_job_name,\n",
    "    project=PROJECT_ID,\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    "    staging_bucket=STAGING_BUCKET,\n",
    ")\n",
    "\n",
    "train_hpt_job = aiplatform.HyperparameterTuningJob(\n",
    "    display_name=train_job_name,\n",
    "    custom_job=train_custom_job,\n",
    "    metric_spec=metric_spec,\n",
    "    parameter_spec=parameter_spec,\n",
    "    max_trial_count=MAX_TRIAL_COUNT,\n",
    "    parallel_trial_count=MAX_TRIAL_COUNT,\n",
    "    project=PROJECT_ID,\n",
    "    search_algorithm=None,\n",
    ")\n",
    "\n",
    "train_hpt_job.run()\n",
    "\n",
    "print(\"model_dir is:\", model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vugUfJEC2HrK"
   },
   "source": [
    "### 以Tensorflow SavedModel格式导出模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "09Rz1AYspK19"
   },
   "outputs": [],
   "source": [
    "# This job will export models from TF checkpoints to TF saved model format.\n",
    "# model_dir is from the section above.\n",
    "best_trial_dir, best_trial_evaluation_results = get_best_trial(\n",
    "    model_dir, MAX_TRIAL_COUNT, EVALUATION_METRIC\n",
    ")\n",
    "best_checkpoint_path = find_checkpoint_in_dir(f\"{best_trial_dir}/best_ckpt/\")\n",
    "print(\"best_trial_dir: \", best_trial_dir)\n",
    "print(\"best_trial_evaluation_results: \", best_trial_evaluation_results)\n",
    "print(\"best_checkpoint: \", best_checkpoint_path)\n",
    "\n",
    "container_args = {\n",
    "    \"export_path\": f\"{model_dir}/best_model\",\n",
    "    \"model_id\": model_id,\n",
    "    \"num_classes\": num_classes,\n",
    "    \"causal\": is_stream,\n",
    "    \"checkpoint_path\": best_checkpoint_path,\n",
    "    \"assert_checkpoint_objects_matched\": False,\n",
    "    **export_container_args,\n",
    "}\n",
    "\n",
    "worker_pool_specs = [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": EXPORT_MACHINE_TYPE,\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": EXPORT_CONTAINER_URI,\n",
    "            \"args\": [\"--{}={}\".format(k, v) for k, v in container_args.items()],\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "model_export_job_name = get_job_name_with_datetime(EXPORT_JOB_PREFIX + \"_\" + OBJECTIVE)\n",
    "model_export_custom_job = aiplatform.CustomJob(\n",
    "    display_name=model_export_job_name,\n",
    "    project=PROJECT_ID,\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    "    staging_bucket=STAGING_BUCKET,\n",
    ")\n",
    "\n",
    "model_export_custom_job.run()\n",
    "\n",
    "print(\"best model is saved to: \", container_args[\"export_path\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0BGaofgsMsy"
   },
   "source": [
    "## 测试训练好的模型\n",
    "本节展示了如何使用训练好的模型进行测试。\n",
    "1. 将模型上传并部署到[Vertex AI模型注册表](https://cloud.google.com/vertex-ai/docs/model-registry/introduction)\n",
    "2. 运行批量预测\n",
    "\n",
    "**注意：** 预测容器仅适用于基本模型。如果您训练了一个流模型，请下载模型并参考[MoViNet官方指南](https://github.com/tensorflow/models/blob/master/official/projects/movinet/movinet_streaming_model_training_and_inference.ipynb)来在本地进行预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gdlca3BOypXU"
   },
   "source": [
    "### 上传模型到Vertex AI模型注册表\n",
    "\n",
    "以下单元格将训练好的模型上传到Vertex AI模型注册表。如果您想在已上传的模型上运行批处理预测，则可以跳过此步骤。\n",
    "\n",
    "#### 可配置的环境变量\n",
    "\n",
    "* `MODEL_PATH`：MoViNet模型的Cloud Storage URI。\n",
    "* `BATCH_SIZE`：推理批处理大小。使用较大的值可以加速GPU预测。\n",
    "* `NUM_FRAMES`：使用该模型进行单次预测时的帧数。\n",
    "* `FPS`：每秒视频采样帧数。\n",
    "* `OVERLAP_FRAMES`：允许连续预测窗口之间重叠帧数。设置较小的值可加快推理速度，但精度较低。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NYuQowyZEtxK"
   },
   "outputs": [],
   "source": [
    "serving_env = {\n",
    "    \"MODEL_ID\": \"tfvision-movinet-vcn\",\n",
    "    \"MODEL_PATH\": container_args[\"export_path\"],\n",
    "    \"BATCH_SIZE\": 1,  # Select a larger batch size to accelerate GPU prediction.\n",
    "    \"NUM_FRAMES\": 32,\n",
    "    \"FPS\": output_fps,\n",
    "    \"OVERLAP_FRAMES\": 24,\n",
    "    \"OBJECTIVE\": OBJECTIVE,\n",
    "    \"DEPLOY_SOURCE\": \"notebook\",\n",
    "}\n",
    "\n",
    "model = aiplatform.Model.upload(\n",
    "    display_name=model_name,\n",
    "    serving_container_image_uri=PREDICTION_CONTAINER_URI,\n",
    "    serving_container_ports=[PREDICTION_PORT],\n",
    "    serving_container_predict_route=\"/predict\",\n",
    "    serving_container_health_route=\"/ping\",\n",
    "    serving_container_environment_variables=serving_env,\n",
    ")\n",
    "\n",
    "model.wait()\n",
    "\n",
    "print(\"The uploaded model name is: \", model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2b47e629a01"
   },
   "source": [
    "或者，取消以下单元格的注释以使用已上传的模型。将模型名称字符串替换为现有模型的名称。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aa503a565b8f"
   },
   "outputs": [],
   "source": [
    "# model = aiplatform.Model(\"projects/123456789/locations/us-central1/models/12345678901234567890\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9SZsKGeS3x6S"
   },
   "source": [
    "### 运行批量预测\n",
    "\n",
    "我们现在将使用训练好的MoViNet剪辑分类模型进行批量预测，使用[Vertex AI 批量预测](https://cloud.google.com/vertex-ai/docs/predictions/get-batch-predictions)。\n",
    "\n",
    "请准备一个输入的JSONL文件，每行遵循[这种格式](https://cloud.google.com/vertex-ai/docs/video-data/classification/get-predictions?hl=en#input_data_requirements)，并将其存储在Cloud Storage存储桶中。该服务帐户应具有访问包含训练模型和输入数据的存储桶的读取权限。有关更多信息，请参阅[服务帐户概述](https://cloud.google.com/iam/docs/service-account-overview)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vbIW9me1F2RY"
   },
   "outputs": [],
   "source": [
    "# Path to the prediction input JSONL file.\n",
    "test_jsonl_path = \"\"  # @param {type:\"string\"}\n",
    "# Full service account name with the suffix `gserviceaccount.com`.\n",
    "batch_predict_service_account = \"\"  # @param {type:\"string\"}\n",
    "\n",
    "predict_job_name = get_job_name_with_datetime(f\"{PREDICTION_JOB_PREFIX}_{model_name}\")\n",
    "predict_destination_prefix = os.path.join(STAGING_BUCKET, predict_job_name)\n",
    "\n",
    "batch_prediction_job = model.batch_predict(\n",
    "    job_display_name=predict_job_name,\n",
    "    gcs_source=test_jsonl_path,\n",
    "    gcs_destination_prefix=predict_destination_prefix,\n",
    "    machine_type=PREDICTION_MACHINE_TYPE,\n",
    "    accelerator_count=PREDICTION_ACCELERATOR_COUNT,\n",
    "    accelerator_type=PREDICTION_ACCELERATOR_TYPE,\n",
    "    max_replica_count=1,\n",
    "    service_account=batch_predict_service_account,\n",
    ")\n",
    "\n",
    "batch_prediction_job.wait()\n",
    "\n",
    "print(batch_prediction_job.display_name)\n",
    "print(batch_prediction_job.resource_name)\n",
    "print(batch_prediction_job.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ik-XPjfx9OCE"
   },
   "source": [
    "您可以在输出目录中读取预测响应的JSONL文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tdkW9e5B9OU1"
   },
   "outputs": [],
   "source": [
    "# The label map file was generated from the section above (`Prepare input data for training`).\n",
    "for file in tf.io.gfile.glob(os.path.join(predict_destination_prefix, \"*/*\")):\n",
    "    with tf.io.gfile.GFile(file, \"r\") as f:\n",
    "        for line in f:\n",
    "            print_response_instance(line, label_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kkH2nrpdp4sp"
   },
   "source": [
    "清理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ax6vQVZhp9pR"
   },
   "outputs": [],
   "source": [
    "# Delete the trained model.\n",
    "model.delete()\n",
    "# Delete custom and hpt jobs.\n",
    "if data_converter_custom_job.list(filter=f'display_name=\"{data_converter_job_name}\"'):\n",
    "    data_converter_custom_job.delete()\n",
    "if train_hpt_job.list(filter=f'display_name=\"{train_job_name}\"'):\n",
    "    train_hpt_job.delete()\n",
    "if model_export_custom_job.list(filter=f'display_name=\"{model_export_job_name}\"'):\n",
    "    model_export_custom_job.delete()\n",
    "if batch_prediction_job.list(filter=f'display_name=\"{predict_job_name}\"'):\n",
    "    batch_prediction_job.delete()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "model_garden_movinet_clip_classification.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
