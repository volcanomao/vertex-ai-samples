{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7d9bbf86da5e"
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99c1c3fc2ca5"
   },
   "source": [
    "# Vertex AI模型花园 - 綿羊模型\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_vicuna.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> 在Colab中运行\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_vicuna.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      在GitHub上查看\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/notebooks/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/model_garden/model_garden_pytorch_vicuna.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "在Vertex AI Workbench中打开\n",
    "    </a>（建议使用Python-3 GPU笔记本）\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3de7470326a2"
   },
   "source": [
    "## 概述\n",
    "\n",
    "本笔记本演示了如何使用预先构建的Vicuna进行本地推断，并使用[vLLM](https://github.com/vllm-project/vllm)部署预先构建的Vicuna。\n",
    "\n",
    "### 目标\n",
    "\n",
    "- 使用预先构建的Vicuna进行本地推断\n",
    "- 部署预建的Vicuna与[vLLM](https://github.com/vllm-project/vllm)以提高服务吞吐量\n",
    "\n",
    "| 模型 |\n",
    "| :- |\n",
    "| [lmsys/vicuna-7b-v1.5](https://huggingface.co/lmsys/vicuna-7b-v1.5) |\n",
    "| [lmsys/vicuna-7b-v1.5-16k](https://huggingface.co/lmsys/vicuna-7b-v1.5-16k) |\n",
    "| [lmsys/vicuna-13b-v1.5](https://huggingface.co/lmsys/vicuna-13b-v1.5) |\n",
    "| [lmsys/vicuna-13b-v1.5-16k](https://huggingface.co/lmsys/vicuna-13b-v1.5-16k) |\n",
    "\n",
    "### 成本\n",
    "\n",
    "本教程使用Google Cloud的计费组件：\n",
    "\n",
    "* Vertex AI\n",
    "* 云存储\n",
    "\n",
    "了解有关[Vertex AI定价](https://cloud.google.com/vertex-ai/pricing)和[云存储定价](https://cloud.google.com/storage/pricing)的信息，并使用[价格计算器](https://cloud.google.com/products/calculator/)根据您的预期使用量生成费用估计。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "264c07757582"
   },
   "source": [
    "在开始之前\n",
    "\n",
    "**注意**: Jupyter使用以`!`开头的行作为shell命令，并将以`$`开头的Python变量插入这些命令中。\n",
    "\n",
    "使用Vicuna进行本地推理需要GPU。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ioensNKM8ned"
   },
   "source": [
    "只限于Colab\n",
    "如果您正在使用工作台，请运行以下命令并跳过此部分。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2707b02ef5df"
   },
   "outputs": [],
   "source": [
    "! pip3 install transformers==4.31.0\n",
    "! pip3 install sentencepiece==0.1.99\n",
    "! pip3 install accelerate==0.21.0\n",
    "\n",
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    ! pip3 install --upgrade google-cloud-aiplatform\n",
    "    from google.colab import auth as google_auth\n",
    "\n",
    "    google_auth.authenticate_user()\n",
    "\n",
    "    # Restart the notebook kernel after installs.\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bb7adab99e41"
   },
   "source": [
    "### 设置 Google 云项目\n",
    "\n",
    "1. [选择或创建一个Google云项目](https://console.cloud.google.com/cloud-resource-manager)。当您首次创建一个账户时，您将获得300美元的免费信用额用于您的计算/存储成本。\n",
    "\n",
    "2. [确保为您的项目启用了计费](https://cloud.google.com/billing/docs/how-to/modify-project)。\n",
    "\n",
    "3. [启用Vertex AI API和Compute Engine API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,compute_component)。\n",
    "\n",
    "4. [创建一个云存储存储桶](https://cloud.google.com/storage/docs/creating-buckets) 用于存储实验输出。\n",
    "\n",
    "5. [创建一个服务账号](https://cloud.google.com/iam/docs/service-accounts-create#iam-service-accounts-create-console) 并为其分配 `Vertex AI User` 和 `Storage Object Admin` 角色，以部署微调模型到 Vertex AI 端点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6c460088b873"
   },
   "source": [
    "为实验环境设置以下变量。指定的云存储桶(`BUCKET_URI`)应位于指定的区域(`REGION`)。请注意，多区域桶（如\"us\"）不被视为与多区域范围（如\"us-central1\"）覆盖的单一区域相匹配。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "855d6b96f291"
   },
   "outputs": [],
   "source": [
    "# Cloud project id.\n",
    "PROJECT_ID = \"\"  # @param {type:\"string\"}\n",
    "\n",
    "# The region you want to launch jobs in.\n",
    "REGION = \"\"  # @param {type:\"string\"}\n",
    "\n",
    "# The Cloud Storage bucket for storing experiments output.\n",
    "# Start with gs:// prefix, e.g. gs://foo_bucket.\n",
    "BUCKET_URI = \"gs://\"  # @param {type:\"string\"}\n",
    "\n",
    "! gcloud config set project $PROJECT_ID\n",
    "\n",
    "import os\n",
    "\n",
    "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
    "\n",
    "# The service account looks like:\n",
    "# '@.iam.gserviceaccount.com'\n",
    "# Please go to https://cloud.google.com/iam/docs/service-accounts-create#iam-service-accounts-create-console\n",
    "# and create service account with `Vertex AI User` and `Storage Object Admin` roles.\n",
    "# The service account for deploying fine tuned model.\n",
    "SERVICE_ACCOUNT = \"\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e828eb320337"
   },
   "source": [
    "### 初始化 Vertex AI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "12cd25839741"
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cc825514deb"
   },
   "source": [
    "### 定义常数."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b42bd4fa2b2d"
   },
   "outputs": [],
   "source": [
    "# The pre-built serving docker image.\n",
    "VLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20231002_0916_RC00\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0c250872074f"
   },
   "source": [
    "### 定义常见函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "354da31189dc"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "\n",
    "def get_job_name_with_datetime(prefix: str) -> str:\n",
    "    \"\"\"Gets the job name with date time when triggering training or deployment\n",
    "    jobs in Vertex AI.\n",
    "    \"\"\"\n",
    "    return prefix + datetime.now().strftime(\"_%Y%m%d_%H%M%S\")\n",
    "\n",
    "\n",
    "def deploy_model_vllm(\n",
    "    model_name: str,\n",
    "    model_id: str,\n",
    "    service_account: str,\n",
    "    machine_type: str = \"n1-standard-8\",\n",
    "    accelerator_type: str = \"NVIDIA_TESLA_V100\",\n",
    "    accelerator_count: int = 1,\n",
    "    max_model_len: int = 4000,\n",
    ") -> tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
    "    \"\"\"Deploys trained models with vLLM into Vertex AI.\"\"\"\n",
    "    endpoint = aiplatform.Endpoint.create(display_name=f\"{model_name}-endpoint\")\n",
    "\n",
    "    vllm_args = [\n",
    "        \"--host=0.0.0.0\",\n",
    "        \"--port=7080\",\n",
    "        f\"--model={model_id}\",\n",
    "        f\"--tensor-parallel-size={accelerator_count}\",\n",
    "        \"--swap-space=16\",\n",
    "        \"--gpu-memory-utilization=0.90\",\n",
    "        \"--disable-log-stats\",\n",
    "        f\"--max-model-len={max_model_len}\",\n",
    "    ]\n",
    "    serving_env = {\n",
    "        \"MODEL_ID\": \"lmsys/vicuna\",\n",
    "        \"DEPLOY_SOURCE\": \"notebook\",\n",
    "    }\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=model_name,\n",
    "        serving_container_image_uri=VLLM_DOCKER_URI,\n",
    "        serving_container_command=[\"python\", \"-m\", \"vllm.entrypoints.api_server\"],\n",
    "        serving_container_args=vllm_args,\n",
    "        serving_container_ports=[7080],\n",
    "        serving_container_predict_route=\"/generate\",\n",
    "        serving_container_health_route=\"/ping\",\n",
    "        serving_container_environment_variables=serving_env,\n",
    "    )\n",
    "\n",
    "    model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        machine_type=machine_type,\n",
    "        accelerator_type=accelerator_type,\n",
    "        accelerator_count=accelerator_count,\n",
    "        deploy_request_timeout=1800,\n",
    "        service_account=service_account,\n",
    "    )\n",
    "    return model, endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "65eaa62632d1"
   },
   "source": [
    "使用预先构建的维库纳在本地进行推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "339601a9500b"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "prebuilt_model_id = \"lmsys/vicuna-7b-v1.5\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    prebuilt_model_id,\n",
    "    use_fast=True,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    prebuilt_model_id,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"cuda\",\n",
    ")\n",
    "\n",
    "prompt = \"Q: What is the largest animal?\\nA:\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "input_ids = input_ids.to(\"cuda\")\n",
    "generation_output = model.generate(input_ids=input_ids, max_new_tokens=32)\n",
    "print(tokenizer.decode(generation_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V7VOhhHGpUrj"
   },
   "source": [
    "使用vLLM部署预构建的维库纳模型\n",
    "\n",
    "本节在终端点上使用[vLLM](https://github.com/vllm-project/vllm)部署预构建的维库纳模型。模型部署步骤将需要大约15分钟完成。\n",
    "\n",
    "vLLM是一个高度优化的LLM服务框架，可以显著提高服务吞吐量。您的QPS越高，使用vLLM获得的改进越多。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4GTNnnuYqrW_"
   },
   "source": [
    "设置预构建的模型ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kLsRoc4Kqrkx"
   },
   "outputs": [],
   "source": [
    "prebuilt_model_id = \"lmsys/vicuna-7b-v1.5\"  # @param [\"lmsys/vicuna-7b-v1.5\", \"lmsys/vicuna-7b-v1.5-16k\", \"lmsys/vicuna-13b-v1.5\", \"lmsys/vicuna-13b-v1.5-16k\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YI0vaDi6p2fi"
   },
   "outputs": [],
   "source": [
    "# Find Vertex AI supported accelerators and regions in:\n",
    "#  https://cloud.google.com/vertex-ai/docs/predictions/configure-compute\n",
    "\n",
    "# Set max_model_len to the desired context length.\n",
    "max_model_len = 2000\n",
    "\n",
    "# Sets V100s/A100 to deploy Vicuna models.\n",
    "machine_type = \"n1-highmem-8\"\n",
    "accelerator_type = \"NVIDIA_TESLA_V100\"\n",
    "accelerator_count = 1  # for lmsys/vicuna-7b-v1.5\n",
    "\n",
    "# machine_type = \"n1-highmem-16\"\n",
    "# accelerator_type = \"NVIDIA_TESLA_V100\"\n",
    "# accelerator_count = 2  # for lmsys/vicuna-13b-v1.5\n",
    "\n",
    "# machine_type = \"a2-highgpu-1g\"\n",
    "# accelerator_type = \"NVIDIA_TESLA_A100\"\n",
    "# accelerator_count = 1  # for lmsys/vicuna-7b-v1.5-16k, lmsys/vicuna-13b-v1.5-16k\n",
    "# max_model_len = 16000\n",
    "\n",
    "model, endpoint = deploy_model_vllm(\n",
    "    model_name=get_job_name_with_datetime(prefix=\"vicuna-serve-vllm\"),\n",
    "    model_id=prebuilt_model_id,\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    machine_type=machine_type,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    "    max_model_len=max_model_len,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWYmYWoqqBuZ"
   },
   "source": [
    "请注意：在部署成功后，预建模型权重将从原始位置动态下载。因此，在上述模型部署步骤成功之后，需要额外等待5分钟**才能**运行下面的下一步骤。否则，当您发送请求到端点时，可能会看到 `ServiceUnavailable: 503 502:Bad Gateway` 错误。\n",
    "\n",
    "一旦部署成功，您就可以使用文本提示向端点发送请求。vLLM支持的参数可以在[这里](https://github.com/vllm-project/vllm/blob/2e8e49fce3775e7704d413b2f02da6d7c99525c9/vllm/sampling_params.py#L23-L64)找到。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fjO4z3qAp3pK"
   },
   "outputs": [],
   "source": [
    "instance = {\n",
    "    \"prompt\": \"Q: What is the tallest animal?\\nA:\",\n",
    "    \"n\": 1,\n",
    "    \"max_tokens\": 100,\n",
    "    \"temperature\": 1.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"top_k\": 10,\n",
    "}\n",
    "response = endpoint.predict(instances=[instance])\n",
    "print(response.predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "af21a3cff1e0"
   },
   "source": [
    "清理资源"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "911406c1561e"
   },
   "outputs": [],
   "source": [
    "# Undeploy models and delete endpoints.\n",
    "endpoint.delete(force=True)\n",
    "\n",
    "# Delete models.\n",
    "model.delete()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "model_garden_pytorch_vicuna.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
