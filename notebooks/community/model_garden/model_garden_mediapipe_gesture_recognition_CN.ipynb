{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TirJ-SGQseby"
   },
   "source": [
    "# 使用手势识别的 Vertex AI 模型花园 MediaPipe\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_mediapipe_gesture_recognition.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> 在 Colab 中运行\n",
    "    </a>\n",
    "  </td>\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_mediapipe_gesture_recognition.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      在 GitHub 上查看\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>                                                                                               <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/notebooks/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/model_garden/model_garden_mediapipe_gesture_recognition.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "在 Vertex AI Workbench 中打开\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dwGLvtIeECLK"
   },
   "source": [
    "注意：这个笔记本在以下环境中进行了测试：\n",
    "\n",
    "* Python版本 = 3.9\n",
    "\n",
    "注意：此Colab中链接的检查点和数据集不是由谷歌拥有或分发的，而是由第三方提供。在使用检查点和数据之前，请查阅第三方提供的条款和条件。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## 概述\n",
    "\n",
    "本笔记本演示了如何使用[MediaPipe Model Maker](https://developers.google.com/mediapipe/solutions/model_maker)在Vertex AI Model Garden中训练一个设备上的手势识别模型。\n",
    "\n",
    "### 目标\n",
    "\n",
    "* 训练新模型\n",
    "  * 将输入数据转换为训练格式\n",
    "  * 创建[自定义作业](https://cloud.google.com/vertex-ai/docs/training/create-custom-job)来训练新模型\n",
    "  * 导出模型\n",
    "\n",
    "* 清理资源\n",
    "\n",
    "### 成本\n",
    "\n",
    "本教程使用Google Cloud的计费组件：\n",
    "\n",
    "* Vertex AI\n",
    "* 云存储\n",
    "\n",
    "了解[Vertex AI定价](https://cloud.google.com/vertex-ai/pricing)和[云存储定价](https://cloud.google.com/storage/pricing)，并使用[Pricing Calculator](https://cloud.google.com/products/calculator/)基于您的预期使用量生成成本估算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KEukV6uRk_S3"
   },
   "source": [
    "在你开始之前"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z__i0w0lCAsW"
   },
   "source": [
    "只有 Colab 上才能运行\n",
    "运行以下命令来安装依赖项，并在 Colab 上运行时进行 Google Cloud 身份验证。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jvqs-ehKlaYh"
   },
   "outputs": [],
   "source": [
    "! pip3 install --upgrade pip\n",
    "\n",
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    ! pip3 install --upgrade google-cloud-aiplatform\n",
    "\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)\n",
    "\n",
    "    from google.colab import auth as google_auth\n",
    "\n",
    "    google_auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WReHDGG5g0XY"
   },
   "source": [
    "#### 设置您的项目ID\n",
    "\n",
    "**如果您不知道您的项目ID**，请参考支持页面：[查找项目ID](https://support.google.com/googleapi/answer/7014113)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oM1iC_MfAts1"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
    "\n",
    "# Set the project id\n",
    "! gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "#### 区域\n",
    "\n",
    "您也可以更改 Vertex AI 使用的 `REGION` 变量。 了解有关 [Vertex AI 区域](https://cloud.google.com/vertex-ai/docs/general/locations) 的更多信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tTy1gX11kCJY"
   },
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"  # @param {type: \"string\"}\n",
    "REGION_PREFIX = REGION.split(\"-\")[0]\n",
    "assert REGION_PREFIX in (\n",
    "    \"us\",\n",
    "    \"europe\",\n",
    "    \"asia\",\n",
    "), f'{REGION} is not supported. It must be prefixed by \"us\", \"asia\", or \"europe\".'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "创建一个云存储桶\n",
    "\n",
    "创建一个存储桶来存储中间产物，如数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "BUCKET_URI = f\"gs://your-bucket-name-{PROJECT_ID}-unique\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EcIXiGsCePi"
   },
   "source": [
    "只有当您的存储桶尚不存在时：运行以下单元格以创建您的云存储存储桶。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIq7R4HZCfIc"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l {REGION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "960505627ddf"
   },
   "source": [
    "### 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PyQmSRbKA8r-"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow\n",
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "source": [
    "### 初始化 Python 的 Vertex AI SDK\n",
    "\n",
    "为您的项目初始化 Python 的 Vertex AI SDK。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9wExiMUxFk91"
   },
   "outputs": [],
   "source": [
    "now = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temp/%s\" % now)\n",
    "\n",
    "EVALUATION_RESULT_OUTPUT_DIRECTORY = os.path.join(STAGING_BUCKET, \"evaluation\")\n",
    "EVALUATION_RESULT_OUTPUT_FILE = os.path.join(\n",
    "    EVALUATION_RESULT_OUTPUT_DIRECTORY, \"evaluation.json\"\n",
    ")\n",
    "\n",
    "EXPORTED_MODEL_OUTPUT_DIRECTORY = os.path.join(STAGING_BUCKET, \"model\")\n",
    "EXPORTED_MODEL_OUTPUT_FILE = os.path.join(\n",
    "    EXPORTED_MODEL_OUTPUT_DIRECTORY, \"gesture_recognizer.task\"\n",
    ")\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n6IFz75WGCam"
   },
   "source": [
    "定义培训机器规格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "riG_qUokg0XZ"
   },
   "outputs": [],
   "source": [
    "TRAINING_JOB_DISPLAY_NAME = \"mediapipe_gesture_recognizer_%s\" % now\n",
    "TRAINING_CONTAINER = f\"{REGION_PREFIX}-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/mediapipe-train\"\n",
    "TRAINING_MACHINE_TYPE = \"n1-highmem-16\"\n",
    "TRAINING_ACCELERATOR_TYPE = \"NVIDIA_TESLA_V100\"\n",
    "TRAINING_ACCELERATOR_COUNT = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-rsdAcBV-vlf"
   },
   "source": [
    "训练您定制的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "### 为训练准备输入数据\n",
    "\n",
    "要为手势识别调整模型，需要一个遵循以下模式的目录结构数据集：`<数据集路径>/<标签名>/<图片名>.*`（例如 `my_custom_dataset/thumbs_up/img12.jpg`）。另外，其中一个标签名必须是 none。none 标签代表任何不属于其他手势分类的手势。\n",
    "\n",
    "本例使用了一个可在云存储中找到的石头剪刀布数据集样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IndQ_m6ddUEM"
   },
   "outputs": [],
   "source": [
    "training_data_path = (\n",
    "    \"gs://mediapipe-tasks/gesture_recognizer/rps_data_sample\"  # @param {type:\"string\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ci4IV6vdXRMD"
   },
   "source": [
    "当模型生成器加载数据集时，它会运行MediaPipe Hands中预先打包的手部检测模型，以检测图像中的手部地标。任何没有检测到手的图像将从数据集中省略。最终数据集将包含从每个图像中提取的手部地标位置，而不是图像本身。\n",
    "\n",
    "您可以配置一些选项来确定如何加载数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aNHLSyFtXP7I"
   },
   "outputs": [],
   "source": [
    "# A boolean controlling whether to shuffle the dataset. Defaults to true.\n",
    "shuffle = True  # @param {type:\"boolean\"}\n",
    "# A float between 0 and 1 controlling the confidence threshold for hand detection\n",
    "min_detection_confidence = 0.6  # @param {type:\"number\"}\n",
    "# Configures how to split the dataset between training, validation and test data. Must sum to up 1.\n",
    "split_ratio = \"0.8,0.1,0.1\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aaff6f5be7f6"
   },
   "source": [
    "### 设置微调选项\n",
    "\n",
    "您可以通过指定ModelOptions和HParams来自定义模型。ModelOptions包含与模型本身相关的参数，而HParams包含与训练和保存模型相关的参数。\n",
    "\n",
    "ModelOptions包含这些可定制的参数，这些参数会影响准确性："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bDxsEaoGcibW"
   },
   "outputs": [],
   "source": [
    "# The fraction of the input units to drop. Used in dropout layer.\n",
    "dropout_rate: float = 0.05  # @param {type:\"number\"}\n",
    "# A list of hidden layer widths for the gesture model. Each element\n",
    "# in the list will create a new hidden layer with the specified width.\n",
    "# The hidden layers are separated with BatchNorm, Dropout, and ReLU.\n",
    "layer_widths: str = \"\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fk0TTZbDdJPX"
   },
   "source": [
    "HParams具有以下可自定义参数列表，影响模型精度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "um_XKbmpTaHx"
   },
   "outputs": [],
   "source": [
    "# The learning rate to use for gradient descent training.\n",
    "learning_rate: float = 0.001  # @param {type:\"number\"}\n",
    "# Batch size for training.\n",
    "batch_size: int = 2  # @param {type:\"number\"}\n",
    "# Number of training iterations over the dataset.\n",
    "epochs: int = 10  # @param {type:\"slider\", min:0, max:100, step:1}\n",
    "# An optional integer that indicates the number of training steps per\n",
    "# epoch. If set to 0, the training pipeline calculates the default\n",
    "# steps per epoch as the training dataset size divided by batch size.\n",
    "steps_per_epoch: int = 0  # @param {type:\"number\"}\n",
    "# Whether to shuffle the dataset before training\n",
    "shuffle: bool = False  # @param {type:\"boolean\"}\n",
    "# Learning rate decay to use for gradient descent training.\n",
    "lr_decay: float = 0.99  # @param {type:\"number\"}\n",
    "# Gamma parameter for focal loss. Defaults to 2\n",
    "gamma: float = 2  # @param {type:\"number\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwcCjwlBTQIz"
   },
   "source": [
    "### 运行微调\n",
    "准备好您的训练数据集和微调选项后，您就可以开始微调过程了。这个过程需要大量资源，可能需要几分钟才能完成。在Vertex AI上使用GPU处理时，下面的微调示例大约需要1-2分钟来对大约500张图片进行训练。\n",
    "\n",
    "要开始微调过程，请使用以下代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aec22792ee84"
   },
   "outputs": [],
   "source": [
    "model_export_path = EXPORTED_MODEL_OUTPUT_DIRECTORY\n",
    "evaluation_result_path = EVALUATION_RESULT_OUTPUT_DIRECTORY\n",
    "\n",
    "model_options = {\"dropout_rate\": dropout_rate}\n",
    "if layer_widths:\n",
    "    model_options[\"layer_widths\"] = layer_widths\n",
    "\n",
    "hparams = {\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"epochs\": epochs,\n",
    "    \"shuffle\": shuffle,\n",
    "    \"lr_decay\": lr_decay,\n",
    "    \"gamma\": gamma,\n",
    "}\n",
    "if steps_per_epoch:\n",
    "    hparams[\"steps_per_epoch\"] = steps_per_epoch\n",
    "\n",
    "worker_pool_specs = [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": TRAINING_MACHINE_TYPE,\n",
    "            \"accelerator_type\": TRAINING_ACCELERATOR_TYPE,\n",
    "            \"accelerator_count\": TRAINING_ACCELERATOR_COUNT,\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": TRAINING_CONTAINER,\n",
    "            \"command\": [],\n",
    "            \"args\": [\n",
    "                \"--task_name=gesture_recognizer\",\n",
    "                \"--training_data_path=%s\" % training_data_path,\n",
    "                \"--model_export_path=%s\" % model_export_path,\n",
    "                \"--evaluation_result_path=%s\" % evaluation_result_path,\n",
    "                \"--split_ratio=%s\" % split_ratio,\n",
    "                \"--model_options=%s\" % json.dumps(model_options),\n",
    "                \"--hparams=%s\" % json.dumps(hparams),\n",
    "            ],\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "training_job = aiplatform.CustomJob(\n",
    "    display_name=TRAINING_JOB_DISPLAY_NAME,\n",
    "    project=PROJECT_ID,\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    "    staging_bucket=STAGING_BUCKET,\n",
    ")\n",
    "\n",
    "training_job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXMF2tnV_WS0"
   },
   "source": [
    "评估和导出模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mV-Djz-frBni"
   },
   "source": [
    "### 评估性能\n",
    "\n",
    "在微调模型之后，我们对测试数据集上的训练结果进行评估，通常这是您在训练过程中未使用的原始数据集的一部分。一般认为准确率在0.8至0.9之间是很好的，但是您的用例需求可能会有所不同。您还应该考虑模型能够产生推断的速度。更高的准确率通常会以较长的推断时间为代价。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "09Rz1AYspK19"
   },
   "outputs": [],
   "source": [
    "def get_evaluation_result(evaluation_result_path):\n",
    "    try:\n",
    "        with tensorflow.io.gfile.GFile(evaluation_result_path, \"r\") as input_file:\n",
    "            evalutation_result = json.loads(input_file.read())\n",
    "        return evalutation_result[\"accuracy\"], evalutation_result[\"loss\"]\n",
    "    except:\n",
    "        print(\n",
    "            \"Evaluation result not found. Your test dataset is likely \"\n",
    "            + \"empty. You can adjust the size of your test dataset or adjust \"\n",
    "            + \"how you split your dataset.\"\n",
    "        )\n",
    "        return None\n",
    "\n",
    "\n",
    "evaluation_result = get_evaluation_result(EVALUATION_RESULT_OUTPUT_FILE)\n",
    "\n",
    "if evaluation_result is not None:\n",
    "    print(\"Accuracy:\", evaluation_result[0])\n",
    "    print(\"Loss:\", evaluation_result[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0BGaofgsMsy"
   },
   "source": [
    "导出模型\n",
    "在微调和评估模型之后，您可以保存Tensorflow Lite模型，可以在MediaPipe Studio的[手势识别器](https://mediapipe-studio.webapps.google.com/demo/gesture_recognizer)演示中尝试，或者按照[手势识别器任务指南](https://developers.google.com/mediapipe/solutions/vision/gesture_recognizer)将其集成到您的设备应用程序中。导出的模型包含所需的模型元数据，以及分类标签文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NYuQowyZEtxK"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "def copy_model(model_source, model_dest):\n",
    "    ! gsutil cp {model_source} {model_dest}\n",
    "\n",
    "copy_model(EXPORTED_MODEL_OUTPUT_FILE, \"gesture_recognizer.task\")\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import files\n",
    "\n",
    "    files.download(\"gesture_recognizer.task\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kkH2nrpdp4sp"
   },
   "source": [
    "清理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ax6vQVZhp9pR"
   },
   "outputs": [],
   "source": [
    "# Delete training data and jobs.\n",
    "if training_job.list(filter=f'display_name=\"{TRAINING_JOB_DISPLAY_NAME}\"'):\n",
    "    training_job.delete()\n",
    "\n",
    "!gsutil rm -r {STAGING_BUCKET}"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "model_garden_mediapipe_gesture_recognition.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
