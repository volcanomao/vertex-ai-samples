{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TirJ-SGQseby"
   },
   "source": [
    "# Vertex AI 模型花园 TFVision 图像目标检测\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_tfvision_image_object_detection.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> 在 Colab 中运行\n",
    "    </a>\n",
    "  </td>\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_tfvision_image_object_detection.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      在 GitHub 上查看\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/notebooks/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/model_garden/model_garden_tfvision_image_object_detection.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "在 Vertex AI 工作台中打开\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dwGLvtIeECLK"
   },
   "source": [
    "注意：此笔记本已在以下环境中进行了测试：\n",
    "\n",
    "- Python 版本 = 3.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## 概览\n",
    "\n",
    "本笔记本演示了如何在Vertex AI模型花园中使用[TFVision](https://github.com/tensorflow/models/blob/master/official/vision/MODEL_GARDEN.md)。\n",
    "\n",
    "### 目标\n",
    "\n",
    "* 训练新模型\n",
    "  * 将输入数据转换为训练格式\n",
    "  * 创建[超参数调整作业](https://cloud.google.com/vertex-ai/docs/training/hyperparameter-tuning-overview)来训练新模型\n",
    "  * 查找并导出最佳模型\n",
    "\n",
    "* 测试训练模型\n",
    "  * 上传模型到模型注册表\n",
    "  * 部署上传的模型\n",
    "  * 进行预测\n",
    "\n",
    "* 清理资源\n",
    "\n",
    "### 成本\n",
    "\n",
    "此教程使用Google Cloud的计费组件：\n",
    "\n",
    "* Vertex AI\n",
    "* 云存储\n",
    "\n",
    "了解[Vertex AI价格](https://cloud.google.com/vertex-ai/pricing)和[云存储价格](https://cloud.google.com/storage/pricing)，并使用[Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "根据您的预计使用量生成成本估算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KEukV6uRk_S3"
   },
   "source": [
    "在你开始之前"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z__i0w0lCAsW"
   },
   "source": [
    "仅限Colab\n",
    "如果您使用Workbench，请运行以下命令并跳过本部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jvqs-ehKlaYh"
   },
   "outputs": [],
   "source": [
    "if \"google.colab\" in str(get_ipython()):\n",
    "    ! pip3 install --upgrade google-cloud-aiplatform\n",
    "\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)\n",
    "\n",
    "    from google.colab import auth as google_auth\n",
    "\n",
    "    google_auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF1j6f9HApxa"
   },
   "source": [
    "### 设置您的Google Cloud项目\n",
    "\n",
    "**无论您使用的是哪种笔记本环境，都需要按照以下步骤进行操作。**\n",
    "\n",
    "1. [选择或创建一个Google Cloud项目](https://console.cloud.google.com/cloud-resource-manager)。当您首次创建帐户时，您将获得$300的免费信用额度，可用于支付计算/存储费用。\n",
    "\n",
    "2. [确保您的项目已启用计费](https://cloud.google.com/billing/docs/how-to/modify-project)。\n",
    "\n",
    "3. [启用Vertex AI API和Compute Engine API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,compute_component)。\n",
    "\n",
    "4. 如果您正在本地运行这个笔记本，您需要安装[Cloud SDK](https://cloud.google.com/sdk)。\n",
    "\n",
    "5. 在下面的单元格中输入您的项目ID。然后运行单元格，确保Cloud SDK在本笔记本中的所有命令中都使用正确的项目。\n",
    "\n",
    "**注意**：Jupyter会将以`!`为前缀的行视为shell命令，并将以`$`为前缀的Python变量插入这些命令中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9wExiMUxFk91"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "# The project and bucket are for experiments below.\n",
    "PROJECT_ID = \"\"  # @param {type:\"string\"}\n",
    "BUCKET_URI = \"\"  # @param {type:\"string\"}\n",
    "\n",
    "# You can choose a region from https://cloud.google.com/about/locations.\n",
    "# Only regions prefixed by \"us\", \"asia\", or \"europe\" are supported.\n",
    "REGION = \"us-central1\"  # @param {type:\"string\"}\n",
    "REGION_PREFIX = REGION.split(\"-\")[0]\n",
    "assert REGION_PREFIX in (\n",
    "    \"us\",\n",
    "    \"europe\",\n",
    "    \"asia\",\n",
    "), f'{REGION} is not supported. It must be prefixed by \"us\", \"asia\", or \"europe\".'\n",
    "\n",
    "! gcloud config set project $PROJECT_ID\n",
    "\n",
    "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
    "CHECKPOINT_BUCKET = os.path.join(BUCKET_URI, \"ckpt\")\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)\n",
    "\n",
    "# Download config files.\n",
    "CONFIG_DIR = os.path.join(BUCKET_URI, \"config\")\n",
    "! wget https://raw.githubusercontent.com/tensorflow/models/master/official/vision/configs/experiments/retinanet/coco_spinenet49_gpu_multiworker_mirrored.yaml\n",
    "! gsutil cp coco_spinenet49_gpu_multiworker_mirrored.yaml $CONFIG_DIR/\n",
    "\n",
    "! wget https://raw.githubusercontent.com/tensorflow/models/master/official/vision/configs/experiments/retinanet/coco_spinenet96_gpu_multiworker_mirrored.yaml\n",
    "! gsutil cp coco_spinenet96_gpu_multiworker_mirrored.yaml $CONFIG_DIR/\n",
    "\n",
    "! wget https://raw.githubusercontent.com/tensorflow/models/master/official/vision/configs/experiments/retinanet/coco_spinenet143_gpu_multiworker_mirrored.yaml\n",
    "! gsutil cp coco_spinenet143_gpu_multiworker_mirrored.yaml $CONFIG_DIR/\n",
    "\n",
    "! wget https://raw.githubusercontent.com/tensorflow/models/master/official/projects/yolo/configs/experiments/yolov4/detection/scaled_yolov4_1280_gpu.yaml\n",
    "! gsutil cp scaled_yolov4_1280_gpu.yaml $CONFIG_DIR/\n",
    "\n",
    "! wget https://raw.githubusercontent.com/tensorflow/models/master/official/projects/yolo/configs/experiments/yolov7/detection/yolov7_gpu.yaml\n",
    "! gsutil cp yolov7_gpu.yaml $CONFIG_DIR/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n6IFz75WGCam"
   },
   "source": [
    "### 定义常量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "riG_qUokg0XZ"
   },
   "outputs": [],
   "source": [
    "OBJECTIVE = \"iod\"\n",
    "\n",
    "# Data converter constants.\n",
    "DATA_CONVERTER_JOB_PREFIX = \"data_converter\"\n",
    "DATA_CONVERTER_CONTAINER = f\"{REGION_PREFIX}-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/data-converter\"\n",
    "DATA_CONVERTER_MACHINE_TYPE = \"n1-highmem-8\"\n",
    "\n",
    "\n",
    "# Training constants.\n",
    "TRAINING_JOB_PREFIX = \"train\"\n",
    "TRAIN_CONTAINER_URI = f\"{REGION_PREFIX}-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/tfvision-oss\"\n",
    "TRAIN_MACHINE_TYPE = \"n1-highmem-16\"\n",
    "TRAIN_ACCELERATOR_TYPE = \"NVIDIA_TESLA_V100\"\n",
    "TRAIN_NUM_GPU = 2\n",
    "TRAIN_SPINENET49_CONFIG = os.path.join(\n",
    "    CONFIG_DIR, \"coco_spinenet49_gpu_multiworker_mirrored.yaml\"\n",
    ")\n",
    "TRAIN_SPINENET96_CONFIG = os.path.join(\n",
    "    CONFIG_DIR, \"coco_spinenet96_gpu_multiworker_mirrored.yaml\"\n",
    ")\n",
    "TRAIN_SPINENET143_CONFIG = os.path.join(\n",
    "    CONFIG_DIR, \"coco_spinenet143_gpu_multiworker_mirrored.yaml\"\n",
    ")\n",
    "TRAIN_YOLOV4_CONFIG = os.path.join(CONFIG_DIR, \"scaled_yolov4_1280_gpu.yaml\")\n",
    "TRAIN_YOLOV7_CONFIG = os.path.join(CONFIG_DIR, \"yolov7_gpu.yaml\")\n",
    "\n",
    "# Evaluation constants.\n",
    "EVALUATION_METRIC = \"AP50\"\n",
    "\n",
    "# Export constants.\n",
    "EXPORT_JOB_PREFIX = \"export\"\n",
    "EXPORT_CONTAINER_URI = f\"{REGION_PREFIX}-docker.pkg.dev/vertex-ai-restricted/vertex-vision-model-garden-dockers/tfvision-model-export\"\n",
    "EXPORT_MACHINE_TYPE = \"n1-highmem-8\"\n",
    "\n",
    "# Prediction constants.\n",
    "# You can deploy models with\n",
    "#   pre-build-dockers: https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers.\n",
    "#   and optimized tensorflow runtime dockers: https://cloud.google.com/vertex-ai/docs/predictions/optimized-tensorflow-runtime.\n",
    "# The example in this notebook uses optimized tensorflow runtime dockers.\n",
    "# You can adjust accelerator types and machine types to get faster predictions.\n",
    "PREDICTION_CONTAINER_URI = f\"{REGION_PREFIX}-docker.pkg.dev/vertex-ai-restricted/prediction/tf_opt-gpu.2-11:latest\"\n",
    "PREDICTION_ACCELERATOR_TYPE = \"NVIDIA_TESLA_T4\"\n",
    "PREDICTION_MACHINE_TYPE = \"n1-standard-4\"\n",
    "UPLOAD_JOB_PREFIX = \"upload\"\n",
    "DEPLOY_JOB_PREFIX = \"deploy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZFPe_GezXg8"
   },
   "source": [
    "### 定义常用的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XcYUGwr-AJGY"
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from io import BytesIO\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import yaml\n",
    "from google.cloud import aiplatform\n",
    "from google.protobuf import json_format\n",
    "from google.protobuf.struct_pb2 import Value\n",
    "from PIL import Image, ImageColor, ImageDraw, ImageFont\n",
    "\n",
    "\n",
    "def get_job_name_with_datetime(prefix: str):\n",
    "    return prefix + datetime.now().strftime(\"_%Y%m%d_%H%M%S\")\n",
    "\n",
    "\n",
    "def predict_custom_trained_model(\n",
    "    project: str,\n",
    "    endpoint_id: str,\n",
    "    instances: Union[Dict, List[Dict]],\n",
    "    location: str = \"us-central1\",\n",
    "):\n",
    "    # The AI Platform services require regional API endpoints.\n",
    "    client_options = {\"api_endpoint\": f\"{location}-aiplatform.googleapis.com\"}\n",
    "    # Initialize client that will be used to create and send requests.\n",
    "    # This client only needs to be created once, and can be reused for multiple requests.\n",
    "    client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)\n",
    "    parameters_dict = {}\n",
    "    parameters = json_format.ParseDict(parameters_dict, Value())\n",
    "    endpoint = client.endpoint_path(\n",
    "        project=project, location=location, endpoint=endpoint_id\n",
    "    )\n",
    "    response = client.predict(\n",
    "        endpoint=endpoint, instances=instances, parameters=parameters\n",
    "    )\n",
    "    return response.predictions, response.deployed_model_id\n",
    "\n",
    "\n",
    "def load_img(path):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    return Image.fromarray(np.uint8(img)).convert(\"RGB\")\n",
    "\n",
    "\n",
    "def display_image(image):\n",
    "    _ = plt.figure(figsize=(20, 15))\n",
    "    plt.grid(False)\n",
    "    plt.imshow(image)\n",
    "\n",
    "\n",
    "def get_prediction_instances(test_filepath, new_width=-1):\n",
    "    if new_width <= 0:\n",
    "        with tf.io.gfile.GFile(test_filepath, \"rb\") as input_file:\n",
    "            encoded_string = base64.b64encode(input_file.read()).decode(\"utf-8\")\n",
    "    else:\n",
    "        img = load_img(test_filepath)\n",
    "        width, height = img.size\n",
    "        print(\"original input image size: \", width, \" , \", height)\n",
    "        new_height = int(height * new_width / width)\n",
    "        new_img = img.resize((new_width, new_height))\n",
    "        print(\"resized input image size: \", new_width, \" , \", new_height)\n",
    "        buffered = BytesIO()\n",
    "        new_img.save(buffered, format=\"JPEG\")\n",
    "        encoded_string = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "    instances = [\n",
    "        {\n",
    "            \"encoded_image\": {\"b64\": encoded_string},\n",
    "        }\n",
    "    ]\n",
    "    return instances\n",
    "\n",
    "\n",
    "def get_label_map(label_map_yaml_filepath):\n",
    "    with tf.io.gfile.GFile(label_map_yaml_filepath, \"rb\") as input_file:\n",
    "        label_map = yaml.safe_load(input_file.read())\n",
    "    return label_map\n",
    "\n",
    "\n",
    "def get_best_trial(model_dir, max_trial_count, evaluation_metric):\n",
    "    best_trial_dir = \"\"\n",
    "    best_trial_evaluation_results = {}\n",
    "    best_performance = -1\n",
    "\n",
    "    for i in range(max_trial_count):\n",
    "        current_trial = i + 1\n",
    "        current_trial_dir = os.path.join(model_dir, \"trial_\" + str(current_trial))\n",
    "        current_trial_best_ckpt_dir = os.path.join(current_trial_dir, \"best_ckpt\")\n",
    "        current_trial_best_ckpt_evaluation_filepath = os.path.join(\n",
    "            current_trial_best_ckpt_dir, \"info.json\"\n",
    "        )\n",
    "        with tf.io.gfile.GFile(current_trial_best_ckpt_evaluation_filepath, \"rb\") as f:\n",
    "            eval_metric_results = json.load(f)\n",
    "            current_performance = eval_metric_results[evaluation_metric]\n",
    "            if current_performance > best_performance:\n",
    "                best_performance = current_performance\n",
    "                best_trial_dir = current_trial_dir\n",
    "                best_trial_evaluation_results = eval_metric_results\n",
    "    return best_trial_dir, best_trial_evaluation_results\n",
    "\n",
    "\n",
    "def draw_bounding_box_on_image(\n",
    "    image, ymin, xmin, ymax, xmax, color, font, thickness=4, display_str_list=()\n",
    "):\n",
    "    \"\"\"Adds a bounding box to an image.\"\"\"\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    im_width, im_height = image.size\n",
    "    (left, right, top, bottom) = (\n",
    "        xmin * im_width,\n",
    "        xmax * im_width,\n",
    "        ymin * im_height,\n",
    "        ymax * im_height,\n",
    "    )\n",
    "    draw.line(\n",
    "        [(left, top), (left, bottom), (right, bottom), (right, top), (left, top)],\n",
    "        width=thickness,\n",
    "        fill=color,\n",
    "    )\n",
    "\n",
    "    # If the total height of the display strings added to the top of the bounding\n",
    "    # box exceeds the top of the image, stack the strings below the bounding box\n",
    "    # instead of above.\n",
    "    display_str_heights = [font.getsize(ds)[1] for ds in display_str_list]\n",
    "    # Each display_str has a top and bottom margin of 0.05x.\n",
    "    total_display_str_height = (1 + 2 * 0.05) * sum(display_str_heights)\n",
    "\n",
    "    if top > total_display_str_height:\n",
    "        text_bottom = top\n",
    "    else:\n",
    "        text_bottom = top + total_display_str_height\n",
    "    # Reverse list and print from bottom to top.\n",
    "    for display_str in display_str_list[::-1]:\n",
    "        text_width, text_height = font.getsize(display_str)\n",
    "        margin = np.ceil(0.05 * text_height)\n",
    "        draw.rectangle(\n",
    "            [\n",
    "                (left, text_bottom - text_height - 2 * margin),\n",
    "                (left + text_width, text_bottom),\n",
    "            ],\n",
    "            fill=color,\n",
    "        )\n",
    "        draw.text(\n",
    "            (left + margin, text_bottom - text_height - margin),\n",
    "            display_str,\n",
    "            fill=\"black\",\n",
    "            font=font,\n",
    "        )\n",
    "        text_bottom -= text_height - 2 * margin\n",
    "\n",
    "\n",
    "def draw_boxes(image, boxes, class_names, scores, max_boxes=40, min_score=0.05):\n",
    "    \"\"\"Overlay labeled boxes on an image with formatted scores and label names.\"\"\"\n",
    "    colors = list(ImageColor.colormap.values())\n",
    "    try:\n",
    "        font = ImageFont.truetype(\n",
    "            \"/usr/share/fonts/truetype/liberation/LiberationSansNarrow-Regular.ttf\", 25\n",
    "        )\n",
    "    except OSError:\n",
    "        print(\"Font not found, using default font.\")\n",
    "        font = ImageFont.load_default()\n",
    "\n",
    "    for i in range(min(len(boxes), max_boxes)):\n",
    "        if scores[i] >= min_score:\n",
    "            ymin, xmin, ymax, xmax = boxes[i]\n",
    "            display_str = \"{}: {}%\".format(class_names[i], int(100 * scores[i]))\n",
    "            color = colors[hash(class_names[i]) % len(colors)]\n",
    "            draw_bounding_box_on_image(\n",
    "                image,\n",
    "                ymin,\n",
    "                xmin,\n",
    "                ymax,\n",
    "                xmax,\n",
    "                color,\n",
    "                font,\n",
    "                display_str_list=[display_str],\n",
    "            )\n",
    "    return image\n",
    "\n",
    "\n",
    "def upload_checkpoint_to_gcs(checkpoint_url):\n",
    "    filename = os.path.basename(checkpoint_url)\n",
    "    checkpoint_name = filename.replace(\".tar.gz\", \"\")\n",
    "    print(\"Download checkpoint from\", checkpoint_url, \"and store to\", CHECKPOINT_BUCKET)\n",
    "    ! wget $checkpoint_url -O $filename\n",
    "    ! mkdir -p $checkpoint_name\n",
    "    ! tar -xvzf $filename -C $checkpoint_name\n",
    "\n",
    "    # Search for relative path to the checkpoint.\n",
    "    checkpoint_path = None\n",
    "    for root, dirs, files in os.walk(checkpoint_name):\n",
    "        for file in files:\n",
    "            if file.endswith(\".index\"):\n",
    "                checkpoint_path = os.path.join(root, os.path.splitext(file)[0])\n",
    "                checkpoint_path = os.path.relpath(checkpoint_path, checkpoint_name)\n",
    "                break\n",
    "\n",
    "    ! gsutil cp -r $checkpoint_name $CHECKPOINT_BUCKET/\n",
    "    checkpoint_uri = os.path.join(CHECKPOINT_BUCKET, checkpoint_name, checkpoint_path)\n",
    "    print(\"Checkpoint uploaded to\", checkpoint_uri)\n",
    "    return checkpoint_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RB_xY9ipr7ZU"
   },
   "source": [
    "## 训练新模型\n",
    "该部分展示了如何训练新模型。\n",
    "1. 将输入数据转换为训练格式\n",
    "2. 创建超参数调整作业以训练新模型\n",
    "3. 查找并导出最佳模型\n",
    "\n",
    "如果您已经训练了模型，请转到“测试训练过的模型”部分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "### 为训练准备输入数据\n",
    "\n",
    "按照[此处](https://cloud.google.com/vertex-ai/docs/image-data/classification/prepare-data)描述的格式准备数据，然后将其转换为以下训练格式：\n",
    "\n",
    "* `input_file_path`：用于准备数据的输入文件路径。\n",
    "* `input_file_type`：输入文件类型，例如 csv 或 jsonl。\n",
    "* `split_ratio`：要拆分为训练/验证/测试的数据比例。\n",
    "* `num_shard`：用于训练/验证/测试的分片数量。\n",
    "* `output_dir`：包含准备好的训练/测试/验证数据的输出目录。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IndQ_m6ddUEM"
   },
   "outputs": [],
   "source": [
    "# This job will convert input data as training format, with given split ratios\n",
    "# and number of shards on train/test/validation.\n",
    "data_converter_job_name = get_job_name_with_datetime(\n",
    "    DATA_CONVERTER_JOB_PREFIX + \"_\" + OBJECTIVE\n",
    ")\n",
    "\n",
    "input_file_path = \"\"  # @param {type:\"string\"}\n",
    "input_file_type = \"csv\"  # @param ['csv', 'jsonl', 'coco_json']\n",
    "split_ratio = \"0.8,0.1,0.1\"\n",
    "num_shard = \"10,10,10\"\n",
    "data_converter_output_dir = os.path.join(BUCKET_URI, data_converter_job_name)\n",
    "\n",
    "\n",
    "worker_pool_specs = [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": DATA_CONVERTER_MACHINE_TYPE,\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": DATA_CONVERTER_CONTAINER,\n",
    "            \"command\": [],\n",
    "            \"args\": [\n",
    "                \"--input_file_path=%s\" % input_file_path,\n",
    "                \"--input_file_type=%s\" % input_file_type,\n",
    "                \"--objective=%s\" % OBJECTIVE,\n",
    "                \"--num_shard=%s\" % num_shard,\n",
    "                \"--split_ratio=%s\" % split_ratio,\n",
    "                \"--output_dir=%s\" % data_converter_output_dir,\n",
    "            ],\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "data_converter_custom_job = aiplatform.CustomJob(\n",
    "    display_name=data_converter_job_name,\n",
    "    project=PROJECT_ID,\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    "    staging_bucket=STAGING_BUCKET,\n",
    ")\n",
    "\n",
    "data_converter_custom_job.run()\n",
    "\n",
    "input_train_data_path = os.path.join(data_converter_output_dir, \"train.tfrecord*\")\n",
    "input_validation_data_path = os.path.join(data_converter_output_dir, \"val.tfrecord*\")\n",
    "label_map_path = os.path.join(data_converter_output_dir, \"label_map.yaml\")\n",
    "print(\"input_train_data_path for training: \", input_train_data_path)\n",
    "print(\"input_validation_data_path for training: \", input_validation_data_path)\n",
    "print(\"label_map_path for prediction: \", label_map_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SA8DVTn7j69v"
   },
   "source": [
    "使用Vertex AI SDK创建并运行具有超参数调整功能的Vertex AI Model Garden Training Dockers自定义作业。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aaff6f5be7f6"
   },
   "source": [
    "#### 定义以下规格\n",
    "\n",
    "* `worker_pool_specs`: 字典指定机器类型和Docker镜像。这个例子定义了一个使用一个 `n1-highmem-16` 机器和两个 `NVIDIA_TESLA_V100` GPU 的单节点集群。\n",
    "* `parameter_spec`: 字典指定要优化的参数。字典键是分配给训练应用代码中每个超参数的命令行参数的字符串，字典值是参数规范。参数规范包括超参数的类型、最小/最大值和尺度。\n",
    "* `metric_spec`: 字典指定要优化的指标。字典键是你在训练应用代码中设置的 `hyperparameter_metric_tag`，值是优化目标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "um_XKbmpTaHx"
   },
   "outputs": [],
   "source": [
    "from google.cloud.aiplatform import hyperparameter_tuning as hpt\n",
    "\n",
    "label_map = get_label_map(label_map_path)\n",
    "num_classes = len(label_map[\"label_map\"]) + 1\n",
    "\n",
    "# Input train and validation datasets can be found from the section above\n",
    "# `Convert input data for training`.\n",
    "# Set prepared datasets if exists.\n",
    "# input_train_data_path = ''\n",
    "# input_validation_data_path = ''\n",
    "\n",
    "# Refer to https://github.com/tensorflow/models/blob/master/official/vision/MODEL_GARDEN.md\n",
    "# for more model details.\n",
    "experiment = \"retinanet_spinenet96\"  # @param ['retinanet_spinenet49', \"retinanet_spinenet96\", 'retinanet_spinenet143', 'scaled_yolo_v4', 'yolov7']\n",
    "\n",
    "train_job_name = get_job_name_with_datetime(TRAINING_JOB_PREFIX + \"_\" + OBJECTIVE)\n",
    "model_dir = os.path.join(BUCKET_URI, train_job_name)\n",
    "\n",
    "# The arguments here are mainly for test purposes. Please update them\n",
    "# to get better performances.\n",
    "common_args = {\n",
    "    \"input_train_data_path\": input_train_data_path,\n",
    "    \"input_validation_data_path\": input_validation_data_path,\n",
    "    \"objective\": OBJECTIVE,\n",
    "    \"model_dir\": model_dir,\n",
    "    \"num_classes\": num_classes,\n",
    "    \"global_batch_size\": 2,\n",
    "    \"prefetch_buffer_size\": 6,\n",
    "    \"train_steps\": 2000,\n",
    "    \"input_size\": \"1024,1024\",\n",
    "}\n",
    "\n",
    "experiment_container_args_dict = {\n",
    "    # retinanet_spinenet49 experiment args.\n",
    "    \"retinanet_spinenet49\": dict(\n",
    "        common_args,\n",
    "        **{\n",
    "            \"experiment\": \"retinanet_spinenet_coco\",\n",
    "            \"config_file\": TRAIN_SPINENET49_CONFIG,\n",
    "            \"anchor_size\": 4,\n",
    "        },\n",
    "    ),\n",
    "    # retinanet_spinenet96 experiment args.\n",
    "    \"retinanet_spinenet96\": dict(\n",
    "        common_args,\n",
    "        **{\n",
    "            \"experiment\": \"retinanet_spinenet_coco\",\n",
    "            \"config_file\": TRAIN_SPINENET96_CONFIG,\n",
    "            \"anchor_size\": 4,\n",
    "        },\n",
    "    ),\n",
    "    # retinanet_spinenet143 experiment args.\n",
    "    \"retinanet_spinenet143\": dict(\n",
    "        common_args,\n",
    "        **{\n",
    "            \"experiment\": \"retinanet_spinenet_coco\",\n",
    "            \"config_file\": TRAIN_SPINENET143_CONFIG,\n",
    "            \"anchor_size\": 4,\n",
    "        },\n",
    "    ),\n",
    "    # scaled_yolo_v4 experiment args.\n",
    "    \"scaled_yolo_v4\": dict(\n",
    "        common_args,\n",
    "        **{\n",
    "            \"experiment\": \"scaled_yolo\",\n",
    "            \"config_file\": TRAIN_YOLOV4_CONFIG,\n",
    "            \"init_checkpoint\": \"https://storage.googleapis.com/tf_model_garden/vision/yolo/scaled-yolov4/scaled-yolov4-l-p6-i1280.tar.gz\",\n",
    "            \"input_size\": \"1280,1280\",\n",
    "        },\n",
    "    ),\n",
    "    # yolov7 experiment args.\n",
    "    \"yolov7\": dict(\n",
    "        common_args,\n",
    "        **{\n",
    "            \"experiment\": \"coco_yolov7\",\n",
    "            \"config_file\": TRAIN_YOLOV7_CONFIG,\n",
    "            \"init_checkpoint\": \"https://storage.googleapis.com/tf_model_garden/vision/yolo/yolov7/yolov7.tar.gz\",\n",
    "            \"input_size\": \"640,640\",\n",
    "        },\n",
    "    ),\n",
    "}\n",
    "experiment_container_args = experiment_container_args_dict[experiment]\n",
    "\n",
    "# Copy checkpoint to GCS bucket if specified.\n",
    "init_checkpoint = experiment_container_args.get(\"init_checkpoint\")\n",
    "if init_checkpoint:\n",
    "    experiment_container_args[\"init_checkpoint\"] = upload_checkpoint_to_gcs(\n",
    "        init_checkpoint\n",
    "    )\n",
    "if \"yolov7\" in experiment:\n",
    "    TRAIN_CONTAINER_URI = f\"{REGION_PREFIX}-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/tfvision-oss-v2\"\n",
    "\n",
    "params_override = \"runtime.num_gpus=%s\" % TRAIN_NUM_GPU\n",
    "eval_params_override = \"runtime.num_gpus=1,runtime.distribution_strategy=mirrored\"\n",
    "worker_pool_specs = [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": TRAIN_MACHINE_TYPE,\n",
    "            \"accelerator_type\": TRAIN_ACCELERATOR_TYPE,\n",
    "            \"accelerator_count\": TRAIN_NUM_GPU,\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": TRAIN_CONTAINER_URI,\n",
    "            \"args\": [\n",
    "                \"--mode=train\",\n",
    "                \"--params_override=%s\" % params_override,\n",
    "            ]\n",
    "            + [\"--{}={}\".format(k, v) for k, v in experiment_container_args.items()],\n",
    "        },\n",
    "    },\n",
    "    {},\n",
    "    {},\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": \"n1-highmem-4\",\n",
    "            \"accelerator_type\": TRAIN_ACCELERATOR_TYPE,\n",
    "            \"accelerator_count\": 1,\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": TRAIN_CONTAINER_URI,\n",
    "            \"args\": [\n",
    "                \"--mode=continuous_eval\",\n",
    "                \"--params_override=%s\" % eval_params_override,\n",
    "            ]\n",
    "            + [\"--{}={}\".format(k, v) for k, v in experiment_container_args.items()],\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "metric_spec = {\"model_performance\": \"maximize\"}\n",
    "\n",
    "LEARNING_RATES = [0.001, 0.01]\n",
    "# Models will be trained with each learning rate separately and max trial count is the number of learning rates.\n",
    "MAX_TRIAL_COUNT = len(LEARNING_RATES)\n",
    "parameter_spec = {\n",
    "    \"learning_rate\": hpt.DiscreteParameterSpec(values=LEARNING_RATES, scale=\"linear\"),\n",
    "}\n",
    "\n",
    "print(worker_pool_specs, metric_spec, parameter_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwcCjwlBTQIz"
   },
   "source": [
    "#### 运行超参数调整任务\n",
    "* `max_trial_count`: 设置服务运行的试验次数上限。建议的做法是从较少的试验次数开始，了解选择的超参数对结果的影响程度，然后再逐步增加试验次数。\n",
    "\n",
    "* `parallel_trial_count`: 如果使用并行试验，服务会提供多个训练处理集群。在创建作业时指定的工作池规范将用于每个单独的训练集群。增加并行试验的数量可以减少超参数调整作业所需的时间；然而，这可能会降低作业的效果。这是因为默认的调整策略使用先前试验的结果来指导后续试验中值的分配。\n",
    "\n",
    "* `search_algorithm`: 可用的搜索算法有网格、随机或默认（无）。默认选项应用贝叶斯优化来搜索可能超参数值的空间，是推荐的算法。\n",
    "\n",
    "点击输出中生成的链接，在云控制台中查看您的运行情况。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aec22792ee84"
   },
   "outputs": [],
   "source": [
    "train_custom_job = aiplatform.CustomJob(\n",
    "    display_name=train_job_name,\n",
    "    project=PROJECT_ID,\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    "    staging_bucket=STAGING_BUCKET,\n",
    ")\n",
    "\n",
    "train_hpt_job = aiplatform.HyperparameterTuningJob(\n",
    "    display_name=train_job_name,\n",
    "    custom_job=train_custom_job,\n",
    "    metric_spec=metric_spec,\n",
    "    parameter_spec=parameter_spec,\n",
    "    max_trial_count=MAX_TRIAL_COUNT,\n",
    "    parallel_trial_count=1,\n",
    "    project=PROJECT_ID,\n",
    "    search_algorithm=None,\n",
    ")\n",
    "\n",
    "train_hpt_job.run()\n",
    "\n",
    "print(\"experiment is: \", experiment)\n",
    "print(\"model_dir is: \", model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mV-Djz-frBni"
   },
   "source": [
    "将最佳模型导出为TF Saved Model格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "09Rz1AYspK19"
   },
   "outputs": [],
   "source": [
    "# This job will export models from TF checkpoints to TF saved model format.\n",
    "# model_dir is from the section above.\n",
    "best_trial_dir, best_trial_evaluation_results = get_best_trial(\n",
    "    model_dir, MAX_TRIAL_COUNT, EVALUATION_METRIC\n",
    ")\n",
    "print(\"best_trial_dir: \", best_trial_dir)\n",
    "print(\"best_trial_evaluation_results: \", best_trial_evaluation_results)\n",
    "\n",
    "worker_pool_specs = [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": EXPORT_MACHINE_TYPE,\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": EXPORT_CONTAINER_URI,\n",
    "            \"command\": [],\n",
    "            \"args\": [\n",
    "                \"--objective=%s\" % OBJECTIVE,\n",
    "                \"--input_image_size=1024,1024\",\n",
    "                \"--experiment=%s\" % experiment_container_args[\"experiment\"],\n",
    "                \"--config_file=%s/params.yaml\" % best_trial_dir,\n",
    "                \"--checkpoint_path=%s/best_ckpt\" % best_trial_dir,\n",
    "                \"--export_dir=%s/best_model\" % model_dir,\n",
    "            ],\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "model_export_name = get_job_name_with_datetime(EXPORT_JOB_PREFIX + \"_\" + OBJECTIVE)\n",
    "model_export_custom_job = aiplatform.CustomJob(\n",
    "    display_name=model_export_name,\n",
    "    project=PROJECT_ID,\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    "    staging_bucket=STAGING_BUCKET,\n",
    ")\n",
    "\n",
    "\n",
    "model_export_custom_job.run()\n",
    "\n",
    "print(\"best model is saved to: \", os.path.join(model_dir, \"best_model\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0BGaofgsMsy"
   },
   "source": [
    "## 测试训练好的模型\n",
    "本部分将展示如何使用训练好的模型进行测试。\n",
    "1. 上传并部署模型\n",
    "2. 运行预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NYuQowyZEtxK"
   },
   "outputs": [],
   "source": [
    "# @title Upload and deploy models\n",
    "# model_dir is from the section above.\n",
    "trained_model_dir = os.path.join(model_dir, \"best_model/saved_model\")\n",
    "\n",
    "upload_job_name = get_job_name_with_datetime(UPLOAD_JOB_PREFIX + \"_\" + OBJECTIVE)\n",
    "\n",
    "if \"yolo\" in experiment:\n",
    "    SERVING_CONTAINER_ARGS = [\"--allow_precompilation\"]\n",
    "else:\n",
    "    SERVING_CONTAINER_ARGS = [\"--allow_precompilation\", \"--allow_compression\"]\n",
    "\n",
    "\n",
    "serving_env = {\n",
    "    \"MODEL_ID\": \"tfvision-scaled-yolo\",\n",
    "    \"DEPLOY_SOURCE\": \"notebook\",\n",
    "}\n",
    "\n",
    "model = aiplatform.Model.upload(\n",
    "    display_name=upload_job_name,\n",
    "    artifact_uri=trained_model_dir,\n",
    "    serving_container_image_uri=PREDICTION_CONTAINER_URI,\n",
    "    serving_container_args=SERVING_CONTAINER_ARGS,\n",
    "    serving_container_environment_variables=serving_env,\n",
    ")\n",
    "\n",
    "model.wait()\n",
    "\n",
    "print(\"The uploaded model name is: \", upload_job_name)\n",
    "\n",
    "deploy_model_name = get_job_name_with_datetime(DEPLOY_JOB_PREFIX + \"_\" + OBJECTIVE)\n",
    "print(\"The deployed job name is: \", deploy_model_name)\n",
    "\n",
    "endpoint = model.deploy(\n",
    "    deployed_model_display_name=deploy_model_name,\n",
    "    machine_type=PREDICTION_MACHINE_TYPE,\n",
    "    traffic_split={\"0\": 100},\n",
    "    accelerator_type=PREDICTION_ACCELERATOR_TYPE,\n",
    "    accelerator_count=1,\n",
    "    min_replica_count=1,\n",
    "    max_replica_count=1,\n",
    ")\n",
    "\n",
    "endpoint_id = endpoint.name\n",
    "print(\"endpoint id is: \", endpoint_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vbIW9me1F2RY"
   },
   "outputs": [],
   "source": [
    "# @title Run predictions\n",
    "\n",
    "# endpoint_id was generated in the section above (`Upload and deploy models`).\n",
    "endpoint_id = endpoint.name\n",
    "\n",
    "# The test image file path.\n",
    "test_filepath = \"\"  # @param {type:\"string\"}\n",
    "score_threshold = 0.2  # @param {type:\"number\"}\n",
    "# If the input image is too large, we will resize it for prediction.\n",
    "instances = get_prediction_instances(test_filepath, new_width=1000)\n",
    "\n",
    "# The label map file was generated from the section above (`Convert input data for training`).\n",
    "label_map = get_label_map(label_map_path)[\"label_map\"]\n",
    "\n",
    "predictions, _ = predict_custom_trained_model(\n",
    "    project=PROJECT_ID, location=REGION, endpoint_id=endpoint_id, instances=instances\n",
    ")\n",
    "\n",
    "img = load_img(test_filepath)\n",
    "detection_boxes = predictions[0][\"detection_boxes\"]\n",
    "detection_scores = predictions[0][\"detection_scores\"]\n",
    "detection_classes_as_text = []\n",
    "\n",
    "for detection_class in predictions[0][\"detection_classes\"]:\n",
    "    detection_classes_as_text.append(label_map[int(detection_class)])\n",
    "\n",
    "img = draw_boxes(\n",
    "    img,\n",
    "    detection_boxes,\n",
    "    detection_classes_as_text,\n",
    "    detection_scores,\n",
    "    min_score=score_threshold,\n",
    ")\n",
    "display_image(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kkH2nrpdp4sp"
   },
   "source": [
    "清理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ax6vQVZhp9pR"
   },
   "outputs": [],
   "source": [
    "# Undeploy model and delete endpoint.\n",
    "endpoint.delete(force=True)\n",
    "# Delete models.\n",
    "model.delete()\n",
    "# Delete custom and hpt jobs.\n",
    "if data_converter_custom_job.list(filter=f'display_name=\"{data_converter_job_name}\"'):\n",
    "    data_converter_custom_job.delete()\n",
    "if train_hpt_job.list(filter=f'display_name=\"{train_job_name}\"'):\n",
    "    train_hpt_job.delete()\n",
    "if model_export_custom_job.list(filter=f'display_name=\"{model_export_name}\"'):\n",
    "    model_export_custom_job.delete()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "model_garden_tfvision_image_object_detection.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
