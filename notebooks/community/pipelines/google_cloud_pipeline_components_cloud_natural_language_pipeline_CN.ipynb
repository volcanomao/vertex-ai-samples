{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "请在Colab中打开：https://colab.research.google.com/github/Narwhalprime/vertex-ai-samples/blob/main/notebooks/community/pipelines/google_cloud_pipeline_components_cloud_natural_language_pipeline.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1142fd18"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BwO30Ag12YcB"
   },
   "source": [
    "# 顶点管道：云自然语言模型训练管道\n",
    "<table align=\"left\">\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/pipelines/google_cloud_pipeline_components_cloud_natural_language_pipeline.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> 在 Colab 中运行\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/pipelines/google_cloud_pipeline_components_cloud_natural_language_pipeline.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      在 GitHub 上查看\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/ai/platform/notebooks/deploy-notebook?download_url=https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/natural_language/cloud_natural_language_pipeline.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      在 Vertex AI Workbench 中打开\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "概述\n",
    "本笔记本展示如何使用[Google Cloud Pipeline Components SDK](https://cloud.google.com/vertex-ai/docs/pipelines/components-introduction)和本目录中的附加组件，在[Vertex AI Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines/introduction)中运行一个机器学习管道，用于训练一个TensorFlow文本分类模型。\n",
    "\n",
    "在这个管道中，模型训练Docker镜像利用[TFHub](https://tfhub.dev/)模型来执行最先进的文本分类训练。这个镜像是预构建的且就绪可用，因此不需要额外的Docker设置。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d975e698c9a4"
   },
   "source": [
    "### 目标\n",
    "\n",
    "在本教程中，您将学习如何在Vertex AI管道中构建一个端到端的训练管道，该管道摄取一个数据集，在其上训练一个文本分类模型，并输出评估指标。\n",
    "\n",
    "本教程使用以下Google Cloud ML服务和资源：\n",
    "\n",
    "- Vertex AI管道\n",
    "- Vertex AI数据集\n",
    "\n",
    "执行的步骤包括：\n",
    "\n",
    "- 定义Kubeflow管道组件\n",
    "- 设置Kubeflow管道\n",
    "- 在Vertex AI上运行管道"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08d289fa873f"
   },
   "source": [
    "数据集\n",
    "\n",
    "这个笔记本要求用户有两个从Vertex AI导出的数据集[托管数据集](https://cloud.google.com/vertex-ai/docs/training/using-managed-datasets)：一个包含训练和验证数据拆分，另一个包含用于评估的测试数据。请确保两个数据集之间没有共享数据（特别是评估数据不应该是训练或验证拆分的一部分）。要导出一个Vertex AI数据集，请按照以下公共文档操作：\n",
    "* [准备数据](https://cloud.google.com/vertex-ai/docs/text-data/classification/prepare-data)\n",
    "* [创建一个Vertex AI数据集](https://cloud.google.com/vertex-ai/docs/text-data/classification/create-dataset) 使用上述数据\n",
    "* [导出数据集及其注释](https://cloud.google.com/vertex-ai/docs/datasets/export-metadata-annotations); 确保导出的结果位于您拥有的Google Cloud Storage（GCS）存储桶中。您可能需要手动将测试拆分数据分离到自己的文件中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aed92deeb4a0"
   },
   "source": [
    "成本\n",
    "\n",
    "本教程使用 Google Cloud 的收费组件：\n",
    "\n",
    "* Vertex AI\n",
    "* 云存储\n",
    "\n",
    "了解[Vertex AI\n",
    "定价](https://cloud.google.com/vertex-ai/pricing)和[云存储\n",
    "定价](https://cloud.google.com/storage/pricing)，并使用[定价\n",
    "计算器](https://cloud.google.com/products/calculator/)\n",
    "根据您的预期使用量生成成本估算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_local"
   },
   "source": [
    "## 设置\n",
    "\n",
    "如果您正在使用Colab或Google Vertex AI Workbench Notebooks，则您的环境已经满足运行此笔记本的所有要求。您可以跳过此步骤。\n",
    "\n",
    "***注意***：此笔记本已在以下环境中进行了测试：\n",
    "\n",
    "* Python版本 = 3.8\n",
    "\n",
    "否则，请确保您的环境满足此笔记本的要求。您需要以下内容：\n",
    "\n",
    "- 云存储 SDK\n",
    "- Python 3\n",
    "- virtualenv\n",
    "- 在使用Python 3的虚拟环境中运行的Jupyter笔记本\n",
    "\n",
    "[设置Python开发环境的云存储指南](https://cloud.google.com/python/setup)和[Jupyter安装指南](https://jupyter.org/install)提供了满足这些要求的详细说明。以下步骤提供了一套简要的说明：\n",
    "\n",
    "1. [安装并初始化SDK](https://cloud.google.com/sdk/docs/)。\n",
    "\n",
    "2. [安装Python 3](https://cloud.google.com/python/setup#installing_python)。\n",
    "\n",
    "3. [安装virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv)，并创建一个使用Python 3的虚拟环境。激活虚拟环境。\n",
    "\n",
    "4. 激活该环境，并在终端中运行 `pip3 install Jupyter` 来安装Jupyter。\n",
    "\n",
    "5. 在终端中的命令行上运行 `jupyter notebook` 来启动Jupyter。\n",
    "\n",
    "6. 在Jupyter Notebook仪表板中打开此笔记本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "568d5c16"
   },
   "source": [
    "### 安装附加包\n",
    "\n",
    "运行以下命令来设置这个笔记本的包。请注意，本节中的最后一个代码片段将重新启动您的内核以正确加载安装内容，因此在从头开始初始化这个笔记本时，建议运行到该单元格，然后之后可以开始运行该单元格之后的单元格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dac98aac"
   },
   "outputs": [],
   "source": [
    "# Install using pip3\n",
    "!pip3 install -U tensorflow google-cloud-pipeline-components google-cloud-aiplatform kfp==1.8.16 \"shapely<2\" -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "alRWYgYTdz7P"
   },
   "outputs": [],
   "source": [
    "# Version check\n",
    "# This has been tested with KFP 1.8.16\n",
    "! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "! python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d0a15440"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9IYalYObAbY"
   },
   "source": [
    "## 开始之前\n",
    "\n",
    "### 设置您的谷歌云项目\n",
    "\n",
    "**无论您使用什么笔记本环境，下面的步骤都是必要的。**\n",
    "\n",
    "1. [选择或创建一个谷歌云项目](https://console.cloud.google.com/cloud-resource-manager)。当您第一次创建帐户时，您将获得$300的免费信用用于支付计算/存储成本。\n",
    "\n",
    "2. [确保为您的项目启用了计费](https://cloud.google.com/billing/docs/how-to/modify-project)。\n",
    "\n",
    "3. [启用 Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,storage.googleapis.com)。\n",
    "\n",
    "4. 如果您在本地运行此笔记本，您需要安装[Cloud SDK](https://cloud.google.com/sdk)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VA_kzAIIj2G_"
   },
   "source": [
    "### 验证您的Google云帐号\n",
    "\n",
    "**如果您正在使用Vertex AI Workbench笔记本**，您的环境已经通过验证。跳过这一步。\n",
    "\n",
    "**如果您正在使用Colab**，运行下面的单元格，并按照提示进行验证您的帐号。\n",
    "\n",
    "**否则**，请按照以下步骤操作：\n",
    "\n",
    "1. 在Cloud控制台中，转到[**创建服务帐号密钥**页面](https://console.cloud.google.com/apis/credentials/serviceaccountkey)。\n",
    "\n",
    "2. 点击**创建服务帐号**。\n",
    "\n",
    "3. 在**服务帐号名称**字段中输入一个名称，然后点击**创建**。\n",
    "\n",
    "4. 在**授予此服务帐号对项目的访问权限**部分，点击**角色**下拉列表。在筛选框中键入\"Vertex AI\"，并选择\n",
    "   **Vertex AI管理员**。在筛选框中键入\"Storage Object Admin\"，并选择**存储对象管理员**。\n",
    "\n",
    "5. 点击*创建*。将包含您密钥的JSON文件下载到您的本地环境。\n",
    "\n",
    "6. 在下面的单元格中将您的服务帐号密钥路径作为\n",
    "`GOOGLE_APPLICATION_CREDENTIALS` 变量输入并运行该单元格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PyQmSRbKA8r-"
   },
   "outputs": [],
   "source": [
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# If on Vertex AI Workbench, then don't execute this code\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\") and not os.getenv(\n",
    "    \"DL_ANACONDA_HOME\"\n",
    "):\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\"):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "set_service_account"
   },
   "source": [
    "### 设置项目ID\n",
    "\n",
    "在这里设置您的项目ID。如果您不知道项目ID，下面的代码将尝试从您的gcloud配置中确定。只有在笔记本可以看到您想要的项目时，请继续。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AkqEd5Gin9mn"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"your-project-id\"  # @param {type:\"string\"}\n",
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "print(\"Project ID:\", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OVO_gUqpFEP2"
   },
   "outputs": [],
   "source": [
    "!gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a27d4cee"
   },
   "source": [
    "### 设置项目信息\n",
    "\n",
    "在这里输入有关您的项目和数据集的信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7e9477a2"
   },
   "outputs": [],
   "source": [
    "REGION = \"us\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "TRAINING_DATA_LOCATION = \"gs://your-training-data-location\"  # @param {type:\"string\"}\n",
    "TASK_TYPE = \"CLASSIFICATION\"  # @param [\"CLASSIFICATION\", \"MULTILABEL_CLASSIFICATION\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o-MZnHsimbOH"
   },
   "outputs": [],
   "source": [
    "# Since we are training a custom model, we need to specify the list of possible\n",
    "# classes/labels.\n",
    "# e.g, [\"FirstClass\", \"SecondClass\"]\n",
    "# An additional class \"[UNK]\" will be added to the list indicating that none of\n",
    "# the specified labels are a match.\n",
    "CLASS_NAMES = [\"\"]\n",
    "\n",
    "# This is a list of GCS URIs; e.g., [\"gs://your-bucket-name-here/your-input-file.jsonl\"].\n",
    "TEST_DATA_URIS = [\"gs://your-bucket-name-here/your-input-file.jsonl\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "timestamp"
   },
   "source": [
    "UUID\n",
    "\n",
    "为了避免在项目中与其他资源发生名称冲突，您可以使用以下代码创建一个UUID，并将其附加到在此笔记本中创建的存储桶名称后面。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wh9sgzemwLXE"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "\n",
    "# Generate a uuid of a specifed length(default=8)\n",
    "def generate_uuid(length: int = 8) -> str:\n",
    "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
    "\n",
    "\n",
    "UUID = generate_uuid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bucket:mbsdk"
   },
   "source": [
    "### 创建云存储桶\n",
    "\n",
    "**以下步骤是必需的，无论您的笔记本环境如何。**\n",
    "\n",
    "当您初始化用于 Python 的 Vertex AI SDK 时，您会指定一个云存储暂存桶。暂存桶是您的数据集和模型资源相关数据在会话中保留的地方。\n",
    "\n",
    "请在下面设置您的云存储桶的名称。存储桶的名称必须在所有 Google Cloud 项目中全局唯一，包括组织外部的项目。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bucket"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"[your-bucket-name]\"  # @param {type:\"string\"}\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "autoset_bucket"
   },
   "outputs": [],
   "source": [
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"[your-bucket-name]\":\n",
    "    BUCKET_NAME = PROJECT_ID + \"aip-\" + UUID\n",
    "    BUCKET_URI = \"gs://\" + BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_bucket"
   },
   "source": [
    "只有当您的存储桶尚不存在时，请运行以下单元格以创建您的云存储存储桶。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dO0NV93IwLXF"
   },
   "outputs": [],
   "source": [
    "!gsutil mb -l $REGION $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "validate_bucket"
   },
   "source": [
    "最后，通过检查存储桶的内容来验证对云存储桶的访问权限。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hg5f2oKBwLXG"
   },
   "outputs": [],
   "source": [
    "!gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EuFETRptyKXc"
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f3a09765"
   },
   "source": [
    "创建训练管道##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "89bb4a50"
   },
   "source": [
    "### 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0f361e65"
   },
   "outputs": [],
   "source": [
    "from google_cloud_pipeline_components.aiplatform import ModelBatchPredictOp\n",
    "from google_cloud_pipeline_components.experimental import natural_language\n",
    "from google_cloud_pipeline_components.experimental.evaluation import (\n",
    "    GetVertexModelOp, ModelEvaluationClassificationOp,\n",
    "    TargetFieldDataRemoverOp)\n",
    "from kfp import components\n",
    "from kfp.v2 import compiler, dsl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d33c87e4-2ada-4b87-bf75-064247f3162d"
   },
   "source": [
    "### 定义常量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "36ceb9f8"
   },
   "outputs": [],
   "source": [
    "# Worker pool specs\n",
    "TRAINING_MACHINE_TYPE = \"n1-highmem-8\"\n",
    "ACCELERATOR_TYPE = \"NVIDIA_TESLA_T4\"\n",
    "ACCELERATOR_COUNT = 1\n",
    "EVAL_MACHINE_TYPE = \"n1-highmem-8\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zAaMJKrhAe5L"
   },
   "source": [
    "## 定义组件\n",
    "\n",
    "该流水线由以下组件组成：\n",
    "\n",
    "- **train-tfhub-model** - 使用预构建的 Docker 镜像训练新的 Tensorflow 模型，使用 TFHub 层\n",
    "- **upload-tensorflow-model-to-google-cloud-vertex-ai** - 将生成的模型上传到 Vertex AI 模型注册表\n",
    "- **get-vertex-model** - 获取刚刚上传为管道中的一个工件的模型\n",
    "- **convert-dataset-export-for-batch-predict** - 预处理组件，接收从 Vertex 数据集导出的测试数据集，并将其转换为可读取的简化兼容数据集，用于批量预测组件\n",
    "- **target-field-data-remover** - 移除测试数据集中的目标字段（即标签），用于下游批处理预测组件\n",
    "- **model-batch-predict** - 执行批量预测作业\n",
    "- **model-evaluation-classification** - 从上述批量预测作业计算评估指标，并导出指标工件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DKe2iQNKgpKG"
   },
   "outputs": [],
   "source": [
    "# Load upload TF model component\n",
    "upload_tensorflow_model_to_vertex_op = components.load_component_from_url(\n",
    "    \"https://raw.githubusercontent.com/Ark-kun/pipeline_components/c6a8b67d1ada2cc17665c99ff6b410df588bee28/components/google-cloud/Vertex_AI/Models/Upload_Tensorflow_model/workaround_for_buggy_KFPv2_compiler/component.yaml\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TEnh9Pcx6Xfi"
   },
   "source": [
    "### 定义管道\n",
    "\n",
    "该管道执行以下步骤：\n",
    "- 训练新的文本分类模型\n",
    "- 将模型上传至 Vertex AI 模型注册表\n",
    "- 对测试数据集进行预处理步骤导出：对于批量预测格式化数据，移除目标字段\n",
    "- 对预处理后的测试数据进行批量预测\n",
    "- 根据批量预测输出评估模型的表现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2a67cde8"
   },
   "outputs": [],
   "source": [
    "@dsl.pipeline(name=\"text-classification-model\")\n",
    "def pipeline():\n",
    "    train_task = natural_language.TrainTextClassificationOp()(\n",
    "        project=PROJECT_ID,\n",
    "        location=LOCATION,\n",
    "        machine_type=TRAINING_MACHINE_TYPE,\n",
    "        accelerator_type=ACCELERATOR_TYPE,\n",
    "        accelerator_count=ACCELERATOR_COUNT,\n",
    "        input_data_path=TRAINING_DATA_LOCATION,\n",
    "        input_format=\"jsonl\",\n",
    "        natural_language_task_type=TASK_TYPE,\n",
    "    )\n",
    "\n",
    "    upload_task = upload_tensorflow_model_to_vertex_op(\n",
    "        model=train_task.outputs[\"model_output\"]\n",
    "    )\n",
    "\n",
    "    get_model_task = GetVertexModelOp(\n",
    "        model_resource_name=upload_task.outputs[\"model_name\"]\n",
    "    )\n",
    "\n",
    "    classification_type = (\n",
    "        \"multilabel\" if TASK_TYPE == \"MULTILABEL_CLASSIFICATION\" else \"multiclass\"\n",
    "    )\n",
    "\n",
    "    convert_dataset_task = natural_language.ConvertDatasetExportForBatchPredictOp(\n",
    "        file_paths=TEST_DATA_URIS, classification_type=classification_type\n",
    "    )\n",
    "\n",
    "    target_field_remover_task = TargetFieldDataRemoverOp(\n",
    "        project=PROJECT_ID,\n",
    "        location=LOCATION,\n",
    "        root_dir=BUCKET_URI,\n",
    "        gcs_source_uris=convert_dataset_task.outputs[\"output_files\"],\n",
    "        target_field_name=\"labels\",\n",
    "        instances_format=\"jsonl\",\n",
    "    )\n",
    "\n",
    "    # Note: ModelBatchPredictOp doesn't support accelerators currently.\n",
    "    batch_predict_task = ModelBatchPredictOp(\n",
    "        project=PROJECT_ID,\n",
    "        location=LOCATION,\n",
    "        model=get_model_task.outputs[\"model\"],\n",
    "        job_display_name=\"nl-batch-predict-evaluation\",\n",
    "        gcs_source_uris=target_field_remover_task.outputs[\"gcs_output_directory\"],\n",
    "        instances_format=\"jsonl\",\n",
    "        predictions_format=\"jsonl\",\n",
    "        gcs_destination_output_uri_prefix=BUCKET_URI,\n",
    "        machine_type=EVAL_MACHINE_TYPE,\n",
    "    )\n",
    "\n",
    "    # Note: Because we're running a custom training pipeline, the model source\n",
    "    # is detected as Custom and thus it doesn't use AutoML NL's default settings\n",
    "    # and fails if class_labels is excluded.\n",
    "    ModelEvaluationClassificationOp(\n",
    "        project=PROJECT_ID,\n",
    "        location=LOCATION,\n",
    "        root_dir=BUCKET_URI,\n",
    "        class_labels=CLASS_NAMES + [\"[UNK]\"],\n",
    "        predictions_gcs_source=batch_predict_task.outputs[\"gcs_output_directory\"],\n",
    "        predictions_format=\"jsonl\",\n",
    "        prediction_label_column=\"prediction.displayNames\",\n",
    "        prediction_score_column=\"prediction.confidences\",\n",
    "        ground_truth_gcs_source=convert_dataset_task.outputs[\"output_files\"],\n",
    "        ground_truth_format=\"jsonl\",\n",
    "        target_field_name=\"labels\",\n",
    "        classification_type=TASK_TYPE,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3211ba19"
   },
   "source": [
    "### 编译管道"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c368c73f"
   },
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(pipeline, \"nl_pipeline.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_Vxwz5cdF5f"
   },
   "source": [
    "运行上面的代码行将在本地或Colab的目录中生成一个文件。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ax0jOxIaholy"
   },
   "source": [
    "### 运行管道\n",
    "\n",
    "这将向 Vertex Pipelines 发送一个创建管道作业请求。请注意，此任务是同步运行的，可能需要一段时间才能完成。\n",
    "\n",
    "您可以随时通过单击生成的链接（在下面单元格的控制台输出中“查看管道作业”之后）来查看作业的进度。一旦管道完成，您可以检查从该管道产生的产物。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wfs7QOSxhp_n"
   },
   "outputs": [],
   "source": [
    "job = aiplatform.PipelineJob(\n",
    "    display_name=\"nl_pipeline\",\n",
    "    template_path=\"nl_pipeline.json\",\n",
    "    location=LOCATION,\n",
    "    enable_caching=True,\n",
    "    parameter_values={},\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UIyGPaihWJWn"
   },
   "source": [
    "一旦管道成功完成，请转到管道并检查结果指标的结果物。否则，请参考管道中失败的步骤，以确定任何错误的原因。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OoexTJTy9jnH"
   },
   "source": [
    "## 查看模型评估结果\n",
    "\n",
    "要在管道执行后检查评估结果，请在由该管道创建的云存储存储桶中找到\"model-evaluation-classification\"子目录。您也可以运行以下命令直接输出度量文件的内容："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h9EqPCQF9lN9"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "EVAL_TASK_NAME = \"model-evaluation-classification\"\n",
    "PROJECT_NUMBER = job.gca_resource.name.split(\"/\")[1]\n",
    "for _ in range(len(job.gca_resource.job_detail.task_details)):\n",
    "    TASK_ID = job.gca_resource.job_detail.task_details[_].task_id\n",
    "    EVAL_METRICS = (\n",
    "        BUCKET_URI\n",
    "        + \"/\"\n",
    "        + PROJECT_NUMBER\n",
    "        + \"/\"\n",
    "        + job.name\n",
    "        + \"/\"\n",
    "        + EVAL_TASK_NAME\n",
    "        + \"_\"\n",
    "        + str(TASK_ID)\n",
    "        + \"/executor_output.json\"\n",
    "    )\n",
    "    if tf.io.gfile.exists(EVAL_METRICS):\n",
    "        ! gsutil cat $EVAL_METRICS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "清理资源\n",
    "\n",
    "要清理此流水线使用的资源，请运行以下命令："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sx_vKniMq9ZX"
   },
   "outputs": [],
   "source": [
    "# Delete GCS bucket.\n",
    "!gsutil -m rm -r {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMuyzrnZLoUa"
   },
   "source": [
    "下一步\n",
    "\n",
    "如需另一种方法，请查看[\"ready-to-go\"文本分类管道](https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/master/notebooks/community/pipelines/google_cloud_pipeline_components_ready_to_go_text_classification_pipeline.ipynb)。该管道提供了模型逻辑以供进一步定制，如果需要的话，并添加了一个额外的管道步骤来部署模型以启用在线预测。"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "d975e698c9a4",
    "08d289fa873f",
    "d33c87e4-2ada-4b87-bf75-064247f3162d",
    "3211ba19",
    "TpV-iwP9qw9c",
    "UMuyzrnZLoUa"
   ],
   "name": "google_cloud_pipeline_components_cloud_natural_language_pipeline.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
