{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# 使用BigQuery ML和Vertex AI进行异常检测\n",
    "\n",
    "<table align=\"left\">\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/tree/main/notebooks/community/pipelines/google_cloud_pipeline_components_bqml_pipeline_anomaly_detection.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> 在Colab中运行\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/pipelines/google_cloud_pipeline_components_bqml_pipeline_anomaly_detection.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      在GitHub上查看\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/pipelines/google_cloud_pipeline_components_bqml_pipeline_anomaly_detection.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      在Vertex AI Workbench中打开\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24743cf4a1e1"
   },
   "source": [
    "**_注意_**：此笔记本已在以下环境中进行测试：\n",
    "\n",
    "* Python版本 = 3.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## 概述\n",
    "\n",
    "异常检测是利用机器学习识别与数据明显偏离的罕见观测值。异常检测可以以多种方式进行。有监督的、无监督的、基于图的方式。对于某些行业，如电信、制造业和金融服务，异常检测尤为重要。\n",
    "\n",
    "例如，在制造业场景中，您可能收集一些传感器数据来预测发动机故障前剩余循环次数（TTF）。通过这种方式，您可以采取行动并做出关于维护计划的决策。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d975e698c9a4"
   },
   "source": [
    "### 目标\n",
    "\n",
    "在没有标记数据的情况下，您可能想知道如何最好地创建异常检测器。\n",
    "\n",
    "在本笔记中，您将学习如何使用自动编码器来检测涡轮风扇发动机数据中的异常，并在此基础上构建异常检测管道。\n",
    "\n",
    "本教程使用以下谷歌云ML服务和资源：\n",
    "\n",
    "- `Vertex AI Pipelines`\n",
    "- `BigQuery ML pipeline components`\n",
    "\n",
    "执行的步骤包括：\n",
    "\n",
    "- 定义自定义评估和指标可视化组件\n",
    "- 定义管道：\n",
    "    - 在BigQuery中构建训练数据集\n",
    "    - 训练BigQuery自动编码器模型\n",
    "    - 评估BigQuery自动编码器模型\n",
    "    - 检查模型性能\n",
    "    - 在BigQuery中构建测试数据集\n",
    "    - 检测异常\n",
    "    - 生成MSE图以评估预测\n",
    "- 编译管道。\n",
    "- 执行管道。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08d289fa873f"
   },
   "source": [
    "数据集\n",
    "\n",
    "[`NASA涡轮风扇喷气发动机数据集`](https://www.kaggle.com/datasets/behrad3d/nasa-cmaps)是一个多变量时间序列数据集，其中每个时间序列描述一个不同的发动机。\n",
    "\n",
    "该数据集包含26列，其中包含了单个操作循环期间采集的数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aed92deeb4a0"
   },
   "source": [
    "费用\n",
    "\n",
    "此教程使用Google Cloud的可收费组件：\n",
    "\n",
    "- Vertex AI\n",
    "- BigQuery\n",
    "- Cloud Storage\n",
    "\n",
    "了解有关[Vertex AI价格](https://cloud.google.com/vertex-ai/pricing)，\n",
    "[BigQuery价格](https://cloud.google.com/bigquery/pricing)和\n",
    "[Cloud Storage价格](https://cloud.google.com/storage/pricing)的信息，\n",
    "并使用[定价计算器](https://cloud.google.com/products/calculator/)\n",
    "基于您的预期使用量生成费用估算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "安装以下所需的包以执行这个笔记本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2b4ef9b72d43"
   },
   "outputs": [],
   "source": [
    "# Install the packages\n",
    "! pip3 install --user --upgrade jinja2 google-cloud-bigquery kfp google-cloud-aiplatform google_cloud_pipeline_components -q --no-warn-conflicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58707a750154"
   },
   "source": [
    "### 仅限 Colab 使用：请取消注释以下单元格以重启核心。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f200f10a1da3"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs so that your environment can access the new packages\n",
    "# import IPython\n",
    "\n",
    "# app = IPython.Application.instance()\n",
    "# app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF1j6f9HApxa"
   },
   "source": [
    "## 开始之前\n",
    "\n",
    "### 设置您的 Google Cloud 项目\n",
    "\n",
    "**无论您使用什么笔记本环境，都需进行以下步骤。**\n",
    "\n",
    "1. [选择或创建一个 Google Cloud 项目](https://console.cloud.google.com/cloud-resource-manager)。当您第一次创建一个账号时，您将获得 $300 的免费信用用于计算/存储成本。\n",
    "\n",
    "2. [确保您的项目已启用计费功能](https://cloud.google.com/billing/docs/how-to/modify-project)。\n",
    "\n",
    "3. [启用 Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com)。\n",
    "\n",
    "4. 如果您是在本地运行此笔记本，您需要安装 [Cloud SDK](https://cloud.google.com/sdk)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WReHDGG5g0XY"
   },
   "source": [
    "#### 设置你的项目ID\n",
    "\n",
    "**如果你不知道你的项目ID**，请尝试以下操作：\n",
    "* 运行 `gcloud config list`。\n",
    "* 运行 `gcloud projects list`。\n",
    "* 查看支持页面：[查找项目ID](https://support.google.com/googleapi/answer/7014113)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oM1iC_MfAts1"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
    "\n",
    "# Set the project id\n",
    "! gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "#### 地区\n",
    "\n",
    "您还可以更改 Vertex AI 使用的 `REGION` 变量。了解有关 [Vertex AI 地区](https://cloud.google.com/vertex-ai/docs/general/locations) 的更多信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RXtUY-LAEB7c"
   },
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBCra4QMA2wR"
   },
   "source": [
    "### 验证您的Google云账户\n",
    "\n",
    "根据您的Jupyter环境，您可能需要手动进行身份验证。请按照以下相关指示进行操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74ccc9e52986"
   },
   "source": [
    "1. 顶点 AI 工作台\n",
    "* 无需操作，因为您已经通过身份验证。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "de775a3773ba"
   },
   "source": [
    "2. 本地的JupyterLab实例，请取消注释并运行:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "254614fa0c46"
   },
   "outputs": [],
   "source": [
    "# ! gcloud auth login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ef21552ccea8"
   },
   "source": [
    "3. 协同工作，取消注释并运行:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "603adbbf0532"
   },
   "outputs": [],
   "source": [
    "# from google.colab import auth\n",
    "# auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6b2ccc891ed"
   },
   "source": [
    "查看如何在https://cloud.google.com/storage/docs/gsutil/commands/iam#ch-examples 上向您的服务账号授予云存储权限。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "创建一个云存储桶\n",
    "\n",
    "创建一个存储桶，用于存储中间产物，例如数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "BUCKET_URI = \"gs://your-bucket-name-unique\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EcIXiGsCePi"
   },
   "source": [
    "只有当您的存储桶不存在时：运行以下单元格以创建您的云存储桶。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIq7R4HZCfIc"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gc1ubsMoF7wn"
   },
   "source": [
    "### 设置项目模板\n",
    "\n",
    "您创建一个存储库集合以在本地组织您的项目。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "420y8i4KF_z4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "KFP_COMPONENTS_PATH = \"components\"\n",
    "PIPELINES_PATH = \"pipelines\"\n",
    "TRAIN_PIPELINES_PATH = os.path.join(PIPELINES_PATH, \"train_pipelines\")\n",
    "TEST_PIPELINES_PATH = os.path.join(PIPELINES_PATH, \"test_pipelines\")\n",
    "\n",
    "! mkdir -m 777 -p {KFP_COMPONENTS_PATH} {TRAIN_PIPELINES_PATH} {TEST_PIPELINES_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WRRE4t_bdzj_"
   },
   "source": [
    "### 准备训练数据\n",
    "\n",
    "接下来，您将将CSV训练数据复制到您的云存储存储桶中，然后为训练数据创建一个BigQuery数据集表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8u7VsRuLaaVC"
   },
   "outputs": [],
   "source": [
    "PUBLIC_DATA_URI = (\n",
    "    \"gs://cloud-samples-data/vertex-ai/pipeline-deployment/datasets/turbofan_anomaly\"\n",
    ")\n",
    "GCS_TRAIN_URI = f\"{PUBLIC_DATA_URI}/train_FD001.csv\"\n",
    "GCS_TEST_URI = f\"{PUBLIC_DATA_URI}/test_FD001.csv\"\n",
    "GCS_LABELS_URI = f\"{PUBLIC_DATA_URI}/RUL_FD001.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wf3vIGViHYo3"
   },
   "source": [
    "### 设置BigQuery数据集\n",
    "\n",
    "您为本教程创建了以下BigQuery数据集：\n",
    "\n",
    "- `sensors_train_raw_data_<timestamp>` 包含从传感器收集的训练数据\n",
    "- `sensors_test_raw_data_<timestamp>` 包含从传感器收集的测试数据\n",
    "- `sensors_label_data_<timestamp>` 包含用于验证结果的测试标签数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L-2dBWfq1FPq"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ZfxQL8JHdHE"
   },
   "outputs": [],
   "source": [
    "LOCATION = REGION.split(\"-\")[0]\n",
    "BQ_DATASET = \"iot_dataset\"\n",
    "BQ_TRAIN_RAW_TABLE = f\"sensors_train_raw_data_{TIMESTAMP}\"\n",
    "BQ_TEST_RAW_TABLE = f\"sensors_test_raw_data_{TIMESTAMP}\"\n",
    "BQ_LABELS_TABLE = f\"sensors_label_data_{TIMESTAMP}\"\n",
    "\n",
    "! bq mk --location={LOCATION} --dataset {PROJECT_ID}:{BQ_DATASET}\n",
    "\n",
    "! bq load \\\n",
    "  --location={LOCATION} \\\n",
    "  --source_format=CSV \\\n",
    "  --skip_leading_rows=1 \\\n",
    "  {BQ_DATASET}.{BQ_TRAIN_RAW_TABLE} \\\n",
    "  {GCS_TRAIN_URI} \\\n",
    "  id:INT64,cycle:INT64,setting1:FLOAT64,setting2:FLOAT64,setting3:FLOAT64,sensor:STRING,value:FLOAT64\n",
    "\n",
    "! bq load \\\n",
    "  --location={LOCATION} \\\n",
    "  --source_format=CSV \\\n",
    "  --skip_leading_rows=1 \\\n",
    "  {BQ_DATASET}.{BQ_TEST_RAW_TABLE} \\\n",
    "  {GCS_TEST_URI} \\\n",
    "  id:INT64,cycle:INT64,setting1:FLOAT64,setting2:FLOAT64,setting3:FLOAT64,sensor:STRING,value:FLOAT64\n",
    "\n",
    "! bq load \\\n",
    "  --location={LOCATION} \\\n",
    "  --source_format=CSV \\\n",
    "  --skip_leading_rows=1 \\\n",
    "  {BQ_DATASET}.{BQ_LABELS_TABLE} \\\n",
    "  {GCS_LABELS_URI} \\\n",
    "  id:INT64,time_to_failure:FLOAT64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "960505627ddf"
   },
   "source": [
    "### 导入库\n",
    "\n",
    "接下来，导入库并设置一些在本教程中使用的变量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PyQmSRbKA8r-"
   },
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "import tensorflow as tf\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud import bigquery\n",
    "from google_cloud_pipeline_components.v1.bigquery import (\n",
    "    BigqueryCreateModelJobOp, BigqueryEvaluateModelJobOp, BigqueryQueryJobOp)\n",
    "from jinja2 import Template\n",
    "from kfp.v2 import compiler, dsl\n",
    "from kfp.v2.dsl import (HTML, Artifact, Condition, Input, Metrics, Output,\n",
    "                        component)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SOskzw0enAyi"
   },
   "source": [
    "设定变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Sf2DSzwnD8H"
   },
   "outputs": [],
   "source": [
    "# SQL templates\n",
    "SENSORS = (\n",
    "    \"s1\",\n",
    "    \"s2\",\n",
    "    \"s3\",\n",
    "    \"s4\",\n",
    "    \"s5\",\n",
    "    \"s6\",\n",
    "    \"s7\",\n",
    "    \"s8\",\n",
    "    \"s9\",\n",
    "    \"s10\",\n",
    "    \"s11\",\n",
    "    \"s12\",\n",
    "    \"s13\",\n",
    "    \"s14\",\n",
    "    \"s15\",\n",
    "    \"s16\",\n",
    "    \"s17\",\n",
    "    \"s18\",\n",
    "    \"s19\",\n",
    "    \"s20\",\n",
    "    \"s21\",\n",
    ")\n",
    "WINDOW = 5\n",
    "PERIOD = 30\n",
    "TARGET = \"is_anomalous_ttf\"\n",
    "EXCLUDED_VARIABLES = \"id, cycle, setting1, setting2, setting3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tyDIqmRWsjL4"
   },
   "source": [
    "### 辅助函数\n",
    "\n",
    "`print_pipeline_output` 辅助函数允许验证管道运行，检查已执行的作业。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3m59SG8Vsk50"
   },
   "outputs": [],
   "source": [
    "def print_pipeline_output(pipeline_root, job, output_task_name):\n",
    "    JOB_ID = job.name\n",
    "    print(JOB_ID)\n",
    "    for _ in range(len(job.gca_resource.job_detail.task_details)):\n",
    "        TASK_ID = job.gca_resource.job_detail.task_details[_].task_id\n",
    "        EXECUTE_OUTPUT = (\n",
    "            pipeline_root\n",
    "            + \"/\"\n",
    "            + PROJECT_NUMBER\n",
    "            + \"/\"\n",
    "            + JOB_ID\n",
    "            + \"/\"\n",
    "            + output_task_name\n",
    "            + \"_\"\n",
    "            + str(TASK_ID)\n",
    "            + \"/executor_output.json\"\n",
    "        )\n",
    "        GCP_RESOURCES = (\n",
    "            pipeline_root\n",
    "            + \"/\"\n",
    "            + PROJECT_NUMBER\n",
    "            + \"/\"\n",
    "            + JOB_ID\n",
    "            + \"/\"\n",
    "            + output_task_name\n",
    "            + \"_\"\n",
    "            + str(TASK_ID)\n",
    "            + \"/gcp_resources\"\n",
    "        )\n",
    "        EVAL_METRICS = (\n",
    "            pipeline_root\n",
    "            + \"/\"\n",
    "            + PROJECT_NUMBER\n",
    "            + \"/\"\n",
    "            + JOB_ID\n",
    "            + \"/\"\n",
    "            + output_task_name\n",
    "            + \"_\"\n",
    "            + str(TASK_ID)\n",
    "            + \"/evaluation_metrics\"\n",
    "        )\n",
    "        if tf.io.gfile.exists(EXECUTE_OUTPUT):\n",
    "            ! gsutil cat $EXECUTE_OUTPUT\n",
    "            return EXECUTE_OUTPUT\n",
    "        elif tf.io.gfile.exists(GCP_RESOURCES):\n",
    "            ! gsutil cat $GCP_RESOURCES\n",
    "            return GCP_RESOURCES\n",
    "        elif tf.io.gfile.exists(EVAL_METRICS):\n",
    "            ! gsutil cat $EVAL_METRICS\n",
    "            return EVAL_METRICS\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "source": [
    "### 初始化用于 Python 的 Vertex AI SDK\n",
    "\n",
    "为您的项目和相应的存储桶初始化 Python 版的 Vertex AI SDK。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OTWzEnp4EB7e"
   },
   "outputs": [],
   "source": [
    "vertex_ai.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L9iMCmfFBn6e"
   },
   "source": [
    "### 初始化Python的BigQuery SDK\n",
    "\n",
    "为您的项目初始化Python的BigQuery SDK。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Md0UdedBn6f"
   },
   "outputs": [],
   "source": [
    "bq_client = bigquery.Client(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QpNpPUp0m80Y"
   },
   "source": [
    "## BigQuery ML管道规范化\n",
    "\n",
    "在接下来的单元格中，您将构建组件和管道，以训练和评估异常检测模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RLwG5IBnF_f0"
   },
   "source": [
    "### 为运行管道设置变量\n",
    "\n",
    "在这里，您初始化了一组特定于本教程中要运行的管道的变量。例如，您定义了管道配置，传递了训练表名称、模型配置和性能阈值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E2mtPcwyGE1P"
   },
   "outputs": [],
   "source": [
    "# BQML pipeline job configuation\n",
    "TRAIN_PIPELINE_NAME = \"bqml-anomaly-detection-train-pipeline\"\n",
    "TRAIN_PIPELINE_ROOT = (\n",
    "    urlparse(BUCKET_URI)._replace(path=\"pipelines/train_pipelines\").geturl()\n",
    ")\n",
    "TRAIN_PIPELINE_PACKAGE = os.path.join(\n",
    "    TRAIN_PIPELINES_PATH, f\"{TRAIN_PIPELINE_NAME}.json\"\n",
    ")\n",
    "\n",
    "# BQML pipeline conponent configuration\n",
    "BQ_TRAIN_FEATURES_TABLE_PREFIX = \"train_features\"\n",
    "BQ_TEST_FEATURES_TABLE_PREFIX = \"test_features\"\n",
    "BQ_TRAIN_TABLE_PREFIX = \"train_dataset\"\n",
    "BQ_TEST_TABLE_PREFIX = \"test_dataset\"\n",
    "BQ_RECOSTRUCTION_MODEL_TABLE_PREFIX = \"reconstruction_model\"\n",
    "DETECT_ANOMALIES_TABLE_PREFIX = \"detect_anomalies\"\n",
    "BQ_TRAIN_FEATURES_TABLE = f\"{BQ_TRAIN_FEATURES_TABLE_PREFIX}_{TIMESTAMP}\"\n",
    "BQ_TEST_FEATURES_TABLE = f\"{BQ_TEST_FEATURES_TABLE_PREFIX}_{TIMESTAMP}\"\n",
    "BQ_TRAIN_TABLE = f\"{BQ_TRAIN_TABLE_PREFIX}_{TIMESTAMP}\"\n",
    "BQ_TEST_TABLE = f\"{BQ_TEST_TABLE_PREFIX}_{TIMESTAMP}\"\n",
    "BQ_RECOSTRUCTION_MODEL_TABLE = f\"{BQ_RECOSTRUCTION_MODEL_TABLE_PREFIX}_{TIMESTAMP}\"\n",
    "DETECT_ANOMALIES_TABLE = f\"{DETECT_ANOMALIES_TABLE_PREFIX}_{TIMESTAMP}\"\n",
    "CONTAMINATION_THRESHOLD = 0.1\n",
    "PERF_THRESHOLD = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m59LNgggyEYG"
   },
   "source": [
    "### 使用模板设置 SQL 查询\n",
    "\n",
    "在 Vertex AI 上运行 BigQuery 和 BigQuery ML 管道的一种方法是将 SQL 查询定义为 Jinja 模板，并将它们作为 `pipeline components` 的参数传递。\n",
    "\n",
    "在本教程中，您定义以下模板：\n",
    "\n",
    "- `CREATE_FEATURES_SQL_TEMPLATE` 用于运行特征工程\n",
    "- `CREATE_TRAIN_SQL_TEMPLATE` 用于创建训练数据集\n",
    "- `TRAIN_RECONSTRUCTION_MODEL_TEMPLATE` 用于使用 BigQuery ML AutoEncoder 模型构建重建模型\n",
    "- `CREATE_TEST_SQL_TEMPLATE` 用于创建测试数据集\n",
    "- `DETECT_ANOMALIES_TEMPLATE` 用于检测异常\n",
    "- `VISUALIZE_MSE_TEMPLATE` 用于可视化 MSE 图表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P4c0wqn5GE8w"
   },
   "source": [
    "定义SQL查询模板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gss6JR0YyaS3"
   },
   "outputs": [],
   "source": [
    "# Training ---------------------------------------------------------------------\n",
    "CREATE_FEATURES_SQL_TEMPLATE = \"\"\"\n",
    "CREATE OR REPLACE TABLE\n",
    "  `{{project_id}}.{{bq_dataset}}.{{features_table}}` AS\n",
    "WITH\n",
    "  get_long_from_wide_table AS (\n",
    "    SELECT *\n",
    "    FROM `{{project_id}}.{{bq_dataset}}.{{data_table}}`\n",
    "    PIVOT(MAX(value) FOR sensor IN {{sensors}})\n",
    "  ),\n",
    "\n",
    "  get_features_table AS (\n",
    "    SELECT\n",
    "    *,\n",
    "    {%- for sensor in sensors %}\n",
    "    -- calculate rolling average sensor value\n",
    "    AVG({{sensor}}) OVER(PARTITION BY id ORDER BY cycle RANGE BETWEEN {{window}} PRECEDING AND CURRENT ROW) AS {{\"rolling_avg_\" ~ sensor}},\n",
    "    -- calculate rolling stdev sensor value\n",
    "    IFNULL(STDDEV({{sensor}}) OVER(PARTITION BY id ORDER BY cycle RANGE BETWEEN {{window}} PRECEDING AND CURRENT ROW), 0) AS {{\"rolling_sd_\" ~ sensor}}\n",
    "    {%- if not loop.last -%}\n",
    "        ,\n",
    "    {%- endif -%}\n",
    "    {%- endfor %}\n",
    "    FROM get_long_from_wide_table\n",
    "  )\n",
    "\n",
    "  SELECT * FROM get_features_table ORDER BY id, cycle\n",
    "\"\"\"\n",
    "\n",
    "CREATE_TRAIN_SQL_TEMPLATE = \"\"\"\n",
    "DECLARE period INT64 DEFAULT {{period}};\n",
    "\n",
    "CREATE OR REPLACE TABLE\n",
    "  `{{project_id}}.{{bq_dataset}}.{{train_table}}` AS\n",
    "WITH\n",
    "  get_last_cycle AS (\n",
    "    SELECT id, max(cycle) as last_cycle\n",
    "    FROM `{{project_id}}.{{bq_dataset}}.{{features_table}}`\n",
    "    GROUP BY id\n",
    "  ),\n",
    "\n",
    "  get_target_train AS (\n",
    "    SELECT\n",
    "    a.*,\n",
    "    CASE WHEN (b.last_cycle - a.cycle) < period THEN 1 ELSE 0 END AS {{target}},\n",
    "    FROM `{{project_id}}.{{bq_dataset}}.{{features_table}}` as a\n",
    "    LEFT JOIN get_last_cycle as b on a.id = b.id\n",
    "  )\n",
    "\n",
    "  SELECT * EXCEPT({{excluded_variables}}) FROM get_target_train\n",
    "\"\"\"\n",
    "\n",
    "TRAIN_RECONSTRUCTION_MODEL_TEMPLATE = \"\"\"\n",
    "CREATE OR REPLACE MODEL `{{project_id}}.{{bq_dataset}}.{{recostruction_model_name}}`\n",
    "OPTIONS(MODEL_TYPE='AUTOENCODER',\n",
    "        ACTIVATION_FN='RELU',\n",
    "        HIDDEN_UNITS=[32, 16, 4, 16, 32],\n",
    "        BATCH_SIZE=8,\n",
    "        DROPOUT=0.2,\n",
    "        EARLY_STOP=TRUE,\n",
    "        LEARN_RATE=0.001,\n",
    "        L1_REG_ACTIVATION=0.0001,\n",
    "        OPTIMIZER='ADAM',\n",
    "        MODEL_REGISTRY = 'vertex_ai',\n",
    "        VERTEX_AI_MODEL_ID = 'reconstruction_model',\n",
    "        VERTEX_AI_MODEL_VERSION_ALIASES = ['staging']\n",
    "        )\n",
    "AS SELECT * FROM `{{project_id}}.{{bq_dataset}}.{{train_table}}`\n",
    "\"\"\"\n",
    "\n",
    "# Test -------------------------------------------------------------------------\n",
    "CREATE_TEST_SQL_TEMPLATE = \"\"\"\n",
    "DECLARE period INT64 DEFAULT {{period}};\n",
    "\n",
    "CREATE OR REPLACE TABLE\n",
    " `{{project_id}}.{{bq_dataset}}.{{test_table}}` AS\n",
    "WITH\n",
    " get_last_cycle AS (\n",
    "   SELECT id, max(cycle) as last_cycle\n",
    "   FROM `{{project_id}}.{{bq_dataset}}.{{features_table}}`\n",
    "   GROUP BY id\n",
    " ),\n",
    "\n",
    " get_target_test AS (\n",
    "   SELECT\n",
    "   a.*\n",
    "   FROM `{{project_id}}.{{bq_dataset}}.{{features_table}}` as a\n",
    "   LEFT JOIN get_last_cycle as b ON a.id = b.id\n",
    "   WHERE a.cycle = b.last_cycle\n",
    " )\n",
    "\n",
    " SELECT\n",
    " a.*,\n",
    " CASE WHEN b.time_to_failure < period THEN 1 ELSE 0 END AS {{target}}\n",
    " FROM get_target_test as a\n",
    " LEFT JOIN `{{project_id}}.{{bq_dataset}}.{{labels_table}}` as b ON a.id = b.id\n",
    "\"\"\"\n",
    "\n",
    "DETECT_ANOMALIES_TEMPLATE = \"\"\"\n",
    "CREATE OR REPLACE TABLE\n",
    "  `{{project_id}}.{{bq_dataset}}.{{anomalies_table}}` AS\n",
    "SELECT\n",
    "  is_anomaly, mean_squared_error, {{target}}\n",
    "FROM\n",
    "  ML.DETECT_ANOMALIES(MODEL `{{project_id}}.{{bq_dataset}}.{{recostruction_model_name}}`,\n",
    "                      STRUCT({{contamination_thr}} AS contamination),\n",
    "                      TABLE `{{project_id}}.{{bq_dataset}}.{{test_table}}`)\n",
    "\"\"\"\n",
    "\n",
    "VISUALIZE_MSE_TEMPLATE = \"\"\"\n",
    "SELECT\n",
    "  *\n",
    "FROM\n",
    "  `{{project_id}}.{{bq_dataset}}.{{anomalies_table}}`\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KIUYHHbiGPWe"
   },
   "source": [
    "### 编译SQL查询模板\n",
    "\n",
    "在定义SQL查询模板之后，您可以编译它们，并传递训练和测试参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lD-Q1Qek6Xrc"
   },
   "outputs": [],
   "source": [
    "# Training parameters specification\n",
    "TRAIN_SQL_PARAMS = dict(\n",
    "    project_id=PROJECT_ID,\n",
    "    bq_dataset=BQ_DATASET,\n",
    "    sensors=SENSORS,\n",
    "    period=PERIOD,\n",
    "    window=WINDOW,\n",
    "    target=TARGET,\n",
    "    excluded_variables=EXCLUDED_VARIABLES,\n",
    "    contamination_threshold=CONTAMINATION_THRESHOLD,\n",
    "    data_table=BQ_TRAIN_RAW_TABLE,\n",
    "    features_table=BQ_TRAIN_FEATURES_TABLE,\n",
    "    train_table=BQ_TRAIN_TABLE,\n",
    "    recostruction_model_name=BQ_RECOSTRUCTION_MODEL_TABLE,\n",
    "    anomalies_table=DETECT_ANOMALIES_TABLE,\n",
    "    contamination_thr=CONTAMINATION_THRESHOLD,\n",
    ")\n",
    "\n",
    "CREATE_TRAIN_FEATURES_QUERY = Template(CREATE_FEATURES_SQL_TEMPLATE).render(\n",
    "    TRAIN_SQL_PARAMS\n",
    ")\n",
    "CREATE_TRAIN_TABLE_QUERY = Template(CREATE_TRAIN_SQL_TEMPLATE).render(TRAIN_SQL_PARAMS)\n",
    "TRAIN_RECOSTRUCTION_MODEL_QUERY = Template(TRAIN_RECONSTRUCTION_MODEL_TEMPLATE).render(\n",
    "    TRAIN_SQL_PARAMS\n",
    ")\n",
    "\n",
    "# Testing parameters specification\n",
    "TEST_SQL_PARAMS = dict(\n",
    "    project_id=PROJECT_ID,\n",
    "    bq_dataset=BQ_DATASET,\n",
    "    sensors=SENSORS,\n",
    "    period=PERIOD,\n",
    "    window=WINDOW,\n",
    "    data_table=BQ_TEST_RAW_TABLE,\n",
    "    labels_table=BQ_LABELS_TABLE,\n",
    "    target=TARGET,\n",
    "    features_table=BQ_TEST_FEATURES_TABLE,\n",
    "    test_table=BQ_TEST_TABLE,\n",
    "    recostruction_model_name=BQ_RECOSTRUCTION_MODEL_TABLE,\n",
    "    anomalies_table=DETECT_ANOMALIES_TABLE,\n",
    "    contamination_thr=CONTAMINATION_THRESHOLD,\n",
    ")\n",
    "\n",
    "CREATE_TEST_FEATURES_QUERY = Template(CREATE_FEATURES_SQL_TEMPLATE).render(\n",
    "    TEST_SQL_PARAMS\n",
    ")\n",
    "CREATE_TEST_TABLE_QUERY = Template(CREATE_TEST_SQL_TEMPLATE).render(TEST_SQL_PARAMS)\n",
    "DETECT_ANOMALIES_QUERY = Template(DETECT_ANOMALIES_TEMPLATE).render(TEST_SQL_PARAMS)\n",
    "VISUALIZE_MSE_QUERY = Template(VISUALIZE_MSE_TEMPLATE).render(TRAIN_SQL_PARAMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZHOP2TzBn6k"
   },
   "source": [
    "创建一个自定义组件，用于读取模型评估指标\n",
    "\n",
    "使用Kubeflow SDK可视化API，在Vertex AI管道UI中构建一个自定义组件，用于消费模型评估指标并进行可视化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iybwx_Z4Bn6k"
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.8-slim\",\n",
    "    output_component_file=f\"{KFP_COMPONENTS_PATH}/build_bq_evaluate_metrics.yaml\",\n",
    ")\n",
    "def get_model_evaluation_metrics(\n",
    "    metrics_in: Input[Artifact],\n",
    "    metrics_out: Output[Metrics],\n",
    "    model_out: Output[Artifact],\n",
    ") -> NamedTuple(\"Outputs\", [(\"mean_squared_error\", float)]):\n",
    "    \"\"\"\n",
    "    Get the average mean absolute error from the metrics\n",
    "    Args:\n",
    "        metrics_in: metrics artifact\n",
    "        metrics_out: resulting metrics artifact\n",
    "        model_out: resulting model artifact\n",
    "    Returns:\n",
    "        avg_mean_absolute_error: average mean absolute error\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract rows and schema from metrics artifact\n",
    "    rows = metrics_in.metadata[\"rows\"]\n",
    "    schema = metrics_in.metadata[\"schema\"]\n",
    "\n",
    "    # Convert into a dictionary format\n",
    "    columns = [metrics[\"name\"] for metrics in schema[\"fields\"] if \"name\" in metrics]\n",
    "    records = [dl[\"v\"] for dl in rows[0][\"f\"]]\n",
    "    metrics = {key: round(float(value), 3) for key, value in zip(columns, records)}\n",
    "\n",
    "    # Log metrics\n",
    "    for key in metrics.keys():\n",
    "        metrics_out.log_metric(key, metrics[key])\n",
    "\n",
    "    # Return the target metrics\n",
    "    mean_absolute_error = metrics[\"mean_squared_error\"]\n",
    "    component_outputs = NamedTuple(\"Outputs\", [(\"mean_squared_error\", float)])\n",
    "\n",
    "    # model metadata\n",
    "    model_framework = \"BQML\"\n",
    "    model_type = \"AutoEncoder\"\n",
    "    model_user = \"Author\"\n",
    "    model_function = \"Reconstruction model\"\n",
    "    model_out.metadata[\"framework\"] = model_framework\n",
    "    model_out.metadata[\"type\"] = model_type\n",
    "    model_out.metadata[\"model function\"] = model_function\n",
    "    model_out.metadata[\"modified by\"] = model_user\n",
    "\n",
    "    return component_outputs(mean_absolute_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yEVHpbmiUmzB"
   },
   "source": [
    "### 创建一个自定义组件来可视化每个标签的MSE\n",
    "\n",
    "使用Kubeflow SDK的可视化API，在Vertex AI Pipelines UI中构建一个自定义组件来可视化每个标签的MSE。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QjJWfui7U9ux"
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.8-slim\",\n",
    "    packages_to_install=[\"pandas\", \"google-cloud-bigquery[bqstorage,pandas]\", \"plotly\"],\n",
    "    output_component_file=f\"{KFP_COMPONENTS_PATH}/build_evaluation_plot.yaml\",\n",
    ")\n",
    "def get_mse_plots(\n",
    "    query: str,\n",
    "    project: str,\n",
    "    location: str,\n",
    "    metrics_out: Output[HTML],\n",
    "    model_out: Output[Artifact],\n",
    "):\n",
    "    \"\"\"\n",
    "    Get the mean squared error per labels\n",
    "    Args:\n",
    "        query: the query to generate the metrics\n",
    "        project: the project id to iniziate the BQ client\n",
    "        location: the region to iniziate the BQ client\n",
    "        metrics_out: resulting metrics artifact\n",
    "        model_out: resulting model artifact\n",
    "    Returns:\n",
    "        avg_mean_absolute_error: average mean absolute error\n",
    "    \"\"\"\n",
    "\n",
    "    import plotly.graph_objects as go\n",
    "    from google.cloud import bigquery\n",
    "    from plotly.subplots import make_subplots\n",
    "\n",
    "    # Initiate client\n",
    "    client = bigquery.Client(project=project, location=location)\n",
    "\n",
    "    # Run a Standard SQL query using the environment's default project\n",
    "    table_df = client.query(query).to_dataframe()\n",
    "\n",
    "    # Create anomalies/no anomalies datasets\n",
    "    anomalies_df = table_df.query(\"is_anomalous_ttf == 1\")\n",
    "    no_anomalies_df = table_df.query(\"is_anomalous_ttf == 0\")\n",
    "\n",
    "    # Create a figure with subplots\n",
    "    fig = make_subplots(\n",
    "        rows=2,\n",
    "        cols=2,\n",
    "        specs=[[{\"colspan\": 2}, None], [{}, {}]],\n",
    "        subplot_titles=(\n",
    "            \"Distribution of mean squared error (MSE) for anomaly and not anomaly sensor data\",\n",
    "            \"Distribution of mean squared error (MSE) for anomaly sensor data\",\n",
    "            \"Distribution of mean squared error (MSE) for not anomaly sensor data\",\n",
    "        ),\n",
    "        x_title=\"Mean squared error (MSE)\",\n",
    "        y_title=\"Density\",\n",
    "    )\n",
    "\n",
    "    # Add subplots to figure\n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=anomalies_df[\"mean_squared_error\"],\n",
    "            name=\"Anomaly\",\n",
    "            marker_color=\"blue\",\n",
    "            showlegend=True,\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=no_anomalies_df[\"mean_squared_error\"],\n",
    "            name=\"No Anomaly\",\n",
    "            marker_color=\"orange\",\n",
    "            showlegend=True,\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=anomalies_df[\"mean_squared_error\"],\n",
    "            name=\"MSE_1\",\n",
    "            marker_color=\"red\",\n",
    "            showlegend=False,\n",
    "        ),\n",
    "        row=2,\n",
    "        col=1,\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=no_anomalies_df[\"mean_squared_error\"],\n",
    "            name=\"MSE_2\",\n",
    "            marker_color=\"green\",\n",
    "            showlegend=False,\n",
    "        ),\n",
    "        row=2,\n",
    "        col=2,\n",
    "    )\n",
    "\n",
    "    # Update figure properties\n",
    "    fig.update_layout(\n",
    "        title=\"Anomaly detection report\",\n",
    "        title_x=0.5,\n",
    "        bargap=0.2,\n",
    "        bargroupgap=0.1,\n",
    "        showlegend=True,\n",
    "    )\n",
    "\n",
    "    # Save output to static HTML file\n",
    "    fig.write_html(metrics_out.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pcSL1FHk69KT"
   },
   "source": [
    "### 构建BQML训练流水线\n",
    "\n",
    "使用Kubeflow Pipelines DSL包定义您的工作流程。\n",
    "\n",
    "以下是流水线工作流程的步骤：\n",
    "\n",
    "1. 在BigQuery中构建训练数据集\n",
    "2. 训练BigQuery AutoEncoder模型\n",
    "3. 评估BigQuery AutoEncoder模型\n",
    "4. 检查模型性能\n",
    "5. 在BigQuery中构建测试数据集\n",
    "6. 检测异常\n",
    "7. 生成MSE图表以评估预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AlFXqsPIAk0l"
   },
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=TRAIN_PIPELINE_NAME,\n",
    "    description=\"A batch pipeline to train recostruction model using BQML\",\n",
    ")\n",
    "def pipeline(\n",
    "    create_train_features_query: str,\n",
    "    create_train_table_query: str,\n",
    "    train_recostruction_model_query: str,\n",
    "    create_test_features_query: str,\n",
    "    create_test_table_query: str,\n",
    "    generate_anomalies_query: str,\n",
    "    performance_thr: float,\n",
    "    visualize_mse_query: str,\n",
    "    project: str,\n",
    "    location: str,\n",
    "):\n",
    "\n",
    "    # Create training features\n",
    "    create_train_features_op = BigqueryQueryJobOp(\n",
    "        query=create_train_features_query,\n",
    "        project=project,\n",
    "        location=location,\n",
    "    ).set_display_name(\"build train features\")\n",
    "\n",
    "    # Create train dataset\n",
    "    create_train_dataset_op = (\n",
    "        BigqueryQueryJobOp(\n",
    "            query=create_train_table_query, project=project, location=location\n",
    "        )\n",
    "        .set_display_name(\"build train table\")\n",
    "        .after(create_train_features_op)\n",
    "    )\n",
    "\n",
    "    # Train the recostruction model\n",
    "    bq_recostruction_model_op = (\n",
    "        BigqueryCreateModelJobOp(\n",
    "            query=train_recostruction_model_query,\n",
    "            project=project,\n",
    "            location=location,\n",
    "        )\n",
    "        .set_display_name(\"train reconstruction model\")\n",
    "        .after(create_train_dataset_op)\n",
    "    )\n",
    "\n",
    "    # Evaluate recostruction model\n",
    "    bq_arima_evaluate_model_op = (\n",
    "        BigqueryEvaluateModelJobOp(\n",
    "            model=bq_recostruction_model_op.outputs[\"model\"],\n",
    "            project=project,\n",
    "            location=location,\n",
    "        )\n",
    "        .set_display_name(\"evaluate reconstruction model\")\n",
    "        .after(bq_recostruction_model_op)\n",
    "    )\n",
    "\n",
    "    # Plot model metrics\n",
    "    get_evaluation_model_metrics_op = (\n",
    "        get_model_evaluation_metrics(\n",
    "            bq_arima_evaluate_model_op.outputs[\"evaluation_metrics\"]\n",
    "        )\n",
    "        .after(bq_arima_evaluate_model_op)\n",
    "        .set_display_name(\"generate evaluation metrics\")\n",
    "    )\n",
    "\n",
    "    # Check the model performance. If AUTOENCODER MSE metric is below to a minimal threshold\n",
    "    with Condition(\n",
    "        get_evaluation_model_metrics_op.outputs[\"mean_squared_error\"] < performance_thr,\n",
    "        name=\"MSE good\",\n",
    "    ):\n",
    "\n",
    "        # Create test features dataset\n",
    "        create_test_features_op = BigqueryQueryJobOp(\n",
    "            query=create_test_features_query,\n",
    "            project=project,\n",
    "            location=location,\n",
    "        ).set_display_name(\"build test features\")\n",
    "\n",
    "        # Create test dataset\n",
    "        create_test_dataset_op = (\n",
    "            BigqueryQueryJobOp(\n",
    "                query=create_test_table_query, project=project, location=location\n",
    "            )\n",
    "            .set_display_name(\"build test table\")\n",
    "            .after(create_test_features_op)\n",
    "        )\n",
    "\n",
    "        # Generate anomalies\n",
    "        generate_anomalies_op = (\n",
    "            BigqueryQueryJobOp(\n",
    "                query=generate_anomalies_query,\n",
    "                project=project,\n",
    "                location=location,\n",
    "            )\n",
    "            .after(create_test_dataset_op)\n",
    "            .set_display_name(\"generate anomalies\")\n",
    "        )\n",
    "\n",
    "        # Plot mse graph of anomalies\n",
    "        _ = (\n",
    "            get_mse_plots(query=visualize_mse_query, project=project, location=location)\n",
    "            .after(generate_anomalies_op)\n",
    "            .set_display_name(\"plot mse report\")\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nghLONQX7JNg"
   },
   "source": [
    "将管道编译成一个 JSON 文件\n",
    "\n",
    "接下来，您将编译管道，这将为您的管道生成一个 JSON 规范。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8l6IR7OoADJV"
   },
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(pipeline_func=pipeline, package_path=TRAIN_PIPELINE_PACKAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gtlwu0Xo1WcT"
   },
   "source": [
    "执行您的流水线\n",
    "\n",
    "接下来，执行流水线。它使用您设置的以下参数作为默认值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zzg2JDlsG2cd"
   },
   "source": [
    "提交管道作业"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8meEMA6NO3aO"
   },
   "outputs": [],
   "source": [
    "TRAIN_PIPELINE_RUN_PARAMS = dict(\n",
    "    create_train_features_query=CREATE_TRAIN_FEATURES_QUERY,\n",
    "    create_train_table_query=CREATE_TRAIN_TABLE_QUERY,\n",
    "    train_recostruction_model_query=TRAIN_RECOSTRUCTION_MODEL_QUERY,\n",
    "    create_test_features_query=CREATE_TEST_FEATURES_QUERY,\n",
    "    create_test_table_query=CREATE_TEST_TABLE_QUERY,\n",
    "    generate_anomalies_query=DETECT_ANOMALIES_QUERY,\n",
    "    performance_thr=PERF_THRESHOLD,\n",
    "    visualize_mse_query=VISUALIZE_MSE_QUERY,\n",
    "    project=PROJECT_ID,\n",
    "    location=LOCATION,\n",
    ")\n",
    "\n",
    "bqml_train_pipeline = vertex_ai.PipelineJob(\n",
    "    display_name=f\"{TRAIN_PIPELINE_PACKAGE}-job\",\n",
    "    template_path=TRAIN_PIPELINE_PACKAGE,\n",
    "    parameter_values=TRAIN_PIPELINE_RUN_PARAMS,\n",
    "    pipeline_root=TRAIN_PIPELINE_ROOT,\n",
    "    enable_caching=True,\n",
    ")\n",
    "\n",
    "bqml_train_pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKgPkm4cgbHd"
   },
   "source": [
    "查看BigQuery ML训练管道结果\n",
    "\n",
    "最后，您将查看管道中每个任务的工件输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "apGt59bCgbHd"
   },
   "outputs": [],
   "source": [
    "PROJECT_NUMBER = bqml_train_pipeline.gca_resource.name.split(\"/\")[1]\n",
    "print(\"PROJECT NUMBER: \", PROJECT_NUMBER)\n",
    "print(\"\\n\\n\")\n",
    "print(\"bigquery-create-model-job\")\n",
    "artifacts = print_pipeline_output(\n",
    "    TRAIN_PIPELINE_ROOT, bqml_train_pipeline, \"bigquery-create-model-job\"\n",
    ")\n",
    "print(\"\\n\\n\")\n",
    "print(\"bigquery-ml-evaluate-job\")\n",
    "artifacts = print_pipeline_output(\n",
    "    TRAIN_PIPELINE_ROOT, bqml_train_pipeline, \"bigquery-evaluate-model-job\"\n",
    ")\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JcGuzM7nEqmj"
   },
   "source": [
    "## 结论\n",
    "\n",
    "在本笔记本中，您使用Vertex AI Pipelines和BigQuery ML构建了一个ML流水线，用于训练自动编码器以检测异常。\n",
    "\n",
    "现在您知道如何利用预构建的 `google_cloud_components` 来训练BigQuery ML模型，以及如何构建自定义组件来评估和可视化性能指标。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "清理\n",
    "\n",
    "要清理此项目中使用的所有Google Cloud资源，您可以[删除用于教程的Google Cloud项目](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects)。\n",
    "\n",
    "否则，您可以删除在本教程中创建的各个资源。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P0Ks1UZpoRXS"
   },
   "outputs": [],
   "source": [
    "# delete pipeline\n",
    "delete_pipeline = False\n",
    "if delete_pipeline:\n",
    "    vertex_ai_pipeline_jobs = vertex_ai.PipelineJob.list(\n",
    "        filter=f'pipeline_name=\"{TRAIN_PIPELINE_NAME}\"'\n",
    "    )\n",
    "    for pipeline_job in vertex_ai_pipeline_jobs:\n",
    "        pipeline_job.delete()\n",
    "\n",
    "# delete model\n",
    "delete_model = False\n",
    "if delete_model:\n",
    "    DELETE_MODEL_SQL = f\"DROP MODEL {BQ_DATASET}.{BQ_RECOSTRUCTION_MODEL_TABLE}\"\n",
    "    try:\n",
    "        delete_model_query_job = bq_client.query(DELETE_MODEL_SQL)\n",
    "        delete_model_query_result = delete_model_query_job.result()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "# delete bucket\n",
    "delete_bucket = False\n",
    "if os.getenv(\"IS_TESTING\") or delete_bucket:\n",
    "    ! gsutil -m rm -r $BUCKET_URI\n",
    "\n",
    "# Remove local resorces\n",
    "delete_local_resources = False\n",
    "if delete_local_resources:\n",
    "    ! rm -rf {KFP_COMPONENTS_PATH}\n",
    "    ! rm -rf {TRAIN_PIPELINES_PATH}\n",
    "    ! rm -rf {TEST_PIPELINES_PATH}"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "google_cloud_pipeline_components_bqml_pipeline_anomaly_detection.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
