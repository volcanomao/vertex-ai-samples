{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "<table align=\"left\">\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/pipelines/google_cloud_pipeline_components_bqml_pipeline_demand_forecasting.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> 在Colab中运行\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/pipelines/google_cloud_pipeline_components_bqml_pipeline_demand_forecasting.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      在GitHub上查看\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/pipelines/google_cloud_pipeline_components_bqml_pipeline_demand_forecasting.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      在Vertex AI Workbench中打开\n",
    "    </a>\n",
    "  </td>                                                                                               \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "239ba71252d3"
   },
   "source": [
    "## 概述\n",
    "\n",
    "本笔记本展示如何使用`Vertex AI Pipelines`和`BigQuery ML pipeline components`来训练和评估需求预测模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25c28706c23e"
   },
   "source": [
    "### 目标\n",
    "\n",
    "在本教程中，您将学习如何使用Vertex AI管道和BigQuery ML管道组件训练和评估一个BigQuery ML模型。\n",
    "\n",
    "本教程使用以下 Google Cloud ML 服务和资源：\n",
    "\n",
    "- `Vertex AI管道`\n",
    "- `BigQuery ML管道组件`\n",
    "\n",
    "执行的步骤包括：\n",
    "\n",
    "- 定义自定义评估组件\n",
    "- 定义一个流水线：\n",
    "  - 获取BigQuery训练数据\n",
    "  - 训练一个BigQuery Arima Plus模型\n",
    "  - 评估BigQuery Arima Plus模型\n",
    "  - 绘制评估结果\n",
    "  - 检查模型性能\n",
    "  - 生成ARIMA Plus预测\n",
    "  - 生成ARIMA PLUS预测解释\n",
    "- 编译流水线。\n",
    "- 执行流水线。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "586acfa9b502"
   },
   "source": [
    "数据集\n",
    "\n",
    "该数据集是[使用Datastream、Dataflow、BigQuery ML和Looker构建和可视化需求预测预测的解决方案架构](https://cloud.google.com/architecture/build-visualize-demand-forecast-prediction-datastream-dataflow-bigqueryml-looker)中的数据集的修改版本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "### 成本\n",
    "\n",
    "本教程使用Google Cloud的计费组件：\n",
    "\n",
    "* Vertex AI\n",
    "* 云存储\n",
    "* BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ze4-nDLfK4pw"
   },
   "source": [
    "### 设置本地开发环境\n",
    "\n",
    "如果你正在使用Colab或Vertex AI工作台笔记本，你的环境已经满足运行此笔记本的所有要求。你可以跳过此步骤。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCuSR8GkAgzl"
   },
   "source": [
    "否则，请确保您的环境符合此笔记本的要求。\n",
    "您需要以下内容：\n",
    "\n",
    "* Google Cloud SDK\n",
    "* Git\n",
    "* Python 3\n",
    "* virtualenv\n",
    "* 在Python 3虚拟环境中运行的Jupyter笔记本\n",
    "\n",
    "Google Cloud指南[设置Python开发环境](https://cloud.google.com/python/setup)和[Jupyter安装指南](https://jupyter.org/install)提供了满足这些要求的详细说明。以下步骤提供了一组简化的说明：\n",
    "\n",
    "1. [安装并初始化Cloud SDK。](https://cloud.google.com/sdk/docs/)\n",
    "\n",
    "2. [安装Python 3。](https://cloud.google.com/python/setup#installing_python)\n",
    "\n",
    "3. [安装virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv)并创建一个使用Python 3的虚拟环境。激活虚拟环境。\n",
    "\n",
    "4. 要安装Jupyter，请在终端shell中的命令行上运行`pip3 install jupyter`。\n",
    "\n",
    "5. 要启动Jupyter，请在终端shell中的命令行上运行`jupyter notebook`。\n",
    "\n",
    "6. 在Jupyter Notebook仪表板中打开此笔记本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "### 安装额外的软件包\n",
    "\n",
    "安装以下软件包以执行此笔记本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2b4ef9b72d43"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Vertex AI Workbench Notebook product has specific requirements\n",
    "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\")\n",
    "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
    "    \"/opt/deeplearning/metadata/env_version\"\n",
    ")\n",
    "\n",
    "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_WORKBENCH_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\"\n",
    "    \n",
    "! pip3 install --upgrade \"kfp\" \\\n",
    "                         \"google-cloud-aiplatform\" \\\n",
    "                         \"google_cloud_pipeline_components\" \\\n",
    "                         \"google-cloud-bigquery\" {USER_FLAG} -q\n",
    "\n",
    "! pip3 install --upgrade \"tensorflow<2.8.0\" {USER_FLAG} -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhq5zEbGg0XX"
   },
   "source": [
    "### 重新启动内核\n",
    "\n",
    "安装额外的包之后，您需要重新启动笔记本内核，以便它能够找到这些包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EzrelQZ22IZj"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWEdiXsJg0XY"
   },
   "source": [
    "在开始之前"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF1j6f9HApxa"
   },
   "source": [
    "### 设置你的Google Cloud项目\n",
    "\n",
    "**无论你使用哪种笔记本环境，以下步骤都是必需的。**\n",
    "\n",
    "1. [选择或创建一个Google Cloud项目](https://console.cloud.google.com/cloud-resource-manager)。当你第一次创建账户时，你会获得$300的免费信用额用于计算/存储成本。\n",
    "\n",
    "1. [确保你的项目启用了计费](https://cloud.google.com/billing/docs/how-to/modify-project)。\n",
    "\n",
    "1. [启用API](https://console.cloud.google.com/flows/enableapi?apiid=cloudresourcemanager.googleapis.com,aiplatform.googleapis.com,notebooks.googleapis.com)。\n",
    "\n",
    "1. 如果你在本地运行这个笔记本，你需要安装[Cloud SDK](https://cloud.google.com/sdk)。\n",
    "\n",
    "1. 在下面的单元格中输入你的项目ID。然后运行单元格，确保Cloud SDK在这个笔记本中的所有命令中使用正确的项目。\n",
    "\n",
    "**注意**: Jupyter以`!`为前缀运行带有shell命令的行，并将以`$`为前缀的Python变量插入这些命令中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WReHDGG5g0XY"
   },
   "source": [
    "#### 设置您的项目ID\n",
    "\n",
    "**如果您不知道您的项目ID**，您可以使用`gcloud`来获取您的项目ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oM1iC_MfAts1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_ID = \"\"\n",
    "\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJYoRfYng0XZ"
   },
   "source": [
    "否则，请在这里设置您的项目ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "riG_qUokg0XZ"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None:\n",
    "    PROJECT_ID = \"[your-project]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_gcloud_project_id"
   },
   "outputs": [],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "#### 区域\n",
    "\n",
    "您也可以更改`REGION`变量，该变量用于本笔记本其余部分的操作。以下是Vertex AI支持的区域。我们建议您选择距离您最近的区域。\n",
    "\n",
    "- 美洲：`us-central1`\n",
    "- 欧洲：`europe-west4`\n",
    "- 亚太：`asia-east1`\n",
    "\n",
    "您可能无法使用多区域存储桶进行Vertex AI的训练。并非所有区域都支持所有Vertex AI服务。\n",
    "\n",
    "了解更多有关[Vertex AI 区域](https://cloud.google.com/vertex-ai/docs/general/locations)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R4aDZuADOWzi"
   },
   "outputs": [],
   "source": [
    "REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
    "\n",
    "if REGION == \"[your-region]\":\n",
    "    REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06571eb4063b"
   },
   "source": [
    "### UUID\n",
    "如果您正在进行直播教程会话，您可能会使用共享的测试帐户或项目。为了避免在创建的资源之间发生名称冲突，您可以为每个实例会话创建一个uuid，并将其附加到您在此教程中创建的资源名称上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "697568e92bd6"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "\n",
    "# Generate a uuid of a specifed length(default=8)\n",
    "def generate_uuid(length: int = 8) -> str:\n",
    "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
    "\n",
    "\n",
    "UUID = generate_uuid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dr--iN2kAylZ"
   },
   "source": [
    "验证您的Google Cloud账户\n",
    "\n",
    "**如果您正在使用Vertex AI Workbench笔记本**，则您的环境已经经过验证。请跳过此步骤。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBCra4QMA2wR"
   },
   "source": [
    "如果您正在使用Colab，请运行下面的单元格，并按照提示进行oAuth身份验证。\n",
    "\n",
    "否则，请按照以下步骤操作：\n",
    "\n",
    "1. 在Cloud Console中，转到[**创建服务帐号密钥**页面](https://console.cloud.google.com/apis/credentials/serviceaccountkey)。\n",
    "\n",
    "2. 点击**创建服务帐号**。\n",
    "\n",
    "3. 在**服务帐号名称**字段中输入一个名称，然后点击**创建**。\n",
    "\n",
    "4. 在**为此服务帐号授予访问权限到项目**部分，点击**角色**下拉列表。在过滤框中输入并选择以下角色：\n",
    "\n",
    "  - BigQuery数据编辑器\n",
    "  - BigQuery作业用户\n",
    "  - 服务帐号用户\n",
    "  - 存储对象管理员\n",
    "  - 存储管理员\n",
    "  - Vertex AI管理员\n",
    "\n",
    "5. 点击*创建*。一个包含您密钥的JSON文件将下载到您的本地环境。\n",
    "\n",
    "6. 在下面的单元格中将您的服务帐号密钥路径作为`GOOGLE_APPLICATION_CREDENTIALS`变量输入，并运行该单元格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PyQmSRbKA8r-"
   },
   "outputs": [],
   "source": [
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# If on Vertex AI Workbench, then don't execute this code\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\") and not os.getenv(\n",
    "    \"DL_ANACONDA_HOME\"\n",
    "):\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\"):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "### 创建云存储存储桶\n",
    "\n",
    "**无论您使用的是哪种笔记本环境，都需要执行以下步骤。**\n",
    "\n",
    "在下面设置您的云存储存储桶名称。它必须在所有云存储存储桶中是唯一的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"[your-bucket-name]\"  # @param {type:\"string\"}\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cf221059d072"
   },
   "outputs": [],
   "source": [
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"[your-bucket-name]\":\n",
    "    BUCKET_NAME = PROJECT_ID + \"-aip-\" + UUID\n",
    "    BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EcIXiGsCePi"
   },
   "source": [
    "只有当您的存储桶尚不存在时：运行以下单元格以创建您的云存储桶。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIq7R4HZCfIc"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucvCsknMCims"
   },
   "source": [
    "最后，通过检查其内容来验证对您的云存储桶的访问权限。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vhOb7YnwClBb"
   },
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "set_service_account"
   },
   "source": [
    "获取您的服务账号\n",
    "\n",
    "如果您不想使用您的项目的计算引擎服务账号，请将`SERVICE_ACCOUNT`设置为另一个服务账号ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vqUH5pk0OWzl"
   },
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT = \"[your-service-account]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "autoset_service_account"
   },
   "outputs": [],
   "source": [
    "if (\n",
    "    SERVICE_ACCOUNT == \"\"\n",
    "    or SERVICE_ACCOUNT is None\n",
    "    or SERVICE_ACCOUNT == \"[your-service-account]\"\n",
    "):\n",
    "    # Get your service account from gcloud\n",
    "    if not IS_COLAB:\n",
    "        shell_output = !gcloud auth list 2>/dev/null\n",
    "        SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
    "\n",
    "    else:  # IS_COLAB:\n",
    "        shell_output = ! gcloud projects describe  $PROJECT_ID\n",
    "        project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
    "        SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
    "\n",
    "    print(\"Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "set_service_account:pipelines"
   },
   "source": [
    "### 设置服务账号访问\n",
    "\n",
    "运行以下命令，将您的服务账号访问权限授予您在之前步骤创建的存储桶。您每个服务账号只需要运行此步骤一次。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nGfP_UQEOWzm"
   },
   "outputs": [],
   "source": [
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator {BUCKET_URI}\n",
    "\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_vars"
   },
   "source": [
    "### 导入库和设置变量\n",
    "\n",
    "接下来，导入库并设置一些在整个教程中使用的变量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRUOFELefqf1"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path as path\n",
    "from typing import NamedTuple\n",
    "# General\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import google.cloud.aiplatform as vertex_ai\n",
    "# Check components\n",
    "import tensorflow as tf\n",
    "# Simulate operations\n",
    "from google.cloud import bigquery\n",
    "# ML pipeline\n",
    "from google_cloud_pipeline_components.v1.bigquery import (\n",
    "    BigqueryCreateModelJobOp, BigqueryEvaluateModelJobOp,\n",
    "    BigqueryExplainForecastModelJobOp, BigqueryForecastModelJobOp,\n",
    "    BigqueryMLArimaEvaluateJobOp, BigqueryQueryJobOp)\n",
    "from kfp.v2 import compiler, dsl\n",
    "from kfp.v2.dsl import HTML, Artifact, Condition, Input, Output, component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "source": [
    "### 初始化用于 Python 的 Vertex AI SDK\n",
    "\n",
    "为您的项目和相应的存储桶初始化 Python 版本的 Vertex AI SDK。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Egd-0fYCOWzm"
   },
   "outputs": [],
   "source": [
    "vertex_ai.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L9iMCmfFBn6e"
   },
   "source": [
    "### 初始化Python的BigQuery SDK\n",
    "\n",
    "为您的项目初始化Python的BigQuery SDK。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Md0UdedBn6f"
   },
   "outputs": [],
   "source": [
    "bq_client = bigquery.Client(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FutXJbmAa1NC"
   },
   "source": [
    "创建本地目录\n",
    "\n",
    "接下来，您需要创建一些在本教程中会用到的本地目录。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Kms5lCRn7N-"
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"data\"\n",
    "KFP_COMPONENTS_PATH = \"components\"\n",
    "PIPELINES_PATH = \"pipelines\"\n",
    "\n",
    "! mkdir -m 777 -p {DATA_PATH}\n",
    "! mkdir -m 777 -p {KFP_COMPONENTS_PATH}\n",
    "! mkdir -m 777 -p {PIPELINES_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WRRE4t_bdzj_"
   },
   "source": [
    "### 准备训练数据\n",
    "\n",
    "接下来，您将把CSV训练数据复制到您的云存储桶中，然后为训练数据创建一个BigQuery数据集表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8u7VsRuLaaVC"
   },
   "outputs": [],
   "source": [
    "PUBLIC_DATA_URI = \"gs://cloud-samples-data/vertex-ai/pipeline-deployment/datasets/oracle_retail/orders.csv\"\n",
    "RAW_DATA_URI = f\"{BUCKET_URI}/{DATA_PATH}/raw/orders.csv\"\n",
    "\n",
    "! gsutil -m cp -R $PUBLIC_DATA_URI $RAW_DATA_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1lgNhMlvgbHX"
   },
   "source": [
    "快速查看CSV数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mdcdUwNwgbHX"
   },
   "outputs": [],
   "source": [
    "! gsutil cat {RAW_DATA_URI} | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AgUlIYgufhnT"
   },
   "outputs": [],
   "source": [
    "LOCATION = REGION.split('-')[0]\n",
    "\n",
    "! bq mk --location={LOCATION} --dataset {PROJECT_ID}:fast_fresh \n",
    "\n",
    "! bq load \\\n",
    "  --location={LOCATION} \\\n",
    "  --source_format=CSV \\\n",
    "  --skip_leading_rows=1\\\n",
    "  fast_fresh.orders_{UUID} \\\n",
    "  {RAW_DATA_URI} \\\n",
    "  time_of_sale:DATETIME,order_id:INTEGER,product_name:STRING,price:NUMERIC,quantity:NUMERIC,payment_method:STRING,store_id:INTEGER,user_id:INTEGER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZrgOD30o7HcL"
   },
   "source": [
    "在接下来的单元格中，您将构建组件和管道，用于训练和评估BQML需求预测模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A9qoVV5YedRi"
   },
   "source": [
    "### 设置管道变量\n",
    "\n",
    "为管道设置一些特定的变量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xYJhVJf5eguR"
   },
   "outputs": [],
   "source": [
    "# BQML pipeline job configuation\n",
    "PIPELINE_NAME = \"bqml-forecast-pipeline\"\n",
    "PIPELINE_ROOT = urlparse(BUCKET_URI)._replace(path=\"pipeline_root\").geturl()\n",
    "PIPELINE_PACKAGE = str(path(PIPELINES_PATH) / f\"{PIPELINE_NAME}.json\")\n",
    "\n",
    "# BQML pipeline conponent configuration\n",
    "BQ_DATASET = \"fast_fresh\"\n",
    "BQ_ORDERS_TABLE_PREFIX = \"orders\"\n",
    "BQ_TRAINING_TABLE_PREFIX = \"orders_training\"\n",
    "BQ_MODEL_TABLE_PREFIX = \"orders_forecast_arima\"\n",
    "BQ_EVALUATE_TS_TABLE_PREFIX = \"orders_arima_time_series_evaluate\"\n",
    "BQ_EVALUATE_MODEL_TABLE_PREFIX = \"orders_arima_model_evaluate\"\n",
    "BQ_FORECAST_TABLE_PREFIX = \"orders_arima_forecast\"\n",
    "BQ_EXPLAIN_FORECAST_TABLE_PREFIX = \"orders_arima_explain_forecast\"\n",
    "BQ_ORDERS_TABLE = f\"{BQ_ORDERS_TABLE_PREFIX}_{UUID}\"\n",
    "BQ_TRAINING_TABLE = f\"{BQ_TRAINING_TABLE_PREFIX}_{UUID}\"\n",
    "BQ_MODEL_TABLE = f\"{BQ_MODEL_TABLE_PREFIX}_{UUID}\"\n",
    "BQ_EVALUATE_TS_TABLE = f\"{BQ_EVALUATE_TS_TABLE_PREFIX}_{UUID}\"\n",
    "BQ_EVALUATE_MODEL_TABLE = f\"{BQ_EVALUATE_MODEL_TABLE_PREFIX}_{UUID}\"\n",
    "BQ_FORECAST_TABLE = f\"{BQ_FORECAST_TABLE_PREFIX}_{UUID}\"\n",
    "BQ_EXPLAIN_FORECAST_TABLE = f\"{BQ_EXPLAIN_FORECAST_TABLE_PREFIX}_{UUID}\"\n",
    "\n",
    "BQ_TRAIN_CONFIGURATION = {\n",
    "    \"destinationTable\": {\n",
    "        \"projectId\": PROJECT_ID,\n",
    "        \"datasetId\": BQ_DATASET,\n",
    "        \"tableId\": BQ_TRAINING_TABLE,\n",
    "    },\n",
    "    \"writeDisposition\": \"WRITE_TRUNCATE\",\n",
    "}\n",
    "\n",
    "BQ_EVALUATE_TS_CONFIGURATION = {\n",
    "    \"destinationTable\": {\n",
    "        \"projectId\": PROJECT_ID,\n",
    "        \"datasetId\": BQ_DATASET,\n",
    "        \"tableId\": BQ_EVALUATE_TS_TABLE,\n",
    "    },\n",
    "    \"writeDisposition\": \"WRITE_TRUNCATE\",\n",
    "}\n",
    "BQ_EVALUATE_MODEL_CONFIGURATION = {\n",
    "    \"destinationTable\": {\n",
    "        \"projectId\": PROJECT_ID,\n",
    "        \"datasetId\": BQ_DATASET,\n",
    "        \"tableId\": BQ_EVALUATE_MODEL_TABLE,\n",
    "    },\n",
    "    \"writeDisposition\": \"WRITE_TRUNCATE\",\n",
    "}\n",
    "BQ_FORECAST_CONFIGURATION = {\n",
    "    \"destinationTable\": {\n",
    "        \"projectId\": PROJECT_ID,\n",
    "        \"datasetId\": BQ_DATASET,\n",
    "        \"tableId\": BQ_FORECAST_TABLE,\n",
    "    },\n",
    "    \"writeDisposition\": \"WRITE_TRUNCATE\",\n",
    "}\n",
    "BQ_EXPLAIN_FORECAST_CONFIGURATION = {\n",
    "    \"destinationTable\": {\n",
    "        \"projectId\": PROJECT_ID,\n",
    "        \"datasetId\": BQ_DATASET,\n",
    "        \"tableId\": BQ_EXPLAIN_FORECAST_TABLE,\n",
    "    },\n",
    "    \"writeDisposition\": \"WRITE_TRUNCATE\",\n",
    "}\n",
    "PERF_THRESHOLD = 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zb3047PEgbHY"
   },
   "source": [
    "### 创建位置以存储组件定义\n",
    "\n",
    "接下来，您将创建一个存储云位置，用于存储您在本教程中创建的自定义组件的YAML组件定义。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ayXuNmkwKD3T"
   },
   "outputs": [],
   "source": [
    "! mkdir -m 777 -p {KFP_COMPONENTS_PATH}/custom_components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZHOP2TzBn6k"
   },
   "source": [
    "### 创建一个自定义组件来读取模型评估指标\n",
    "\n",
    "使用Kubeflow SDK可视化API构建一个自定义组件，用于在Vertex AI流水线UI中消费模型评估指标并进行可视化展示。实际上，Vertex AI允许您在Google Cloud控制台中轻松访问渲染HTML在输出页面中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iybwx_Z4Bn6k"
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.8-slim\",\n",
    "    packages_to_install=[\"jinja2\", \"pandas\", \"matplotlib\"],\n",
    "    output_component_file=f\"{KFP_COMPONENTS_PATH}/custom_components/build_bq_evaluate_metrics.yaml\",\n",
    ")\n",
    "def get_model_evaluation_metrics(\n",
    "    metrics_in: Input[Artifact], metrics_out: Output[HTML]\n",
    ") -> NamedTuple(\"Outputs\", [(\"avg_mean_absolute_error\", float)]):\n",
    "    \"\"\"\n",
    "    Get the average mean absolute error from the metrics\n",
    "    Args:\n",
    "        metrics_in: metrics artifact\n",
    "        metrics_out: metrics artifact\n",
    "    Returns:\n",
    "        avg_mean_absolute_error: average mean absolute error\n",
    "    \"\"\"\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    # Helpers\n",
    "    def prettyfier(styler):\n",
    "        \"\"\"\n",
    "        Helper function to prettify the metrics table.\n",
    "        Args:\n",
    "            styler: Styler object\n",
    "        Returns:\n",
    "            Styler object\n",
    "        \"\"\"\n",
    "        caption = {\n",
    "            \"selector\": \"caption\",\n",
    "            \"props\": [\n",
    "                (\"caption-side\", \"top\"),\n",
    "                (\"font-size\", \"150%\"),\n",
    "                (\"font-weight\", \"bold\"),\n",
    "                (\"font-family\", \"arial\"),\n",
    "            ],\n",
    "        }\n",
    "        headers = {\n",
    "            \"selector\": \"th\",\n",
    "            \"props\": [(\"color\", \"black\"), (\"font-family\", \"arial\")],\n",
    "        }\n",
    "        rows = {\n",
    "            \"selector\": \"td\",\n",
    "            \"props\": [(\"text-align\", \"center\"), (\"font-family\", \"arial\")],\n",
    "        }\n",
    "        styler.set_table_styles([caption, headers, rows])\n",
    "        styler.set_caption(\"Forecasting accuracy report <br><br>\")\n",
    "        styler.hide(axis=\"index\")\n",
    "        styler.format(precision=2)\n",
    "        styler.background_gradient(cmap=\"Blues\")\n",
    "        return styler\n",
    "\n",
    "    def get_column_names(header):\n",
    "        \"\"\"\n",
    "        Helper function to get the column names from the metrics table.\n",
    "        Args:\n",
    "            header: header\n",
    "        Returns:\n",
    "            column_names: column names\n",
    "        \"\"\"\n",
    "        header_clean = header.replace(\"_\", \" \")\n",
    "        header_abbrev = \"\".join([h[0].upper() for h in header_clean.split()])\n",
    "        header_prettied = f\"{header_clean} ({header_abbrev})\"\n",
    "        return header_prettied\n",
    "\n",
    "    # Extract rows and schema from metrics artifact\n",
    "    rows = metrics_in.metadata[\"rows\"]\n",
    "    schema = metrics_in.metadata[\"schema\"]\n",
    "\n",
    "    # Convert into a tabular format\n",
    "    columns = [metrics[\"name\"] for metrics in schema[\"fields\"] if \"name\" in metrics]\n",
    "    records = []\n",
    "    for row in rows:\n",
    "        records.append([dl[\"v\"] for dl in row[\"f\"]])\n",
    "    metrics = (\n",
    "        pd.DataFrame.from_records(records, columns=columns, index=\"product_name\")\n",
    "        .astype(float)\n",
    "        .round(3)\n",
    "    )\n",
    "    metrics = metrics.reset_index()\n",
    "\n",
    "    # Create the HTML artifact for the metrics\n",
    "    pretty_columns = list(\n",
    "        map(\n",
    "            lambda h: get_column_names(h)\n",
    "            if h != columns[0]\n",
    "            else h.replace(\"_\", \" \").capitalize(),\n",
    "            columns,\n",
    "        )\n",
    "    )\n",
    "    pretty_metrics = metrics.copy()\n",
    "    pretty_metrics.columns = pretty_columns\n",
    "    html_metrics = pretty_metrics.style.pipe(prettyfier).to_html()\n",
    "    with open(metrics_out.path, \"w\") as f:\n",
    "        f.write(html_metrics)\n",
    "\n",
    "    # Create metrics dictionary for the model\n",
    "    avg_mean_absolute_error = round(float(metrics.mean_absolute_error.mean()), 0)\n",
    "    component_outputs = NamedTuple(\"Outputs\", [(\"avg_mean_absolute_error\", float)])\n",
    "\n",
    "    return component_outputs(avg_mean_absolute_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pcSL1FHk69KT"
   },
   "source": [
    "### 构建BigQuery ML训练流水线\n",
    "\n",
    "使用Kubeflow Pipelines DSL包定义您的工作流程。\n",
    "\n",
    "以下是流水线工作流程的步骤：\n",
    "\n",
    "1. 获取BigQuery训练数据\n",
    "2. 训练一个BigQuery Arima Plus模型\n",
    "3. 评估BigQuery Arima Plus模型\n",
    "4. 绘制评估结果\n",
    "5. 检查模型性能\n",
    "6. 生成ARIMA Plus预测\n",
    "7. 生成ARIMA PLUS预测解释"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AlFXqsPIAk0l"
   },
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=PIPELINE_NAME,\n",
    "    description=\"A batch pipeline to train ARIMA PLUS using BQML\",\n",
    ")\n",
    "def pipeline(\n",
    "    bq_dataset: str = BQ_DATASET,\n",
    "    bq_orders_table: str = BQ_ORDERS_TABLE,\n",
    "    bq_training_table: str = BQ_TRAINING_TABLE,\n",
    "    bq_train_configuration: dict = BQ_TRAIN_CONFIGURATION,\n",
    "    bq_model_table: str = BQ_MODEL_TABLE,\n",
    "    bq_evaluate_time_series_configuration: dict = BQ_EVALUATE_TS_CONFIGURATION,\n",
    "    bq_evaluate_model_configuration: dict = BQ_EVALUATE_MODEL_CONFIGURATION,\n",
    "    performance_threshold: float = PERF_THRESHOLD,\n",
    "    bq_forecast_configuration: dict = BQ_FORECAST_CONFIGURATION,\n",
    "    bq_explain_forecast_configuration: dict = BQ_EXPLAIN_FORECAST_CONFIGURATION,\n",
    "    project: str = PROJECT_ID,\n",
    "    location: str = LOCATION,\n",
    "):\n",
    "\n",
    "    # Create the training dataset\n",
    "    create_training_dataset_op = BigqueryQueryJobOp(\n",
    "        query=f\"\"\"\n",
    "        -- create the training table\n",
    "        WITH \n",
    "        -- get 90% percentile for time series splitting\n",
    "        get_split AS (\n",
    "          SELECT APPROX_QUANTILES(DATETIME_TRUNC(time_of_sale, HOUR), 100)[OFFSET(90)] as split\n",
    "          FROM `{project}.{bq_dataset}.{bq_orders_table}`\n",
    "        ),\n",
    "        -- get train table\n",
    "        get_train AS (\n",
    "          SELECT\n",
    "            DATETIME_TRUNC(time_of_sale, HOUR) as hourly_timestamp,\n",
    "            product_name,\n",
    "            SUM(quantity) AS total_sold,\n",
    "            FROM `{project}.{bq_dataset}.{bq_orders_table}`\n",
    "        GROUP BY hourly_timestamp, product_name\n",
    "        )\n",
    "        SELECT\n",
    "          *,\n",
    "          CASE WHEN hourly_timestamp < (SELECT split FROM get_split) THEN 'TRAIN' ELSE 'TEST' END AS split\n",
    "        FROM get_train\n",
    "        ORDER BY hourly_timestamp\n",
    "        \"\"\",\n",
    "        job_configuration_query=bq_train_configuration,\n",
    "        project=project,\n",
    "        location=location,\n",
    "    ).set_display_name(\"get train data\")\n",
    "\n",
    "    # Run an ARIMA PLUS experiment\n",
    "    bq_arima_model_exp_op = (\n",
    "        BigqueryCreateModelJobOp(\n",
    "            query=f\"\"\"\n",
    "        -- create model table\n",
    "        CREATE OR REPLACE MODEL `{project}.{bq_dataset}.{bq_model_table}`\n",
    "        OPTIONS(\n",
    "        MODEL_TYPE = \\'ARIMA_PLUS\\',\n",
    "        TIME_SERIES_TIMESTAMP_COL = \\'hourly_timestamp\\',\n",
    "        TIME_SERIES_DATA_COL = \\'total_sold\\',\n",
    "        TIME_SERIES_ID_COL = [\\'product_name\\']\n",
    "        ) AS\n",
    "        SELECT\n",
    "          hourly_timestamp,\n",
    "          product_name,\n",
    "          total_sold\n",
    "        FROM `{project}.{bq_dataset}.{bq_training_table}`\n",
    "        WHERE split='TRAIN';\n",
    "        \"\"\",\n",
    "            project=project,\n",
    "            location=location,\n",
    "        )\n",
    "        .set_display_name(\"run arima+ model experiment\")\n",
    "        .after(create_training_dataset_op)\n",
    "    )\n",
    "\n",
    "    # Evaluate ARIMA PLUS time series\n",
    "    _ = (\n",
    "        BigqueryMLArimaEvaluateJobOp(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            model=bq_arima_model_exp_op.outputs[\"model\"],\n",
    "            show_all_candidate_models=False,\n",
    "            job_configuration_query=bq_evaluate_time_series_configuration,\n",
    "        )\n",
    "        .set_display_name(\"evaluate arima plus time series\")\n",
    "        .after(bq_arima_model_exp_op)\n",
    "    )\n",
    "\n",
    "    # Evaluate ARIMA Plus model\n",
    "    bq_arima_evaluate_model_op = (\n",
    "        BigqueryEvaluateModelJobOp(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            model=bq_arima_model_exp_op.outputs[\"model\"],\n",
    "            query_statement=f\"\"\"SELECT * FROM `{project}.{bq_dataset}.{bq_training_table}` WHERE split='TEST'\"\"\",\n",
    "            job_configuration_query=bq_evaluate_model_configuration,\n",
    "        )\n",
    "        .set_display_name(\"evaluate arima plus model\")\n",
    "        .after(bq_arima_model_exp_op)\n",
    "    )\n",
    "\n",
    "    # Plot model metrics\n",
    "    get_evaluation_model_metrics_op = (\n",
    "        get_model_evaluation_metrics(\n",
    "            bq_arima_evaluate_model_op.outputs[\"evaluation_metrics\"]\n",
    "        )\n",
    "        .after(bq_arima_evaluate_model_op)\n",
    "        .set_display_name(\"plot evaluation metrics\")\n",
    "    )\n",
    "\n",
    "    # Check the model performance. If ARIMA_PLUS average MAE metric is below to a minimal threshold\n",
    "    with Condition(\n",
    "        get_evaluation_model_metrics_op.outputs[\"avg_mean_absolute_error\"]\n",
    "        < PERF_THRESHOLD,\n",
    "        name=\"avg. mae good\",\n",
    "    ):\n",
    "        # Train the ARIMA PLUS model\n",
    "        bq_arima_model_op = (\n",
    "            BigqueryCreateModelJobOp(\n",
    "                query=f\"\"\"\n",
    "        -- create model table\n",
    "        CREATE OR REPLACE MODEL `{project}.{bq_dataset}.{bq_model_table}`\n",
    "        OPTIONS(\n",
    "        MODEL_TYPE = \\'ARIMA_PLUS\\',\n",
    "        TIME_SERIES_TIMESTAMP_COL = \\'hourly_timestamp\\',\n",
    "        TIME_SERIES_DATA_COL = \\'total_sold\\',\n",
    "        TIME_SERIES_ID_COL = [\\'product_name\\'],\n",
    "        MODEL_REGISTRY = \\'vertex_ai\\',\n",
    "        VERTEX_AI_MODEL_ID = \\'order_demand_forecasting\\',\n",
    "        VERTEX_AI_MODEL_VERSION_ALIASES = [\\'staging\\']\n",
    "        ) AS\n",
    "        SELECT\n",
    "          DATETIME_TRUNC(time_of_sale, HOUR) as hourly_timestamp,\n",
    "          product_name,\n",
    "          SUM(quantity) AS total_sold,\n",
    "          FROM `{project}.{bq_dataset}.{bq_orders_table}`\n",
    "        GROUP BY hourly_timestamp, product_name;\n",
    "        \"\"\",\n",
    "                project=project,\n",
    "                location=location,\n",
    "            )\n",
    "            .set_display_name(\"train arima+ model\")\n",
    "            .after(get_evaluation_model_metrics_op)\n",
    "        )\n",
    "\n",
    "        # Generate the ARIMA PLUS forecasts\n",
    "        bq_arima_forecast_op = (\n",
    "            BigqueryForecastModelJobOp(\n",
    "                project=project,\n",
    "                location=location,\n",
    "                model=bq_arima_model_op.outputs[\"model\"],\n",
    "                horizon=1,  # 1 hour\n",
    "                confidence_level=0.9,\n",
    "                job_configuration_query=bq_forecast_configuration,\n",
    "            )\n",
    "            .set_display_name(\"generate hourly forecasts\")\n",
    "            .after(get_evaluation_model_metrics_op)\n",
    "        )\n",
    "\n",
    "        # Generate the ARIMA PLUS forecast explainations\n",
    "        _ = (\n",
    "            BigqueryExplainForecastModelJobOp(\n",
    "                project=project,\n",
    "                location=location,\n",
    "                model=bq_arima_model_op.outputs[\"model\"],\n",
    "                horizon=1,  # 1 hour\n",
    "                confidence_level=0.9,\n",
    "                job_configuration_query=bq_explain_forecast_configuration,\n",
    "            )\n",
    "            .set_display_name(\"explain hourly forecasts\")\n",
    "            .after(bq_arima_forecast_op)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nghLONQX7JNg"
   },
   "source": [
    "将管道编译成JSON文件\n",
    "\n",
    "接下来，您编译管道，这将为您的管道生成一个JSON规范。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8l6IR7OoADJV"
   },
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(pipeline_func=pipeline, package_path=PIPELINE_PACKAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gtlwu0Xo1WcT"
   },
   "source": [
    "### 执行你的流水线\n",
    "\n",
    "接下来，我们执行流水线。它需要以下参数，我们设置为默认值：\n",
    "\n",
    "- `bq_dataset`：要训练的BigQuery数据集。\n",
    "- `bq_orders_table`：原始数据的BigQuery表。\n",
    "- `bq_training_table`：预处理后的训练数据的BigQuery表。\n",
    "- `bq_train_configuration`：训练组件的作业配置。\n",
    "- `bq_model_table`：训练模型的BigQuery表。\n",
    "- `bq_evaluate_time_series_configuration`：ARIMA时间序列评估的作业配置。\n",
    "- `bq_evaluate_model_configuration`：ARIMA模型评估的作业配置。\n",
    "- `performance_threshold`：平均MAE阈值的值。\n",
    "- `bq_forecast_configuration`：预测组件的作业配置。\n",
    "- `bq_explain_forecast_configuration`：预测组件评估的作业配置。\n",
    "- `project`：项目ID\n",
    "- `location`：地点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6rJ0NRuWwfnp"
   },
   "outputs": [],
   "source": [
    "bqml_pipeline = vertex_ai.PipelineJob(\n",
    "    display_name=f\"{PIPELINE_NAME}-job\",\n",
    "    template_path=PIPELINE_PACKAGE,\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    enable_caching=False,\n",
    ")\n",
    "\n",
    "bqml_pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKgPkm4cgbHd"
   },
   "source": [
    "### 查看BigQuery ML训练管道结果\n",
    "\n",
    "最后，您可以查看管道中每个任务的工件输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "apGt59bCgbHd"
   },
   "outputs": [],
   "source": [
    "PROJECT_NUMBER = bqml_pipeline.gca_resource.name.split(\"/\")[1]\n",
    "print(\"PROJECT NUMBER: \", PROJECT_NUMBER)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "def print_pipeline_output(job, output_task_name):\n",
    "    JOB_ID = job.name\n",
    "    print(JOB_ID)\n",
    "    for _ in range(len(job.gca_resource.job_detail.task_details)):\n",
    "        TASK_ID = job.gca_resource.job_detail.task_details[_].task_id\n",
    "        EXECUTE_OUTPUT = (\n",
    "            PIPELINE_ROOT\n",
    "            + \"/\"\n",
    "            + PROJECT_NUMBER\n",
    "            + \"/\"\n",
    "            + JOB_ID\n",
    "            + \"/\"\n",
    "            + output_task_name\n",
    "            + \"_\"\n",
    "            + str(TASK_ID)\n",
    "            + \"/executor_output.json\"\n",
    "        )\n",
    "        GCP_RESOURCES = (\n",
    "            PIPELINE_ROOT\n",
    "            + \"/\"\n",
    "            + PROJECT_NUMBER\n",
    "            + \"/\"\n",
    "            + JOB_ID\n",
    "            + \"/\"\n",
    "            + output_task_name\n",
    "            + \"_\"\n",
    "            + str(TASK_ID)\n",
    "            + \"/gcp_resources\"\n",
    "        )\n",
    "        EVAL_METRICS = (\n",
    "            PIPELINE_ROOT\n",
    "            + \"/\"\n",
    "            + PROJECT_NUMBER\n",
    "            + \"/\"\n",
    "            + JOB_ID\n",
    "            + \"/\"\n",
    "            + output_task_name\n",
    "            + \"_\"\n",
    "            + str(TASK_ID)\n",
    "            + \"/evaluation_metrics\"\n",
    "        )\n",
    "        if tf.io.gfile.exists(EXECUTE_OUTPUT):\n",
    "            ! gsutil cat $EXECUTE_OUTPUT\n",
    "            return EXECUTE_OUTPUT\n",
    "        elif tf.io.gfile.exists(GCP_RESOURCES):\n",
    "            ! gsutil cat $GCP_RESOURCES\n",
    "            return GCP_RESOURCES\n",
    "        elif tf.io.gfile.exists(EVAL_METRICS):\n",
    "            ! gsutil cat $EVAL_METRICS\n",
    "            return EVAL_METRICS\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "print(\"bigquery-create-model-job\")\n",
    "artifacts = print_pipeline_output(bqml_pipeline, \"bigquery-create-model-job\")\n",
    "print(\"\\n\\n\")\n",
    "print(\"bigquery-ml-arima-evaluate-job\")\n",
    "artifacts = print_pipeline_output(bqml_pipeline, \"bigquery-ml-arima-evaluate-job\")\n",
    "print(\"\\n\\n\")\n",
    "print(\"bigquery-evaluate-model-job\")\n",
    "artifacts = print_pipeline_output(bqml_pipeline, \"bigquery-evaluate-model-job\")\n",
    "print(\"\\n\\n\")\n",
    "print(\"bigquery-forecast-model-job\")\n",
    "artifacts = print_pipeline_output(bqml_pipeline, \"bigquery-forecast-model-job\")\n",
    "print(\"\\n\\n\")\n",
    "print(\"bigquery-explain-forecast-model-job\")\n",
    "artifacts = print_pipeline_output(bqml_pipeline, \"bigquery-explain-forecast-model-job\")\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "清理工作\n",
    "\n",
    "要清理此项目中使用的所有Google Cloud资源，您可以删除用于本教程的[Google Cloud项目](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects)。\n",
    "\n",
    "否则，您可以删除在本教程中创建的各个资源。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P0Ks1UZpoRXS"
   },
   "outputs": [],
   "source": [
    "# delete pipeline\n",
    "vertex_ai_pipeline_jobs = vertex_ai.PipelineJob.list(\n",
    "    filter=f'pipeline_name=\"{PIPELINE_NAME}\"'\n",
    ")\n",
    "for pipeline_job in vertex_ai_pipeline_jobs:\n",
    "    pipeline_job.delete()\n",
    "\n",
    "# delete model\n",
    "DELETE_MODEL_SQL = f\"DROP MODEL {BQ_DATASET}.{BQ_MODEL_TABLE}\"\n",
    "try:\n",
    "    delete_model_query_job = bq_client.query(DELETE_MODEL_SQL)\n",
    "    delete_model_query_result = delete_model_query_job.result()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# delete dataset\n",
    "try:\n",
    "    delete_detaset_query_result = bq_client.delete_dataset(\n",
    "        BQ_DATASET, delete_contents=True, not_found_ok=True\n",
    "    )\n",
    "    print(delete_detaset_query_result)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# delete bucket\n",
    "delete_bucket = True\n",
    "if os.getenv(\"IS_TESTING\") or delete_bucket:\n",
    "    ! gsutil -m rm -r $BUCKET_URI\n",
    "\n",
    "\n",
    "# Remove local resorces\n",
    "! rm -rf {KFP_COMPONENTS_PATH}\n",
    "! rm -rf {PIPELINES_PATH}\n",
    "! rm -rf {DATA_PATH}"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "google_cloud_pipeline_components_bqml_pipeline_demand_forecasting.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
