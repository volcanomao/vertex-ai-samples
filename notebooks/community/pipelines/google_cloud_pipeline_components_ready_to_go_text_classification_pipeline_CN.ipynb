{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Narwhalprime/vertex-ai-samples/blob/ready_to_go_notebook_fix/notebooks/community/pipelines/google_cloud_pipeline_components_ready_to_go_text_classification_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"在Colab中打开\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tb01JWKr4ima"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# 顶点管道：Ready-to-go文本分类模型训练管道\n",
    "\n",
    "<table align=\"left\">\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/pipelines/google_cloud_pipeline_components_ready_to_go_text_classification_pipeline.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> 在 Colab 中运行\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/pipelines/google_cloud_pipeline_components_ready_to_go_text_classification_pipeline.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      在 GitHub 上查看\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/ai/platform/notebooks/deploy-notebook?download_url=https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/pipelines/google_cloud_pipeline_components_ready_to_go_text_classification_pipeline.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"顶点 AI logo\">\n",
    "      在顶点 AI 工作台中打开\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## 概述\n",
    "\n",
    "本演示展示了使用[Google Cloud Pipeline Components（GCPC）](https://pypi.org/project/google-cloud-pipeline-components/)、[Kubeflow Pipelines（KFP）](https://pypi.org/project/kfp/)以及各种Vertex AI服务，如[Vertex Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines/introduction)、[Vertex Tensorboard](https://cloud.google.com/vertex-ai/docs/experiments/tensorboard-overview)、[Vertex Training for distributed training](https://cloud.google.com/vertex-ai/docs/training/distributed-training)等加速器，[Vertex Online Prediction](https://cloud.google.com/vertex-ai/docs/predictions/getting-predictions)，以及[Vertex Model Evaluation components](https://cloud.google.com/vertex-ai/docs/pipelines/model-evaluation-component)，构建了一个端到端文本分类流水线，该流水线根据新闻文章的标题和简短描述确定其类别。该演示旨在向开发人员展示如何使用KFP和Vertex Pipelines构建端到端流水线，以对其自己的文本数据进行分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YLMHSSKNGuB-"
   },
   "source": [
    "### 目标\n",
    "\n",
    "在本教程中，您将构建一个端到端的文本分类管道，用于分类新闻标题和描述。您将使用KFP、Vertex AI和Vertex Pipelines生成一个经过管理且高度可扩展的解决方案。\n",
    "\n",
    "文本分类管道包括以下步骤：\n",
    "\n",
    "- 将数据分割为训练集和验证集\n",
    "- 调整预训练的 [BERT](https://www.tensorflow.org/text/tutorials/classify_text_with_bert) 模型\n",
    "- 将您的模型上传至Vertex\n",
    "- 将您的模型部署到一个Vertex端点\n",
    "- 执行模型评估"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eMxTrJCKGqsD"
   },
   "source": [
    "### 数据集\n",
    "\n",
    "该演示使用了一个 Kaggle [新闻分类数据集](https://www.kaggle.com/datasets/rmisra/news-category-dataset)，其中包含从2012年至2018年从 HuffPost 获取的约20万条新闻标题。它位于公共示例云存储桶中，路径为 `gs://cloud-samples-data/vertex-ai/community-content/datasets/news/news_category_data.json`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yPPEG6lwGxPv"
   },
   "source": [
    "### 成本\n",
    "\n",
    "本教程使用 Google Cloud 的计费组件：\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "请了解[Vertex AI\n",
    "定价](https://cloud.google.com/vertex-ai/pricing)和[Cloud Storage\n",
    "定价](https://cloud.google.com/storage/pricing)，并使用[Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "根据您的预期使用量生成成本估算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ze4-nDLfK4pw"
   },
   "source": [
    "### 设置您的本地开发环境\n",
    "\n",
    "**如果您正在使用Colab或Vertex AI Workbench笔记本**，您的环境已经满足运行此笔记本的所有要求。请跳转到下面的“安装额外软件包”部分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCuSR8GkAgzl"
   },
   "source": [
    "否则，请确保您的环境符合本笔记本的要求。\n",
    "您需要以下内容：\n",
    "\n",
    "* Google Cloud SDK\n",
    "* Git\n",
    "* Python 3\n",
    "* virtualenv\n",
    "* 在使用Python 3的虚拟环境中运行Jupyter笔记本\n",
    "\n",
    "Google Cloud指南中提供了[设置Python开发环境的详细说明](https://cloud.google.com/python/setup)，以及[Jupyter安装指南](https://jupyter.org/install)，\n",
    "提供了满足这些要求的详细说明。以下步骤提供了一套简化的指令：\n",
    "\n",
    "1. [安装并初始化Cloud SDK。](https://cloud.google.com/sdk/docs/)\n",
    "\n",
    "2. [安装Python 3。](https://cloud.google.com/python/setup#installing_python)\n",
    "\n",
    "3. [安装virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv) 并创建一个使用Python 3的虚拟环境。激活虚拟环境。\n",
    "\n",
    "4. 要安装Jupyter，请在终端窗口的命令行中运行 `pip3 install jupyter`。\n",
    "\n",
    "5. 要启动Jupyter，请在终端窗口的命令行中运行 `jupyter notebook`。\n",
    "\n",
    "6. 在Jupyter Notebook Dashboard中打开这个笔记本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "### 安装额外的包\n",
    "\n",
    "注意：此笔记依赖于较旧版本的 Google Cloud Pipeline Components library (GCPC) 中的组件。\n",
    "\n",
    "请安装以下包以执行此笔记。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2b4ef9b72d43"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Vertex AI Workbench Notebook product has specific requirements\n",
    "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\")\n",
    "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
    "    \"/opt/deeplearning/metadata/env_version\"\n",
    ")\n",
    "\n",
    "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_WORKBENCH_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LoSXmPuYg5zA"
   },
   "outputs": [],
   "source": [
    "! pip3 install --upgrade pip google-cloud-aiplatform 'google-cloud-pipeline-components<2' kfp tensorflow tensorboard numpy {USER_FLAG} -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhq5zEbGg0XX"
   },
   "source": [
    "### 重新启动内核\n",
    "\n",
    "重新启动您的笔记本内核，以确保可以找到所有新安装的软件包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EzrelQZ22IZj"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWEdiXsJg0XY"
   },
   "source": [
    "在你开始之前"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF1j6f9HApxa"
   },
   "source": [
    "### 设置您的Google Cloud项目\n",
    "\n",
    "**无论您的笔记本环境如何，都需要执行以下步骤。**\n",
    "\n",
    "1. [选择或创建一个Google Cloud项目](https://console.cloud.google.com/cloud-resource-manager)。当您首次创建帐户时，您将获得$300免费信用额度用于计算/存储成本。\n",
    "\n",
    "1. [确保您的项目已启用计费](https://cloud.google.com/billing/docs/how-to/modify-project)。\n",
    "\n",
    "1. [启用Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,storage.googleapis.com)。\n",
    "\n",
    "1. 如果您在本地运行此笔记本，您需要安装[Cloud SDK](https://cloud.google.com/sdk)。\n",
    "\n",
    "1. 在下面的单元格中输入您的项目ID。然后运行单元格，确保Cloud SDK在本笔记本中的所有命令中使用正确的项目。\n",
    "\n",
    "**注意**：Jupyter会将以`!`为前缀的行视为shell命令，并将以`$`为前缀的Python变量插入这些命令中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WReHDGG5g0XY"
   },
   "source": [
    "设置您的项目ID\n",
    "\n",
    "在这里设置您的项目ID（*不是*项目编号）。 如果您不知道您的项目ID，请尝试以下操作：\n",
    "\n",
    "* 运行`gcloud config list`。\n",
    "* 运行`gcloud projects list`。\n",
    "* 查看支持页面：查找项目ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "riG_qUokg0XZ"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id-here]\"  # @param {type:\"string\"}\n",
    "!gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9LYAz8zhg5zC"
   },
   "source": [
    "#### 区域\n",
    "\n",
    "您还可以更改`REGION`变量，该变量用于笔记本其余部分的操作。以下是支持Vertex AI的区域。我们建议您选择最靠近您的区域。\n",
    "\n",
    "- 美洲：`us-central1`\n",
    "- 欧洲：`europe-west4`\n",
    "- 亚太：`asia-east1`\n",
    "\n",
    "您可能不会在Vertex AI上使用多区域存储桶进行训练。并非所有区域都支持所有Vertex AI服务。\n",
    "\n",
    "了解更多关于[Vertex AI区域](https://cloud.google.com/vertex-ai/docs/general/locations)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "id": "zW8Byl_jg5zC"
   },
   "outputs": [],
   "source": [
    "REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
    "\n",
    "if REGION == \"[your-region]\":\n",
    "    REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "id": "A75Sm4Zpg5zC"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "\n",
    "# Generate a uuid of a specifed length(default=8)\n",
    "def generate_uuid(length: int = 8) -> str:\n",
    "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
    "\n",
    "\n",
    "UUID = generate_uuid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dr--iN2kAylZ"
   },
   "source": [
    "### 验证您的谷歌云账户\n",
    "\n",
    "**如果您正在使用Vertex AI工作台笔记本**，您的环境已经经过验证。跳过此步骤。\n",
    "\n",
    "**如果您正在使用Colab**，运行下面的单元格，并按照提示进行身份验证以通过oAuth进行验证您的账户。\n",
    "\n",
    "**否则**，请按照以下步骤操作：\n",
    "\n",
    "1. 在云控制台中，转到[**创建服务账户密钥**页面](https://console.cloud.google.com/apis/credentials/serviceaccountkey)。\n",
    "\n",
    "2. 点击**创建服务账户**。\n",
    "\n",
    "3. 在**服务账户名称**字段中输入一个名称，并点击**创建**。\n",
    "\n",
    "4. 在**授予此服务账户对项目的访问权限**部分，点击**角色**下拉列表。在过滤框中输入“Vertex AI”，并选择**Vertex AI管理员**。在过滤框中输入“存储对象管理员”，并选择**存储对象管理员**。\n",
    "\n",
    "5. 点击*创建*。一个包含您密钥的JSON文件将下载到您的本地环境中。\n",
    "\n",
    "6. 在下面的单元格中，将您的服务账户密钥的路径作为`GOOGLE_APPLICATION_CREDENTIALS`变量输入，并运行该单元格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PyQmSRbKA8r-"
   },
   "outputs": [],
   "source": [
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# If on Vertex AI Workbench, then don't execute this code\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\") and not os.getenv(\n",
    "    \"DL_ANACONDA_HOME\"\n",
    "):\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\"):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "### 创建一个云存储桶\n",
    "\n",
    "**无论你使用的是哪个笔记本环境，以下步骤都是必需的。**\n",
    "\n",
    "在本教程中，一个云存储桶保存了用于训练模型的新闻类别数据集文件。Vertex AI 还会将由预处理组件生成的拆分训练和验证数据集等工件保存在同一个存储桶中。使用这个模型工件，你可以创建一个 Vertex AI 模型和端点，以便提供在线预测。\n",
    "\n",
    "请在下面设置你的云存储桶的名称。它在所有的云存储桶中必须是唯一的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"[your-bucket-name]\"  # @param {type:\"string\"}\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cf221059d072"
   },
   "outputs": [],
   "source": [
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"[your-bucket-name]\":\n",
    "    BUCKET_NAME = PROJECT_ID + \"aip-\" + UUID\n",
    "    BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EcIXiGsCePi"
   },
   "source": [
    "只有当您的存储桶尚不存在时：运行以下单元格来创建您的云存储桶。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIq7R4HZCfIc"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucvCsknMCims"
   },
   "source": [
    "最后，通过检查其内容来验证对云存储存储桶的访问。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vhOb7YnwClBb"
   },
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "set_service_account"
   },
   "source": [
    "设置服务账号和权限\n",
    "将使用服务账号来创建自定义培训作业。如果您不想使用项目的计算引擎服务账号，请将 SERVICE_ACCOUNT 设置为另一个服务账号 ID。您可以按照[说明](https://cloud.google.com/iam/docs/creating-managing-service-accounts#creating)创建服务账号。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EGFb5_BNg5zD"
   },
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT = \"[your-service-account]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "autoset_service_account"
   },
   "outputs": [],
   "source": [
    "if (\n",
    "    SERVICE_ACCOUNT == \"\"\n",
    "    or SERVICE_ACCOUNT is None\n",
    "    or SERVICE_ACCOUNT == \"[your-service-account]\"\n",
    "):\n",
    "    # Get your service account from gcloud\n",
    "    if not IS_COLAB:\n",
    "        shell_output = !gcloud auth list 2>/dev/null\n",
    "        SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
    "\n",
    "    else:  # IS_COLAB:\n",
    "        shell_output = ! gcloud projects describe  $PROJECT_ID\n",
    "        project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
    "        SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
    "\n",
    "    print(\"Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "set_service_account:pipelines"
   },
   "source": [
    "设置服务帐户访问权限\n",
    "\n",
    "运行以下命令，将读写管道工件的访问权限授予您的服务帐户，该服务帐户位于您在上一步创建的存储桶中。 您只需对每个服务帐户运行此步骤一次。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VxXITjj2g5zE"
   },
   "outputs": [],
   "source": [
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
    "\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRUOFELefqf1"
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "from kfp import components\n",
    "from kfp.v2 import compiler, dsl\n",
    "from kfp.v2.dsl import InputPath, component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kCGX-LsRg5zE"
   },
   "outputs": [],
   "source": [
    "# Model evaluation components\n",
    "from google_cloud_pipeline_components.experimental.evaluation import \\\n",
    "    GetVertexModelOp as get_vertex_model_op\n",
    "from google_cloud_pipeline_components.experimental.evaluation import \\\n",
    "    ModelEvaluationClassificationOp as evaluation_classification_op\n",
    "from google_cloud_pipeline_components.experimental.evaluation import \\\n",
    "    ModelImportEvaluationOp as model_import_evaluation_op\n",
    "from google_cloud_pipeline_components.experimental.evaluation import \\\n",
    "    TargetFieldDataRemoverOp as target_field_data_remover_op\n",
    "# Text Classification components\n",
    "from google_cloud_pipeline_components.experimental.sklearn import \\\n",
    "    SklearnTrainTestSplitJsonlOp as train_test_split_op\n",
    "from google_cloud_pipeline_components.experimental.text_classification import \\\n",
    "    TextClassificationTrainingOp as text_classification_training_op\n",
    "from google_cloud_pipeline_components.v1.batch_predict_job import \\\n",
    "    ModelBatchPredictOp as batch_prediction_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5IFdqoBqg5zE"
   },
   "source": [
    "### 填写以下必填配置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YoWhYTV3g5zE"
   },
   "source": [
    "这个管道接受一个 JSONL 数据集，其中每个 JSON 对象示例都有两个必需的键：`text` 和 `label`。`text` 键应映射到样本的文本数据，而 `label` 键应映射到其分类类别。\n",
    "\n",
    "以下是在此演示中使用的数据集中的 JSON 对象示例：\n",
    "\n",
    "{\n",
    "**\"label\"**:\"CRIME\",\n",
    "**\"text\"**:\"There Were 2 Mass Shootings In Texas Last Week, But Only 1 On TV\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T17qYMhxg5zE"
   },
   "outputs": [],
   "source": [
    "BASE_OUTPUT_DIR = f\"gs://{BUCKET_NAME}\"\n",
    "\n",
    "SAMPLE_DATA_URI = \"gs://cloud-samples-data/vertex-ai/community-content/datasets/news/news_category_data.json\"\n",
    "TRAINING_DATA_URI = (\n",
    "    f\"{BASE_OUTPUT_DIR}/data/news_category_data.json\"  # @param {type:\"string\"}\n",
    ")\n",
    "\n",
    "# The GCS directory for keeping staging files for model evaluation.\n",
    "ROOT_DIR = f\"{BASE_OUTPUT_DIR}/root\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rcimKmcMg5zE"
   },
   "source": [
    "将示例数据复制到TRAINING_DATA_URI。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nr3KLUEyg5zE"
   },
   "outputs": [],
   "source": [
    "! gsutil cp {SAMPLE_DATA_URI} {TRAINING_DATA_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3pIcOb86g5zE"
   },
   "source": [
    "`CLASS_NAMES` 应该是一个列表，包含一个文本样本可以分类为的所有类别。\n",
    "\n",
    "在这个演示中，有 41 个题材类别（如下所示）可以将一个标题分类为。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CZTJ1NwIg5zE"
   },
   "outputs": [],
   "source": [
    "CLASS_NAMES = [\n",
    "    \"GOOD NEWS\",\n",
    "    \"STYLE\",\n",
    "    \"STYLE & BEAUTY\",\n",
    "    \"ARTS\",\n",
    "    \"IMPACT\",\n",
    "    \"WEIRD NEWS\",\n",
    "    \"FIFTY\",\n",
    "    \"ENTERTAINMENT\",\n",
    "    \"ARTS & CULTURE\",\n",
    "    \"HEALTHY LIVING\",\n",
    "    \"WEDDINGS\",\n",
    "    \"PARENTING\",\n",
    "    \"BLACK VOICES\",\n",
    "    \"GREEN\",\n",
    "    \"RELIGION\",\n",
    "    \"POLITICS\",\n",
    "    \"PARENTS\",\n",
    "    \"BUSINESS\",\n",
    "    \"DIVORCE\",\n",
    "    \"WELLNESS\",\n",
    "    \"FOOD & DRINK\",\n",
    "    \"THE WORLDPOST\",\n",
    "    \"MEDIA\",\n",
    "    \"COLLEGE\",\n",
    "    \"WOMEN\",\n",
    "    \"TASTE\",\n",
    "    \"WORLDPOST\",\n",
    "    \"TRAVEL\",\n",
    "    \"CULTURE & ARTS\",\n",
    "    \"SPORTS\",\n",
    "    \"CRIME\",\n",
    "    \"QUEER VOICES\",\n",
    "    \"TECH\",\n",
    "    \"COMEDY\",\n",
    "    \"MONEY\",\n",
    "    \"WORLD NEWS\",\n",
    "    \"LATINO VOICES\",\n",
    "    \"SCIENCE\",\n",
    "    \"EDUCATION\",\n",
    "    \"HOME & LIVING\",\n",
    "    \"ENVIRONMENT\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JqSq5J7pg5zF"
   },
   "source": [
    "### 加载组件\n",
    "KFP SDK提供了各种方法来[加载组件](https://www.kubeflow.org/docs/components/pipelines/sdk/component-development/#using-your-component-in-a-pipeline)以在流水线中使用。在此演示中，我们将加载五个组件。\n",
    "\n",
    "此流水线由以下组件组成：\n",
    "\n",
    "- **train_test_split_jsonl_with_sklearn** - 将数据分割为训练和验证数据集。\n",
    "- **train_tensorflow_text_classification_model** - 创建经过训练的文本分类TensorFlow模型。\n",
    "- **upload_Tensorflow_model_to_Google_Cloud_Vertex_AI** - 将TensorFlow模型转换为Vertex模型并上传到Vertex。\n",
    "- **deploy_model_to_endpoint** - 将Vertex模型部署到用于在线预测的端点。\n",
    "- **get_gcs_uris_from_jsonl_artifact** - 基于Python函数的操作，将数据资源转换为模型评估组件可接受的格式。\n",
    "- **target_field_data_remover** - 删除验证数据中的目标（标签）字段，以进行下游Vertex批处理预测。\n",
    "- **model_batch_predict** - 提交批量预测作业。\n",
    "- **model_evaluation_classification** - 计算并导出评估指标。\n",
    "- **model_evaluation_import** - 导入模型评估指标结果。\n",
    "\n",
    "对于已由GCPC提供并发布的组件，请使用`load_component_from_url`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aQtIfCuqg5zF"
   },
   "outputs": [],
   "source": [
    "upload_tensorflow_model_to_vertex_op = components.load_component_from_url(\n",
    "    \"https://raw.githubusercontent.com/Ark-kun/pipeline_components/c6a8b67d1ada2cc17665c99ff6b410df588bee28/components/google-cloud/Vertex_AI/Models/Upload_Tensorflow_model/workaround_for_buggy_KFPv2_compiler/component.yaml\"\n",
    ")\n",
    "deploy_model_to_endpoint_op = components.load_component_from_url(\n",
    "    \"https://raw.githubusercontent.com/Ark-kun/pipeline_components/27a5ea25e849c9e8c0cb6ed65518bc3ece259aaf/components/google-cloud/Vertex_AI/Models/Deploy_to_endpoint/workaround_for_buggy_KFPv2_compiler/component.yaml\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pdi60Egyg5zM"
   },
   "source": [
    "将验证数据（具有\"JSONLines\"注释的Artifact）转换为可由下游的\"target_field_data_remover_op\"和\"model_evaluation_op\"接收的输出参数gcs_source_uris（类型：Sequence[str]）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lw5YefFeg5zM"
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9\",\n",
    ")\n",
    "def get_gcs_uris_from_jsonl_artifact(input_jsonl: InputPath(\"JSONLines\")) -> list:\n",
    "    return [\"gs://\" + input_jsonl[5:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xX9pQ0T_g5zM"
   },
   "source": [
    "### 建立一个流水线\n",
    "以下流水线代码链接加载组件的输入和输出。生成的流水线执行以下步骤：\n",
    "- 将数据分成训练集和测试集。\n",
    "- 训练新的文本分类模型。\n",
    "- 将模型上传到 Vertex AI Model Registry。\n",
    "- 使用测试数据执行批量预测。\n",
    "- 使用上述批量预测评估模型的性能。\n",
    "- 将评估指标导入模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j_Ev_TOug5zM"
   },
   "outputs": [],
   "source": [
    "@dsl.pipeline(name=\"text-classification-pipeline\")\n",
    "def text_pipeline(\n",
    "    project: str,\n",
    "    training_data_uri: str,\n",
    "    class_names: list,\n",
    "    root_dir: str,\n",
    "    target_field_name: str,\n",
    "    batch_predict_display_name: str,\n",
    "    batch_predict_instances_format: str = \"jsonl\",\n",
    "    batch_predictions_format: str = \"jsonl\",\n",
    "    model_name: str = \"small_bert/bert_en_uncased_L-2_H-128_A-2\",\n",
    "    validation_split: float = 0.2,\n",
    "    batch_size: int = 256,\n",
    "    learning_rate: float = 3e-4,\n",
    "    num_epochs: int = 5,\n",
    "    random_seed: int = 0,\n",
    ") -> None:\n",
    "    \"\"\"End-to-end text classification pipeline.\n",
    "\n",
    "    Args:\n",
    "    project: Required. GCP project ID.\n",
    "    training_data_uri: Required. Data in JSON lines format.\n",
    "    class_names: Required. List of categories (string) for classification.\n",
    "    root_dir: Required. The GCS directory for keeping staging files for model evaluation.\n",
    "    target_field_name: Required. The name of the features target field in the predictions file (e.g. 'label').\n",
    "    batch_predict_instances_format: The file format for the ground truth files.\n",
    "    batch_predictions_format: The file format for the batch prediction results.\n",
    "    batch_predict_display_name: Required. The user-defined name of this BatchPredictionJob.\n",
    "    model_name: Optional. Name of pre-trained BERT model to be used.\n",
    "                Default: \"small_bert/bert_en_uncased_L-2_H-128_A-2\"\n",
    "    validation_split: Optional. Fraction of data that will make up validation dataset.\n",
    "                      Default: 0.2\n",
    "    batch_size: Optional. Batch size\n",
    "                Default: 256\n",
    "    learning_rate: Optional. Learning rate\n",
    "                   Default: 3e-4\n",
    "    num_epochs: Optional. Number of epochs\n",
    "                Default: 10\n",
    "    random_seed: Optional. Random seed\n",
    "                 Default: 0\n",
    "    \"\"\"\n",
    "\n",
    "    text_data_preprocess_task = train_test_split_op(\n",
    "        input_data_path=training_data_uri,\n",
    "    )\n",
    "\n",
    "    training_data = text_data_preprocess_task.outputs[\"training_data_path\"]\n",
    "\n",
    "    validation_data = text_data_preprocess_task.outputs[\"validation_data_path\"]\n",
    "\n",
    "    # Set CPU, memory, and GPU configuration settings for this step (https://cloud.google.com/vertex-ai/docs/pipelines/machine-types)\n",
    "    model = (\n",
    "        text_classification_training_op(\n",
    "            preprocessed_training_data_path=training_data,\n",
    "            preprocessed_validation_data_path=validation_data,\n",
    "            model_name=model_name,\n",
    "            class_names=class_names,\n",
    "            batch_size=batch_size,\n",
    "            learning_rate=learning_rate,\n",
    "            num_epochs=num_epochs,\n",
    "            random_seed=random_seed,\n",
    "        )\n",
    "    ).add_node_selector_constraint(\n",
    "        \"cloud.google.com/gke-accelerator\", \"NVIDIA_TESLA_A100\"\n",
    "    )  # Note that A100 is available on us-central1\n",
    "\n",
    "    vertex_model_name = upload_tensorflow_model_to_vertex_op(\n",
    "        model=model.outputs[\"trained_model_path\"],\n",
    "    ).outputs[\"model_name\"]\n",
    "\n",
    "    # Model evaluation\n",
    "    # Need a component to convert Artifact('JsonLinesDataset') to JsonArray\n",
    "    validation_data_uris = get_gcs_uris_from_jsonl_artifact(validation_data).output\n",
    "\n",
    "    evaluation_data_for_batch_predict = target_field_data_remover_op(\n",
    "        project=project,\n",
    "        root_dir=root_dir,\n",
    "        target_field_name=target_field_name,\n",
    "        gcs_source_uris=validation_data_uris,\n",
    "    ).outputs[\"gcs_output_directory\"]\n",
    "\n",
    "    vertex_model = get_vertex_model_op(\n",
    "        model_resource_name=vertex_model_name,\n",
    "    ).outputs[\"model\"]\n",
    "\n",
    "    batch_prediction_task = batch_prediction_op(\n",
    "        project=project,\n",
    "        model=vertex_model,\n",
    "        job_display_name=batch_predict_display_name,\n",
    "        gcs_source_uris=evaluation_data_for_batch_predict,\n",
    "        gcs_destination_output_uri_prefix=root_dir,\n",
    "        instances_format=batch_predict_instances_format,\n",
    "        predictions_format=batch_predictions_format,\n",
    "        machine_type=\"n1-standard-32\",\n",
    "        starting_replica_count=5,\n",
    "        max_replica_count=10,\n",
    "    )\n",
    "\n",
    "    # Run the evaluation based on prediction type\n",
    "    eval_task = evaluation_classification_op(\n",
    "        project=project,\n",
    "        root_dir=root_dir,\n",
    "        ground_truth_gcs_source=validation_data_uris,\n",
    "        target_field_name=target_field_name,\n",
    "        prediction_score_column=\"prediction\",\n",
    "        prediction_label_column=\"\",\n",
    "        class_labels=class_names,\n",
    "        ground_truth_format=batch_predict_instances_format,\n",
    "        predictions_format=batch_predictions_format,\n",
    "        predictions_gcs_source=batch_prediction_task.outputs[\"gcs_output_directory\"],\n",
    "    )\n",
    "    # Import the model evaluations to the Vertex AI model\n",
    "    model_import_evaluation_op(\n",
    "        classification_metrics=eval_task.outputs[\"evaluation_metrics\"],\n",
    "        model=vertex_model,\n",
    "        dataset_type=\"jsonl\",\n",
    "    )\n",
    "\n",
    "    # For online predictions\n",
    "    _ = deploy_model_to_endpoint_op(\n",
    "        model_name=vertex_model_name,\n",
    "    ).outputs[\"endpoint_name\"]\n",
    "\n",
    "\n",
    "pipeline_func = text_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DiuoDAI8g5zM"
   },
   "source": [
    "运行管线"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tjbfj0Dwg5zM"
   },
   "source": [
    "以下的代码块从上面的pipeline函数创建一个流水线运行，并提交到Vertex AI平台。您可以在[Vertex ML Metadata (MLMD)](https://cloud.google.com/vertex-ai/docs/ml-metadata/introduction) 中查看该流水线的工件，在运行下一个代码块时会输出链接。 \n",
    "\n",
    "只需要修改这个流水线的参数以适应您特定的用例。在`parameter_values`字典中指定所需的流水线参数和任何可选的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OMZJdsVCg5zM"
   },
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline_func,\n",
    "    package_path=\"text_classification_pipeline.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UnpTVTkZg5zM"
   },
   "outputs": [],
   "source": [
    "PIPELINE_DISPLAY_NAME = f\"text-classification-train-evaluate-{UUID}\"  # \"[your-pipeline-display-name]\"  # @param {type:\"string\"}\n",
    "\n",
    "BATCH_PREDICTION_DISPLAY_NAME = f\"batch-prediction-on-pipelines-model-{UUID}\"\n",
    "\n",
    "parameters = {\n",
    "    \"project\": PROJECT_ID,\n",
    "    \"training_data_uri\": TRAINING_DATA_URI,\n",
    "    \"class_names\": CLASS_NAMES,\n",
    "    \"num_epochs\": 5,\n",
    "    \"root_dir\": ROOT_DIR,\n",
    "    \"target_field_name\": \"label\",\n",
    "    \"batch_predict_display_name\": BATCH_PREDICTION_DISPLAY_NAME,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6tjK6MUYg5zM"
   },
   "outputs": [],
   "source": [
    "job = aiplatform.PipelineJob(\n",
    "    display_name=PIPELINE_DISPLAY_NAME,\n",
    "    template_path=\"text_classification_pipeline.json\",\n",
    "    location=REGION,\n",
    "    enable_caching=True,\n",
    "    parameter_values=parameters,\n",
    "    project=PROJECT_ID,\n",
    ")\n",
    "\n",
    "job.submit(service_account=SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tNkT5JDHg5zM"
   },
   "source": [
    "## 进行预测\n",
    "\n",
    "一旦您的模型部署到一个端点，它可以使用UI或KFP SDK进行预测。\n",
    "\n",
    "要使用UI，请观看这个简短的[教程](https://screencast.googleplex.com/cast/NDY5Nzk3NzUzNzk1Mzc5MnxiNDMxZTFkNi1lNg)或按照[这里](https://cloud.google.com/vertex-ai/docs/predictions/online-predictions-custom-models)所示的步骤进行操作。\n",
    "\n",
    "要使用KFP SDK 进行预测，需要知道部署模型的端点。以下代码从PipelineJob中提取`ENDPOINT_ID`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r9y_riwHg5zM"
   },
   "outputs": [],
   "source": [
    "task_id = \"deploy-model-to-endpoint-for-google-cloud-vertex-ai-model\"\n",
    "deploy_task_detail = [\n",
    "    task_details\n",
    "    for task_details in job.task_details\n",
    "    if task_details.task_name == task_id\n",
    "][0]\n",
    "ENDPOINT_ID = deploy_task_detail.execution.metadata[\"output:endpoint_name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "skY2oAy8g5zN"
   },
   "source": [
    "每个请求必须是一个带有`text`键的JSON对象。`instances`应该是一个包含所有请求的列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E-GNIT9Ng5zN"
   },
   "outputs": [],
   "source": [
    "instances = [\n",
    "    {\n",
    "        \"text\": \"Irish Voters Set To Liberalize Abortion Laws In Landslide, Exit Poll Signals Vote counting will begin Saturday.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5DXeesSog5zN"
   },
   "source": [
    "以上順序結構化的要求列表中的預測清單。每個預測都是一個向量，其中包含 `class_names` 中每個類別作為正確標籤的概率。概率與 `class_names` 中每個類別的順序對應。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nRMUx8tng5zN"
   },
   "outputs": [],
   "source": [
    "endpoint = aiplatform.Endpoint(ENDPOINT_ID)\n",
    "prediction = endpoint.predict(instances=instances)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "清理\n",
    "\n",
    "要清理此项目中使用的所有Google Cloud资源，您可以[删除用于教程的Google Cloud项目](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects)。\n",
    "\n",
    "否则，您可以删除在本教程中创建的各个资源。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sx_vKniMq9ZX"
   },
   "outputs": [],
   "source": [
    "# Delete GCS bucket.\n",
    "! gsutil -m rm -r {BUCKET_URI}\n",
    "\n",
    "# Delete endpoint resource.\n",
    "! gcloud ai endpoints delete $ENDPOINT_ID --quiet --region $REGION"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "google_cloud_pipeline_components_ready_to_go_text_classification_pipeline.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
