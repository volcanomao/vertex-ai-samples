{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "使用Vertex AI模型注册表进行模型版本控制\n",
    "\n",
    "\n",
    "<table align=\"left\">\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/notebook_template.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> 在Colab中运行\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/notebook_template.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      在GitHub上查看\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/notebook_template.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      在Vertex AI工作台中打开\n",
    "    </a>\n",
    "  </td>                                                                                               \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95963630ed29"
   },
   "source": [
    "## 概述\n",
    "\n",
    "在这本笔记本中，我们将展示Vertex AI Model Registry与AutoML模型的模型版本控制能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6d0af502c20f"
   },
   "source": [
    "### 目标\n",
    "\n",
    "在本教程中，您将学习如何使用Vertex AI SDK和Vertex AI模型注册表来管理您的模型。\n",
    "\n",
    "本教程使用以下Google Cloud ML服务和资源：\n",
    "\n",
    "- Vertex AI AutoML\n",
    "- Vertex AI模型注册表\n",
    "\n",
    "执行的步骤包括：\n",
    "\n",
    "- 使用SparkNLP预处理数据并将其加载到BQML中\n",
    "- 使用Vertex AI AutoML训练和注册一个AutoML分类器\n",
    "- 提名一位冠军并通过更新别名为 `production` 的模型批准将其投入生产\n",
    "- 部署模型资源的默认/生产版本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b62a8b01d816"
   },
   "source": [
    "数据集\n",
    "\n",
    "[BBC](http://mlg.ucd.ie/datasets/bbc.html) 数据集包括来自BBC新闻网站的2225篇文章，涵盖了2004年至2005年的五个主题领域（商业、娱乐、政治、体育、科技）。每篇文章都保存在一个.txt文件中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8d26a1808b7c"
   },
   "source": [
    "### 成本\n",
    "\n",
    "本教程使用 Google Cloud 的可计费组件：\n",
    "\n",
    "* Vertex AI\n",
    "* BigQuery\n",
    "* Dataproc\n",
    "* Cloud Storage\n",
    "\n",
    "了解 [Vertex AI 价格](https://cloud.google.com/vertex-ai/pricing)、[BigQuery 价格](https://cloud.google.com/bigquery/pricing)、[Dataproc 价格](https://cloud.google.com/dataproc/pricing) 和 [Cloud Storage 价格](https://cloud.google.com/storage/pricing)，并使用 [定价计算器](https://cloud.google.com/products/calculator/) 来根据您的预期使用情况生成成本估算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ze4-nDLfK4pw"
   },
   "source": [
    "### 配置你的本地开发环境\n",
    "\n",
    "**如果你正在使用Colab或者Vertex AI Workbench笔记本**，你的环境已经满足运行这个笔记本的所有要求。你可以跳过这一步。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCuSR8GkAgzl"
   },
   "source": [
    "否则，请确保您的环境满足该笔记本的要求。\n",
    "您需要以下内容：\n",
    "\n",
    "* Google Cloud SDK\n",
    "* Git\n",
    "* Python 3\n",
    "* virtualenv\n",
    "* 在使用Python 3的虚拟环境中运行的Jupyter笔记本\n",
    "\n",
    "Google Cloud指南[设置Python开发环境](https://cloud.google.com/python/setup)和[Jupyter安装指南](https://jupyter.org/install)提供了满足这些要求的详细说明。以下步骤提供了一套简明的说明：\n",
    "\n",
    "1. [安装并初始化Cloud SDK。](https://cloud.google.com/sdk/docs/)\n",
    "\n",
    "2. [安装Python 3。](https://cloud.google.com/python/setup#installing_python)\n",
    "\n",
    "3. [安装virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv)并创建一个使用Python 3的虚拟环境。激活虚拟环境。\n",
    "\n",
    "4. 要安装Jupyter，请在终端窗口中的命令行上运行`pip3 install jupyter`。\n",
    "\n",
    "5. 要启动Jupyter，请在终端窗口的命令行上运行`jupyter notebook`。\n",
    "\n",
    "6. 在Jupyter Notebook仪表板中打开此笔记本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "## 安装\n",
    "\n",
    "安装下面所需的包以执行这个笔记本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2b4ef9b72d43"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Vertex AI Workbench Notebook product has specific requirements\n",
    "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\")\n",
    "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
    "    \"/opt/deeplearning/metadata/env_version\"\n",
    ")\n",
    "\n",
    "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_WORKBENCH_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\"\n",
    "\n",
    "! pip3 install --upgrade tensorflow google-cloud-bigquery google-cloud-aiplatform {USER_FLAG} -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhq5zEbGg0XX"
   },
   "source": [
    "### 重新启动内核\n",
    "\n",
    "在安装完额外的软件包后，您需要重新启动笔记本内核，以便它可以找到这些软件包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EzrelQZ22IZj"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF1j6f9HApxa"
   },
   "source": [
    "## 在开始之前\n",
    "\n",
    "### 设置您的谷歌云项目\n",
    "\n",
    "**无论您使用什么笔记本环境，以下步骤都是必需的。**\n",
    "\n",
    "1. [选择或创建谷歌云项目](https://console.cloud.google.com/cloud-resource-manager)。当您首次创建帐户时，您将获得300美元的免费信用额度，用于支付计算/存储成本。\n",
    "\n",
    "1. [确保为您的项目启用了结算功能](https://cloud.google.com/billing/docs/how-to/modify-project)。\n",
    "\n",
    "1. [启用 API](https://console.cloud.google.com/flows/enableapi?apiid=iam.googleapis.com,aiplatform.googleapis.com,artifactregistry.googleapis.com,dataproc.googleapis.com,cloudbuild.googleapis.com)\n",
    "\n",
    "1. 如果您在本地运行此笔记本，您需要安装 [Cloud SDK](https://cloud.google.com/sdk)。\n",
    "\n",
    "1. 在下面的单元格中输入您的项目ID。然后运行该单元格，以确保\n",
    "Cloud SDK在此笔记本中的所有命令中使用正确的项目。\n",
    "\n",
    "**注意**：Jupyter将以 `!` 为前缀的行视为shell命令，并将以 `$` 为前缀的Python变量插入这些命令中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WReHDGG5g0XY"
   },
   "source": [
    "设置您的项目ID\n",
    "\n",
    "**如果您不知道您的项目ID**，您可以使用`gcloud`来获取您的项目ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oM1iC_MfAts1"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JX9pSDAbZ7_r"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID:\", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_gcloud_project_id"
   },
   "outputs": [],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "### 区域\n",
    "\n",
    "您还可以更改“REGION”变量，该变量用于此笔记本的其余部分操作。以下是支持Vertex AI的区域。我们建议您选择最靠近您的区域。\n",
    "\n",
    "- 美洲：`us-central1`\n",
    "- 欧洲：`europe-west4`\n",
    "- 亚太：`asia-east1`\n",
    "\n",
    "您可能不会使用多区域存储桶进行Vertex AI的培训。并非所有区域都支持所有Vertex AI服务。\n",
    "\n",
    "了解有关[Vertex AI 区域](https://cloud.google.com/vertex-ai/docs/general/locations)的更多信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vprrvv0Ey1CU"
   },
   "outputs": [],
   "source": [
    "REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
    "\n",
    "if REGION == \"[your-region]\":\n",
    "    REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06571eb4063b"
   },
   "source": [
    "UUID\n",
    "\n",
    "如果您正在参加实时教程会话，您可能在使用共享测试账户或项目。为了避免用户在创建的资源之间发生名称冲突，您需要为每个实例会话创建一个UUID，并将其附加到您在本教程中创建的资源名称之后。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "697568e92bd6"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "\n",
    "# Generate a uuid of a specifed length(default=8)\n",
    "def generate_uuid(length: int = 8) -> str:\n",
    "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
    "\n",
    "\n",
    "UUID = generate_uuid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dr--iN2kAylZ"
   },
   "source": [
    "### 验证您的谷歌云账户\n",
    "\n",
    "**如果您正在使用Vertex AI工作台笔记本**，您的环境已经通过身份验证。跳过此步骤。\n",
    "\n",
    "**如果您正在使用Colab**，运行下面的单元格并按照提示进行身份验证。\n",
    "\n",
    "**否则**，请按照以下步骤操作：\n",
    "\n",
    "1. 在云控制台中，转到[**创建服务账户密钥**页面](https://console.cloud.google.com/apis/credentials/serviceaccountkey)。\n",
    "\n",
    "2. 点击**创建服务账户**。\n",
    "\n",
    "3. 在**服务账户名称**字段中输入名称，并点击**创建**。\n",
    "\n",
    "4. 在**授予此服务账户访问项目的权限**部分，点击**角色**下拉列表。在筛选框中输入并选中以下角色：\n",
    "\n",
    "    *   BigQuery管理员\n",
    "    *   Dataproc管理员\n",
    "    *   Dataproc工作台\n",
    "    *   存储管理员\n",
    "    *   存储对象管理员\n",
    "    *   Vertex AI管理员\n",
    "\n",
    "5. 点击*创建*。一个包含您密钥的JSON文件将下载到您的本地环境。\n",
    "\n",
    "6. 在下面的单元格中输入您的服务账户密钥的路径作为`GOOGLE_APPLICATION_CREDENTIALS`变量，并运行该单元格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PyQmSRbKA8r-"
   },
   "outputs": [],
   "source": [
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# If on Vertex AI Workbench, then don't execute this code\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\") and not os.getenv(\n",
    "    \"DL_ANACONDA_HOME\"\n",
    "):\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\"):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23988890fef6"
   },
   "source": [
    "获取项目编号\n",
    "\n",
    "现在项目ID已设置，您可以获得相应的项目编号。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2d6950574e1d"
   },
   "outputs": [],
   "source": [
    "shell_output = ! gcloud projects list --filter=\"PROJECT_ID:'{PROJECT_ID}'\" --format='value(PROJECT_NUMBER)'\n",
    "PROJECT_NUMBER = shell_output[0]\n",
    "print(\"Project Number:\", PROJECT_NUMBER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "### 创建一个云存储桶\n",
    "\n",
    "**无论您使用的是哪种笔记本环境，以下步骤都是必需的。**\n",
    "\n",
    "在下方设置您的云存储桶的名称。它必须在所有云存储桶中是唯一的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"[your-bucket-name]\"  # @param {type:\"string\"}\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cf221059d072"
   },
   "outputs": [],
   "source": [
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"[your-bucket-name]\":\n",
    "    BUCKET_NAME = PROJECT_ID + \"-aip-\" + UUID\n",
    "    BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EcIXiGsCePi"
   },
   "source": [
    "只有当您的存储桶不存在时才执行以下单元格以创建您的云存储存储桶。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIq7R4HZCfIc"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucvCsknMCims"
   },
   "source": [
    "最后，通过检查云存储桶中的内容来验证对其的访问权限。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vhOb7YnwClBb"
   },
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "set_service_account"
   },
   "source": [
    "服务账号\n",
    "\n",
    "如果您不想使用您的项目的Compute Engine服务账号，则将`SERVICE_ACCOUNT`设置为另一个服务账号ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6tfyNMaIy1CW"
   },
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT = \"[your-service-account]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "autoset_service_account"
   },
   "outputs": [],
   "source": [
    "if (\n",
    "    SERVICE_ACCOUNT == \"\"\n",
    "    or SERVICE_ACCOUNT is None\n",
    "    or SERVICE_ACCOUNT == \"[your-service-account]\"\n",
    "):\n",
    "    # Get your service account from gcloud\n",
    "    if not IS_COLAB:\n",
    "        shell_output = !gcloud auth list 2>/dev/null\n",
    "        SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
    "\n",
    "    else:  # IS_COLAB:\n",
    "        shell_output = ! gcloud projects describe  $PROJECT_ID\n",
    "        project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
    "        SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
    "\n",
    "    print(\"Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "set_service_account:pipelines"
   },
   "source": [
    "设置服务账户访问权限\n",
    "\n",
    "运行以下命令，将您的服务账户访问权限授予您在上一步中创建的存储桶。您只需要对每个服务账户运行这一步一次。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RL4BUlkPy1CX"
   },
   "outputs": [],
   "source": [
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-TRRTduWyx7f"
   },
   "source": [
    "启用Dataproc无服务器的专用Google访问权限\n",
    "\n",
    "要执行无服务器Spark工作负载，VPC子网络必须满足Dataproc无服务器的Spark网络配置中列出的[要求](https://cloud.google.com/dataproc-serverless/docs/concepts/network)。在本教程中，我们将使用默认设置并启用专用IP访问。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d-1ZXXtTyzI5"
   },
   "outputs": [],
   "source": [
    "SUBNETWORK = \"default\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3mSeas5axzyg"
   },
   "outputs": [],
   "source": [
    "!gcloud compute networks subnets list --regions=$REGION --filter=$SUBNETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bTsxWbNMyFKy"
   },
   "outputs": [],
   "source": [
    "!gcloud compute networks subnets update $SUBNETWORK \\\n",
    "--region=$REGION \\\n",
    "--enable-private-ip-google-access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VsdtU6E-yjxl"
   },
   "outputs": [],
   "source": [
    "!gcloud compute networks subnets describe $SUBNETWORK \\\n",
    "--region=$REGION \\\n",
    "--format=\"get(privateIpGoogleAccess)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o7p_RIzNM02N"
   },
   "source": [
    "### 创建和配置 Docker 存储库\n",
    "\n",
    "您可以在 Artefact Registry 中为将为 NLP 数据预处理创建的自定义 dataproc 映像创建一个 Docker 存储库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KfTW_fMeWq3e"
   },
   "outputs": [],
   "source": [
    "REPO_NAME = \"vertex-ai-model-registry-demo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oyXCqeEPT-Cu"
   },
   "outputs": [],
   "source": [
    "!gcloud artifacts repositories create $REPO_NAME \\\n",
    "    --repository-format=docker \\\n",
    "    --location=$REGION \\\n",
    "    --description=\"vertex ai model registry spark docker repository\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gc1ubsMoF7wn"
   },
   "source": [
    "### 设置项目模板\n",
    "\n",
    "您可以创建一组存储库来在本地组织您的项目。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "420y8i4KF_z4"
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"data\"\n",
    "SRC_PATH = \"src\"\n",
    "BUILD_PATH = \"build\"\n",
    "CONFIG_PATH = \"config\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MHvsHGncGB-B"
   },
   "outputs": [],
   "source": [
    "!mkdir -m 777 -p $DATA_PATH $SRC_PATH $BUILD_PATH $CONFIG_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "grDrP5pcGH2m"
   },
   "source": [
    "### 获取输入数据\n",
    "\n",
    "在以下代码中，您将下载并提取教程数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x_Nnlf0P4wnS"
   },
   "outputs": [],
   "source": [
    "RAW_DATA_URI = \"http://mlg.ucd.ie/files/datasets/bbc-fulltext.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ML7i-NVFGLS6"
   },
   "outputs": [],
   "source": [
    "!rm -Rf {DATA_PATH}/raw \n",
    "!wget --no-parent {RAW_DATA_URI} --directory-prefix={DATA_PATH}/raw \n",
    "!unzip -qo {DATA_PATH}/raw/bbc-fulltext.zip -d {DATA_PATH}/raw && mv {DATA_PATH}/raw/bbc/* {DATA_PATH}/raw/\n",
    "!rm -Rf {DATA_PATH}/raw/bbc-fulltext.zip {DATA_PATH}/raw/bbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wf3vIGViHYo3"
   },
   "source": [
    "设置 BigQuery 数据集\n",
    "\n",
    "您为本教程创建了 BigQuery 数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ZfxQL8JHdHE"
   },
   "outputs": [],
   "source": [
    "LOCATION = REGION.split(\"-\")[0]\n",
    "BQ_DATASET = \"bcc_sport\"\n",
    "\n",
    "! bq mk --location={LOCATION} --dataset {PROJECT_ID}:{BQ_DATASET}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### 引入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRUOFELefqf1"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import glob\n",
    "# General\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 3000)\n",
    "\n",
    "# Model Training\n",
    "import tensorflow as tf\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XPYUKWxx70wo"
   },
   "outputs": [],
   "source": [
    "print(\"BigQuery library version:\", bigquery.__version__)\n",
    "print(\"Vertex AI library version:\", vertex_ai.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcv3P_PTWCeR"
   },
   "source": [
    "设定变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K3tPJ5YcWEys"
   },
   "outputs": [],
   "source": [
    "# General\n",
    "STAGING_BUCKET = f\"{BUCKET_URI}/jobs\"\n",
    "RAW_PATH = os.path.join(DATA_PATH, \"raw\")\n",
    "DATAPROC_IMAGE_BUILD_PATH = os.path.join(BUILD_PATH, \"dataproc_image\")\n",
    "PREPROCESS_DOCKERFILE_PATH = os.path.join(DATAPROC_IMAGE_BUILD_PATH, \"Dockerfile\")\n",
    "DATAPROC_RUNTIME_IMAGE = \"dataproc_serverless_custom_runtime\"\n",
    "IMAGE_TAG = \"1.0.0\"\n",
    "DATAPROC_RUNTIME_CONTAINER_IMAGE = (\n",
    "    f\"gcr.io/{PROJECT_ID}/{DATAPROC_RUNTIME_IMAGE}:{IMAGE_TAG}\"\n",
    ")\n",
    "INIT_PATH = os.path.join(SRC_PATH, \"__init__.py\")\n",
    "MODULE_URI = f\"{BUCKET_URI}/{SRC_PATH}\"\n",
    "VERTEX_AI_MODEL_ID = \"text-classifier-model\"\n",
    "\n",
    "# Ingest\n",
    "PREPARED_PATH = os.path.join(DATA_PATH, \"prepared\")\n",
    "PREPARED_FILE = \"prepared_data.csv\"\n",
    "PREPARED_FILE_PATH = os.path.join(PREPARED_PATH, PREPARED_FILE)\n",
    "PREPARED_FILE_URI = f\"{BUCKET_URI}/{PREPARED_FILE_PATH}\"\n",
    "\n",
    "# Preprocess\n",
    "PREPROCESS_MODULE_PATH = os.path.join(SRC_PATH, \"preprocess.py\")\n",
    "LEMMA_DICTIONARY_PATH = os.path.join(CONFIG_PATH, \"lemmas.txt\")\n",
    "LEMMA_DICTIONARY_URI = f\"{BUCKET_URI}/{CONFIG_PATH}/lemmas.txt\"\n",
    "PROCESS_PYTHON_FILE_URI = f\"{MODULE_URI}/preprocess.py\"\n",
    "PROCESS_DATA_PATH = os.path.join(DATA_PATH, \"processed\")\n",
    "BQ_OUTPUT_TABLE_URI = f\"{BQ_DATASET}.news_processed_{UUID}\"\n",
    "PROCESS_DATA_URI = f\"{BUCKET_URI}/{PROCESS_DATA_PATH}\"\n",
    "PROCESS_FILE_URI = f\"{PROCESS_DATA_URI}/*.parquet\"\n",
    "PREPROCESS_BATCH_ID = f\"nlp-preprocess-{UUID}\"\n",
    "\n",
    "# Training\n",
    "AUTOML_BQ_TABLE_URI = f\"{BQ_DATASET}.news_automl_dataset_table_{UUID}\"\n",
    "AUTOML_BQ_SOURCE = f\"bq://{PROJECT_ID}.{AUTOML_BQ_TABLE_URI}\"\n",
    "AUTOML_TEXT_DATASET = f\"sport_news_dataset_{UUID}\"\n",
    "AUTOML_BQ_EVALUATION_TABLE = (\n",
    "    f\"bq://{PROJECT_ID}.{BQ_DATASET}.news_automl_eval_table_{UUID}\"\n",
    ")\n",
    "\n",
    "# Deployment\n",
    "ENDPOINT_NAME = \"text-classifier-endpoint\"\n",
    "DEPLOYED_MODEL_NAME = \"naive-bayes-text-classifier\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "source": [
    "### 初始化用于Python的Vertex AI SDK\n",
    "\n",
    "为您的项目和相应的存储桶初始化Vertex AI SDK。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gb46F2fk-ZTR"
   },
   "outputs": [],
   "source": [
    "vertex_ai.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-4MN3EoDvjga"
   },
   "source": [
    "###助手\n",
    "\n",
    "一组助手，可以简化一些任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1I4LpAJg78Fp"
   },
   "outputs": [],
   "source": [
    "def prepare_data(input_path: str, output_path: str, file_name: str):\n",
    "    \"\"\"\n",
    "    This function prepares the data for the model registry demo.\n",
    "    Args:\n",
    "        input_path: The directory where the raw data is stored.\n",
    "        output_path: The directory where the prepared data will be stored.\n",
    "        file_name: The name of the file to be prepared.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Read folder names\n",
    "    categories = [f.name for f in os.scandir(input_path) if f.is_dir()]\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "\n",
    "    # Create output file\n",
    "    with open(output_path + \"/\" + file_name, \"w\") as output_file:\n",
    "        csv_writer = csv.writer(output_file)\n",
    "        csv_writer.writerow([\"category\", \"text\"])\n",
    "\n",
    "        # For each category, read all files and write to output file\n",
    "        for category in categories:\n",
    "            # Read all files in category\n",
    "            for filename in glob.glob(os.path.join(input_path, category, \"*.txt\")):\n",
    "                # Read file\n",
    "                with open(filename, \"r\") as input_file:\n",
    "                    output_text = \"\".join([line.rstrip() for line in input_file])\n",
    "                    # Write to output file\n",
    "                    csv_writer.writerow([category, output_text])\n",
    "                    input_file.close()\n",
    "\n",
    "        # Close output file\n",
    "        output_file.close()\n",
    "\n",
    "\n",
    "def run_query(query):\n",
    "\n",
    "    \"\"\"\n",
    "    This function runs a query on the prepared data.\n",
    "    Args:\n",
    "        query: The query to be run.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Construct a BigQuery client object.\n",
    "    client = bigquery.Client(project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "    # Run the query_job\n",
    "    query_job = client.query(query)\n",
    "\n",
    "    # Wait for the query to finish\n",
    "    result = query_job.result()\n",
    "\n",
    "    # Return table\n",
    "    table = query_job.ddl_target_table\n",
    "\n",
    "    return table, result\n",
    "\n",
    "\n",
    "def read_metrics_file(metrics_file_uri):\n",
    "    \"\"\"\n",
    "    This function reads metrics file on bucket\n",
    "    Args:\n",
    "      metrics_file_uri: The uri of the metrics file\n",
    "    Returns:\n",
    "      metrics_str: metrics string\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.io.gfile.GFile(metrics_file_uri, \"r\") as metrics_file:\n",
    "        metrics = metrics_file.read().replace(\"'\", '\"')\n",
    "    metrics_file.close()\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "brRJTOTQmrcp"
   },
   "source": [
    "使用Dataproc Serverless进行数据工程\n",
    "\n",
    "在构建NLP机器学习模型之前，有一些常见的预处理步骤：\n",
    "\n",
    "1. 初步处理，如句子切分和单词标记化\n",
    "2. 常见步骤，如去除停用词，词干提取和词形还原，去除数字/标点符号，转换为小写等。\n",
    "\n",
    "其他步骤包括归一化、语言检测以及词性标注之外的语法分析。\n",
    "\n",
    "在接下来的部分中，您将摄取数据集，并使用Dataproc Serverless上的SparkNLP构建和执行一个简单的NLP预处理流水线。为此，您需要：\n",
    "\n",
    "1. 在Google Cloud Bucket上上传数据\n",
    "2. 创建一个自定义Dataproc Serverless映像\n",
    "3. 创建并上传`preprocess`模块及其依赖项到Google Cloud Bucket\n",
    "\n",
    "然后，您将运行Dataproc Serverless作业，并将结果数据加载到Bigquery中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pt4-0iDRwxSl"
   },
   "source": [
    "摄入数据\n",
    "\n",
    "接下来，你需要做以下事情：\n",
    "\n",
    "1. 从目录中提取新闻，创建相应的csv文件来准备数据。\n",
    "2. 将数据上传至Google Cloud Bucket。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hmSQUovoD8_C"
   },
   "source": [
    "准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yCSAJMkQ9X0G"
   },
   "outputs": [],
   "source": [
    "prepare_data(RAW_PATH, PREPARED_PATH, PREPARED_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2oeNQhmcEAtB"
   },
   "source": [
    "快速浏览CSV数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VKkkdTmiGacU"
   },
   "outputs": [],
   "source": [
    "! head $PREPARED_FILE_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cdov1WbQF5L1"
   },
   "source": [
    "将数据上传到存储桶####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1-kp6EeiAF5C"
   },
   "outputs": [],
   "source": [
    "! gsutil cp $PREPARED_FILE_PATH $PREPARED_FILE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JeAdjD55KhTY"
   },
   "source": [
    "### 基本数据和特征工程\n",
    "\n",
    "在这种情况下，您将使用Spark管道来使用Spark NLP覆盖以下步骤\n",
    "\n",
    "1. 句子分割\n",
    "2. 词语分割\n",
    "3. 标准化\n",
    "4. 停用词移除\n",
    "5. 词干提取\n",
    "6. 词形还原\n",
    "\n",
    "最后，您将使用`CountVectorizer`对象创建一个词袋（BOW）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68nYBB5GS9TB"
   },
   "source": [
    "构建自定义Dataproc无服务器镜像\n",
    "\n",
    "`DataprocPySparkBatchOp`允许您传递自定义镜像，当提供的Dataproc无服务器运行时版本不符合您的要求时可以使用。 在这种情况下，需要一个带有Spark NLP库的镜像。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C8DsE4wHqI7B"
   },
   "source": [
    "下载Spark作业所需的依赖项\n",
    "\n",
    "您可以下载运行NLP预处理流程所需的Spark依赖项。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Wxd4fqvl8CF"
   },
   "outputs": [],
   "source": [
    "! rm -rf $DATAPROC_IMAGE_BUILD_PATH\n",
    "! mkdir $DATAPROC_IMAGE_BUILD_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4N8m-K1ldt11"
   },
   "outputs": [],
   "source": [
    "!gsutil cp gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.22.2.jar $DATAPROC_IMAGE_BUILD_PATH\n",
    "!wget -P $DATAPROC_IMAGE_BUILD_PATH https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-assembly-4.0.2.jar\n",
    "!wget -P $DATAPROC_IMAGE_BUILD_PATH https://repo.anaconda.com/miniconda/Miniconda3-py38_4.9.2-Linux-x86_64.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GF9_5IGYqLAX"
   },
   "source": [
    "将Dataproc无服务器自定义运行时图像定义为（Define）Dataproc无服务器自定义运行时图像（Dataproc serverless custom runtime image）。\n",
    "\n",
    "您需要定义 Dockerfile 来创建自定义图像。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nWiGHdEibCcv"
   },
   "outputs": [],
   "source": [
    "dataproc_serverless_custom_runtime_image = \"\"\"\n",
    "# Debian 11 is recommended.\n",
    "FROM debian:11-slim\n",
    "\n",
    "# Suppress interactive prompts\n",
    "ENV DEBIAN_FRONTEND=noninteractive\n",
    "\n",
    "# (Required) Install utilities required by Spark scripts.\n",
    "RUN apt update && apt install -y procps tini\n",
    "\n",
    "# (Optional) Add extra jars.\n",
    "ENV SPARK_EXTRA_JARS_DIR=/opt/spark/jars/\n",
    "ENV SPARK_EXTRA_CLASSPATH='/opt/spark/jars/*'\n",
    "RUN mkdir -p \"${SPARK_EXTRA_JARS_DIR}\"\n",
    "COPY spark-bigquery-with-dependencies_2.12-0.22.2.jar \"${SPARK_EXTRA_JARS_DIR}\"\n",
    "COPY spark-nlp-assembly-4.0.2.jar \"${SPARK_EXTRA_JARS_DIR}\"\n",
    "\n",
    "# (Optional) Install and configure Miniconda3.\n",
    "ENV CONDA_HOME=/opt/miniconda3\n",
    "ENV PYSPARK_PYTHON=${CONDA_HOME}/bin/python\n",
    "ENV PATH=${CONDA_HOME}/bin:${PATH}\n",
    "COPY Miniconda3-py38_4.9.2-Linux-x86_64.sh .\n",
    "RUN bash Miniconda3-py38_4.9.2-Linux-x86_64.sh -b -p /opt/miniconda3 \\\n",
    "  && ${CONDA_HOME}/bin/conda config --system --set always_yes True \\\n",
    "  && ${CONDA_HOME}/bin/conda config --system --set auto_update_conda False \\\n",
    "  && ${CONDA_HOME}/bin/conda config --system --prepend channels conda-forge \\\n",
    "  && ${CONDA_HOME}/bin/conda config --system --set channel_priority strict\n",
    "\n",
    "# (Optional) Install Conda packages.\n",
    "#\n",
    "# The following packages are installed in the default image, it is strongly\n",
    "# recommended to include all of them.\n",
    "#\n",
    "# Use mamba to install packages quickly.\n",
    "RUN ${CONDA_HOME}/bin/conda install mamba -n base -c conda-forge \\\n",
    "    && ${CONDA_HOME}/bin/mamba install \\\n",
    "      conda \\\n",
    "      cython \\\n",
    "      gcsfs \\\n",
    "      google-cloud-bigquery-storage \\\n",
    "      google-cloud-bigquery[pandas] \\\n",
    "      google-cloud-dataproc \\\n",
    "      numpy \\\n",
    "      pandas \\\n",
    "      python \\\n",
    "      pyspark \\\n",
    "      findspark\n",
    "\n",
    "# Use conda to install spark-nlp\n",
    "RUN ${CONDA_HOME}/bin/conda install -n base -c johnsnowlabs spark-nlp\n",
    "\n",
    "# Add lemma dictionary\n",
    "# ENV CONFIG_DIR='/home/app/build'\n",
    "# RUN mkdir -p \"${CONFIG_DIR}\"\n",
    "# COPY lemmas.txt \"${CONFIG_DIR}\"\n",
    "\n",
    "# (Required) Create the 'spark' group/user.\n",
    "# The GID and UID must be 1099. Home directory is required.\n",
    "RUN groupadd -g 1099 spark\n",
    "RUN useradd -u 1099 -g 1099 -d /home/spark -m spark\n",
    "USER spark\n",
    "\"\"\"\n",
    "\n",
    "with open(PREPROCESS_DOCKERFILE_PATH, \"w\") as f:\n",
    "    f.write(dataproc_serverless_custom_runtime_image)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXzI2xInqb3V"
   },
   "source": [
    "##### 使用Google Cloud Build构建Dataproc无服务器自定义运行时\n",
    "\n",
    "您可以使用云构建来创建并注册容器映像到Artefact注册表。\n",
    "\n",
    "请注意，`<PROJECT_ID>@cloudbuild.gserviceaccount.com`需要具有对Google Cloud Storage对象的storage.objects.get访问权限。\n",
    "\n",
    "**注意**：此步骤将需要约5分钟。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U5QT7D1LkG7L"
   },
   "outputs": [],
   "source": [
    "CLOUD_BUILD_SERVICE_ACCOUNT = f\"{PROJECT_NUMBER}@cloudbuild.gserviceaccount.com\"\n",
    "\n",
    "! gsutil iam ch serviceAccount:{CLOUD_BUILD_SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
    "! gsutil iam ch serviceAccount:{CLOUD_BUILD_SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RnpRqGrFkPaH"
   },
   "outputs": [],
   "source": [
    "!gcloud builds submit --tag $DATAPROC_RUNTIME_CONTAINER_IMAGE $DATAPROC_IMAGE_BUILD_PATH --machine-type=N1_HIGHCPU_32 --timeout=900s --verbosity=info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_lsCPf1KwImo"
   },
   "source": [
    "#### 准备 `preprocess` 模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AFWE_I8VfQFO"
   },
   "source": [
    "创建预处理模块\n",
    "\n",
    "该模块将预处理数据，包括以下步骤：\n",
    "\n",
    "1. 句子分割\n",
    "2. 单词分词\n",
    "3. 规范化\n",
    "4. 停用词移除\n",
    "5. 词干提取\n",
    "6. 词形还原"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X_-MYpcbr_1N"
   },
   "outputs": [],
   "source": [
    "with open(INIT_PATH, \"w\") as init_file:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j9Ouh6VMr8cB"
   },
   "outputs": [],
   "source": [
    "process_module = \"\"\"\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "'''\n",
    "This is a simple module to preprocess the data for the model registry demo.\n",
    "Steps:\n",
    "1. Sentence segmentation\n",
    "2. Word tokenization\n",
    "3. Normalization\n",
    "4. Stopword removal\n",
    "5. Stemming\n",
    "6. Lemmatization\n",
    "'''\n",
    "\n",
    "# Libraries\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "from pyspark.sql.types import StructType, StringType\n",
    "from pyspark.sql.functions import col, concat_ws, rand\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Variables ------------------------------------------------------------------------------------------------------------\n",
    "DATA_SCHEMA = (StructType()\n",
    "               .add(\"category\", StringType(), True)\n",
    "               .add(\"text\", StringType(), True))\n",
    "SEED=8\n",
    "\n",
    "# Helper functions -----------------------------------------------------------------------------------------------------\n",
    "def get_logger():\n",
    "    '''\n",
    "    This function returns a logger object.\n",
    "    Returns:\n",
    "        logger: The logger object.\n",
    "    '''\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    handler = logging.StreamHandler()\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "    return logger\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    '''\n",
    "    This function returns the arguments from the command line.\n",
    "    Returns:\n",
    "        args: The arguments from the command line.\n",
    "    '''\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--input_path', type=str, help='The input path uri without bucket prefix')\n",
    "    parser.add_argument('--lemmas_path', type=str, help='The lemma dictionary path without bucket prefix')\n",
    "    parser.add_argument('--gcs_output_path', type=str, help='The gcs path for preprocessed data without bucket prefix')\n",
    "    parser.add_argument('--bq_output_table_uri', type=str, help='The Bigquery output table URI')\n",
    "    parser.add_argument('--bucket', type=str, help='The staging bucket')\n",
    "    parser.add_argument('--project', type=str, help='The project id')\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def build_preliminary_steps():\n",
    "    '''\n",
    "    This function builds the preliminary steps for the preprocessing.\n",
    "    Returns:\n",
    "        preliminary_steps: The preliminary steps for the preprocessing.\n",
    "    '''\n",
    "\n",
    "    document_assembler = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\").setCleanupMode('shrink_full')\n",
    "    sentence_detector = SentenceDetector().setInputCols(\"document\").setOutputCol(\"sentence\")\n",
    "    tokenizer = Tokenizer().setInputCols(\"sentence\").setOutputCol(\"token\")\n",
    "    preliminary_steps = [document_assembler, sentence_detector, tokenizer]\n",
    "    return preliminary_steps\n",
    "\n",
    "\n",
    "def build_common_preprocess_steps(lemma_uri):\n",
    "    '''\n",
    "    This function builds the common preprocessing steps.\n",
    "    Args:\n",
    "        lemma_uri: The uri of lemma dictionary\n",
    "    Returns:\n",
    "        common_preprocess_steps: The common preprocessing steps.\n",
    "    '''\n",
    "\n",
    "    normalizer = Normalizer().setInputCols(\"token\").setOutputCol(\"normalized_token\").setLowercase(True)\n",
    "    stopwords_cleaner = StopWordsCleaner().setInputCols(\"normalized_token\").setOutputCol(\n",
    "        \"cleaned_tokens\").setCaseSensitive(False)\n",
    "    stemmer = Stemmer().setInputCols(\"cleaned_tokens\").setOutputCol(\"stem\")\n",
    "    lemmatizer = Lemmatizer().setInputCols(\"stem\").setOutputCol(\"lemma\").setDictionary(lemma_uri, \"->\", \"\\t\")\n",
    "    finisher = Finisher().setInputCols(\"lemma\").setOutputCols([\"lemma_features\"]).setIncludeMetadata(\n",
    "        False).setOutputAsArray(True)\n",
    "    common_preprocess_steps = [normalizer, stopwords_cleaner, stemmer, lemmatizer, finisher]\n",
    "    return common_preprocess_steps\n",
    "\n",
    "\n",
    "def build_feature_extraction_steps():\n",
    "    '''\n",
    "    This function builds the feature extraction steps.\n",
    "    Returns:\n",
    "        feature_extraction_steps: The feature extraction steps.\n",
    "    '''\n",
    "\n",
    "    count_vectorizer = CountVectorizer().setInputCol(\"lemma_features\").setOutputCol(\"features\").setVocabSize(30)\n",
    "    feature_extraction_steps = [count_vectorizer]\n",
    "    return feature_extraction_steps\n",
    "\n",
    "\n",
    "def read_data(spark_session, data_schema, input_dir):\n",
    "    '''\n",
    "    This function reads the data from the input directory.\n",
    "    Args:\n",
    "        spark_session: The SparkSession object.\n",
    "        data_schema: The data schema.\n",
    "        input_dir: The input directory.\n",
    "    Returns:\n",
    "        raw_df: The raw dataframe.\n",
    "    '''\n",
    "\n",
    "    raw_df = (spark_session.read.option(\"header\", True)\n",
    "              .option(\"delimiter\", ',')\n",
    "              .schema(data_schema)\n",
    "              .csv(input_dir))\n",
    "    return raw_df\n",
    "\n",
    "\n",
    "def prepare_train_df(df):\n",
    "    '''\n",
    "    This function prepares the training dataframe.\n",
    "    Args:\n",
    "        df: The dataframe.\n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "    train_df = (df.withColumn(\"bow_col\", vector_to_array(\"features\"))\n",
    "                .withColumn(\"lemmas\", concat_ws(\" \", col(\"lemma_features\")))\n",
    "                .select([\"text\"] + [\"lemmas\"] + [col(\"bow_col\")[i] for i in range(30)] + [\"category\"]))\n",
    "\n",
    "    return train_df\n",
    "\n",
    "\n",
    "def save_data(data, bucket, gcs_path, bigquery_uri):\n",
    "    '''\n",
    "    This function saves the data to Bigquery.\n",
    "    Args:\n",
    "        data: The data to save.\n",
    "        bucket: The bucket.\n",
    "        gcs_path: The path to store processed data.\n",
    "        bigquery_uri: The URI of the Bigquery table.\n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "    # df_sample = data.sample(withReplacement=False, fraction=0.7, seed=SEED)\n",
    "    df_sample = data.orderBy(rand(SEED)).limit(1000)\n",
    "    df_sample.write.format('bigquery') \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"persistentGcsBucket\", bucket) \\\n",
    "        .option(\"persistentGcsPath\", gcs_path) \\\n",
    "        .save(bigquery_uri)\n",
    "\n",
    "\n",
    "# Main function --------------------------------------------------------------------------------------------------------\n",
    "def preprocess(args):\n",
    "    '''\n",
    "    preprocess function.\n",
    "    Args:\n",
    "        args: The arguments from the command line.\n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "    # Get logger\n",
    "    logger = get_logger()\n",
    "\n",
    "    # Initialize variables\n",
    "    input_path = args.input_path\n",
    "    lemma_path = args.lemmas_path\n",
    "    gcs_output_path = args.gcs_output_path\n",
    "    bq_output_table_uri = args.bq_output_table_uri\n",
    "    bucket = args.bucket\n",
    "    project = args.project\n",
    "    lemma_uri = f'gs://{bucket}/{lemma_path}'\n",
    "    input_uri = f'gs://{bucket}/{input_path}'\n",
    "\n",
    "    # Initialize SparkSession\n",
    "    logger.info('Starting preprocessing')\n",
    "    spark = sparknlp.start()\n",
    "    print(f\"Spark NLP version: {sparknlp.version()}\")\n",
    "    print(f\"Spark version: {spark.version}\")\n",
    "\n",
    "    # Build pipeline steps\n",
    "    logger.info('Building pipeline steps')\n",
    "    preliminary_steps = build_preliminary_steps()\n",
    "    common_preprocess_steps = build_common_preprocess_steps(lemma_uri)\n",
    "    feature_extraction_steps = build_feature_extraction_steps()\n",
    "    pipeline = Pipeline(stages=preliminary_steps + common_preprocess_steps + feature_extraction_steps)\n",
    "\n",
    "    # Read data\n",
    "    logger.info('Reading data')\n",
    "    raw_df = read_data(spark, DATA_SCHEMA, input_uri)\n",
    "\n",
    "    # Preprocess data\n",
    "    logger.info('Preprocessing data')\n",
    "    processed_pipeline = pipeline.fit(raw_df)\n",
    "    preprocessed_df = processed_pipeline.transform(raw_df)\n",
    "    preprocessed_df.show(10, truncate=False)\n",
    "\n",
    "    # Save data to Bigquery\n",
    "    logger.info('Saving data to Bigquery')\n",
    "    train_df = prepare_train_df(preprocessed_df)\n",
    "    save_data(train_df, bucket, gcs_output_path, bq_output_table_uri)\n",
    "    logging.info('done.')\n",
    "    spark.stop()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Get args\n",
    "    args = get_args()\n",
    "    preprocess(args)\n",
    "\"\"\"\n",
    "\n",
    "with open(PREPROCESS_MODULE_PATH, \"w\") as process_file:\n",
    "    process_file.write(process_module)\n",
    "process_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8Ic6_s3wPUC"
   },
   "source": [
    "将模块上传到存储桶中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SpLfaU6Xw4oF"
   },
   "outputs": [],
   "source": [
    "!gsutil cp $SRC_PATH/__init__.py $MODULE_URI/__init__.py\n",
    "!gsutil cp $SRC_PATH/preprocess.py $MODULE_URI/preprocess.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W1K3F7QidiCd"
   },
   "source": [
    "上传配置文件\n",
    "\n",
    "您根据Spark NLP文档使用lemma字典，并将其上传至Google Cloud存储桶。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4vxZuoy0dlfS"
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/mahavivo/vocabulary/master/lemmas/AntBNC_lemmas_ver_001.txt -O $LEMMA_DICTIONARY_PATH\n",
    "!gsutil cp $LEMMA_DICTIONARY_PATH $LEMMA_DICTIONARY_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQVJBaXTz4sV"
   },
   "source": [
    "使用Dataproc无服务器运行一个预处理的Spark作业\n",
    "\n",
    "现在您已经准备好执行，可以提交预处理的Dataproc无服务器作业。关于这个cli命令的解释超出了范围，但可以查看官方文档中的所有选项。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8fbik13otuQ7"
   },
   "outputs": [],
   "source": [
    "! gcloud beta dataproc batches submit pyspark $PROCESS_PYTHON_FILE_URI \\\n",
    "  --batch=$PREPROCESS_BATCH_ID \\\n",
    "  --container-image=$DATAPROC_RUNTIME_CONTAINER_IMAGE \\\n",
    "  --region=$REGION \\\n",
    "  --subnet='default' \\\n",
    "  --properties spark.executor.instances=2,spark.driver.cores=4,spark.executor.cores=4,spark.app.name=spark_preprocessing_job \\\n",
    "  -- --input_path=$PREPARED_FILE_PATH --lemmas_path=$LEMMA_DICTIONARY_PATH --gcs_output_path=$PROCESS_DATA_PATH --bq_output_table_uri=$BQ_OUTPUT_TABLE_URI --bucket=$BUCKET_NAME --project=$PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZhMnSORpPkO0"
   },
   "source": [
    "## 文本分类的模型训练\n",
    "\n",
    "根据[Practical Natural Language Processing: A Comprehensive Guide to Building Real-World NLP Systems](https://www.oreilly.com/library/view/practical-natural-language/9781492054047/)，有不同的方法可以训练文本分类器。\n",
    "\n",
    "例如，您可以使用\n",
    "\n",
    "- 传统方法，如逻辑回归或朴素贝叶斯分类器\n",
    "- 神经嵌入方法\n",
    "- 深度学习方法\n",
    "- 大型、预训练的语言模型\n",
    "\n",
    "在接下来的部分中，您将使用Vertex AI AutoML，并展示Vertex AI模型注册表将如何管理它。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8-Ycf1BLVvt"
   },
   "source": [
    "使用 Vertex AI AutoML 训练一个深度文本分类器模型的方法非常简单。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6vJ5nRSgfn-C"
   },
   "source": [
    "准备Biguery中的文本数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DSuusK2fful9"
   },
   "outputs": [],
   "source": [
    "automl_table_query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE {AUTOML_BQ_TABLE_URI} AS\n",
    "  SELECT text, category\n",
    "  FROM `{PROJECT_ID}.{BQ_OUTPUT_TABLE_URI}`\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nMGmYJT3kCjT"
   },
   "outputs": [],
   "source": [
    "table, result = run_query(query=automl_table_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KrgrTU7zXBaJ"
   },
   "source": [
    "创建一个表格数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tUqRP5UuLvag"
   },
   "outputs": [],
   "source": [
    "automl_bq_dataset = vertex_ai.TabularDataset.create(\n",
    "    display_name=AUTOML_TEXT_DATASET, bq_source=AUTOML_BQ_SOURCE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ry534SFldiTb"
   },
   "source": [
    "##### 训练AutoML文本分类器\n",
    "\n",
    "您将训练一个AutoML分类器，该分类器将最小化对数损失。最终，它将注册为文本分类器模型的新版本。\n",
    "\n",
    "请注意，这将需要**~4小时**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NoYzbaoGLvSk"
   },
   "outputs": [],
   "source": [
    "automl_pipeline_job = vertex_ai.AutoMLTabularTrainingJob(\n",
    "    display_name=f\"deep_text_classifier_{UUID}\",\n",
    "    optimization_prediction_type=\"classification\",\n",
    "    optimization_objective=\"minimize-log-loss\",\n",
    "    column_specs={\"text\": \"auto\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JXSN1sbpXSSR"
   },
   "outputs": [],
   "source": [
    "automl_model = automl_pipeline_job.run(\n",
    "    dataset=automl_bq_dataset,\n",
    "    target_column=\"category\",\n",
    "    training_fraction_split=0.8,\n",
    "    validation_fraction_split=0.1,\n",
    "    test_fraction_split=0.1,\n",
    "    parent_model=VERTEX_AI_MODEL_ID,\n",
    "    model_version_aliases=[\"automl\", \"deep_classifier\"],\n",
    "    model_version_description=\"An Vertex AI AutoML text classifier\",\n",
    "    model_labels={\"created_by\": \"inardini\", \"team\": \"advocacy\"},\n",
    "    is_default_version=False,\n",
    "    disable_early_stopping=False,\n",
    "    export_evaluated_data_items=True,\n",
    "    export_evaluated_data_items_bigquery_destination_uri=AUTOML_BQ_EVALUATION_TABLE,\n",
    "    sync=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KgsDPyfG-zjP"
   },
   "source": [
    "使用Vertex AI 模型注册表对模型管理进行规范化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oV9FimH-vrps"
   },
   "source": [
    "初始化Vertex AI模型注册表\n",
    "\n",
    "要访问Vertex AI模型资源的不同模型版本，您可以初始化一个模型注册表实例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O32IGV9tgXw8"
   },
   "outputs": [],
   "source": [
    "registry = vertex_ai.models.ModelRegistry(VERTEX_AI_MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11xaykBni5mJ"
   },
   "source": [
    "####比较模型版本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-bvStzdzPs9"
   },
   "source": [
    "评估新的候选人\n",
    "\n",
    "您将使用`list_model_evaluations`方法来评估新模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PKOmAsgv2OeH"
   },
   "outputs": [],
   "source": [
    "automl_evaluations = automl_model.list_model_evaluations()\n",
    "\n",
    "for model_evaluation in automl_evaluations:\n",
    "    print(model_evaluation.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_nr1xRQbcm4i"
   },
   "source": [
    "注册`champion`模型版本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tJIURXCyzevK"
   },
   "source": [
    "验证新候选人"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gn9LIgPoVK-1"
   },
   "outputs": [],
   "source": [
    "versions = registry.list_versions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KsB-BEXI1ECG"
   },
   "outputs": [],
   "source": [
    "CANDIDATE_VERSION_ID = versions[-1].version_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w0FBdyEb1ECG"
   },
   "outputs": [],
   "source": [
    "candidate_model_version_info = registry.get_version_info(CANDIDATE_VERSION_ID)\n",
    "candidate_model_version_info_df = pd.DataFrame(\n",
    "    candidate_model_version_info,\n",
    "    columns=[\"model_version\"],\n",
    "    index=[\n",
    "        \"version_id\",\n",
    "        \"created_at\",\n",
    "        \"updated_at\",\n",
    "        \"model_display_name\",\n",
    "        \"model_resource_name\",\n",
    "        \"version_aliases\",\n",
    "        \"version_description\",\n",
    "    ],\n",
    ")\n",
    "candidate_model_version_info_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0dLEuDn1XkU"
   },
   "source": [
    "推广到生产"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UWpPmReM1cbx"
   },
   "outputs": [],
   "source": [
    "registry.add_version_aliases([\"default\", \"production\"], version=CANDIDATE_VERSION_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JqbiAONcwpM1"
   },
   "source": [
    "部署新候选人"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lKOPUzyI1kp0"
   },
   "outputs": [],
   "source": [
    "candidate_model = registry.get_model(version=\"candidate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YwErIIsOGMzk"
   },
   "source": [
    "创建终端点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5LWhrPXfGMGu"
   },
   "outputs": [],
   "source": [
    "endpoint = vertex_ai.Endpoint.create(\n",
    "    display_name=ENDPOINT_NAME,\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqhjQSrMKGyl"
   },
   "source": [
    "部署冠军模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EvMabDk0GxgV"
   },
   "outputs": [],
   "source": [
    "endpoint.deploy(\n",
    "    model=candidate_model,\n",
    "    deployed_model_display_name=DEPLOYED_MODEL_NAME,\n",
    "    machine_type=\"n1-standard-8\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iytsp3RN15aE"
   },
   "source": [
    "生成预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "34wRUZkfbp05"
   },
   "outputs": [],
   "source": [
    "text = \"\"\"The singer to headline the event halftime show: 'It's on\"\"\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UqgNOvZ815aE"
   },
   "outputs": [],
   "source": [
    "instances = [{\"text\": text}]\n",
    "predictions = endpoint.predict(instances)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dksu_4joK2Hv"
   },
   "source": [
    "最后的想法\n",
    "\n",
    "正如您所想象的，您也可以上传外部模型。请查看文档示例和[示例笔记本](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage3/get_started_with_model_registry.ipynb)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "清理\n",
    "\n",
    "要清理此项目中使用的所有谷歌云资源，您可以[删除用于本教程的谷歌云项目](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects)。\n",
    "\n",
    "否则，您可以删除创建的各个资源。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t3GoM5NIrMML"
   },
   "outputs": [],
   "source": [
    "endpoint.undeploy_all()\n",
    "\n",
    "endpoint.delete()\n",
    "\n",
    "versions = registry.list_versions()\n",
    "for version in versions:\n",
    "    registry.delete_version(version=version.version_id)\n",
    "\n",
    "automl_pipeline_job.delete()\n",
    "\n",
    "automl_bq_dataset.delete()\n",
    "\n",
    "!gcloud dataproc batches delete $PREPROCESS_BATCH_ID --region=$REGION --quiet\n",
    "\n",
    "! bq rm -r -f -d $PROJECT_ID:$BQ_DATASET\n",
    "\n",
    "! gcloud artifacts repositories delete $REPO_NAME --location=$REGION --quiet\n",
    "\n",
    "!rm -rf $DATA_PATH $SRC_PATH $BUILD_PATH $CONFIG_PATH"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "vertex_ai_model_registry_automl_model_versioning.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
