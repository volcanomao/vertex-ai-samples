{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# 使用Vertex AI模型注册中心进行模型管理\n",
    "\n",
    "<table align=\"left\">\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/notebook_template.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> 在Colab中运行\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/notebook_template.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      在GitHub上查看\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/notebook_template.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      在Vertex AI工作台中打开\n",
    "    </a>\n",
    "  </td>                                                                                               \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1c282a4cf6d0"
   },
   "source": [
    "## 概述\n",
    "\n",
    "本笔记本将展示 Vertex AI Model Registry 在 BQML 和自定义模型中的模型版本控制功能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b534af67fe8a"
   },
   "source": [
    "### 目标\n",
    "\n",
    "在本教程中，您将学习如何使用Vertex AI SDK和Vertex AI模型注册表来管理您的模型。\n",
    "\n",
    "本教程使用以下Google Cloud ML服务和资源：\n",
    "\n",
    "- BigQuery\n",
    "- Vertex AI 训练\n",
    "- Vertex AI 模型注册表\n",
    "\n",
    "执行的步骤包括：\n",
    "\n",
    "- 使用SparkNLP预处理数据并将其载入BQML\n",
    "- 使用BQML训练和注册Logistic回归\n",
    "- 使用scikit-learn训练和注册朴素贝叶斯分类器\n",
    "- 回顾和验证BQML和scikit-learn模型\n",
    "- 提名一名冠军，并通过更新别名为`production`别名来批准该模型进入生产\n",
    "- 部署Model资源的默认/生产版本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7556b88bb30a"
   },
   "source": [
    "数据集\n",
    "\n",
    "[BBC](http://mlg.ucd.ie/datasets/bbc.html) 包含来自BBC新闻网站的2225篇文章，涵盖了2004年至2005年的五个主题领域（商业、娱乐、政治、体育、科技）的报道。每篇文章都保存在一个 .txt 文件中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4f5b82fcc00"
   },
   "source": [
    "费用\n",
    "\n",
    "本教程使用谷歌云的可计费组件：\n",
    "\n",
    "* Vertex AI\n",
    "* BigQuery\n",
    "* Dataproc\n",
    "* 云存储\n",
    "\n",
    "了解[Vertex AI价格](https://cloud.google.com/vertex-ai/pricing)和[云存储价格](https://cloud.google.com/storage/pricing)，并使用[定价计算器](https://cloud.google.com/products/calculator/)根据您的预计使用量生成费用估算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ze4-nDLfK4pw"
   },
   "source": [
    "### 设置本地开发环境\n",
    "\n",
    "**如果您正在使用 Colab 或 Vertex AI Workbench 笔记本**，您的环境已满足运行此笔记本的所有要求。您可以跳过此步骤。\n",
    "\n",
    "**否则**，请确保您的环境满足此笔记本的要求。您需要以下内容：\n",
    "\n",
    "- Google Cloud SDK\n",
    "- Git\n",
    "- Python 3\n",
    "- virtualenv\n",
    "- 在使用 Python 3 的虚拟环境中运行的 Jupyter 笔记本\n",
    "\n",
    "Google Cloud 的[设置 Python 开发环境](https://cloud.google.com/python/setup)指南和[Jupyter 安装指南](https://jupyter.org/install)提供了满足这些要求的详细说明。以下步骤提供了一套精简的说明：\n",
    "\n",
    "1. [安装和初始化 Cloud SDK。](https://cloud.google.com/sdk/docs/)\n",
    "\n",
    "2. [安装 Python 3。](https://cloud.google.com/python/setup#installing_python)\n",
    "\n",
    "3. [安装 virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv) 并创建一个使用 Python 3 的虚拟环境。激活虚拟环境。\n",
    "\n",
    "4. 要安装 Jupyter，在终端 shell 中运行 `pip3 install jupyter`。\n",
    "\n",
    "5. 要启动 Jupyter，在终端 shell 中运行 `jupyter notebook`。\n",
    "\n",
    "6. 在 Jupyter Notebook 仪表板中打开此笔记本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "## 安装\n",
    "\n",
    "安装以下所需的软件包以执行此笔记本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2b4ef9b72d43"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Vertex AI Workbench Notebook product has specific requirements\n",
    "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\")\n",
    "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
    "    \"/opt/deeplearning/metadata/env_version\"\n",
    ")\n",
    "\n",
    "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_WORKBENCH_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\"\n",
    "\n",
    "! pip3 install --upgrade tensorflow google-cloud-bigquery google-cloud-aiplatform \"shapely<2\" {USER_FLAG} -q --no-warn-conflicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhq5zEbGg0XX"
   },
   "source": [
    "### 重新启动内核\n",
    "\n",
    "安装完附加包之后，您需要重新启动笔记本内核，以便它可以找到这些包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EzrelQZ22IZj"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWEdiXsJg0XY"
   },
   "source": [
    "## 开始之前\n",
    "\n",
    "### 设置您的谷歌云项目\n",
    "\n",
    "**无论您使用什么笔记本环境，以下步骤都是必要的。**\n",
    "\n",
    "1. [选择或创建谷歌云项目](https://console.cloud.google.com/cloud-resource-manager)。当您首次创建账户时，您会获得$300的免费信用，用于支付计算/存储成本。\n",
    "\n",
    "1. [确保为您的项目启用了计费](https://cloud.google.com/billing/docs/how-to/modify-project)。\n",
    "\n",
    "1. [启用API](https://console.cloud.google.com/flows/enableapi?apiid=iam.googleapis.com,aiplatform.googleapis.com,cloudresourcemanager.googleapis.com,artifactregistry.googleapis.com,dataproc.googleapis.com,cloudbuild.googleapis.com)。\n",
    "\n",
    "1. 如果您是在本地运行此笔记本，您需要安装[Cloud SDK](https://cloud.google.com/sdk)。\n",
    "\n",
    "1. 在下面的单元格中输入您的项目ID。然后运行该单元格，以确保Cloud SDK对本笔记本中的所有命令使用正确的项目。\n",
    "\n",
    "**注意**：Jupyter会将以`!`开头的行作为shell命令运行，并将以`$`开头的Python变量插入这些命令中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WReHDGG5g0XY"
   },
   "source": [
    "设置您的项目ID\n",
    "\n",
    "**如果您不知道您的项目ID**，您可以使用`gcloud`获取您的项目ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "173f251c128d"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oM1iC_MfAts1"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID:\", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_gcloud_project_id"
   },
   "outputs": [],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "#### 地区\n",
    "\n",
    "您也可以更改 `REGION` 变量，在本笔记本的其余部分中将使用该变量进行操作。以下是Vertex AI支持的地区。我们建议您选择距离您最近的地区。\n",
    "\n",
    "- 美洲：`us-central1`\n",
    "- 欧洲：`europe-west4`\n",
    "- 亚太：`asia-east1`\n",
    "\n",
    "您可能无法使用多区域存储桶进行使用Vertex AI进行训练。并非所有地区都支持所有Vertex AI服务。\n",
    "\n",
    "了解更多关于[Vertex AI地区](https://cloud.google.com/vertex-ai/docs/general/locations)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vprrvv0Ey1CU"
   },
   "outputs": [],
   "source": [
    "REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
    "\n",
    "if REGION == \"[your-region]\":\n",
    "    REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55gWWcI15ZFH"
   },
   "source": [
    "UUID\n",
    "\n",
    "如果你正在参加现场教程会话，你可能正在使用共享的测试账户或项目。为了避免用户在创建的资源上发生名称冲突，你为每个实例会话创建一个UUID，并将其附加到你在本教程中创建的资源的名称上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uScy6YmD5ZFI"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "\n",
    "# Generate a uuid of a specifed length(default=8)\n",
    "def generate_uuid(length: int = 8) -> str:\n",
    "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
    "\n",
    "\n",
    "UUID = generate_uuid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dr--iN2kAylZ"
   },
   "source": [
    "###验证您的Google Cloud帐户\n",
    "\n",
    "**如果您正在使用Vertex AI Workbench笔记本**，您的环境已经得到验证。\n",
    "\n",
    "**如果您正在使用Colab**，请运行下面的单元格，并按提示进行身份验证，通过oAuth验证您的帐户。\n",
    "\n",
    "**否则**，请按照以下步骤操作：\n",
    "\n",
    "1. 在Cloud控制台中，转到[**创建服务帐户密钥**页面](https://console.cloud.google.com/apis/credentials/serviceaccountkey)。\n",
    "\n",
    "2. 点击**创建服务帐户**。\n",
    "\n",
    "3. 在**服务帐户名称**字段中输入名称，然后点击**创建**。\n",
    "\n",
    "4. 在**授予此服务帐户对项目的访问权限**部分，点击**角色**下拉列表。在过滤框中输入并选择以下角色：\n",
    "\n",
    "    * Artifact Registry管理员\n",
    "    * Artifact Registry存储库管理员\n",
    "    * BigQuery管理员\n",
    "    * 计算网络管理员\n",
    "    * Cloud Build编辑器\n",
    "    * Dataproc管理员\n",
    "    * Dataproc Worker\n",
    "    * 服务帐户用户\n",
    "    * 服务使用管理员\n",
    "    * 存储管理员\n",
    "    * 存储对象管理员\n",
    "    * Vertex AI管理员\n",
    "\n",
    "5. 点击*创建*。一个包含您密钥的JSON文件会下载到您的本地环境中。\n",
    "\n",
    "6. 在下面的单元格中，将您的服务帐户密钥路径输入为`GOOGLE_APPLICATION_CREDENTIALS`变量并运行单元格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PyQmSRbKA8r-"
   },
   "outputs": [],
   "source": [
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# If on Vertex AI Workbench, then don't execute this code\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\") and not os.getenv(\n",
    "    \"DL_ANACONDA_HOME\"\n",
    "):\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\"):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23988890fef6"
   },
   "source": [
    "请获取您的项目编号\n",
    "\n",
    "现在项目ID已设置好，您可以获取对应的项目编号。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2d6950574e1d"
   },
   "outputs": [],
   "source": [
    "shell_output = ! gcloud projects list --filter=\"PROJECT_ID:'{PROJECT_ID}'\" --format='value(PROJECT_NUMBER)'\n",
    "PROJECT_NUMBER = shell_output[0]\n",
    "print(\"Project Number:\", PROJECT_NUMBER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "### 创建一个云存储存储桶\n",
    "\n",
    "**无论您的笔记本环境如何，都需要以下步骤。**\n",
    "\n",
    "在下方设置您的云存储存储桶的名称。它必须在所有云存储存储桶中是唯一的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"[your-bucket-name]\"  # @param {type:\"string\"}\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cf221059d072"
   },
   "outputs": [],
   "source": [
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"[your-bucket-name]\":\n",
    "    BUCKET_NAME = PROJECT_ID + \"-aip-\" + UUID\n",
    "    BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EcIXiGsCePi"
   },
   "source": [
    "只有当您的存储桶不存在时才运行以下单元格以创建您的云存储存储桶。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIq7R4HZCfIc"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucvCsknMCims"
   },
   "source": [
    "最后，通过检查存储桶的内容来验证对您的云存储桶的访问权限。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vhOb7YnwClBb"
   },
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "set_service_account"
   },
   "source": [
    "服务账户\n",
    "\n",
    "如果您不想使用您项目的计算引擎服务账户，请将`SERVICE_ACCOUNT`设置为另一个服务账户ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6tfyNMaIy1CW"
   },
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT = \"[your-service-account]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "autoset_service_account"
   },
   "outputs": [],
   "source": [
    "if (\n",
    "    SERVICE_ACCOUNT == \"\"\n",
    "    or SERVICE_ACCOUNT is None\n",
    "    or SERVICE_ACCOUNT == \"[your-service-account]\"\n",
    "):\n",
    "    # Get your service account from gcloud\n",
    "    if not IS_COLAB:\n",
    "        shell_output = !gcloud auth list 2>/dev/null\n",
    "        SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
    "\n",
    "    else:  # IS_COLAB:\n",
    "        shell_output = ! gcloud projects describe  $PROJECT_ID\n",
    "        project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
    "        SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
    "\n",
    "    print(\"Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "set_service_account:pipelines"
   },
   "source": [
    "#### 设置服务账号访问权限\n",
    "\n",
    "运行以下命令，为您的服务账号授予访问您在上一步创建的存储桶的权限。您只需要为每个服务账号运行此步骤一次。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RL4BUlkPy1CX"
   },
   "outputs": [],
   "source": [
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-TRRTduWyx7f"
   },
   "source": [
    "为Dataproc Serverless启用私有Google访问\n",
    "\n",
    "要执行Serverless Spark工作负载，VPC子网络必须符合Dataproc Serverless for Spark网络配置中列出的要求。在本教程中，我们将使用默认设置并将其启用为私有IP访问。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d-1ZXXtTyzI5"
   },
   "outputs": [],
   "source": [
    "SUBNETWORK = \"default\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3mSeas5axzyg"
   },
   "outputs": [],
   "source": [
    "!gcloud compute networks subnets list --regions=$REGION --filter=$SUBNETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bTsxWbNMyFKy"
   },
   "outputs": [],
   "source": [
    "!gcloud compute networks subnets update $SUBNETWORK \\\n",
    "--region=$REGION \\\n",
    "--enable-private-ip-google-access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VsdtU6E-yjxl"
   },
   "outputs": [],
   "source": [
    "!gcloud compute networks subnets describe $SUBNETWORK \\\n",
    "--region=$REGION \\\n",
    "--format=\"get(privateIpGoogleAccess)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o7p_RIzNM02N"
   },
   "source": [
    "### 创建并配置Docker代码库\n",
    "\n",
    "您可以在Artefact Registry中为准备创建的用于NLP数据预处理的自定义dataproc镜像创建一个Docker代码库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KfTW_fMeWq3e"
   },
   "outputs": [],
   "source": [
    "REPO_NAME = \"vertex-ai-model-registry-demo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oyXCqeEPT-Cu"
   },
   "outputs": [],
   "source": [
    "!gcloud artifacts repositories create $REPO_NAME \\\n",
    "    --repository-format=docker \\\n",
    "    --location=$REGION \\\n",
    "    --description=\"vertex ai model registry spark docker repository\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gc1ubsMoF7wn"
   },
   "source": [
    "### 设定项目模板\n",
    "\n",
    "您可以创建一组仓库来在本地组织您的项目。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "420y8i4KF_z4"
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"data\"\n",
    "SRC_PATH = \"src\"\n",
    "BUILD_PATH = \"build\"\n",
    "CONFIG_PATH = \"config\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MHvsHGncGB-B"
   },
   "outputs": [],
   "source": [
    "!mkdir -m 777 -p $DATA_PATH $SRC_PATH $BUILD_PATH $CONFIG_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "grDrP5pcGH2m"
   },
   "source": [
    "获取输入数据\n",
    "\n",
    "在以下代码中，您下载并提取教程数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x_Nnlf0P4wnS"
   },
   "outputs": [],
   "source": [
    "RAW_DATA_URI = \"http://mlg.ucd.ie/files/datasets/bbc-fulltext.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ML7i-NVFGLS6"
   },
   "outputs": [],
   "source": [
    "!rm -Rf {DATA_PATH}/raw \n",
    "!wget --no-parent {RAW_DATA_URI} --directory-prefix={DATA_PATH}/raw \n",
    "!unzip -qo {DATA_PATH}/raw/bbc-fulltext.zip -d {DATA_PATH}/raw && mv {DATA_PATH}/raw/bbc/* {DATA_PATH}/raw/\n",
    "!rm -Rf {DATA_PATH}/raw/bbc-fulltext.zip {DATA_PATH}/raw/bbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wf3vIGViHYo3"
   },
   "source": [
    "设置 Bigquery 数据集\n",
    "\n",
    "您为本教程创建 BigQuery 数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ZfxQL8JHdHE"
   },
   "outputs": [],
   "source": [
    "LOCATION = REGION.split(\"-\")[0]\n",
    "BQ_DATASET = \"bcc_sport\"\n",
    "\n",
    "! bq mk --location={LOCATION} --dataset {PROJECT_ID}:{BQ_DATASET}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRUOFELefqf1"
   },
   "outputs": [],
   "source": [
    "# General\n",
    "import csv\n",
    "import datetime as dt\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 3000)\n",
    "\n",
    "# Model Training\n",
    "import tensorflow as tf\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XPYUKWxx70wo"
   },
   "outputs": [],
   "source": [
    "print(\"BigQuery library version:\", bigquery.__version__)\n",
    "print(\"Vertex AI library version:\", vertex_ai.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcv3P_PTWCeR"
   },
   "source": [
    "### 设定变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K3tPJ5YcWEys"
   },
   "outputs": [],
   "source": [
    "# General\n",
    "STAGING_BUCKET = f\"{BUCKET_URI}/jobs\"\n",
    "RAW_PATH = os.path.join(DATA_PATH, \"raw\")\n",
    "DATAPROC_IMAGE_BUILD_PATH = os.path.join(BUILD_PATH, \"dataproc_image\")\n",
    "PREPROCESS_DOCKERFILE_PATH = os.path.join(DATAPROC_IMAGE_BUILD_PATH, \"Dockerfile\")\n",
    "DATAPROC_RUNTIME_IMAGE = \"dataproc_serverless_custom_runtime\"\n",
    "IMAGE_TAG = \"1.0.0\"\n",
    "DATAPROC_RUNTIME_CONTAINER_IMAGE = (\n",
    "    f\"gcr.io/{PROJECT_ID}/{DATAPROC_RUNTIME_IMAGE}:{IMAGE_TAG}\"\n",
    ")\n",
    "INIT_PATH = os.path.join(SRC_PATH, \"__init__.py\")\n",
    "MODULE_URI = f\"{BUCKET_URI}/{SRC_PATH}\"\n",
    "VERTEX_AI_MODEL_ID = \"text-classifier-model\"\n",
    "\n",
    "# Ingest\n",
    "PREPARED_PATH = os.path.join(DATA_PATH, \"prepared\")\n",
    "PREPARED_FILE = \"prepared_data.csv\"\n",
    "PREPARED_FILE_PATH = os.path.join(PREPARED_PATH, PREPARED_FILE)\n",
    "PREPARED_FILE_URI = f\"{BUCKET_URI}/{PREPARED_FILE_PATH}\"\n",
    "\n",
    "# Preprocess\n",
    "PREPROCESS_MODULE_PATH = os.path.join(SRC_PATH, \"preprocess.py\")\n",
    "LEMMA_DICTIONARY_PATH = os.path.join(CONFIG_PATH, \"lemmas.txt\")\n",
    "LEMMA_DICTIONARY_URI = f\"{BUCKET_URI}/{CONFIG_PATH}/lemmas.txt\"\n",
    "PROCESS_PYTHON_FILE_URI = f\"{MODULE_URI}/preprocess.py\"\n",
    "PROCESS_DATA_PATH = os.path.join(DATA_PATH, \"processed\")\n",
    "BQ_OUTPUT_TABLE_URI = f\"{BQ_DATASET}.news_processed_{UUID}\"\n",
    "PROCESS_DATA_URI = f\"{BUCKET_URI}/{PROCESS_DATA_PATH}\"\n",
    "PROCESS_FILE_URI = f\"{PROCESS_DATA_URI}/*.parquet\"\n",
    "PREPROCESS_BATCH_ID = f\"nlp-preprocess-{UUID}\"\n",
    "\n",
    "# Training\n",
    "TRAIN_NAIVE_MODULE_PATH = os.path.join(SRC_PATH, \"train_naive.py\")\n",
    "NAIVE_TRAIN_JOB_NAME = f\"naive_training_job_{UUID}\"\n",
    "TRAIN_VERSION = \"scikit-learn-cpu.0-23\"\n",
    "NAIVE_TRAIN_CONTAINER_URI = (\n",
    "    f\"{REGION.split('-')[0]}-docker.pkg.dev/vertex-ai/training/{TRAIN_VERSION}:latest\"\n",
    ")\n",
    "NAIVE_TRAIN_REQUIREMENTS = [\"pyarrow\", \"fastparquet\", \"gcsfs\"]\n",
    "DEPLOY_VERSION = \"sklearn-cpu.0-23\"\n",
    "NAIVE_DEPLOY_CONTAINER_URI = f\"{REGION.split('-')[0]}-docker.pkg.dev/vertex-ai/prediction/{DEPLOY_VERSION}:latest\"\n",
    "NAIVE_MODEL_BASE_URI = f\"{BUCKET_URI}/deliverables/naive\"\n",
    "NAIVE_MODEL_URI = f\"{BUCKET_URI}/deliverables/naive/model\"\n",
    "NAIVE_METRICS_FILE_URI = f\"{NAIVE_MODEL_URI}/metrics.json\"\n",
    "\n",
    "# Deployment\n",
    "SERVING_BUILD_PATH = os.path.join(BUILD_PATH, \"serving\")\n",
    "SERVING_APP_BUILD_PATH = os.path.join(SERVING_BUILD_PATH, \"app\")\n",
    "SERVE_NAIVE_MODULE_PATH = os.path.join(SERVING_APP_BUILD_PATH, \"main.py\")\n",
    "SERVE_REQUIREMENTS_PATH = os.path.join(SERVING_BUILD_PATH, \"requirements.txt\")\n",
    "SERVE_DOCKERFILE_PATH = os.path.join(SERVING_BUILD_PATH, \"Dockerfile\")\n",
    "SERVE_AUTH_PATH = os.path.join(SERVING_BUILD_PATH, \"key.json\")\n",
    "SERVE_SCRIPT_PATH = os.path.join(SERVING_BUILD_PATH, \"copy_model.sh\")\n",
    "SERVING_RUNTIME_IMAGE = \"serving_custom_naive\"\n",
    "IMAGE_TAG = \"1.0.0\"\n",
    "SERVING_NAIVE_RUNTIME_CONTAINER_IMAGE = (\n",
    "    f\"gcr.io/{PROJECT_ID}/{SERVING_RUNTIME_IMAGE}:{IMAGE_TAG}\"\n",
    ")\n",
    "ENDPOINT_NAME = \"text-classifier-endpoint\"\n",
    "DEPLOYED_MODEL_NAME = \"naive-bayes-text-classifier\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "source": [
    "### 初始化Python版的Vertex AI SDK\n",
    "\n",
    "为您的项目和对应的存储桶初始化Python版SDK。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gb46F2fk-ZTR"
   },
   "outputs": [],
   "source": [
    "vertex_ai.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-4MN3EoDvjga"
   },
   "source": [
    "### 助手\n",
    "\n",
    "一组助手，用于简化一些任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1I4LpAJg78Fp"
   },
   "outputs": [],
   "source": [
    "def prepare_data(input_path: str, output_path: str, file_name: str):\n",
    "    \"\"\"\n",
    "    This function prepares the data for the model registry demo.\n",
    "    Args:\n",
    "        input_path: The directory where the raw data is stored.\n",
    "        output_path: The directory where the prepared data will be stored.\n",
    "        file_name: The name of the file to be prepared.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Read folder names\n",
    "    categories = [f.name for f in os.scandir(input_path) if f.is_dir()]\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "\n",
    "    # Create output file\n",
    "    with open(output_path + \"/\" + file_name, \"w\") as output_file:\n",
    "        csv_writer = csv.writer(output_file)\n",
    "        csv_writer.writerow([\"category\", \"text\"])\n",
    "\n",
    "        # For each category, read all files and write to output file\n",
    "        for category in categories:\n",
    "            # Read all files in category\n",
    "            for filename in glob.glob(os.path.join(input_path, category, \"*.txt\")):\n",
    "                # Read file\n",
    "                with open(filename, \"r\") as input_file:\n",
    "                    output_text = \"\".join([line.rstrip() for line in input_file])\n",
    "                    # Write to output file\n",
    "                    csv_writer.writerow([category, output_text])\n",
    "                    input_file.close()\n",
    "\n",
    "    # Close output file\n",
    "    output_file.close()\n",
    "\n",
    "\n",
    "def run_query(query):\n",
    "\n",
    "    \"\"\"\n",
    "    This function runs a query on the prepared data.\n",
    "    Args:\n",
    "        query: The query to be run.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Construct a BigQuery client object.\n",
    "    client = bigquery.Client(project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "    # Run the query_job\n",
    "    query_job = client.query(query)\n",
    "\n",
    "    # Wait for the query to finish\n",
    "    result = query_job.result()\n",
    "\n",
    "    # Return table\n",
    "    table = query_job.ddl_target_table\n",
    "\n",
    "    return table, result\n",
    "\n",
    "\n",
    "def read_metrics_file(metrics_file_uri):\n",
    "    \"\"\"\n",
    "    This function reads metrics file on bucket\n",
    "    Args:\n",
    "      metrics_file_uri: The uri of the metrics file\n",
    "    Returns:\n",
    "      metrics_str: metrics string\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.io.gfile.GFile(metrics_file_uri, \"r\") as metrics_file:\n",
    "        metrics = metrics_file.read().replace(\"'\", '\"')\n",
    "    metrics_file.close()\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "brRJTOTQmrcp"
   },
   "source": [
    "使用Dataproc Serverless进行数据工程\n",
    "\n",
    "在构建NLP机器学习模型之前，有一些常见的预处理步骤可以使用：\n",
    "\n",
    "1. 初始步骤，如句子分割和单词标记化\n",
    "2. 频繁步骤，如停用词去除，词干提取和词形还原，去除数字和标点，转小写等。\n",
    "\n",
    "其他步骤包括标准化，语言检测以及除词性标注之外的解析。\n",
    "\n",
    "在接下来的部分中，您将导入您的数据集，并使用Dataproc服务器上的SparkNLP构建和执行一个简单的NLP预处理管道。为此，您需要：\n",
    "\n",
    "1. 在Google Cloud存储桶上上传数据\n",
    "2. 创建一个自定义的Dataproc Serverless映像\n",
    "3. 创建并上传`preprocess`模块及其依赖项到Google Cloud存储桶\n",
    "\n",
    "然后您将运行Dataproc Serverless作业，并将生成的数据加载到BigQuery中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pt4-0iDRwxSl"
   },
   "source": [
    "### 导入数据\n",
    "\n",
    "接下来您将会：\n",
    "\n",
    "1.  通过从目录中提取新闻来准备数据，并生成相应的csv文件。\n",
    "2.  将数据上传至Google Cloud Bucket。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hmSQUovoD8_C"
   },
   "source": [
    "准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yCSAJMkQ9X0G"
   },
   "outputs": [],
   "source": [
    "prepare_data(RAW_PATH, PREPARED_PATH, PREPARED_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2oeNQhmcEAtB"
   },
   "source": [
    "快速查看CSV数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VKkkdTmiGacU"
   },
   "outputs": [],
   "source": [
    "! head $PREPARED_FILE_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cdov1WbQF5L1"
   },
   "source": [
    "将数据上传到存储桶中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1-kp6EeiAF5C"
   },
   "outputs": [],
   "source": [
    "! gsutil cp $PREPARED_FILE_PATH $PREPARED_FILE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JeAdjD55KhTY"
   },
   "source": [
    "### 基础数据和特征工程\n",
    "\n",
    "在这种情况下，您将使用一个Spark管道来使用Spark NLP覆盖以下步骤：\n",
    "\n",
    "1. 句子分割\n",
    "2. 单词分词\n",
    "3. 标准化\n",
    "4. 停用词移除\n",
    "5. 词干提取\n",
    "6. 词形归并\n",
    "\n",
    "最后，您将使用`CountVectorizer`对象创建一个词袋（BOW）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68nYBB5GS9TB"
   },
   "source": [
    "#### 构建自定义 Dataproc 无服务器镜像\n",
    "\n",
    "`DataprocPySparkBatchOp` 允许您传递自定义镜像，以便在[提供的 Dataproc 无服务器运行时版本](https://cloud.google.com/dataproc-serverless/docs/concepts/versions/spark-runtime-versions)不符合您的要求时使用。\n",
    "\n",
    "在这种情况下，需要一个带有 Spark NLP 库的镜像。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C8DsE4wHqI7B"
   },
   "source": [
    "下载Spark作业所需的依赖项\n",
    "\n",
    "您下载了运行NLP预处理流水线所需的Spark依赖项。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Wxd4fqvl8CF"
   },
   "outputs": [],
   "source": [
    "! rm -rf $DATAPROC_IMAGE_BUILD_PATH\n",
    "! mkdir $DATAPROC_IMAGE_BUILD_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4N8m-K1ldt11"
   },
   "outputs": [],
   "source": [
    "!gsutil cp gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.22.2.jar $DATAPROC_IMAGE_BUILD_PATH\n",
    "!wget -P $DATAPROC_IMAGE_BUILD_PATH https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-assembly-4.0.2.jar\n",
    "!wget -P $DATAPROC_IMAGE_BUILD_PATH https://repo.anaconda.com/miniconda/Miniconda3-py38_4.9.2-Linux-x86_64.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GF9_5IGYqLAX"
   },
   "source": [
    "定义Dataproc无服务器自定义运行时映像\n",
    "\n",
    "您可以定义Dockerfile来创建自定义映像。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nWiGHdEibCcv"
   },
   "outputs": [],
   "source": [
    "dataproc_serverless_custom_runtime_image = \"\"\"\n",
    "# Debian 11 is recommended.\n",
    "FROM debian:11-slim\n",
    "\n",
    "# Suppress interactive prompts\n",
    "ENV DEBIAN_FRONTEND=noninteractive\n",
    "\n",
    "# (Required) Install utilities required by Spark scripts.\n",
    "RUN apt update && apt install -y procps tini\n",
    "\n",
    "# (Optional) Add extra jars.\n",
    "ENV SPARK_EXTRA_JARS_DIR=/opt/spark/jars/\n",
    "ENV SPARK_EXTRA_CLASSPATH='/opt/spark/jars/*'\n",
    "RUN mkdir -p \"${SPARK_EXTRA_JARS_DIR}\"\n",
    "COPY spark-bigquery-with-dependencies_2.12-0.22.2.jar \"${SPARK_EXTRA_JARS_DIR}\"\n",
    "COPY spark-nlp-assembly-4.0.2.jar \"${SPARK_EXTRA_JARS_DIR}\"\n",
    "\n",
    "# (Optional) Install and configure Miniconda3.\n",
    "ENV CONDA_HOME=/opt/miniconda3\n",
    "ENV PYSPARK_PYTHON=${CONDA_HOME}/bin/python\n",
    "ENV PATH=${CONDA_HOME}/bin:${PATH}\n",
    "COPY Miniconda3-py38_4.9.2-Linux-x86_64.sh .\n",
    "RUN bash Miniconda3-py38_4.9.2-Linux-x86_64.sh -b -p /opt/miniconda3 \\\n",
    "  && ${CONDA_HOME}/bin/conda config --system --set always_yes True \\\n",
    "  && ${CONDA_HOME}/bin/conda config --system --set auto_update_conda False \\\n",
    "  && ${CONDA_HOME}/bin/conda config --system --prepend channels conda-forge \\\n",
    "  && ${CONDA_HOME}/bin/conda config --system --set channel_priority strict\n",
    "\n",
    "# (Optional) Install Conda packages.\n",
    "#\n",
    "# The following packages are installed in the default image, it is strongly\n",
    "# recommended to include all of them.\n",
    "#\n",
    "# Use mamba to install packages quickly.\n",
    "RUN ${CONDA_HOME}/bin/conda install mamba -n base -c conda-forge \\\n",
    "    && ${CONDA_HOME}/bin/mamba install \\\n",
    "      conda \\\n",
    "      cython \\\n",
    "      gcsfs \\\n",
    "      google-cloud-bigquery-storage \\\n",
    "      google-cloud-bigquery[pandas] \\\n",
    "      google-cloud-dataproc \\\n",
    "      numpy \\\n",
    "      pandas \\\n",
    "      python \\\n",
    "      pyspark \\\n",
    "      findspark \n",
    "\n",
    "# Use conda to install spark-nlp\n",
    "RUN ${CONDA_HOME}/bin/conda install -n base -c johnsnowlabs 'spark-nlp=4.0.2'\n",
    "\n",
    "# Add lemma dictionary\n",
    "# ENV CONFIG_DIR='/home/app/build'\n",
    "# RUN mkdir -p \"${CONFIG_DIR}\"\n",
    "# COPY lemmas.txt \"${CONFIG_DIR}\"\n",
    "\n",
    "# (Required) Create the 'spark' group/user.\n",
    "# The GID and UID must be 1099. Home directory is required.\n",
    "RUN groupadd -g 1099 spark\n",
    "RUN useradd -u 1099 -g 1099 -d /home/spark -m spark\n",
    "USER spark\n",
    "\"\"\"\n",
    "\n",
    "with open(PREPROCESS_DOCKERFILE_PATH, \"w\") as f:\n",
    "    f.write(dataproc_serverless_custom_runtime_image)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXzI2xInqb3V"
   },
   "source": [
    "使用Google Cloud Build构建Dataproc无服务器自定义运行时\n",
    "\n",
    "您可以使用云构建来创建并注册容器镜像到Artifact注册表。\n",
    "\n",
    "请注意，`<PROJECT_ID>@cloudbuild.gserviceaccount.com`需要具有对Google Cloud Storage对象的storage.objects.get访问权限。\n",
    "\n",
    "**注意**: 这一步可能需要大约5分钟。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U5QT7D1LkG7L"
   },
   "outputs": [],
   "source": [
    "CLOUD_BUILD_SERVICE_ACCOUNT = f\"{PROJECT_NUMBER}@cloudbuild.gserviceaccount.com\"\n",
    "\n",
    "! gsutil iam ch serviceAccount:{CLOUD_BUILD_SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
    "! gsutil iam ch serviceAccount:{CLOUD_BUILD_SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RnpRqGrFkPaH"
   },
   "outputs": [],
   "source": [
    "!gcloud builds submit --tag $DATAPROC_RUNTIME_CONTAINER_IMAGE $DATAPROC_IMAGE_BUILD_PATH --machine-type=N1_HIGHCPU_32 --timeout=900s --verbosity=info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_lsCPf1KwImo"
   },
   "source": [
    "####准备`preprocess`模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AFWE_I8VfQFO"
   },
   "source": [
    "创建预处理模块\n",
    "\n",
    "此模块将对数据进行预处理，包括以下步骤：\n",
    "\n",
    "1. 句子分割\n",
    "2. 词语标记\n",
    "3. 正规化\n",
    "4. 去除停用词\n",
    "5. 词干提取\n",
    "6. 词形还原"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X_-MYpcbr_1N"
   },
   "outputs": [],
   "source": [
    "with open(INIT_PATH, \"w\") as init_file:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j9Ouh6VMr8cB"
   },
   "outputs": [],
   "source": [
    "process_module = \"\"\"\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "'''\n",
    "This is a simple module to preprocess the data for the model registry demo.\n",
    "Steps:\n",
    "1. Sentence segmentation\n",
    "2. Word tokenization\n",
    "3. Normalization\n",
    "4. Stopword removal\n",
    "5. Stemming\n",
    "6. Lemmatization\n",
    "'''\n",
    "\n",
    "# Libraries\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "from pyspark.sql.types import StructType, StringType\n",
    "from pyspark.sql.functions import col, concat_ws, rand\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.ml.feature import CountVectorizer, SQLTransformer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Variables ------------------------------------------------------------------------------------------------------------\n",
    "DATA_SCHEMA = (StructType()\n",
    "               .add(\"category\", StringType(), True)\n",
    "               .add(\"text\", StringType(), True))\n",
    "SEED=8\n",
    "\n",
    "# Helper functions -----------------------------------------------------------------------------------------------------\n",
    "def get_logger():\n",
    "    '''\n",
    "    This function returns a logger object.\n",
    "    Returns:\n",
    "        logger: The logger object.\n",
    "    '''\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    handler = logging.StreamHandler()\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "    return logger\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    '''\n",
    "    This function returns the arguments from the command line.\n",
    "    Returns:\n",
    "        args: The arguments from the command line.\n",
    "    '''\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--input_path', type=str, help='The input path uri without bucket prefix')\n",
    "    parser.add_argument('--lemmas_path', type=str, help='The lemma dictionary path without bucket prefix')\n",
    "    parser.add_argument('--gcs_output_path', type=str, help='The gcs path for preprocessed data without bucket prefix')\n",
    "    parser.add_argument('--bq_output_table_uri', type=str, help='The Bigquery output table URI')\n",
    "    parser.add_argument('--bucket', type=str, help='The staging bucket')\n",
    "    parser.add_argument('--project', type=str, help='The project id')\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def build_preliminary_steps():\n",
    "    '''\n",
    "    This function builds the preliminary steps for the preprocessing.\n",
    "    Returns:\n",
    "        preliminary_steps: The preliminary steps for the preprocessing.\n",
    "    '''\n",
    "    \n",
    "    document_assembler = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\").setCleanupMode('shrink_full')\n",
    "    sentence_detector = SentenceDetector().setInputCols(\"document\").setOutputCol(\"sentence\")\n",
    "    tokenizer = Tokenizer().setInputCols(\"sentence\").setOutputCol(\"token\")\n",
    "    preliminary_steps = [document_assembler, sentence_detector, tokenizer]\n",
    "    return preliminary_steps\n",
    "\n",
    "\n",
    "def build_common_preprocess_steps(lemma_uri):\n",
    "    '''\n",
    "    This function builds the common preprocessing steps.\n",
    "    Args:\n",
    "        lemma_uri: The uri of lemma dictionary\n",
    "    Returns:\n",
    "        common_preprocess_steps: The common preprocessing steps.\n",
    "    '''\n",
    "\n",
    "    normalizer = Normalizer().setInputCols(\"token\").setOutputCol(\"normalized_token\").setLowercase(True)\n",
    "    stopwords_cleaner = StopWordsCleaner().setInputCols(\"normalized_token\").setOutputCol(\n",
    "        \"cleaned_tokens\").setCaseSensitive(False)\n",
    "    stemmer = Stemmer().setInputCols(\"cleaned_tokens\").setOutputCol(\"stem\")\n",
    "    lemmatizer = Lemmatizer().setInputCols(\"stem\").setOutputCol(\"lemma\").setDictionary(lemma_uri, \"->\", \"\\t\")\n",
    "    finisher = Finisher().setInputCols(\"lemma\").setOutputCols([\"lemma_features\"]).setIncludeMetadata(\n",
    "        False).setOutputAsArray(True)\n",
    "    common_preprocess_steps = [normalizer, stopwords_cleaner, stemmer, lemmatizer, finisher]\n",
    "    return common_preprocess_steps\n",
    "\n",
    "\n",
    "def build_feature_extraction_steps():\n",
    "    '''\n",
    "    This function builds the feature extraction steps.\n",
    "    Returns:\n",
    "        feature_extraction_steps: The feature extraction steps.\n",
    "    '''\n",
    "\n",
    "    count_vectorizer = CountVectorizer().setInputCol(\"lemma_features\").setOutputCol(\"features\").setVocabSize(30)\n",
    "    feature_extraction_steps = [count_vectorizer]\n",
    "    return feature_extraction_steps\n",
    "\n",
    "def build_postprocessing_steps():\n",
    "    '''\n",
    "    This function builds the postprocessing steps.\n",
    "    Returns:\n",
    "        target_conversion_step: The target conversion step.\n",
    "    '''\n",
    "\n",
    "    sql_transformer = SQLTransformer(statement=\"SELECT CASE WHEN (category != 'business') THEN 'other' ELSE category END AS category, text, lemma_features, features  FROM __THIS__\")\n",
    "    build_postprocessing_steps = [sql_transformer]\n",
    "    return build_postprocessing_steps\n",
    "\n",
    "def read_data(spark_session, data_schema, input_dir):\n",
    "    '''\n",
    "    This function reads the data from the input directory.\n",
    "    Args:\n",
    "        spark_session: The SparkSession object.\n",
    "        data_schema: The data schema.\n",
    "        input_dir: The input directory.\n",
    "    Returns:\n",
    "        raw_df: The raw dataframe.\n",
    "    '''\n",
    "\n",
    "    raw_df = (spark_session.read.option(\"header\", True)\n",
    "              .option(\"delimiter\", ',')\n",
    "              .schema(data_schema)\n",
    "              .csv(input_dir))\n",
    "    return raw_df\n",
    "\n",
    "\n",
    "def prepare_train_df(df):\n",
    "    '''\n",
    "    This function prepares the training dataframe.\n",
    "    Args:\n",
    "        df: The dataframe.\n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "    train_df = (df.withColumn(\"bow_col\", vector_to_array(\"features\"))\n",
    "                .withColumn(\"lemmas\", concat_ws(\" \", col(\"lemma_features\")))\n",
    "                .select([\"text\"] + [\"lemmas\"] + [col(\"bow_col\")[i] for i in range(30)] + [\"category\"]))\n",
    "\n",
    "    return train_df\n",
    "\n",
    "\n",
    "def save_data(data, bucket, gcs_path, bigquery_uri):\n",
    "    '''\n",
    "    This function saves the data to Bigquery.\n",
    "    Args:\n",
    "        data: The data to save.\n",
    "        bucket: The bucket.\n",
    "        gcs_path: The path to store processed data.\n",
    "        bigquery_uri: The URI of the Bigquery table.\n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "    # df_sample = data.sample(withReplacement=False, fraction=0.7, seed=SEED)\n",
    "    df_sample = data.orderBy(rand(SEED)).limit(1000)\n",
    "    df_sample.write.format('bigquery') \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"persistentGcsBucket\", bucket) \\\n",
    "        .option(\"persistentGcsPath\", gcs_path) \\\n",
    "        .save(bigquery_uri)\n",
    "\n",
    "\n",
    "# Main function --------------------------------------------------------------------------------------------------------\n",
    "def preprocess(args):\n",
    "    '''\n",
    "    preprocess function.\n",
    "    Args:\n",
    "        args: The arguments from the command line.\n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "    # Get logger\n",
    "    logger = get_logger()\n",
    "\n",
    "    # Initialize variables\n",
    "    input_path = args.input_path\n",
    "    lemma_path = args.lemmas_path\n",
    "    gcs_output_path = args.gcs_output_path\n",
    "    bq_output_table_uri = args.bq_output_table_uri\n",
    "    bucket = args.bucket\n",
    "    project = args.project\n",
    "    lemma_uri = f'gs://{bucket}/{lemma_path}'\n",
    "    input_uri = f'gs://{bucket}/{input_path}'\n",
    "\n",
    "    # Initialize SparkSession\n",
    "    logger.info('Starting preprocessing')\n",
    "    spark = sparknlp.start()\n",
    "    print(f\"Spark NLP version: {sparknlp.version()}\")\n",
    "    print(f\"Spark version: {spark.version}\")\n",
    "\n",
    "    # Build pipeline steps\n",
    "    logger.info('Building pipeline steps')\n",
    "    preliminary_steps = build_preliminary_steps()\n",
    "    common_preprocess_steps = build_common_preprocess_steps(lemma_uri)\n",
    "    feature_extraction_steps = build_feature_extraction_steps()\n",
    "    postprocessing_steps = build_postprocessing_steps()\n",
    "    pipeline = Pipeline(stages=preliminary_steps + common_preprocess_steps + feature_extraction_steps + postprocessing_steps)\n",
    "\n",
    "    # Read data\n",
    "    logger.info('Reading data')\n",
    "    raw_df = read_data(spark, DATA_SCHEMA, input_uri)\n",
    "\n",
    "    # Preprocess data\n",
    "    logger.info('Preprocessing data')\n",
    "    processed_pipeline = pipeline.fit(raw_df)\n",
    "    preprocessed_df = processed_pipeline.transform(raw_df)\n",
    "    preprocessed_df.show(10, truncate=False)\n",
    "\n",
    "    # Save data to Bigquery\n",
    "    logger.info('Saving data to Bigquery')\n",
    "    train_df = prepare_train_df(preprocessed_df)\n",
    "    save_data(train_df, bucket, gcs_output_path, bq_output_table_uri)\n",
    "    logging.info('done.')\n",
    "    spark.stop()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Get args\n",
    "    args = get_args()\n",
    "    preprocess(args)\n",
    "\"\"\"\n",
    "\n",
    "with open(PREPROCESS_MODULE_PATH, \"w\") as process_file:\n",
    "    process_file.write(process_module)\n",
    "process_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8Ic6_s3wPUC"
   },
   "source": [
    "##### 将模块上传到存储桶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SpLfaU6Xw4oF"
   },
   "outputs": [],
   "source": [
    "!gsutil cp $SRC_PATH/__init__.py $MODULE_URI/__init__.py\n",
    "!gsutil cp $SRC_PATH/preprocess.py $MODULE_URI/preprocess.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W1K3F7QidiCd"
   },
   "source": [
    "上传配置文件\n",
    "\n",
    "根据Spark NLP文档，您将使用引理词典，并将其上传到Google Cloud存储桶。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4vxZuoy0dlfS"
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/mahavivo/vocabulary/master/lemmas/AntBNC_lemmas_ver_001.txt -O $LEMMA_DICTIONARY_PATH\n",
    "!gsutil cp $LEMMA_DICTIONARY_PATH $LEMMA_DICTIONARY_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQVJBaXTz4sV"
   },
   "source": [
    "利用 Dataproc 无服务器运行一个预处理 Spark 作业\n",
    "\n",
    "现在你已经准备好执行，可以提交预处理 Dataproc 无服务器作业。这个 CLI 命令的解释超出范围，但你可以查看所有选项[官方文档](https://cloud.google.com/dataproc-serverless/docs/quickstarts/spark-batch)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8fbik13otuQ7"
   },
   "outputs": [],
   "source": [
    "! gcloud beta dataproc batches submit pyspark $PROCESS_PYTHON_FILE_URI \\\n",
    "  --batch=$PREPROCESS_BATCH_ID \\\n",
    "  --container-image=$DATAPROC_RUNTIME_CONTAINER_IMAGE \\\n",
    "  --region=$REGION \\\n",
    "  --version='1.0.21' \\\n",
    "  --subnet='default' \\\n",
    "  --properties spark.executor.instances=2,spark.driver.cores=4,spark.executor.cores=4,spark.app.name=spark_preprocessing_job \\\n",
    "  -- --input_path=$PREPARED_FILE_PATH --lemmas_path=$LEMMA_DICTIONARY_PATH --gcs_output_path=$PROCESS_DATA_PATH --bq_output_table_uri=$BQ_OUTPUT_TABLE_URI --bucket=$BUCKET_NAME --project=$PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZhMnSORpPkO0"
   },
   "source": [
    "## 文本分类模型训练\n",
    "\n",
    "根据《实用自然语言处理：构建现实世界NLP系统的全面指南》(Practical Natural Language Processing: A Comprehensive Guide to Building Real-World NLP Systems)，有不同的方法可以训练文本分类器。例如，\n",
    "\n",
    "- 传统方法，如逻辑回归或朴素贝叶斯分类器\n",
    "- 神经嵌入方法\n",
    "- 深度学习方法\n",
    "- 大型、预训练的语言模型\n",
    "\n",
    "在接下来的章节中，你将使用传统方法，并展示Vertex AI模型注册表将如何管理所有这些方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bGnLq4PEjOuT"
   },
   "source": [
    "使用BQML进行逻辑回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJv2kKf4SGca"
   },
   "source": [
    "训练并注册模型\n",
    "\n",
    "要将BigQuery ML模型注册到Vertex AI模型注册表，您必须使用`model_registry=\"vertex_ai\"`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FaDx8SdGSU3V"
   },
   "outputs": [],
   "source": [
    "train_lr_query = f\"\"\"\n",
    "CREATE OR REPLACE MODEL\n",
    "  `{PROJECT_ID}.{BQ_DATASET}.text_logit_classifier`\n",
    "OPTIONS\n",
    "  ( MODEL_TYPE='LOGISTIC_REG',\n",
    "    AUTO_CLASS_WEIGHTS=TRUE,\n",
    "    DATA_SPLIT_METHOD='RANDOM',\n",
    "    DATA_SPLIT_EVAL_FRACTION = .10,\n",
    "    INPUT_LABEL_COLS=['category'],\n",
    "    ENABLE_GLOBAL_EXPLAIN=TRUE,\n",
    "    MODEL_REGISTRY='vertex_ai',\n",
    "    VERTEX_AI_MODEL_ID='{VERTEX_AI_MODEL_ID}',\n",
    "    VERTEX_AI_MODEL_VERSION_ALIASES=['experimental', 'baseline', 'BQML', 'logistic_regression']\n",
    "  ) AS\n",
    "    SELECT * EXCEPT(text, lemmas)\n",
    "    FROM `{PROJECT_ID}.{BQ_OUTPUT_TABLE_URI}`\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6DJtOe1uUtgm"
   },
   "outputs": [],
   "source": [
    "model_table, result = run_query(query=train_lr_query)\n",
    "print(f\"The {model_table.dataset_id}.{model_table.table_id} successfully created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y6hcXNGITTym"
   },
   "source": [
    "朴素贝叶斯分类器与scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EBHgAA0xqyi-"
   },
   "source": [
    "创建朴素训练模块\n",
    "\n",
    "使用这个模块，您将为文本分类训练一个简单的scikit-learn朴素贝叶斯估计器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NuFd9fJAq4FV"
   },
   "outputs": [],
   "source": [
    "train_naive_module = \"\"\"\n",
    "#!/usr/bin/env python3\n",
    "'''\n",
    "This is a simple module to train a naive bayes model.\n",
    "'''\n",
    "\n",
    "import logging\n",
    "import argparse\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, log_loss, roc_auc_score\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "# Variables\n",
    "RANDOM_STATE = 8\n",
    "TEST_SIZE = 0.2\n",
    "EVAL_SIZE = 0.25\n",
    "\n",
    "\n",
    "\n",
    "# Helpers --------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def get_logger():\n",
    "    '''\n",
    "    This function returns a logger object.\n",
    "    Returns:\n",
    "        logger: The logger object.\n",
    "    '''\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    handler = logging.StreamHandler()\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "    return logger\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    '''\n",
    "    This function parses and return arguments passed in command line.\n",
    "    Returns:\n",
    "        args: Arguments list.\n",
    "    '''\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--data_path\",\n",
    "                        type=str, help=\"The path of the training data.\")\n",
    "    parser.add_argument('--model_dir',\n",
    "                        type=str, help='The path of the model directory.')\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def read_data(data_path: str):\n",
    "    '''\n",
    "    This function reads the data from the provided data path.\n",
    "    Args:\n",
    "        data_path: The path of the data.\n",
    "    Returns:\n",
    "        x_train: The training data.\n",
    "        y_train: The training labels.\n",
    "        x_test: The test data.\n",
    "        y_test: The test labels.\n",
    "    '''\n",
    "    # Read data\n",
    "    gs_prefix = 'gs://'\n",
    "    gcsfuse_prefix = '/gcs/'\n",
    "    if data_path.startswith(gs_prefix):\n",
    "        data_path = data_path.replace(gs_prefix, gcsfuse_prefix)\n",
    "    parquet_files = glob.glob(data_path)\n",
    "    dataframes = []\n",
    "    for parquet_file_path in parquet_files:\n",
    "        parquet_file_path = parquet_file_path.replace(gcsfuse_prefix, gs_prefix)\n",
    "        dataframes.append(pd.read_parquet(parquet_file_path, engine='fastparquet'))\n",
    "    df = pd.concat(dataframes, axis=0)\n",
    "    x = df.text\n",
    "    # y = np.where(df.category == 'sport', 1, 0)\n",
    "    y = df.category\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=RANDOM_STATE, test_size=TEST_SIZE)\n",
    "    x_train, x_eval, y_train, y_eval = train_test_split(x_train, y_train, random_state=RANDOM_STATE, test_size=EVAL_SIZE)\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "def get_weights(y_train):\n",
    "    '''\n",
    "    This function returns the class weights for the model.\n",
    "    Returns:\n",
    "        weights: The class weights.\n",
    "    '''\n",
    "    weights = compute_sample_weight('balanced', y_train)\n",
    "    return weights\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    '''\n",
    "    This function builds the model.\n",
    "    Returns:\n",
    "        model: The model.\n",
    "    '''\n",
    "    model = Pipeline([\n",
    "        ('count_vectorizer', CountVectorizer()),\n",
    "        ('classifier', MultinomialNB())\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(x_train, y_train, model):\n",
    "    '''\n",
    "    This function trains the model.\n",
    "    Args:\n",
    "        x_train: The training data.\n",
    "        y_train: The training labels.\n",
    "        model: The model to train.\n",
    "    Returns:\n",
    "        model: The trained model.\n",
    "    '''\n",
    "    model = model.fit(x_train, y_train, classifier__sample_weight=get_weights(y_train))\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(model, x_test, y_test):\n",
    "    '''\n",
    "    This function evaluates the model on the test data.\n",
    "    Parameters:\n",
    "        model: The model to evaluate.\n",
    "        x_test: The test data.\n",
    "        y_test: The test labels.\n",
    "    '''\n",
    "\n",
    "    y_pred = model.predict(x_test)\n",
    "    y_pred_proba = model.predict_proba(x_test)\n",
    "    metrics = {\n",
    "        \"precision\": round(precision_score(y_test, y_pred, sample_weight=get_weights(y_test), average=\"weighted\"), 5),\n",
    "        \"recall\": round(recall_score(y_test, y_pred, sample_weight=get_weights(y_test), average=\"weighted\"), 5),\n",
    "        \"accuracy\": round(accuracy_score(y_test, y_pred, sample_weight=get_weights(y_test)), 5),\n",
    "        \"f1_score\": round(f1_score(y_test, y_pred, sample_weight=get_weights(y_test), average=\"weighted\"), 5),\n",
    "        \"log_loss\": round(log_loss(y_test, y_pred_proba, sample_weight=get_weights(y_test)), 5),\n",
    "        \"roc_auc\": round(roc_auc_score(y_test, y_pred_proba[:,1], sample_weight=get_weights(y_test), average=\"weighted\"), 5)\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def save_model(model, model_dir):\n",
    "    '''\n",
    "    This function saves the model to the provided model directory.\n",
    "    Parameters:\n",
    "        model: The model to save.\n",
    "        model_dir: The directory to save the model to.\n",
    "    '''\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    gs_prefix = 'gs://'\n",
    "    gcsfuse_prefix = '/gcs/'\n",
    "    if model_dir.startswith(gs_prefix):\n",
    "        model_dir = model_dir.replace(gs_prefix, gcsfuse_prefix)\n",
    "    model_dir = os.path.join(model_dir, 'model')\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    model_path = os.path.join(model_dir, 'model.pkl')\n",
    "    with open(model_path, 'wb') as model_file:\n",
    "        pickle.dump(model, model_file)\n",
    "\n",
    "def save_metrics(metrics, model_dir):\n",
    "    '''\n",
    "    This function saves the metrics to the provided model directory.\n",
    "    Parameters:\n",
    "        metrics: The metrics to save.\n",
    "        model_dir: The directory to save the metrics to.\n",
    "    '''\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    gs_prefix = 'gs://'\n",
    "    gcsfuse_prefix = '/gcs/'\n",
    "    if model_dir.startswith(gs_prefix):\n",
    "        model_dir = model_dir.replace(gs_prefix, gcsfuse_prefix)\n",
    "    metrics_path = os.path.join(model_dir, 'model', 'metrics.json')\n",
    "    with open(metrics_path, 'w') as f:\n",
    "        f.write(str(metrics))\n",
    "\n",
    "\n",
    "def train_naive(args):\n",
    "    '''\n",
    "    This function trains the model and saves it to the provided model directory.\n",
    "    Parameters:\n",
    "        args: The arguments from the command line.\n",
    "    '''\n",
    "    # Get logger\n",
    "    logger = get_logger()\n",
    "    logger.info('Starting model training...')\n",
    "\n",
    "    # Initialize variables\n",
    "    data_path = args.data_path\n",
    "    model_dir = args.model_dir\n",
    "\n",
    "    # Build model\n",
    "    model = build_model()\n",
    "\n",
    "    # Read data\n",
    "    logger.info('Reading data')\n",
    "    x_train, y_train, x_test, y_test = read_data(data_path)\n",
    "\n",
    "    # Train model\n",
    "    logger.info('Training model')\n",
    "    model = train_model(x_train, y_train, model)\n",
    "\n",
    "    # Evaluate model\n",
    "    logger.info('Evaluating model')\n",
    "    metrics = evaluate_model(model, x_test, y_test)\n",
    "    for key, value in metrics.items():\n",
    "        print(f'{key}: {value}')\n",
    "\n",
    "    # Save model\n",
    "    logger.info('Saving model')\n",
    "    save_model(model, model_dir)\n",
    "\n",
    "    # Save metrics\n",
    "    logger.info('Saving metrics')\n",
    "    save_metrics(metrics, model_dir)\n",
    "\n",
    "    logger.info('Training complete.')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Get args\n",
    "    args = get_args()\n",
    "    train_naive(args)\n",
    "\"\"\"\n",
    "\n",
    "with open(TRAIN_NAIVE_MODULE_PATH, \"w\") as train_naive_file:\n",
    "    train_naive_file.write(train_naive_module)\n",
    "train_naive_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21VTM2N_1Qtk"
   },
   "source": [
    "使用Vertex AI Training对模型进行训练和注册\n",
    "\n",
    "要注册一个新的自定义模型版本，使用Vertex AI Training对现有模型进行训练，您需要提供以下额外的参数：\n",
    "\n",
    "* `parent_model`：要注册新版本的现有模型的父资源名称。\n",
    "* `model_version_aliases`：要创建的模型版本的别名。\n",
    "* `model_version_description`：模型版本的描述。\n",
    "* `is_default_version`：该模型版本是否为默认版本。\n",
    "\n",
    "一旦您运行训练任务，它将需要**~5分钟**才能完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JhG57TLm1lUV"
   },
   "outputs": [],
   "source": [
    "naive_bayes_train_job = vertex_ai.CustomTrainingJob(\n",
    "    display_name=NAIVE_TRAIN_JOB_NAME,\n",
    "    script_path=TRAIN_NAIVE_MODULE_PATH,\n",
    "    container_uri=NAIVE_TRAIN_CONTAINER_URI,\n",
    "    requirements=NAIVE_TRAIN_REQUIREMENTS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tuEyTkim-GNq"
   },
   "outputs": [],
   "source": [
    "naive_model = naive_bayes_train_job.run(\n",
    "    args=[\"--data_path\", PROCESS_FILE_URI, \"--model_dir\", NAIVE_MODEL_BASE_URI],\n",
    "    replica_count=1,\n",
    "    machine_type=\"n1-standard-4\",\n",
    "    base_output_dir=NAIVE_MODEL_BASE_URI,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KgsDPyfG-zjP"
   },
   "source": [
    "用 Vertex AI Model Registry 进行模型治理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oV9FimH-vrps"
   },
   "source": [
    "#### 初始化Vertex AI模型注册表\n",
    "\n",
    "要访问Vertex AI模型资源的不同模型版本，您可以初始化模型注册表模型的实例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O32IGV9tgXw8"
   },
   "outputs": [],
   "source": [
    "registry = vertex_ai.models.ModelRegistry(VERTEX_AI_MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11xaykBni5mJ"
   },
   "source": [
    "比较模型版本\n",
    "\n",
    "然后，您使用`ML.EVALUATE`生成BQML模型评估指标，并将其与您使用自定义模型创建的相同指标进行比较。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9o0phVemTU0i"
   },
   "outputs": [],
   "source": [
    "evaluation_query = f\"\"\"\n",
    "SELECT *\n",
    "FROM\n",
    "  ML.EVALUATE(MODEL `{BQ_DATASET}.text_logit_classifier`)\n",
    "ORDER BY  roc_auc desc\n",
    "LIMIT 1\n",
    "\"\"\"\n",
    "_, result = run_query(query=evaluation_query)\n",
    "evaluation_df = result.to_dataframe().rename(index={0: \"bqml_text_logit_classifier\"})\n",
    "evaluation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o1H0e8c3VpM8"
   },
   "outputs": [],
   "source": [
    "naive_metrics = read_metrics_file(NAIVE_METRICS_FILE_URI)\n",
    "metrics_dict = [json.loads(naive_metrics)]\n",
    "naive_metrics_df = pd.DataFrame.from_dict(metrics_dict).rename(index={0: \"naive_bayes\"})\n",
    "evaluation_df = evaluation_df.append(naive_metrics_df, ignore_index=False)\n",
    "evaluation_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_nr1xRQbcm4i"
   },
   "source": [
    "### 注册`champion`模型版本\n",
    "\n",
    "根据模型评估，scikit-learn朴素贝叶斯分类器胜过了BQML逻辑回归，因此成为生产候选模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWisrY3svDYA"
   },
   "source": [
    "构建并推送定制服务容器到Artifact Registry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hdzX9WW4lNWK"
   },
   "source": [
    "构建自定义服务图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JP8MQJkenIz2"
   },
   "outputs": [],
   "source": [
    "! rm -rf $SERVING_BUILD_PATH\n",
    "! mkdir $SERVING_BUILD_PATH\n",
    "! mkdir $SERVING_APP_BUILD_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fsqCwvWApgUC"
   },
   "source": [
    "构建提供服务的应用程序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wao9muHfprDL"
   },
   "outputs": [],
   "source": [
    "serve_naive_module = \"\"\"\n",
    "'''\n",
    "This is a simple web application to serve the naive bayes model.\n",
    "'''\n",
    "\n",
    "# Libraries ------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "import logging\n",
    "import os\n",
    "from flask import Flask, Response, request, jsonify\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Helpers --------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def get_probabilities(model_classes, probabilities):\n",
    "    proba_classes = []\n",
    "    for probabilities_list in probabilities:\n",
    "      proba_classes.append({\"classes\": model_classes, \"scores\": probabilities_list})\n",
    "    return proba_classes\n",
    "\n",
    "\n",
    "# App ------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Initialize the app\n",
    "app = Flask(__name__)\n",
    "app.logger.setLevel(logging.INFO)\n",
    "\n",
    "# Load the model\n",
    "app.logger.info(\"Loading model...\")\n",
    "model = pickle.load(open('./model/model.pkl', 'rb'))\n",
    "app.logger.info(\"Model loaded.\")\n",
    "\n",
    "# classes = model.classes_\n",
    "classes = model.classes_.tolist()\n",
    "\n",
    "\n",
    "@app.route(os.environ['AIP_HEALTH_ROUTE'], methods=['GET'])\n",
    "def health():\n",
    "    '''\n",
    "    A health check endpoint.\n",
    "    '''\n",
    "    app.logger.info(\"Health check\")\n",
    "    return Response(response='OK', status=200)\n",
    "\n",
    "\n",
    "@app.route(os.environ['AIP_PREDICT_ROUTE'], methods=['POST'])\n",
    "def predict():\n",
    "    '''\n",
    "    A predict endpoint.\n",
    "    '''\n",
    "    app.logger.info(\"Predict\")\n",
    "\n",
    "    # Get instances\n",
    "    instances_dict = request.get_json()[\"instances\"]\n",
    "\n",
    "    # Generate predictions\n",
    "    instances_df = pd.DataFrame.from_records(instances_dict)\n",
    "    probabilities = model.predict_proba(instances_df.iloc[:, 0])\n",
    "\n",
    "    # Format predictions\n",
    "    fmt_probabilities = get_probabilities(classes, probabilities.tolist())\n",
    "\n",
    "    return jsonify({\"predictions\": fmt_probabilities})\n",
    "\n",
    "\n",
    "if __name__ == \"main\":\n",
    "    app.run(debug=True, host=\"0.0.0.0\", port=9999)\n",
    "\"\"\"\n",
    "\n",
    "with open(SERVE_NAIVE_MODULE_PATH, \"w\") as serve_naive_file:\n",
    "    serve_naive_file.write(serve_naive_module)\n",
    "serve_naive_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mDM7-7KZn9MS"
   },
   "source": [
    "####复制模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KQezoOQMoCEH"
   },
   "outputs": [],
   "source": [
    "!gsutil cp -r $NAIVE_MODEL_URI $SERVING_APP_BUILD_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0HIQD70qqDn"
   },
   "source": [
    "创建 `requirements` 文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j9tJdqu1qtV7"
   },
   "outputs": [],
   "source": [
    "serve_requirements = \"\"\"\n",
    "flask==2.2.2\n",
    "gunicorn==20.1.0\n",
    "numpy==1.22.4\n",
    "pandas==1.4.3\n",
    "scikit-learn==0.23.1\n",
    "\"\"\"\n",
    "\n",
    "with open(SERVE_REQUIREMENTS_PATH, \"w\") as serve_requirements_file:\n",
    "    serve_requirements_file.write(serve_requirements)\n",
    "serve_requirements_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XihT5MpErihk"
   },
   "source": [
    "创建`Dockerfile`文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sj98z12qrubZ"
   },
   "outputs": [],
   "source": [
    "serve_dockerfile = \"\"\"\n",
    "FROM python:3.8-slim\n",
    "\n",
    "# Update pip\n",
    "RUN pip3 install --upgrade pip\n",
    "\n",
    "# Install requirements\n",
    "COPY requirements.txt requirements.txt\n",
    "RUN pip install -r requirements.txt\n",
    "\n",
    "# Create app folder and copy app files\n",
    "RUN mkdir /app\n",
    "COPY app /app\n",
    "WORKDIR /app\n",
    "\n",
    "# Run app\n",
    "EXPOSE 9999\n",
    "CMD [\"gunicorn\", \"main:app\", \"--timeout=0\", \"--preload\", \\\n",
    "     \"--workers=1\", \"--threads=4\", \"--bind=0.0.0.0:9999\"]\n",
    "\"\"\"\n",
    "\n",
    "with open(SERVE_DOCKERFILE_PATH, \"w\") as serve_dockerfile_file:\n",
    "    serve_dockerfile_file.write(serve_dockerfile)\n",
    "serve_dockerfile_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQ5jp2FSvSK7"
   },
   "source": [
    "构建并推送定制图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qAAkoXcHvVVY"
   },
   "outputs": [],
   "source": [
    "!gcloud builds submit --tag $SERVING_NAIVE_RUNTIME_CONTAINER_IMAGE $SERVING_BUILD_PATH --machine-type=N1_HIGHCPU_32 --timeout=900s --verbosity=info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hmY1U5S9g7Li"
   },
   "source": [
    "注册模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QagCSDjXqFgV"
   },
   "outputs": [],
   "source": [
    "naive_model = vertex_ai.Model.upload(\n",
    "    parent_model=VERTEX_AI_MODEL_ID,\n",
    "    is_default_version=False,\n",
    "    version_aliases=[\"experimental\", \"challenger\", \"custom-training\", \"naive-bayes\"],\n",
    "    version_description=\"A Naive Bayes text classifier\",\n",
    "    serving_container_image_uri=SERVING_NAIVE_RUNTIME_CONTAINER_IMAGE,\n",
    "    serving_container_health_route=\"/health\",\n",
    "    serving_container_predict_route=\"/predict\",\n",
    "    serving_container_ports=[9999],\n",
    "    labels={\"created_by\": \"inardini\", \"team\": \"advocacy\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mveSg_Xtdpcq"
   },
   "source": [
    "列出模型版本\n",
    "\n",
    "您可以使用 `list_versions` 方法列出所有模型版本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n-LFYv8gdumh"
   },
   "outputs": [],
   "source": [
    "versions = registry.list_versions()\n",
    "for version in versions:\n",
    "    version_id = version.version_id\n",
    "    version_created_time = dt.datetime.fromtimestamp(\n",
    "        version.version_create_time.timestamp()\n",
    "    ).strftime(\"%m/%d/%Y %H:%M:%S\")\n",
    "    version_aliases = version.version_aliases\n",
    "    print(\n",
    "        \"\\n\",\n",
    "        f\"Model version {version_id} was created at {version_created_time} with aliases {version_aliases}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VyVxqeebhVX0"
   },
   "source": [
    "获取有关“champion”模型版本的所有信息\n",
    "\n",
    "要获取有关您的“champion”模型的所有信息，您可以使用“get_version_info”方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YlTFBPJy0m5u"
   },
   "outputs": [],
   "source": [
    "CHAMPION_VERSION_ID = versions[-1].version_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KGaLLbt9hYNC"
   },
   "outputs": [],
   "source": [
    "champion_model_version_info = registry.get_version_info(CHAMPION_VERSION_ID)\n",
    "champion_model_version_info_df = pd.DataFrame(\n",
    "    champion_model_version_info,\n",
    "    columns=[\"model_version\"],\n",
    "    index=[\n",
    "        \"version_id\",\n",
    "        \"created_at\",\n",
    "        \"updated_at\",\n",
    "        \"model_display_name\",\n",
    "        \"model_resource_name\",\n",
    "        \"version_aliases\",\n",
    "        \"version_description\",\n",
    "    ],\n",
    ")\n",
    "champion_model_version_info_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "etK2i-NA8rtK"
   },
   "source": [
    "### 将冠军模型设置为`production`，状态为`default`\n",
    "\n",
    "为了更新别名并将模型状态从`experimental`更改为`production`，Vertex AI SDK提供了`add_version_aliases`和`remove_version_aliases`方法。\n",
    "\n",
    "请注意，我们根据在线实验阶段中讨论的内容设置这些别名，该内容在[“MLOps从业者指南：持续交付和自动化机器学习的框架”](https://services.google.com/fh/files/misc/practitioners_guide_to_mlops_whitepaper.pdf)中有所介绍。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VniG9U_Q81R6"
   },
   "outputs": [],
   "source": [
    "registry.remove_version_aliases(\n",
    "    [\"experimental\", \"challenger\"], version=CHAMPION_VERSION_ID\n",
    ")\n",
    "registry.add_version_aliases([\"default\", \"production\"], version=CHAMPION_VERSION_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4tiKo0Jk_axs"
   },
   "source": [
    "### 部署 `champion` 模型\n",
    "\n",
    "最后，您启动了准备投入生产的冠军模型，并将其部署到 Vertex AI 端点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8qojYvUbComG"
   },
   "outputs": [],
   "source": [
    "champion_model = registry.get_model(version=\"production\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YwErIIsOGMzk"
   },
   "source": [
    "创建端点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5LWhrPXfGMGu"
   },
   "outputs": [],
   "source": [
    "endpoint = vertex_ai.Endpoint.create(\n",
    "    display_name=ENDPOINT_NAME,\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqhjQSrMKGyl"
   },
   "source": [
    "部署冠军模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EvMabDk0GxgV"
   },
   "outputs": [],
   "source": [
    "endpoint.deploy(\n",
    "    model=champion_model,\n",
    "    deployed_model_display_name=DEPLOYED_MODEL_NAME,\n",
    "    machine_type=\"n1-standard-8\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a53QDmCWKnQD"
   },
   "source": [
    "### 生成预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0a3QkYvyHp5s"
   },
   "outputs": [],
   "source": [
    "text = \"\"\"The singer to headline the event halftime show: 'It's on\"\"\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S_Q47a6R9PPr"
   },
   "outputs": [],
   "source": [
    "instances = [{\"text\": text}]\n",
    "predictions = endpoint.predict(instances)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dksu_4joK2Hv"
   },
   "source": [
    "### 最后的想法 \n",
    "\n",
    "正如你所能想象的，你也可以上传外部模型。查看文档示例和[示例笔记本](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage3/get_started_with_model_registry.ipynb)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "清理工作\n",
    "\n",
    "要清理此项目中使用的所有Google Cloud资源，您可以删除用于本教程的[Google Cloud项目](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects)。\n",
    "\n",
    "否则，您可以删除您创建的各个资源。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t3GoM5NIrMML"
   },
   "outputs": [],
   "source": [
    "endpoint.undeploy_all()\n",
    "\n",
    "endpoint.delete()\n",
    "\n",
    "drop_model_query = f\"DROP MODEL `{PROJECT_ID}.{BQ_DATASET}.text_logit_classifier`\"\n",
    "run_query(drop_model_query)\n",
    "\n",
    "versions = registry.list_versions()\n",
    "for version in versions:\n",
    "    if \"default\" not in version.version_aliases:\n",
    "        registry.delete_version(version=version.version_id)\n",
    "    else:\n",
    "        model = registry.get_model(version=\"default\")\n",
    "        model.delete()\n",
    "\n",
    "naive_bayes_train_job.delete()\n",
    "\n",
    "! gcloud dataproc batches delete $PREPROCESS_BATCH_ID --region=$REGION --quiet\n",
    "\n",
    "! bq rm -r -f -d $PROJECT_ID:$BQ_DATASET\n",
    "\n",
    "! gcloud artifacts repositories delete $REPO_NAME --location=$REGION --quiet\n",
    "\n",
    "delete_bucket = False\n",
    "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
    "    ! gsutil -m rm -r $BUCKET_URI\n",
    "\n",
    "! rm -rf $DATA_PATH $SRC_PATH $BUILD_PATH $CONFIG_PATH"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "vertex_ai_model_registry_bqml_custom_model_versioning.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
