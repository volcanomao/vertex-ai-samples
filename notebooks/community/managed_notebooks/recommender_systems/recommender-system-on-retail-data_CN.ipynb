{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aa9846856f05"
   },
   "source": [
    "使用PySpark在Vertex AI上构建一个零售数据推荐系统"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1caa36b69240"
   },
   "source": [
    "### 目录\n",
    "\n",
    "* [概述](#section-1)\n",
    "* [数据集](#section-2)\n",
    "* [目标](#section-3)\n",
    "* [成本](#section-4)\n",
    "* [创建启用组件网关和JupyterLab扩展的Dataproc集群](#section-5)\n",
    "* [从笔记本连接到集群](#section-6)\n",
    "* [探索数据](#section-7)\n",
    "* [定义ALS模型](#section-8)\n",
    "* [评估模型](#section-9)\n",
    "* [将ALS模型保存到Cloud Storage](#section-10)\n",
    "* [将推荐写入BigQuery](#section-11)\n",
    "* [清理](#section-12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ec25b247838"
   },
   "source": [
    "## 概述\n",
    "<a name=\"section-1\"></a>\n",
    "\n",
    "推荐系统是强大的工具，它们建模现有客户行为以生成推荐内容。这些模型通常构建复杂的矩阵，映射现有客户的偏好，以找到交叉兴趣并提供推荐内容。这些矩阵可能非常庞大，并且将受益于分布式计算和大内存池。在一个Vertex AI Workbench管理的笔记本实例中，您可以通过在一个Dataproc集群上使用PySpark处理数据来实现分布式计算。\n",
    "\n",
    "*注意：这个笔记本文件是设计在一个使用Dataproc运行时生成的Python 3内核的[Vertex AI Workbench管理的笔记本](https://cloud.google.com/vertex-ai/docs/workbench/managed/create-instance)实例中运行的。这个笔记本的一些组件可能无法在其他笔记本环境中运行。*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ccfcd2355f3"
   },
   "source": [
    "## 数据集\n",
    "\n",
    "本笔记本在BigQuery中使用`looker-private-demo.retail`数据集。可以通过在BigQuery中固定`looker-private-demo`项目来访问该数据集。在JupyterLab用户界面上，可以从Vertex AI Workbench托管笔记本实例执行此过程，而无需转到BigQuery用户界面。Vertex AI Workbench托管笔记本实例支持通过其BigQuery集成浏览BigQuery中的数据集和表。\n",
    "\n",
    "在这个数据集中，将使用`retail.order_items`表来使用PySpark训练推荐系统。该表包含与数据集中的用户和商品相关的各种订单信息。\n",
    "\n",
    "## 目标\n",
    "\n",
    "本教程使用由Vertex AI Workbench托管笔记本实例提供的交互式PySpark功能，构建一个基于[协同过滤](https://en.wikipedia.org/wiki/Collaborative_filtering)方法的推荐模型。您将设置一个远程连接的Dataproc集群，并使用PySpark的MLlib库中实现的<a href=\"http://dl.acm.org/citation.cfm?id=1608614\">交替最小二乘(ALS)</a>方法。\n",
    "\n",
    "本笔记本中执行的步骤包括：\n",
    "\n",
    "1. 将托管笔记本实例连接到具有PySpark的Dataproc集群。\n",
    "2. 在笔记本中从BigQuery中探索数据集。\n",
    "3. 预处理数据。\n",
    "4. 在数据上训练一个PySpark ALS模型。\n",
    "5. 评估ALS模型。\n",
    "6. 生成推荐。\n",
    "7. 使用PySpark-BigQuery连接器将推荐保存到BigQuery表中。\n",
    "8. 将ALS模型保存到Cloud Storage存储桶。\n",
    "9. 清理资源。\n",
    "\n",
    "## 成本\n",
    "\n",
    "本教程使用Google Cloud的以下计费组件：\n",
    "\n",
    "* Vertex AI\n",
    "* Dataproc\n",
    "* BigQuery\n",
    "* Cloud Storage\n",
    "\n",
    "了解有关[Vertex AI价格](https://cloud.google.com/vertex-ai/pricing)、[Dataproc价格](https://cloud.google.com/dataproc/pricing)、[BigQuery价格](https://cloud.google.com/bigquery/pricing)和[Cloud Storage价格](https://cloud.google.com/storage/pricing)的信息，并使用[定价计算器](https://cloud.google.com/products/calculator/)根据您的预期使用量生成成本估算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ae6fd90ce34"
   },
   "source": [
    "设置您的项目ID\n",
    "\n",
    "**如果您不知道您的项目ID**，您可以使用`gcloud`来获取您的项目ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0b690120659f"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"\"\n",
    "\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    shell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c23bde1f51c5"
   },
   "source": [
    "否则，将您的项目ID设置在这里。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a258c0ac1442"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None:\n",
    "    PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "826232e3a213"
   },
   "source": [
    "时间戳\n",
    "\n",
    "如果您正在参加现场教程，您可能会使用共享测试帐户或项目。为了避免用户在创建的资源之间发生名称冲突，您需要为每个实例会话创建一个时间戳，并将其附加到在本教程中创建的资源名称上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "47aebdd88e9e"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98ca9d3da7e6"
   },
   "source": [
    "创建一个云存储桶\n",
    "\n",
    "**无论您使用什么笔记本环境，以下步骤都是必需的。**\n",
    "\n",
    "当您使用Cloud SDK提交训练作业时，您需要将包含训练代码的Python包上传到云存储桶中。Vertex AI会从这个包中运行代码。在本教程中，Vertex AI还会将您作业产生的训练模型保存在同一个存储桶中。使用这个模型工件，您可以创建Vertex AI模型和端点资源，以便提供在线预测。\n",
    "\n",
    "在下方设置您的Cloud存储桶的名称。它必须在所有的Cloud存储桶中是唯一的。\n",
    "\n",
    "您也可以更改`REGION`变量，它将在整个笔记本的其余部分中使用。确保[选择一个Vertex AI服务可用的地区](https://cloud.google.com/vertex-ai/docs/general/locations#available_regions)。您不可以使用多区域存储桶来训练Vertex AI。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ead6150209c"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"gs://[your-bucket-name]\"  # @param {type:\"string\"}\n",
    "REGION = \"[your-region]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "933efba860bc"
   },
   "outputs": [],
   "source": [
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"gs://[your-bucket-name]\":\n",
    "    BUCKET_NAME = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "26fd8d22e96f"
   },
   "source": [
    "只有当您的桶不存在时才能运行以下单元格以创建您的云存储桶。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d201bf34f895"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3f62553e5054"
   },
   "source": [
    "最后，通过检查云存储桶的内容来验证访问权限。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "73f53d17ae47"
   },
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ef78f6261a0"
   },
   "source": [
    "## 开始之前\n",
    "\n",
    "ALS模型方法需要大量计算，可能需要很长时间才能在常规笔记本环境上训练，因此本教程使用带有PySpark环境的Dataproc集群。\n",
    "\n",
    "### 创建具有组件网关启用和JupyterLab扩展的Dataproc集群\n",
    "<a name=\"section-5\"></a>\n",
    "\n",
    "使用以下`gcloud`命令创建集群。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "950554272656"
   },
   "outputs": [],
   "source": [
    "CLUSTER_NAME = \"[your-cluster-name]\"\n",
    "CLUSTER_REGION = \"[your-cluster-region]\"\n",
    "CLUSTER_ZONE = \"[your-cluster-zone]\"\n",
    "MACHINE_TYPE = \"[your=machine-type]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1e3e719e27f6"
   },
   "outputs": [],
   "source": [
    "! gcloud dataproc clusters create $CLUSTER_NAME \\\n",
    "--enable-component-gateway \\\n",
    "--region $CLUSTER_REGION \\\n",
    "--zone $CLUSTER_ZONE \\\n",
    "--single-node \\\n",
    "--master-machine-type $MACHINE_TYPE \\\n",
    "--master-boot-disk-size 100 \\\n",
    "--image-version 2.0-debian10 \\\n",
    "--optional-components JUPYTER \\\n",
    "--project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2961b0a96176"
   },
   "source": [
    "另外，也可以通过Dataproc控制台创建集群。如果需要，可以在那里配置额外的设置，如网络配置和服务帐户。在配置集群时，请确保完成以下步骤：\n",
    "\n",
    "- 为集群提供一个名称。\n",
    "- 为集群选择一个区域和区域。\n",
    "- 将集群类型选择为单节点。对于小型和概念验证用例，建议使用单节点集群。\n",
    "- 启用组件网关。\n",
    "- 在可选组件中，选择Jupyter Notebook。\n",
    "- （可选）选择机器类型（最好选择高内存机器类型）。\n",
    "- 创建集群。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b864144904fb"
   },
   "source": [
    "## 从笔记本连接到集群\n",
    "新的Dataproc集群运行时，对应的运行时会出现在笔记本中作为一个内核。创建的集群名称会出现在可以为此笔记本选择的内核列表中。在这个笔记本文件的右上角，点击当前的内核名称 **Python (local)**，然后选择在您的Dataproc集群上运行的Python 3内核。\n",
    "\n",
    "注意以下内容：\n",
    "\n",
    "- 您的Dataproc内核可能需要几分钟才会出现在内核列表中。\n",
    "- 在本教程中的PySpark代码可以在Dataproc集群上的PySpark或Python 3内核上运行，但是为了运行将推荐保存到BigQuery表的可选代码，建议使用Python 3内核。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fc8d6f0cc21b"
   },
   "source": [
    "教程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03c8dc1024f7"
   },
   "source": [
    "## 探索数据\n",
    "<a name=\"section-7\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ffc1113d6504"
   },
   "source": [
    "Vertex AI Workbench管理的笔记本实例使您可以使用BigQuery集成从管理的笔记本实例内部探索BigQuery内容。该功能使您可以查看表内容的元数据和预览，查询表，并获取表中数据的描述。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1fb418e82451"
   },
   "source": [
    "检查`STATUS`字段的分布。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfebfe18e127"
   },
   "source": [
    "#@bigquery\n",
    "SELECT STATUS, COUNT(*) order_count FROM looker-private-demo.retail.order_items GROUP BY 1\n",
    "\n",
    "#@bigquery\n",
    "选择状态，COUNT(*) 订单数量 FROM looker-private-demo.retail.order_items 分组 BY 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3dd73d32bc80"
   },
   "source": [
    "将`order_items`表与相同数据集中的`inventory_items`表连接，以检索订单的产品ID。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4abc4fb13e25"
   },
   "source": [
    "#@bigquery\n",
    "以 user_prod_table 作为 (\n",
    "选择 USER_ID, PRODUCT_ID, STATUS FROM looker-private-demo.retail.order_items AS a\n",
    "加入\n",
    "(SELECT ID, PRODUCT_ID FROM looker-private-demo.retail.inventory_items) AS b\n",
    "在 a.inventory_item_id = b.ID )\n",
    "\n",
    "选择 USER_ID, PRODUCT_ID, STATUS from user_prod_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5072aded172b"
   },
   "source": [
    "一旦在上述单元格中显示了来自BigQuery的结果，点击**查询并加载为DataFrame**按钮，并执行生成的代码存根以将数据获取到当前笔记本中作为一个数据帧。\n",
    "\n",
    "*注意：默认情况下，数据将加载到`df`变量中，但如果需要的话，在执行单元格之前可以更改它。*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ba1a54b04d9a"
   },
   "outputs": [],
   "source": [
    "# The following two lines are only necessary to run once.\n",
    "# Comment out otherwise for speed-up.\n",
    "from google.cloud.bigquery import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "query = \"\"\"WITH user_prod_table AS (\n",
    "SELECT USER_ID, PRODUCT_ID, STATUS FROM looker-private-demo.retail.order_items AS a\n",
    "join\n",
    "(SELECT ID, PRODUCT_ID FROM looker-private-demo.retail.inventory_items) AS b\n",
    "on a.inventory_item_id = b.ID )\n",
    "\n",
    "SELECT USER_ID, PRODUCT_ID, STATUS from user_prod_table\"\"\"\n",
    "job = client.query(query)\n",
    "df = job.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "acb1077cbf1e"
   },
   "source": [
    "### 数据预处理\n",
    "\n",
    "要在现有数据上运行PySpark的ALS方法，必须有一些字段来量化`USER_ID`和`PRODUCT_ID`之间的关系，比如*用户给出的评分*。如果数据中已经存在这样的字段，它们可以被视为ALS模型的*显式反馈*。否则，表明关系的字段可以被视为*隐式反馈*。了解更多关于[PySpark的ALS方法的反馈信息](https://spark.apache.org/docs/2.2.0/ml-collaborative-filtering.html#explicit-vs-implicit-feedback)。\n",
    "\n",
    "在当前数据集中，由于没有这样的数值字段，`STATUS`字段被进一步用于量化`USER_ID`和`PRODUCT_ID`之间的关联。基于订单生命周期中它们发生的时间以及用户可能喜欢订单的可能性，`STATUS`字段被分配以下评分中的一个：\n",
    "\n",
    "- 取消 - 1\n",
    "- 退货 - 2\n",
    "- 处理中 - 3\n",
    "- 已发货 - 4\n",
    "- 完成 - 5\n",
    "\n",
    "给出的评分是主观的，可以根据使用情况进行修改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "365dca51641f"
   },
   "outputs": [],
   "source": [
    "score_mapping = {\n",
    "    \"Cancelled\": 1,\n",
    "    \"Returned\": 2,\n",
    "    \"Processing\": 3,\n",
    "    \"Shipped\": 4,\n",
    "    \"Complete\": 5,\n",
    "}\n",
    "df[\"RATING\"] = df[\"STATUS\"].map(score_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a88eb70c6dec"
   },
   "source": [
    "检查新生成的`RATING`字段的分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cfe29a1f9f16"
   },
   "outputs": [],
   "source": [
    "df[\"RATING\"].plot(kind=\"hist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "367bc4912977"
   },
   "source": [
    "加载PySpark MLlib中所需的方法和类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b674b037dce0"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1c3c74870bcf"
   },
   "source": [
    "使用配置了BigQuery-Spark连接器的Spark会话生成一个Spark会话。\n",
    "\n",
    "*注意：如果笔记本连接到Dataproc集群，会话对象会显示`yarn`作为Master。*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2e55770bb894"
   },
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"Recommendations\")\n",
    "    .config(\"spark.jars\", \"gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "89b6a2146b7c"
   },
   "source": [
    "将pandas dataframe 转换为spark dataframe 以进行进一步处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0d1d5b5e6a56"
   },
   "outputs": [],
   "source": [
    "spark_df = spark.createDataFrame(df[[\"USER_ID\", \"PRODUCT_ID\", \"RATING\"]])\n",
    "spark_df.printSchema()\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5b159af5c085"
   },
   "source": [
    "### 将数据分割成训练集和测试集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "951e7c2bf8fe"
   },
   "outputs": [],
   "source": [
    "(train, test) = spark_df.randomSplit([0.8, 0.2], seed=36)\n",
    "train.count(), test.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dea0589dd474"
   },
   "source": [
    "定义ALS模型\n",
    "<a name=\"section-8\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99e0365f32be"
   },
   "source": [
    "PySpark ALS推荐器，交替最小二乘法（Alternating Least Squares），是一种矩阵分解算法。其思想是构建一个将用户映射到行为的矩阵。这些行为可以是评论、购买、采取的各种选项等。由于矩阵的复杂性和规模，PySpark可以并行运行该算法。\n",
    "\n",
    "ALS将尝试将评分矩阵R估计为两个较低秩矩阵X和Y的乘积。通常提到这些近似值被称为“因子”矩阵。在每次迭代中，其中一个因子矩阵保持不变，而另一个则使用最小二乘法求解。然后保持新解决的因子矩阵不变，同时解决另一个因子矩阵。\n",
    "\n",
    "PySpark使用ALS因子分解算法的分块实现，将两组因子（称为“用户”和“产品”）分组为块，并通过仅在每次迭代中向每个产品块发送每个用户向量的一个副本，以及仅发送给需要该用户特征向量的产品块来减少通信。\n",
    "\n",
    "基本上，与找到评分矩阵R的低秩近似值不同，这会找到偏好矩阵P的近似值，其中P的元素为1，如果r > 0则为1，如果r <= 0则为0。然后，这些评分作为与指示用户偏好强度相关的置信度值，而不是给定给项的明确评分。了解更多关于PySpark的ALS算法的信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b47e5bfe0c30"
   },
   "outputs": [],
   "source": [
    "als = ALS(\n",
    "    userCol=\"USER_ID\",\n",
    "    itemCol=\"PRODUCT_ID\",\n",
    "    ratingCol=\"RATING\",\n",
    "    nonnegative=True,\n",
    "    implicitPrefs=False,\n",
    "    coldStartStrategy=\"drop\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9726f220acb5"
   },
   "source": [
    "ALS 模型试图预测用户和物品之间的评分，因此可以使用 RMSE 来评估模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "50ae46e7db7c"
   },
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\", labelCol=\"RATING\", predictionCol=\"prediction\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01d3b2a2a801"
   },
   "source": [
    "定义一个用于交叉验证的超参数网格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d0397bf82ba6"
   },
   "outputs": [],
   "source": [
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(als.rank, [10, 50])\n",
    "    .addGrid(als.regParam, [0.01, 0.1, 0.2])\n",
    "    .build()\n",
    ")\n",
    "print(\"No. of settings to be tested: \", len(param_grid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98f3c23ff0a1"
   },
   "source": [
    "执行交叉验证并保存最佳模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "13617284cdb3"
   },
   "outputs": [],
   "source": [
    "cv = CrossValidator(\n",
    "    estimator=als, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=3\n",
    ")\n",
    "model = cv.fit(train)\n",
    "best_model = model.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "66bc83eb4adb"
   },
   "outputs": [],
   "source": [
    "print(\"##Parameters for the Best Model##\")\n",
    "print(\"Rank:\", best_model._java_obj.parent().getRank())\n",
    "print(\"MaxIter:\", best_model._java_obj.parent().getMaxIter())\n",
    "print(\"RegParam:\", best_model._java_obj.parent().getRegParam())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12a02668017e"
   },
   "source": [
    "评估模型\n",
    "<a name=\"section-9\"></a>\n",
    "通过计算训练和测试数据上的RMSE评估模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "04d7e30661fa"
   },
   "outputs": [],
   "source": [
    "# View the rating predictions by the model on train and test sets\n",
    "train_predictions = best_model.transform(train)\n",
    "train_RMSE = evaluator.evaluate(train_predictions)\n",
    "\n",
    "test_predictions = best_model.transform(test)\n",
    "test_RMSE = evaluator.evaluate(test_predictions)\n",
    "\n",
    "print(\"Train RMSE \", train_RMSE)\n",
    "print(\"Test RMSE \", test_RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "521dc4a5a688"
   },
   "source": [
    "为所有用户生成推荐\n",
    "\n",
    "可以使用ALS模型的`recommendForAllUsers()`方法生成用户所需数量的推荐。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c7ddfd001900"
   },
   "outputs": [],
   "source": [
    "# Generate 10 product recommendations for all users\n",
    "nrecommendations = best_model.recommendForAllUsers(10)\n",
    "nrecommendations.limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "219e986f5c23"
   },
   "source": [
    "为特定用户生成推荐\n",
    "\n",
    "前面的步骤已经为`nrecommendations`数据框中的所有用户生成并存储了指定数量的产品推荐。要获取单个用户的推荐，可以查询这个数据框对象。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "782b17eb3621"
   },
   "outputs": [],
   "source": [
    "# get product recommendations for the selected user (USER_ID = 1)\n",
    "nrecommendations.where(nrecommendations.USER_ID == 1).select(\n",
    "    \"recommendations.PRODUCT_ID\", \"recommendations.rating\"\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "964c4d610d31"
   },
   "source": [
    "## 将ALS模型保存到云存储（可选）\n",
    "<a name=\"section-10\"></a>\n",
    "\n",
    "PySpark的`ALS.save()`方法会在指定路径创建一个文件夹，并在其中保存训练好的模型。托管笔记本实例的环境中提供了云存储文件浏览器，您可以使用它将模型保存到云存储桶中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73db311320d0"
   },
   "source": [
    "使用ALS对象的` .save（）`函数将模型写入Cloud Storage存储桶。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9f910addca92"
   },
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "GCS_MODEL_PATH = \"gs://\" + BUCKET_NAME + \"/recommender_systems/\"\n",
    "best_model.save(GCS_MODEL_PATH + \"rcmd_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "665a05361fbd"
   },
   "source": [
    "## 将推荐写入BigQuery（可选）\n",
    "<a name=\"section-11\"></a>\n",
    "\n",
    "为了向最终用户或任何应用程序提供推荐，可以使用Spark的BigQuery连接器将`recommendForAllUsers()`方法的输出保存到BigQuery表中。\n",
    "\n",
    "### 在BigQuery中创建数据集\n",
    "\n",
    "以下代码单元格在BigQuery中创建一个新数据集。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22e136731378"
   },
   "source": [
    "#@bigquery\n",
    "-- 在BigQuery中创建一个数据集\n",
    "CREATE SCHEMA recommender_sys\n",
    "OPTIONS(\n",
    "  location=\"us\"\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4910f961001f"
   },
   "source": [
    "### 写入 BigQuery 推荐\n",
    "\n",
    "PySpark的 BigQuery 连接器需要两个必要字段：一个 *BigQuery 表名称* 和一个 *Cloud 存储路径来写入临时文件* 在保存模型时。这两个字段可以在写入到 BigQuery 推荐时提供。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "63b76c37123d"
   },
   "outputs": [],
   "source": [
    "DATASET = \"[your-dataset-name]\"\n",
    "TABLE = \"[your-bigquery-table-name]\"\n",
    "GCS_TEMP_PATH = \"[your-cloud-storage-path]\"\n",
    "\n",
    "nrecommendations.write.format(\"bigquery\").option(\n",
    "    \"table\", \"{}.{}\".format(DATASET, TABLE)\n",
    ").option(\"temporaryGcsBucket\", GCS_TEMP_PATH).mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6c3a6d501600"
   },
   "source": [
    "清理\n",
    "<a name=\"section-12\"></a>\n",
    "要清理此项目中使用的所有Google Cloud资源，您可以[删除用于教程的Google Cloud项目](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects)。\n",
    "\n",
    "否则，您可以删除您在本教程中创建的各个资源："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "72488a2999a6"
   },
   "outputs": [],
   "source": [
    "# remove the BigQuery dataset created for storing the recommendations and all of its tables\n",
    "! bq rm -r -f -d $PROJECT:$DATASET\n",
    "\n",
    "# remove the Cloud Storage bucket created and all of its tables\n",
    "! gsutil rm -r gs://$BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "75cfc772a3f4"
   },
   "outputs": [],
   "source": [
    "# delete the created Dataproc cluster\n",
    "! gcloud dataproc clusters delete $CLUSTER_NAME --region=$CLUSTER_REGION"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "recommender-system-on-retail-data.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
