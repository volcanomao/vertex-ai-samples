{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# 使用Vertex AI Experiments跟踪、比较和管理实验\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/vertex_ai_experiments_classification.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> 在Colab中运行\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/vertex_ai_experiments_classification.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      在GitHub上查看\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/vertex_ai_experiments_classification.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      在Vertex AI工作台中打开\n",
    "    </a>\n",
    "  </td>                                                                                               \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## 概述\n",
    "\n",
    "在为问题开发模型时的目标是识别针对特定用例的最佳模型。为此，Vertex AI实验使您能够跟踪、分析、比较和搜索不同的ML框架（例如TensorFlow、PyTorch、scikit-learn）和训练环境。\n",
    "\n",
    "Vertex AI实验使您能够跟踪\n",
    "\n",
    "*   实验运行的步骤，例如前处理、训练\n",
    "*   输入，例如算法、参数、数据集\n",
    "*   这些步骤的输出，例如模型、检查点、指标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d975e698c9a4"
   },
   "source": [
    "### 目标\n",
    "\n",
    "在本教程中，您将学习如何使用Vertex AI实验来进行模型实验。\n",
    "\n",
    "该教程使用以下 Google Cloud ML 服务和资源：\n",
    "\n",
    "- Cloud 存储\n",
    "- Vertex AI\n",
    "\n",
    "执行的步骤包括：\n",
    "\n",
    "1. 创建实验\n",
    "2. 创建运行\n",
    "3. 记录参数\n",
    "4. 记录指标\n",
    "5. 跟踪工件和执行\n",
    "6. 跟踪Vertex AI流水线"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08d289fa873f"
   },
   "source": [
    "### 数据集\n",
    "\n",
    "该数据集是[心脏病数据集](https://archive.ics.uci.edu/ml/datasets/heart+Disease)。该数据库包含14个属性。\n",
    "\n",
    "“goal”字段指的是患者是否患有心脏病。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aed92deeb4a0"
   },
   "source": [
    "### 费用\n",
    "\n",
    "本教程使用 Google Cloud 的计费组件：\n",
    "\n",
    "* Vertex AI\n",
    "* 云存储\n",
    "\n",
    "了解 [Vertex AI 价格](https://cloud.google.com/vertex-ai/pricing)，\n",
    "以及 [云存储价格](https://cloud.google.com/storage/pricing)，\n",
    "并使用 [价格计算器](https://cloud.google.com/products/calculator/)\n",
    "根据您预计的使用量生成费用估算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCuSR8GkAgzl"
   },
   "source": [
    "设置您的本地开发环境\n",
    "\n",
    "**如果您使用的是Colab或Vertex AI Workbench笔记本**，您的环境已经满足运行此笔记本的所有要求。您可以跳过此步骤。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24743cf4a1e1"
   },
   "source": [
    "注意：此笔记本在以下环境中进行了测试：\n",
    "\n",
    "* Python版本= 3.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fITp_o3JWlA0"
   },
   "source": [
    "否则，请确保您的环境符合本笔记本的要求。\n",
    "您需要以下内容：\n",
    "\n",
    "* Google Cloud SDK\n",
    "\n",
    "Google Cloud指南中 [设置Python开发环境](https://cloud.google.com/python/setup) 和 [Jupyter安装指南](https://jupyter.org/install) 提供了满足这些要求的详细说明。以下步骤提供了压缩的指令：\n",
    "\n",
    "1. [安装和初始化Cloud SDK。](https://cloud.google.com/sdk/docs/)\n",
    "\n",
    "1. [安装Python 3。](https://cloud.google.com/python/setup#installing_python)\n",
    "\n",
    "1. [安装virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv) 并创建一个使用Python 3的虚拟环境。激活虚拟环境。\n",
    "\n",
    "1. 要安装Jupyter，在终端中运行 `pip3 install jupyter`。\n",
    "\n",
    "1. 要启动Jupyter，在终端中运行 `jupyter notebook`。\n",
    "\n",
    "1. 在Jupyter Notebook仪表板中打开这个笔记本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "## 安装\n",
    "\n",
    "安装以下所需的包以执行此笔记本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2b4ef9b72d43"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Vertex AI Workbench Notebook product has specific requirements\n",
    "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\")\n",
    "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
    "    \"/opt/deeplearning/metadata/env_version\"\n",
    ")\n",
    "\n",
    "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_WORKBENCH_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\"\n",
    "\n",
    "! pip3 install --upgrade google-cloud-aiplatform kfp fsspec gcsfs {USER_FLAG} -q --no-warn-conflicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhq5zEbGg0XX"
   },
   "source": [
    "重新启动内核\n",
    "\n",
    "在安装额外的包之后，您需要重新启动笔记本内核，以便它可以找到这些包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EzrelQZ22IZj"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF1j6f9HApxa"
   },
   "source": [
    "## 准备开始之前\n",
    "\n",
    "### 设置您的谷歌云项目\n",
    "\n",
    "**无论您使用哪种笔记本环境，都需要完成以下步骤。**\n",
    "\n",
    "1. [选择或创建一个谷歌云项目](https://console.cloud.google.com/cloud-resource-manager)。当您第一次创建账号时，您将获得$300的免费信用额度，用于支付计算/存储成本。\n",
    "\n",
    "1. [确保为您的项目启用了计费](https://cloud.google.com/billing/docs/how-to/modify-project)。\n",
    "\n",
    "1. [启用 Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com)。\n",
    "\n",
    "1. 如果您正在本地运行此笔记本，您需要安装[Cloud SDK](https://cloud.google.com/sdk)。\n",
    "\n",
    "1. 在下面的单元格中输入您的项目ID。然后运行该单元格，确保Cloud SDK在本笔记本中的所有命令中使用正确的项目。\n",
    "\n",
    "**注意**：Jupyter会将以`!`为前缀的行视为shell命令，并将以`$`为前缀的Python变量插入这些命令中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WReHDGG5g0XY"
   },
   "source": [
    "#### 设置您的项目ID\n",
    "\n",
    "**如果您不知道您的项目ID**，您可以使用`gcloud`命令来获取您的项目ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oM1iC_MfAts1"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "riG_qUokg0XZ"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID:\", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_gcloud_project_id"
   },
   "outputs": [],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "地区\n",
    "\n",
    "您还可以更改“REGION”变量，该变量用于笔记本中的其他操作。以下是Vertex AI支持的地区。建议您选择距离您最近的地区。\n",
    "\n",
    "- 美洲：`us-central1`\n",
    "- 欧洲：`europe-west4`\n",
    "- 亚太地区：`asia-east1`\n",
    "\n",
    "您不能使用多地区存储桶来训练Vertex AI。并非所有地区都支持所有的Vertex AI服务。\n",
    "\n",
    "了解更多有关[Vertex AI地区](https://cloud.google.com/vertex-ai/docs/general/locations)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MiIiKM9LWlA4"
   },
   "outputs": [],
   "source": [
    "REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
    "\n",
    "if REGION == \"[your-region]\":\n",
    "    REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06571eb4063b"
   },
   "source": [
    "UUID\n",
    "\n",
    "如果您正在进行现场教程，您可能正在使用共享测试帐户或项目。为了避免资源名称冲突，您可以为每个实例会话创建一个uuid，并将其附加到您在本教程中创建的资源名称上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "697568e92bd6"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "\n",
    "# Generate a uuid of a specifed length(default=8)\n",
    "def generate_uuid(length: int = 8) -> str:\n",
    "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
    "\n",
    "\n",
    "UUID = generate_uuid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBCra4QMA2wR"
   },
   "source": [
    "### 验证您的谷歌云帐号\n",
    "\n",
    "**如果您正在使用Vertex AI Workbench笔记本**，您的环境已经得到了验证。\n",
    "\n",
    "**如果您正在使用Colab**，请运行下面的单元格，并按照提示进行帐户oAuth验证。\n",
    "\n",
    "**否则**，请按照以下步骤进行操作：\n",
    "\n",
    "1. 在Cloud控制台中，转到[**创建服务帐号密钥**页面](https://console.cloud.google.com/apis/credentials/serviceaccountkey)。\n",
    "\n",
    "2. 点击**创建服务帐号**。\n",
    "\n",
    "3. 在**服务帐号名称**字段中输入一个名称，并点击**创建**。\n",
    "\n",
    "4. 在**将此服务帐号授予对项目的访问权限**部分，点击**角色**下拉列表。在过滤框中输入“Vertex AI”，选择**Vertex AI 管理员**。在过滤框中输入“存储对象管理员”，选择**存储对象管理员**。\n",
    "\n",
    "5. 点击*创建*。一个包含您密钥的JSON文件将下载到您的本地环境。\n",
    "\n",
    "6. 在下面的单元格中将您的服务帐号密钥路径作为`GOOGLE_APPLICATION_CREDENTIALS`变量输入，并运行单元格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PyQmSRbKA8r-"
   },
   "outputs": [],
   "source": [
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# If on Vertex AI Workbench, then don't execute this code\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\") and not os.getenv(\n",
    "    \"DL_ANACONDA_HOME\"\n",
    "):\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\"):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS '[your-service-account-key-path]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23988890fef6"
   },
   "source": [
    "获取您的项目编号\n",
    "\n",
    "现在项目ID已经设置，您可以获得相应的项目编号。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2d6950574e1d"
   },
   "outputs": [],
   "source": [
    "shell_output = ! gcloud projects list --filter=\"PROJECT_ID:'{PROJECT_ID}'\" --format='value(PROJECT_NUMBER)'\n",
    "PROJECT_NUMBER = shell_output[0]\n",
    "print(\"Project Number:\", PROJECT_NUMBER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "### 创建一个云存储桶\n",
    "\n",
    "**无论您使用什么笔记本环境，以下步骤都是必需的。**\n",
    "\n",
    "在下面设置您的云存储桶的名称。它必须在所有云存储桶中保持唯一。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"[your-bucket-name]\"  # @param {type:\"string\"}\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cf221059d072"
   },
   "outputs": [],
   "source": [
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"[your-bucket-name]\":\n",
    "    BUCKET_NAME = PROJECT_ID + \"-aip-\" + UUID\n",
    "    BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EcIXiGsCePi"
   },
   "source": [
    "只有在您的存储桶尚未存在的情况下才运行以下单元格以创建您的云存储存储桶。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIq7R4HZCfIc"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucvCsknMCims"
   },
   "source": [
    "最后，通过检查云存储桶中的内容来验证对其的访问。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vhOb7YnwClBb"
   },
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "set_service_account"
   },
   "source": [
    "服务账户\n",
    "\n",
    "如果你不想使用项目的计算引擎服务账户，请将`SERVICE_ACCOUNT`设置为另一个服务账户ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dxkQNhY0WlA8"
   },
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT = \"[your-service-account]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "autoset_service_account"
   },
   "outputs": [],
   "source": [
    "if (\n",
    "    SERVICE_ACCOUNT == \"\"\n",
    "    or SERVICE_ACCOUNT is None\n",
    "    or SERVICE_ACCOUNT == \"[your-service-account]\"\n",
    "):\n",
    "    # Get your service account from gcloud\n",
    "    if not IS_COLAB:\n",
    "        shell_output = !gcloud auth list 2>/dev/null\n",
    "        SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
    "\n",
    "    else:  # IS_COLAB:\n",
    "        shell_output = ! gcloud projects describe  $PROJECT_ID\n",
    "        project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
    "        SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
    "\n",
    "    print(\"Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "set_service_account:pipelines"
   },
   "source": [
    "### 设置服务账号访问权限\n",
    "\n",
    "运行以下命令，将您的服务账号访问权限授予之前创建的存储桶中的{TODO; 即读取和写入管道工件}。您只需要为每个服务账号运行此步骤一次。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7oFHfGxwWlA8"
   },
   "outputs": [],
   "source": [
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
    "\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRUOFELefqf1"
   },
   "outputs": [],
   "source": [
    "import kfp.v2.compiler as compiler\n",
    "import kfp.v2.dsl as dsl\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from kfp.v2.dsl import Metrics, Model, Output, component\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import IntegerLookup, Normalization, StringLookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hEWIYdFBbUNa"
   },
   "source": [
    "定义常数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jHFGG8X3bVK1"
   },
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"[your-experiment-name]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ogcGyP5XbZil"
   },
   "outputs": [],
   "source": [
    "if EXPERIMENT_NAME == \"[your-experiment-name]\" or EXPERIMENT_NAME is None:\n",
    "    EXPERIMENT_NAME = \"my-experiment-\" + UUID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qy-YBebIewXA"
   },
   "source": [
    "### 帮助函数\n",
    "\n",
    "以下是一些帮助函数：\n",
    "\n",
    "- `dataframe_to_dataset`用于将Pandas数据框转换为tf.data.Dataset\n",
    "- `encode_numerical_feature`用于创建一个标准化器并编码一个数值特征。\n",
    "- `encode_categorical_feature`用于对分类特征进行编码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t9N3mbVPeyQV"
   },
   "outputs": [],
   "source": [
    "def dataframe_to_dataset(dataframe):\n",
    "    \"\"\"\n",
    "    Convert a Pandas dataframe to a tf.data.Dataset.\n",
    "    Args:\n",
    "        dataframe: Pandas dataframe\n",
    "    Returns:\n",
    "        tf.data.Dataset\n",
    "    \"\"\"\n",
    "    dataframe = dataframe.copy()\n",
    "    labels = dataframe.pop(\"target\")\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "    return ds\n",
    "\n",
    "\n",
    "def encode_numerical_feature(feature, name, dataset):\n",
    "    \"\"\"\n",
    "    Create a normalizer and encode a numerical feature.\n",
    "    Args:\n",
    "        feature: the feature to encode\n",
    "        name: the name of the feature\n",
    "        dataset: tf.data.Dataset\n",
    "    Returns:\n",
    "        the encoded feature\n",
    "    \"\"\"\n",
    "    # Create a Normalization layer for our feature\n",
    "    normalizer = Normalization()\n",
    "\n",
    "    # Prepare a Dataset that only yields our feature\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
    "\n",
    "    # Learn the statistics of the data\n",
    "    normalizer.adapt(feature_ds)\n",
    "\n",
    "    # Normalize the input feature\n",
    "    encoded_feature = normalizer(feature)\n",
    "    return encoded_feature\n",
    "\n",
    "\n",
    "def encode_categorical_feature(feature, name, dataset, is_string):\n",
    "    \"\"\"\n",
    "    Encode a categorical feature.\n",
    "    Args:\n",
    "        feature: the feature to encode\n",
    "        name: the name of the feature\n",
    "        dataset: tf.data.Dataset\n",
    "        is_string: whether the feature is a string\n",
    "    Returns:\n",
    "        the encoded feature\n",
    "    \"\"\"\n",
    "    lookup_class = StringLookup if is_string else IntegerLookup\n",
    "    # Create a lookup layer which will turn strings into integer indices\n",
    "    lookup = lookup_class(output_mode=\"binary\")\n",
    "\n",
    "    # Prepare a Dataset that only yields our feature\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
    "\n",
    "    # Learn the set of possible string values and assign them a fixed integer index\n",
    "    lookup.adapt(feature_ds)\n",
    "\n",
    "    # Turn the string input into integer indices\n",
    "    encoded_feature = lookup(feature)\n",
    "    return encoded_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "source": [
    "### 初始化 Python 的 Vertex AI SDK\n",
    "\n",
    "为您的项目和相应的存储桶初始化 Python 的 Vertex AI SDK。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zBXU04iaWlA9"
   },
   "outputs": [],
   "source": [
    "vertex_ai.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bba904753a2c"
   },
   "source": [
    "使用Vertex AI TensorBoard创建TensorBoard实例\n",
    "\n",
    "您可以通过首先创建一个Vertex AI TensorBoard实例来上传您的TensorBoard日志。\n",
    "\n",
    "请注意，如果您还没有激活，Vertex AI TensorBoard将向每个唯一活动用户收取每月300美元的费用。\n",
    "\n",
    "了解有关[TensorBoard概述](https://cloud.google.com/vertex-ai/docs/experiments/tensorboard-overview)的更多信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d9800e386d0c"
   },
   "outputs": [],
   "source": [
    "vertex_ai_tb = vertex_ai.Tensorboard.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a600e376bf37"
   },
   "outputs": [],
   "source": [
    "vertex_ai.init(experiment=EXPERIMENT_NAME, experiment_tensorboard=vertex_ai_tb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12ab31563365"
   },
   "source": [
    "## 使用 Vertex AI 实验进行模型实验和形式化\n",
    "\n",
    "Vertex AI 使用户能够跟踪实验运行的步骤（例如，预处理、训练）以及跟踪这些步骤的输入（例如，算法、参数、数据集）和输出（例如，模型、检查点、度量）。 \n",
    "\n",
    "为了更好地了解参数和度量是如何存储和组织的，以下概念进行了解释：\n",
    "\n",
    "1. **实验** 描述了将你的运行和创建的工件组合成一个逻辑会话的上下文。例如，在这本笔记本中，你创建一个实验并记录数据到该实验中。\n",
    "\n",
    "2. **运行** 代表您在执行实验时执行的单个路径/途径。一个运行包括您用作输入或输出的工件以及您在此执行中使用的参数。一个实验可以包含多个运行。\n",
    "\n",
    "您可以使用 Vertex AI SDK for Python 来跟踪参数和度量，对每个实验在多个实验运行过程中在本地训练的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-iTnzt3B6Z_"
   },
   "source": [
    "开始一个实验并运行实验。\n",
    "\n",
    "您定义了多个实验配置，运行实验并在Vertex AI实验中进行跟踪。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qoKLTdn5cD4p"
   },
   "outputs": [],
   "source": [
    "RUN_NAME = \"run-1\"\n",
    "my_run = vertex_ai.start_run(RUN_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-_upcKFdlwY"
   },
   "source": [
    "准备数据进行模型训练\n",
    "\n",
    "跟踪数据集和数据配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4PujAKQ4coI4"
   },
   "outputs": [],
   "source": [
    "file_url = \"http://storage.googleapis.com/download.tensorflow.org/data/heart.csv\"\n",
    "dataset_artifact = vertex_ai.Artifact.create(\n",
    "    schema_title=\"system.Dataset\",\n",
    "    resource_id=f\"{EXPERIMENT_NAME}-heart-data\",\n",
    "    uri=file_url,\n",
    "    display_name=\"heart data\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "da3f422ed147"
   },
   "source": [
    "开始实验配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c_Z1SbcadDZ3"
   },
   "outputs": [],
   "source": [
    "params = dict(\n",
    "    dataset_uri=dataset_artifact.uri,\n",
    "    dataset_fraction_split=0.2,\n",
    "    dataset_batch=32,\n",
    "    random_state=1337,\n",
    ")\n",
    "\n",
    "vertex_ai.log_params(params)\n",
    "vertex_ai.get_experiment_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1hnbD-eheQan"
   },
   "source": [
    "读取数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JrcH08CLeE22"
   },
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv(file_url)\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yb8VyNdheTDN"
   },
   "source": [
    "创建训练、测试和验证数据集，并在Vertex ML Metadata中跟踪它们的数据谱系。\n",
    "\n",
    "运行一次执行来准备训练数据，并将结果数据集作为实验谱系的一部分进行跟踪。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "interracial-cleanup"
   },
   "outputs": [],
   "source": [
    "with vertex_ai.start_execution(\n",
    "    schema_title=\"system.ContainerExecution\", display_name=f\"{RUN_NAME} data split\"\n",
    ") as exc:\n",
    "    exc.assign_input_artifacts([dataset_artifact])\n",
    "\n",
    "    # Train, test and validation split\n",
    "    val_dataframe = dataframe.sample(\n",
    "        frac=params[\"dataset_fraction_split\"], random_state=params[\"random_state\"]\n",
    "    )\n",
    "    test_dataframe = val_dataframe.sample(\n",
    "        frac=params[\"dataset_fraction_split\"], random_state=params[\"random_state\"]\n",
    "    )\n",
    "    train_dataframe = dataframe.drop(val_dataframe.index)\n",
    "\n",
    "    train_uri = f\"{BUCKET_URI}/{EXPERIMENT_NAME}/{RUN_NAME}/data/heart_train.csv\"\n",
    "    test_uri = f\"{BUCKET_URI}/{EXPERIMENT_NAME}/{RUN_NAME}/data/heart_test.csv\"\n",
    "    val_uri = f\"{BUCKET_URI}/{EXPERIMENT_NAME}/{RUN_NAME}/data/heart_val.csv\"\n",
    "\n",
    "    # Materialize data\n",
    "    train_dataframe.to_csv(train_uri)\n",
    "    val_dataframe.to_csv(val_uri)\n",
    "    test_dataframe.to_csv(val_uri)\n",
    "\n",
    "    # Create Vertex AI Datasets\n",
    "    train_metadata = vertex_ai.Artifact.create(\n",
    "        schema_title=\"system.Dataset\", uri=train_uri, display_name=\"train split\"\n",
    "    )\n",
    "    val_metadata = vertex_ai.Artifact.create(\n",
    "        schema_title=\"system.Dataset\", uri=val_uri, display_name=\"val split\"\n",
    "    )\n",
    "    test_metadata = vertex_ai.Artifact.create(\n",
    "        schema_title=\"system.Dataset\", uri=test_uri, display_name=\"test split\"\n",
    "    )\n",
    "\n",
    "    exc.assign_output_artifacts([train_metadata, val_metadata, test_metadata])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2fasbSpQeq31"
   },
   "source": [
    "#### 特征工程\n",
    "\n",
    "##### 将数据框转换为TF数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Geq2oWte_yi"
   },
   "outputs": [],
   "source": [
    "train_ds = (\n",
    "    dataframe_to_dataset(train_dataframe)\n",
    "    .batch(params[\"dataset_batch\"])\n",
    "    .shuffle(buffer_size=len(train_dataframe))\n",
    ")\n",
    "val_ds = dataframe_to_dataset(val_dataframe).batch(params[\"dataset_batch\"])\n",
    "test_ds = dataframe_to_dataset(test_dataframe).batch(params[\"dataset_batch\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IeVZbTGZiV_3"
   },
   "source": [
    "生成特征\n",
    "\n",
    "此部分中执行的步骤包括分类特征和整数特征的编码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r0hCAxfIfcC5"
   },
   "outputs": [],
   "source": [
    "# Categorical features encoded as integers\n",
    "sex = keras.Input(shape=(1,), name=\"sex\", dtype=\"int64\")\n",
    "cp = keras.Input(shape=(1,), name=\"cp\", dtype=\"int64\")\n",
    "fbs = keras.Input(shape=(1,), name=\"fbs\", dtype=\"int64\")\n",
    "restecg = keras.Input(shape=(1,), name=\"restecg\", dtype=\"int64\")\n",
    "exang = keras.Input(shape=(1,), name=\"exang\", dtype=\"int64\")\n",
    "ca = keras.Input(shape=(1,), name=\"ca\", dtype=\"int64\")\n",
    "\n",
    "# Categorical feature encoded as string\n",
    "thal = keras.Input(shape=(1,), name=\"thal\", dtype=\"string\")\n",
    "\n",
    "# Numerical features\n",
    "age = keras.Input(shape=(1,), name=\"age\")\n",
    "trestbps = keras.Input(shape=(1,), name=\"trestbps\")\n",
    "chol = keras.Input(shape=(1,), name=\"chol\")\n",
    "thalach = keras.Input(shape=(1,), name=\"thalach\")\n",
    "oldpeak = keras.Input(shape=(1,), name=\"oldpeak\")\n",
    "slope = keras.Input(shape=(1,), name=\"slope\")\n",
    "\n",
    "all_inputs = [\n",
    "    sex,\n",
    "    cp,\n",
    "    fbs,\n",
    "    restecg,\n",
    "    exang,\n",
    "    ca,\n",
    "    thal,\n",
    "    age,\n",
    "    trestbps,\n",
    "    chol,\n",
    "    thalach,\n",
    "    oldpeak,\n",
    "    slope,\n",
    "]\n",
    "\n",
    "# Integer categorical features\n",
    "sex_encoded = encode_categorical_feature(sex, \"sex\", train_ds, False)\n",
    "cp_encoded = encode_categorical_feature(cp, \"cp\", train_ds, False)\n",
    "fbs_encoded = encode_categorical_feature(fbs, \"fbs\", train_ds, False)\n",
    "restecg_encoded = encode_categorical_feature(restecg, \"restecg\", train_ds, False)\n",
    "exang_encoded = encode_categorical_feature(exang, \"exang\", train_ds, False)\n",
    "ca_encoded = encode_categorical_feature(ca, \"ca\", train_ds, False)\n",
    "\n",
    "# String categorical features\n",
    "thal_encoded = encode_categorical_feature(thal, \"thal\", train_ds, True)\n",
    "\n",
    "# Numerical features\n",
    "age_encoded = encode_numerical_feature(age, \"age\", train_ds)\n",
    "trestbps_encoded = encode_numerical_feature(trestbps, \"trestbps\", train_ds)\n",
    "chol_encoded = encode_numerical_feature(chol, \"chol\", train_ds)\n",
    "thalach_encoded = encode_numerical_feature(thalach, \"thalach\", train_ds)\n",
    "oldpeak_encoded = encode_numerical_feature(oldpeak, \"oldpeak\", train_ds)\n",
    "slope_encoded = encode_numerical_feature(slope, \"slope\", train_ds)\n",
    "\n",
    "all_features = layers.concatenate(\n",
    "    [\n",
    "        sex_encoded,\n",
    "        cp_encoded,\n",
    "        fbs_encoded,\n",
    "        restecg_encoded,\n",
    "        exang_encoded,\n",
    "        slope_encoded,\n",
    "        ca_encoded,\n",
    "        thal_encoded,\n",
    "        age_encoded,\n",
    "        trestbps_encoded,\n",
    "        chol_encoded,\n",
    "        thalach_encoded,\n",
    "        oldpeak_encoded,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56t6atwMfteE"
   },
   "source": [
    "#### 构建并训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9BXdhgJnsvmZ"
   },
   "source": [
    "###### 跟踪模型配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kPhulhwEgOl4"
   },
   "outputs": [],
   "source": [
    "params.update(n_units=32, activation=\"relu\", dropout_rate=0.5)\n",
    "\n",
    "vertex_ai.log_params(params)\n",
    "vertex_ai.get_experiment_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TTNAzId5tGSn"
   },
   "source": [
    "建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GmFwjbt1fyvj"
   },
   "outputs": [],
   "source": [
    "x = layers.Dense(params[\"n_units\"], activation=params[\"activation\"])(all_features)\n",
    "x = layers.Dropout(params[\"dropout_rate\"])(x)\n",
    "output = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(all_inputs, output)\n",
    "model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DsOMLiv9thdN"
   },
   "source": [
    "训练模型并跟踪指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5WhfzpZEgsec"
   },
   "outputs": [],
   "source": [
    "with vertex_ai.start_execution(\n",
    "    schema_title=\"system.ContainerExecution\", display_name=f\"{RUN_NAME} train\"\n",
    ") as exc:\n",
    "\n",
    "    exc.assign_input_artifacts([train_metadata, val_metadata, test_metadata])\n",
    "\n",
    "    params.update(epochs=10)\n",
    "    vertex_ai.log_params(params)\n",
    "    history = model.fit(train_ds, epochs=params[\"epochs\"], validation_data=val_ds)\n",
    "    for i in range(history.params[\"epochs\"]):\n",
    "        vertex_ai.log_time_series_metrics(\n",
    "            dict(\n",
    "                train_loss=history.history[\"loss\"][i],\n",
    "                train_accuracy=history.history[\"accuracy\"][i],\n",
    "                val_loss=history.history[\"val_loss\"][i],\n",
    "                val_accuracy=history.history[\"val_accuracy\"][i],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    metrics = model.evaluate(test_ds, return_dict=True)\n",
    "    vertex_ai.log_metrics(\n",
    "        dict(\n",
    "            loss=metrics[\"loss\"],\n",
    "            accurancy=metrics[\"accuracy\"],\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model_uri = f\"{BUCKET_URI}/{EXPERIMENT_NAME}/{RUN_NAME}/model/\"\n",
    "    model.save(model_uri)\n",
    "\n",
    "    model_metadata = vertex_ai.Artifact.create(\n",
    "        schema_title=\"system.Model\", uri=model_uri, display_name=\"trained heart model\"\n",
    "    )\n",
    "\n",
    "    exc.assign_output_artifacts([model_metadata])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZvXizXBN0myN"
   },
   "source": [
    "可视化实验结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k1zcDoj6ucIz"
   },
   "source": [
    "绘制指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fFIk3lkC0z__"
   },
   "outputs": [],
   "source": [
    "vertex_ai.get_experiment_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDZ4KXQ-AARJ"
   },
   "source": [
    "每个时期绘制指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sRFz_imruwhT"
   },
   "outputs": [],
   "source": [
    "my_run.get_time_series_data_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jHM1TAbmAGXy"
   },
   "source": [
    "###### 在云控制台中可视化实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yWppCA5p0vHf"
   },
   "outputs": [],
   "source": [
    "print(\"Vertex AI Experiments:\")\n",
    "print(\n",
    "    f\"https://console.cloud.google.com/ai/platform/experiments/experiments?folder=&organizationId=&project={PROJECT_ID}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hTQ7eWDZu45Q"
   },
   "outputs": [],
   "source": [
    "vertex_ai.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gNxc57G-xIlf"
   },
   "source": [
    "将您的实验形式化为一个 Vertex AI 流水线"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Waw0N6qeyMlT"
   },
   "source": [
    "##### 创建自定义组件\n",
    "\n",
    "下面的部分创建了一个自定义的训练器组件，将实验代码包装在管道组件中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dFynCYipxZ1y"
   },
   "outputs": [],
   "source": [
    "@component(packages_to_install=[\"tensorflow\", \"pandas\"])\n",
    "def tabular_trainer(\n",
    "    dataset_uri: str,\n",
    "    dataset_fraction_split: float,\n",
    "    dataset_batch: int,\n",
    "    random_state: int,\n",
    "    n_units: int,\n",
    "    activation: str,\n",
    "    dropout_rate: float,\n",
    "    epochs: int,\n",
    "    metrics: Output[Metrics],\n",
    "    model_metadata: Output[Model],\n",
    "):\n",
    "\n",
    "    import pandas as pd\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    from tensorflow.keras.layers import (IntegerLookup, Normalization,\n",
    "                                         StringLookup)\n",
    "\n",
    "    dataframe = pd.read_csv(dataset_uri)\n",
    "    dataframe.head()\n",
    "\n",
    "    val_dataframe = dataframe.sample(\n",
    "        frac=dataset_fraction_split, random_state=random_state\n",
    "    )\n",
    "    test_dataframe = val_dataframe.sample(\n",
    "        frac=dataset_fraction_split, random_state=random_state\n",
    "    )\n",
    "    train_dataframe = dataframe.drop(val_dataframe.index)\n",
    "\n",
    "    def dataframe_to_dataset(dataframe):\n",
    "        \"\"\"\n",
    "        Convert a Pandas dataframe to a tf.data.Dataset.\n",
    "        Args:\n",
    "            dataframe: Pandas dataframe\n",
    "        Returns:\n",
    "            tf.data.Dataset\n",
    "        \"\"\"\n",
    "        dataframe = dataframe.copy()\n",
    "        labels = dataframe.pop(\"target\")\n",
    "        ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "        return ds\n",
    "\n",
    "    train_ds = (\n",
    "        dataframe_to_dataset(train_dataframe)\n",
    "        .batch(dataset_batch)\n",
    "        .shuffle(buffer_size=len(train_dataframe))\n",
    "    )\n",
    "    val_ds = dataframe_to_dataset(val_dataframe).batch(dataset_batch)\n",
    "    test_ds = dataframe_to_dataset(test_dataframe).batch(dataset_batch)\n",
    "\n",
    "    def encode_numerical_feature(feature, name, dataset):\n",
    "        \"\"\"\n",
    "        Create a normalizer and encode a numerical feature.\n",
    "        Args:\n",
    "          feature: the feature to encode\n",
    "          name: the name of the feature\n",
    "          dataset: tf.data.Dataset\n",
    "        Returns:\n",
    "          the encoded feature\n",
    "        \"\"\"\n",
    "        # Create a Normalization layer for our feature\n",
    "        normalizer = Normalization()\n",
    "\n",
    "        # Prepare a Dataset that only yields our feature\n",
    "        feature_ds = dataset.map(lambda x, y: x[name])\n",
    "        feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
    "\n",
    "        # Learn the statistics of the data\n",
    "        normalizer.adapt(feature_ds)\n",
    "\n",
    "        # Normalize the input feature\n",
    "        encoded_feature = normalizer(feature)\n",
    "        return encoded_feature\n",
    "\n",
    "    def encode_categorical_feature(feature, name, dataset, is_string):\n",
    "        \"\"\"\n",
    "        Encode a categorical feature.\n",
    "        Args:\n",
    "          feature: the feature to encode\n",
    "          name: the name of the feature\n",
    "          dataset: tf.data.Dataset\n",
    "          is_string: whether the feature is a string\n",
    "        Returns:\n",
    "          the encoded feature\n",
    "        \"\"\"\n",
    "        lookup_class = StringLookup if is_string else IntegerLookup\n",
    "        # Create a lookup layer which will turn strings into integer indices\n",
    "        lookup = lookup_class(output_mode=\"binary\")\n",
    "\n",
    "        # Prepare a Dataset that only yields our feature\n",
    "        feature_ds = dataset.map(lambda x, y: x[name])\n",
    "        feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
    "\n",
    "        # Learn the set of possible string values and assign them a fixed integer index\n",
    "        lookup.adapt(feature_ds)\n",
    "\n",
    "        # Turn the string input into integer indices\n",
    "        encoded_feature = lookup(feature)\n",
    "        return encoded_feature\n",
    "\n",
    "    # Categorical features encoded as integers\n",
    "    sex = keras.Input(shape=(1,), name=\"sex\", dtype=\"int64\")\n",
    "    cp = keras.Input(shape=(1,), name=\"cp\", dtype=\"int64\")\n",
    "    fbs = keras.Input(shape=(1,), name=\"fbs\", dtype=\"int64\")\n",
    "    restecg = keras.Input(shape=(1,), name=\"restecg\", dtype=\"int64\")\n",
    "    exang = keras.Input(shape=(1,), name=\"exang\", dtype=\"int64\")\n",
    "    ca = keras.Input(shape=(1,), name=\"ca\", dtype=\"int64\")\n",
    "\n",
    "    # Categorical feature encoded as string\n",
    "    thal = keras.Input(shape=(1,), name=\"thal\", dtype=\"string\")\n",
    "\n",
    "    # Numerical features\n",
    "    age = keras.Input(shape=(1,), name=\"age\")\n",
    "    trestbps = keras.Input(shape=(1,), name=\"trestbps\")\n",
    "    chol = keras.Input(shape=(1,), name=\"chol\")\n",
    "    thalach = keras.Input(shape=(1,), name=\"thalach\")\n",
    "    oldpeak = keras.Input(shape=(1,), name=\"oldpeak\")\n",
    "    slope = keras.Input(shape=(1,), name=\"slope\")\n",
    "\n",
    "    all_inputs = [\n",
    "        sex,\n",
    "        cp,\n",
    "        fbs,\n",
    "        restecg,\n",
    "        exang,\n",
    "        ca,\n",
    "        thal,\n",
    "        age,\n",
    "        trestbps,\n",
    "        chol,\n",
    "        thalach,\n",
    "        oldpeak,\n",
    "        slope,\n",
    "    ]\n",
    "\n",
    "    # Integer categorical features\n",
    "    sex_encoded = encode_categorical_feature(sex, \"sex\", train_ds, False)\n",
    "    cp_encoded = encode_categorical_feature(cp, \"cp\", train_ds, False)\n",
    "    fbs_encoded = encode_categorical_feature(fbs, \"fbs\", train_ds, False)\n",
    "    restecg_encoded = encode_categorical_feature(restecg, \"restecg\", train_ds, False)\n",
    "    exang_encoded = encode_categorical_feature(exang, \"exang\", train_ds, False)\n",
    "    ca_encoded = encode_categorical_feature(ca, \"ca\", train_ds, False)\n",
    "\n",
    "    # String categorical features\n",
    "    thal_encoded = encode_categorical_feature(thal, \"thal\", train_ds, True)\n",
    "\n",
    "    # Numerical features\n",
    "    age_encoded = encode_numerical_feature(age, \"age\", train_ds)\n",
    "    trestbps_encoded = encode_numerical_feature(trestbps, \"trestbps\", train_ds)\n",
    "    chol_encoded = encode_numerical_feature(chol, \"chol\", train_ds)\n",
    "    thalach_encoded = encode_numerical_feature(thalach, \"thalach\", train_ds)\n",
    "    oldpeak_encoded = encode_numerical_feature(oldpeak, \"oldpeak\", train_ds)\n",
    "    slope_encoded = encode_numerical_feature(slope, \"slope\", train_ds)\n",
    "\n",
    "    all_features = layers.concatenate(\n",
    "        [\n",
    "            sex_encoded,\n",
    "            cp_encoded,\n",
    "            fbs_encoded,\n",
    "            restecg_encoded,\n",
    "            exang_encoded,\n",
    "            slope_encoded,\n",
    "            ca_encoded,\n",
    "            thal_encoded,\n",
    "            age_encoded,\n",
    "            trestbps_encoded,\n",
    "            chol_encoded,\n",
    "            thalach_encoded,\n",
    "            oldpeak_encoded,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    x = layers.Dense(n_units, activation=activation)(all_features)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    output = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(all_inputs, output)\n",
    "    model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    model.fit(train_ds, epochs=epochs, validation_data=val_ds)\n",
    "\n",
    "    m = model.evaluate(test_ds, return_dict=True)\n",
    "    metrics.metadata.update(\n",
    "        dict(\n",
    "            loss=m[\"loss\"],\n",
    "            accurancy=m[\"accuracy\"],\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model.save(model_metadata.uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUTnAo1EyVKR"
   },
   "source": [
    "构建机器学习管道\n",
    "\n",
    "构建和编译包括训练组件在内的管道"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p1XdCv2OyZui"
   },
   "outputs": [],
   "source": [
    "@dsl.pipeline(name=\"simple-tabular-pipeline\")\n",
    "def pipeline(\n",
    "    dataset_uri: str,\n",
    "    dataset_fraction_split: float,\n",
    "    dataset_batch: int,\n",
    "    random_state: int,\n",
    "    n_units: int,\n",
    "    activation: str,\n",
    "    dropout_rate: float,\n",
    "    epochs: int,\n",
    "):\n",
    "\n",
    "    tabular_trainer(\n",
    "        dataset_uri=dataset_uri,\n",
    "        dataset_fraction_split=dataset_fraction_split,\n",
    "        dataset_batch=dataset_batch,\n",
    "        random_state=random_state,\n",
    "        n_units=n_units,\n",
    "        activation=activation,\n",
    "        dropout_rate=dropout_rate,\n",
    "        epochs=epochs,\n",
    "    )\n",
    "\n",
    "\n",
    "compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"pipeline.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYbpX6bvyfK8"
   },
   "source": [
    "提交实验流水线运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X8Dtdt6hycgJ"
   },
   "outputs": [],
   "source": [
    "job = vertex_ai.PipelineJob(\n",
    "    display_name=\"my pipeline run\",\n",
    "    template_path=\"pipeline.json\",\n",
    "    job_id=f\"pipeline-{RUN_NAME}\",\n",
    "    pipeline_root=BUCKET_URI,\n",
    "    parameter_values={**params},\n",
    ")\n",
    "\n",
    "job.submit(experiment=EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ClIUq-V4zatH"
   },
   "source": [
    "检查管道实验状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L5jdjRZLzFux"
   },
   "outputs": [],
   "source": [
    "vertex_ai.get_experiment_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BTQPSGHfzLbW"
   },
   "outputs": [],
   "source": [
    "job.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UO6-6EFyzWCD"
   },
   "outputs": [],
   "source": [
    "vertex_ai.get_experiment_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ww-geF9GAviH"
   },
   "source": [
    "在云控制台中进行可视化实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iPqjDzcSAviI"
   },
   "outputs": [],
   "source": [
    "print(\"Vertex AI Experiments:\")\n",
    "print(\n",
    "    f\"https://console.cloud.google.com/ai/platform/experiments/experiments?folder=&organizationId=&project={PROJECT_ID}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "清理工作\n",
    "\n",
    "要清理此项目中使用的所有Google Cloud资源，您可以删除用于教程的[Google Cloud项目](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects)。\n",
    "\n",
    "否则，您可以删除此教程中创建的各个资源。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "659e822f49c1"
   },
   "outputs": [],
   "source": [
    "# Delete pipeline\n",
    "job.delete()\n",
    "\n",
    "# Delete experiment\n",
    "exp = vertex_ai.Experiment(EXPERIMENT_NAME)\n",
    "exp.delete(delete_backing_tensorboard_runs=True)\n",
    "\n",
    "# Delete Tensorboard\n",
    "vertex_ai_tb.delete()\n",
    "\n",
    "# Delete Artifacts\n",
    "artifacts_list = vertex_ai.Artifact.list()\n",
    "for artifact in artifacts_list:\n",
    "    vertex_ai.Artifact.delete(artifact)\n",
    "\n",
    "# Delete Contexts\n",
    "context_list = vertex_ai.Context.list()\n",
    "for context in context_list:\n",
    "    vertex_ai.Context.delete(context)\n",
    "\n",
    "\n",
    "# Delete Cloud Storage objects that were created\n",
    "delete_bucket = False\n",
    "\n",
    "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
    "    ! gsutil rm -rf {BUCKET_URI}"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "vertex_ai_model_experimentation.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
