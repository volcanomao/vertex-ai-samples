{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a6b56b1c7b76"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2d7a1a97d1ee"
   },
   "source": [
    "# Vertex AI：SDK BigQuery 自定义容器训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "et4hRnB9mrau"
   },
   "source": [
    "<table align=\"left\">\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/master/notebooks/community/sdk/SDK_BigQuery_Custom_Container_Training.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> 在Colab中运行\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/master/notebooks/community/sdk/SDK_BigQuery_Custom_Container_Training.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      在GitHub上查看\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/master/notebooks/community/sdk/SDK_BigQuery_Custom_Container_Training.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      在Vertex AI Workbench中打开\n",
    "    </a>\n",
    "  </td>                                                                                               \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wLMxmUTwn1td"
   },
   "source": [
    "### 概述\n",
    "\n",
    "这个笔记本使用bigquery数据集创建自定义容器，它将训练容器，并创建、训练和部署模型以执行预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "989999fbdab3"
   },
   "source": [
    "### 目标\n",
    "\n",
    "在这个笔记本中，您将学习如何使用Vertex AI实验来：\n",
    "\n",
    "* 记录管道作业\n",
    "* 比较不同的管道作业\n",
    "\n",
    "涵盖的步骤包括：\n",
    "\n",
    "* 规范化训练组件\n",
    "* 构建训练模型\n",
    "* 运行多个管道作业并记录它们的结果\n",
    "* 训练模型进行预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3d29af7e49d8"
   },
   "source": [
    "数据集\n",
    "\n",
    "本教程使用的数据集是[TensorFlow Datasets](https://www.tensorflow.org/datasets/catalog/overview)中的[Iris数据集](https://www.tensorflow.org/datasets/catalog/iris)。该数据集不需要任何特征工程。在本教程中使用的数据集版本存储在公共云存储桶中。训练好的模型可以预测出三个品种的鸢尾花物种：山鸢尾、弗吉尼亚鸢尾或变色鸢尾。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e3e924989cce"
   },
   "source": [
    "### 费用\n",
    "\n",
    "本教程使用了Google Cloud的可计费组件：\n",
    "\n",
    "- Vertex AI\n",
    "- 云存储\n",
    "\n",
    "了解[Vertex AI价格](https://cloud.google.com/vertex-ai/pricing)，[Bigquery价格](https://cloud.google.com/bigquery/pricing/)和[云存储价格](https://cloud.google.com/storage/pricing)，并使用[Pricing计算器](https://cloud.google.com/products/calculator/)根据您的预计使用量生成费用估算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g5dkyDy1obku"
   },
   "source": [
    "建立您的本地开发环境\n",
    "\n",
    "**如果您正在使用Colab或Google Cloud笔记本**，您的环境已经满足运行此笔记本的所有要求。 您可以跳过此步骤。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gyja44LCozU_"
   },
   "source": [
    "否则，请确保您的环境满足此笔记本的要求。\n",
    "您需要以下内容：\n",
    "\n",
    "* Google Cloud SDK\n",
    "* Git\n",
    "* Python 3\n",
    "* virtualenv\n",
    "* 在使用Python 3的虚拟环境中运行的Jupyter笔记本\n",
    "\n",
    "Google Cloud指南[设置Python开发环境](https://cloud.google.com/python/setup)和[Jupyter安装指南](https://jupyter.org/install)提供了满足这些要求的详细说明。以下步骤提供了简化的一套说明：\n",
    "\n",
    "1. [安装和初始化Cloud SDK。](https://cloud.google.com/sdk/docs/)\n",
    "\n",
    "2. [安装Python 3。](https://cloud.google.com/python/setup#installing_python)\n",
    "\n",
    "3. [安装virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv)并创建一个使用Python 3的虚拟环境。激活虚拟环境。\n",
    "\n",
    "4. 要安装Jupyter，在终端窗口的命令行中运行`pip3 install jupyter`。\n",
    "\n",
    "5. 要启动Jupyter，在终端窗口的命令行中运行`jupyter notebook`。\n",
    "\n",
    "6. 在Jupyter Notebook仪表板中打开此笔记本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JpYscdzmf4Gu"
   },
   "source": [
    "# 确保以下API已启用：\n",
    "- [BigQuery](https://console.cloud.google.com/apis/library/bigquery.googleapis.com?q=BigQuery)\n",
    "- [Cloudbuild](https://console.cloud.google.com/apis/library/cloudbuild.googleapis.com?q=Cloudbuild)\n",
    "- [Container Registry](https://console.cloud.google.com/apis/library/containerregistry.googleapis.com?q=container%20registry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xOMNWzTbftDr"
   },
   "source": [
    "安装额外的包\n",
    "\n",
    "在您的笔记本环境中安装未安装的额外包依赖项，如XGBoost、AdaNet或TensorFlow Hub。使用每个包的最新正式版本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Be020jY-ftDv"
   },
   "outputs": [],
   "source": [
    "!pip3 uninstall -y google-cloud-aiplatform\n",
    "!pip3 install google-cloud-aiplatform\n",
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4f317591f55"
   },
   "source": [
    "重新启动内核\n",
    "\n",
    "在安装附加包之后，您需要重新启动笔记本内核，这样它才能找到这些包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f731803a16c0"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "181b681faf5c"
   },
   "source": [
    "在下面的单元格中输入您的项目 ID 和 GCS 存储桶。\n",
    "\n",
    "在下面的单元格中输入您的项目 ID，然后运行该单元格，以确保 Cloud SDK 在这个笔记本中对所有命令使用正确的项目。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3c8049930470"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d61oYG3KftDw"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_ID = \"\"\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_HRQg6eXiolk"
   },
   "source": [
    "否则，请在这里设置您的项目ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sg6AbQRviolk"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None:\n",
    "    PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
    "    print(\"Project ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZaQd5jNwjP_0"
   },
   "source": [
    "### 认证您的谷歌云账户\n",
    "\n",
    "**如果您正在使用 Vertex AI Workbench Notebooks**，您的环境已经认证。请跳过此步骤。\n",
    "\n",
    "**如果您正在使用 Colab**，请运行下面的单元格，并按照提示进行身份验证，通过oAuth认证您的账户。\n",
    "\n",
    "**否则**，请按照以下步骤操作：\n",
    "\n",
    "1. 在云控制台中，转到[**创建服务账号密钥**页面](https://console.cloud.google.com/apis/credentials/serviceaccountkey)。\n",
    "\n",
    "2. 点击**创建服务账号**。\n",
    "\n",
    "3. 在**服务账号名称**字段中输入名称，然后点击**创建**。\n",
    "\n",
    "4. 在**授予此服务账号访问项目**部分，点击**角色**下拉列表。在过滤框中输入\"Vertex AI\"，选择**Vertex AI管理员**。在过滤框中输入\" Storage Object Admin\"，选择**存储对象管理员**。\n",
    "\n",
    "5. 点击*创建*。一个包含您密钥的JSON文件将下载到您的本地环境。\n",
    "\n",
    "6. 在下方单元格中将您的服务账号密钥路径输入为`GOOGLE_APPLICATION_CREDENTIALS`变量，并运行该单元格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "idpLDmlyjWyU"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "# The Google Cloud Notebook product has specific requirements\n",
    "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "\n",
    "# If on Google Cloud Notebooks, then don't execute this code\n",
    "if not IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\"):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r2lr6-MVpXLP"
   },
   "source": [
    "UUID\n",
    "\n",
    "如果您正在进行现场教程会话，您可能正在使用一个共享的测试帐户或项目。为了避免在创建的资源上发生名称冲突，您可以为每个实例会话创建一个uuid，并将其附加到您在本教程中创建的资源名称上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "he2lcG3Jpdxu"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "\n",
    "# Generate a uuid of a specifed length(default=8)\n",
    "def generate_uuid(length: int = 8) -> str:\n",
    "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
    "\n",
    "\n",
    "UUID = generate_uuid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VxkQeJLyjgyr"
   },
   "source": [
    "创建存储桶\n",
    "\n",
    "**无论您使用哪种笔记本环境，都需要执行以下步骤。**\n",
    "\n",
    "使用 Cloud SDK 提交训练作业时，您需要将包含训练代码的 Python 包上传到 Cloud 存储桶中。Vertex AI 会从这个包中运行代码。在本教程中，Vertex AI 还会将作业产生的训练模型保存在同一个存储桶中。使用这个模型工件，您可以创建 Vertex AI 模型和端点资源，以便提供在线预测。\n",
    "\n",
    "在下方设置您的 Cloud 存储桶的名称。它必须在所有 Cloud 存储桶中是唯一的。\n",
    "\n",
    "您还可以更改“REGION”变量，该变量会在此笔记本的其余部分中使用。我们建议您[选择一个支持 Vertex AI 服务的地区](https://cloud.google.com/vertex-ai/docs/general/locations#available_regions)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2f6f0f6ec383"
   },
   "outputs": [],
   "source": [
    "BUCKET_URI = \"gs://[your-bucket-name]\"  # @param {type:\"string\"}\n",
    "REGION = \"[your-region]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GF076Vmoioll"
   },
   "outputs": [],
   "source": [
    "if BUCKET_URI == \"\" or BUCKET_URI is None or BUCKET_URI == \"gs://[your-bucket-name]\":\n",
    "    BUCKET_URI = \"gs://\" + PROJECT_ID + \"aip-\" + UUID\n",
    "\n",
    "if REGION == \"[your-region]\":\n",
    "    REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ElTizrkXiolm"
   },
   "source": [
    "只有如果您的存储桶尚不存在：运行以下单元格以创建您的云存储存储桶。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ar8qPT6iolm"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_Hq3rtPiolm"
   },
   "source": [
    "最后，通过检查其内容来验证对您的云存储桶的访问。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_KfwVmnIiolm"
   },
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5T1d5uBoftDw"
   },
   "source": [
    "复制bigquery鸢尾花数据集\n",
    "\n",
    "您创建了一个BigQuery数据集，并将BigQuery的公共鸢尾花表复制到该数据集中。有关此数据集的更多信息，请访问：https://archive.ics.uci.edu/ml/datasets/iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJF047yNftDw"
   },
   "source": [
    "### 制作 BQ 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9yOl-l_oftDx"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
    "!bq mk {PROJECT_ID}:ml_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xn9TuBZAftDx"
   },
   "source": [
    "### 复制bigquery-public-data.ml_datasets.iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ISFR8nFfftDx"
   },
   "outputs": [],
   "source": [
    "!bq cp -n --project_id={PROJECT_ID} bigquery-public-data:ml_datasets.iris {PROJECT_ID}:ml_datasets.iris "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IltwFqKIftDx"
   },
   "source": [
    "# 创建训练容器\n",
    "我们将创建一个目录，并将所有容器构建产物写入该文件夹中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "40BkhtMeftDy"
   },
   "outputs": [],
   "source": [
    "CONTAINER_ARTIFACTS_DIR = \"demo-container-artifacts\"\n",
    "\n",
    "!mkdir {CONTAINER_ARTIFACTS_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iVeG-LPOftDy"
   },
   "source": [
    "创建Cloudbuild YAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4kODuFZCftDy"
   },
   "outputs": [],
   "source": [
    "cloudbuild_yaml = \"\"\"steps:\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  args: [ 'build', '-t', 'gcr.io/{PROJECT_ID}/test-custom-container', '.' ]\n",
    "images: ['gcr.io/{PROJECT_ID}/test-custom-container']\"\"\".format(\n",
    "    PROJECT_ID=PROJECT_ID\n",
    ")\n",
    "\n",
    "with open(f\"{CONTAINER_ARTIFACTS_DIR}/cloudbuild.yaml\", \"w\") as fp:\n",
    "    fp.write(cloudbuild_yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQ_GUCtZftDz"
   },
   "source": [
    "写这个 Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rja_jo3rftDz"
   },
   "outputs": [],
   "source": [
    "%%writefile {CONTAINER_ARTIFACTS_DIR}/Dockerfile\n",
    "\n",
    "# Specifies base image and tag\n",
    "FROM gcr.io/google-appengine/python\n",
    "WORKDIR /root\n",
    "\n",
    "# Installs additional packages\n",
    "RUN pip3 install tensorflow tensorflow-io pyarrow\n",
    "\n",
    "# Copies the trainer code to the docker image.\n",
    "COPY test_script.py /root/test_script.py\n",
    "\n",
    "# Sets up the entry point to invoke the trainer.\n",
    "ENTRYPOINT [\"python3\", \"test_script.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9dfrLShaftDz"
   },
   "source": [
    "### 编写入口脚本调用训练器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5cdc477cd73"
   },
   "source": [
    "入口脚本训练和验证数据，并且编译模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S0jSd8NWftDz"
   },
   "outputs": [],
   "source": [
    "%%writefile {CONTAINER_ARTIFACTS_DIR}/test_script.py\n",
    "\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow_io.bigquery import BigQueryClient\n",
    "from tensorflow_io.bigquery import BigQueryReadSession\n",
    "import tensorflow as tf\n",
    "from tensorflow import feature_column\n",
    "import os\n",
    "\n",
    "training_data_uri = os.environ[\"AIP_TRAINING_DATA_URI\"]\n",
    "validation_data_uri = os.environ[\"AIP_VALIDATION_DATA_URI\"]\n",
    "test_data_uri = os.environ[\"AIP_TEST_DATA_URI\"]\n",
    "data_format = os.environ[\"AIP_DATA_FORMAT\"]\n",
    "\n",
    "def caip_uri_to_fields(uri):\n",
    "    uri = uri[5:]\n",
    "    project, dataset, table = uri.split('.')\n",
    "    return project, dataset, table\n",
    "\n",
    "feature_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "\n",
    "target_name = 'species'\n",
    "\n",
    "def transform_row(row_dict):\n",
    "  # Trim all string tensors\n",
    "  trimmed_dict = { column:\n",
    "                  (tf.strings.strip(tensor) if tensor.dtype == 'string' else tensor) \n",
    "                  for (column,tensor) in row_dict.items()\n",
    "                  }\n",
    "  target = trimmed_dict.pop(target_name)\n",
    "\n",
    "  target_float = tf.cond(tf.equal(tf.strings.strip(target), 'versicolor'), \n",
    "                 lambda: tf.constant(1.0),\n",
    "                 lambda: tf.constant(0.0))\n",
    "  return (trimmed_dict, target_float)\n",
    "\n",
    "def read_bigquery(project, dataset, table):\n",
    "  tensorflow_io_bigquery_client = BigQueryClient()\n",
    "  read_session = tensorflow_io_bigquery_client.read_session(\n",
    "      \"projects/\" + project,\n",
    "      project, table, dataset,\n",
    "      feature_names + [target_name],\n",
    "      [dtypes.float64] * 4 + [dtypes.string],\n",
    "      requested_streams=2)\n",
    "\n",
    "  dataset = read_session.parallel_read_rows()\n",
    "  transformed_ds = dataset.map(transform_row)\n",
    "  return transformed_ds\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "training_ds = read_bigquery(*caip_uri_to_fields(training_data_uri)).shuffle(10).batch(BATCH_SIZE)\n",
    "eval_ds = read_bigquery(*caip_uri_to_fields(validation_data_uri)).batch(BATCH_SIZE)\n",
    "test_ds = read_bigquery(*caip_uri_to_fields(test_data_uri)).batch(BATCH_SIZE)\n",
    "\n",
    "feature_columns = []\n",
    "\n",
    "# numeric cols\n",
    "for header in feature_names:\n",
    "  feature_columns.append(feature_column.numeric_column(header))\n",
    "\n",
    "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n",
    "\n",
    "Dense = tf.keras.layers.Dense\n",
    "model = tf.keras.Sequential(\n",
    "  [\n",
    "    feature_layer,\n",
    "      Dense(16, activation=tf.nn.relu),\n",
    "      Dense(8, activation=tf.nn.relu),\n",
    "      Dense(4, activation=tf.nn.relu),\n",
    "      Dense(1, activation=tf.nn.sigmoid),\n",
    "  ])\n",
    "\n",
    "# Compile Keras model\n",
    "model.compile(\n",
    "    loss='binary_crossentropy', \n",
    "    metrics=['accuracy'],\n",
    "    optimizer='adam')\n",
    "\n",
    "model.fit(training_ds, epochs=5, validation_data=eval_ds)\n",
    "\n",
    "print(model.evaluate(test_ds))\n",
    "\n",
    "tf.saved_model.save(model, os.environ[\"AIP_MODEL_DIR\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6LYlV4D2ftD0"
   },
   "source": [
    "构建容器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9tGdX7B_ftD1"
   },
   "outputs": [],
   "source": [
    "!gcloud builds submit --project={PROJECT_ID} --config {CONTAINER_ARTIFACTS_DIR}/cloudbuild.yaml {CONTAINER_ARTIFACTS_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pf0pugbvftD1"
   },
   "source": [
    "# 运行自定义容器培训"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ee691569d8d"
   },
   "source": [
    "初始化Python的Vertex AI SDK\n",
    "\n",
    "初始化Vertex AI的客户端"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vEEr62NUftD1"
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "736ddff8408b"
   },
   "source": [
    "从前面复制的 iris BigQuery 表中创建一个托管的表格数据集。使用的参数是 BigQuery 的公共 iris 数据集。# 从 bigquery 数据集创建管理的表格数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oBdOv6lWftD1"
   },
   "outputs": [],
   "source": [
    "ds = aiplatform.TabularDataset.create(\n",
    "    display_name=\"bq_iris_dataset\", bq_source=f\"bq://{PROJECT_ID}.ml_datasets.iris\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ee242cc1f74c"
   },
   "source": [
    "启动训练工作以创建模型\n",
    "\n",
    "我们将使用上面构建的容器来训练模型。要训练模型，您可以使用CustomContainer TrainingJob方法，参数为Container Image和Container_uri。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uRGrFdxOftD1"
   },
   "outputs": [],
   "source": [
    "job = aiplatform.CustomContainerTrainingJob(\n",
    "    display_name=\"train-bq-iris\",\n",
    "    container_uri=f\"gcr.io/{PROJECT_ID}/test-custom-container:latest\",\n",
    "    model_serving_container_image_uri=\"gcr.io/cloud-aiplatform/prediction/tf2-cpu.2-2:latest\",\n",
    ")\n",
    "model = job.run(\n",
    "    ds,\n",
    "    replica_count=1,\n",
    "    model_display_name=\"bq-iris-model\",\n",
    "    bigquery_destination=f\"bq://{PROJECT_ID}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7fa9b59f919"
   },
   "source": [
    "部署模型\n",
    "\n",
    "部署您的模型，然后等待模型完成部署后再继续预测。对于预测部署方法，需要传入 machine_type 参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tEg2IDwPftD2"
   },
   "outputs": [],
   "source": [
    "endpoint = model.deploy(machine_type=\"n1-standard-4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4dbd6c650a03"
   },
   "source": [
    "做一个预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e4b04d246ba9"
   },
   "source": [
    "端点预测方法根据长度和宽度特征参数发布预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DKVhGB1PftD2"
   },
   "outputs": [],
   "source": [
    "prediction = endpoint.predict(\n",
    "    [{\"sepal_length\": 5.1, \"sepal_width\": 2.5, \"petal_length\": 3.0, \"petal_width\": 1.1}]\n",
    ")\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MaoIczP8qu--"
   },
   "source": [
    "清理工作\n",
    "\n",
    "要清理此项目中使用的所有谷歌云资源，您可以[删除您用于本教程的谷歌云项目](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects)。\n",
    "\n",
    "否则，您可以删除本教程中创建的各个资源：\n",
    "\n",
    "- 管道\n",
    "- 端点\n",
    "- 云存储桶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-UaP-qoKqzc1"
   },
   "outputs": [],
   "source": [
    "delete_pipeline = True\n",
    "delete_endpoint = True\n",
    "\n",
    "\n",
    "if delete_pipeline:\n",
    "    job.delete()\n",
    "\n",
    "    if delete_endpoint and \"DISPLAY_NAME\" in globals():\n",
    "        endpoints = aip.Endpoint.list(\n",
    "            filter=f\"display_name={DISPLAY_NAME}_endpoint\", order_by=\"create_time\"\n",
    "        )\n",
    "        if endpoints:\n",
    "            endpoint = endpoints[0]\n",
    "            endpoint.undeploy_all()\n",
    "            aip.Endpoint.delete(endpoint.resource_name)\n",
    "            print(\"Deleted endpoint:\", endpoint)\n",
    "\n",
    "\n",
    "# Delete bucket\n",
    "delete_bucket = False\n",
    "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
    "    ! gsutil rm -rf {BUCKET_URI}"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SDK_BigQuery_Custom_Container_Training.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
