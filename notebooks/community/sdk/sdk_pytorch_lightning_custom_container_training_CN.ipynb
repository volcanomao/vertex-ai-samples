{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/master/notebooks/community/sdk/pytorch_lightning_custom_container_training.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> 在Colab中运行\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/master/notebooks/community/sdk/pytorch_lightning_custom_container_training.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      在GitHub上查看\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## 概述\n",
    "\n",
    "本教程演示了如何使用Vertex AI SDK for Python来训练一个使用自定义容器和PyTorch Lightning的ResNet模型。模型训练代码取自PyTorch Lightning文档页面上的CIFAR-10训练示例：\n",
    "https://pytorch-lightning.readthedocs.io/en/stable/notebooks/lightning_examples/cifar10-baseline.html\n",
    "\n",
    "采取了两种训练方法：1）在单台计算机上使用多个GPU进行训练 2）在每个计算机上只使用单个GPU进行多机训练\n",
    "\n",
    "### 数据集\n",
    "\n",
    "以下是网站上的描述：CIFAR-10数据集由10个类别中的60000张32x32彩色图片组成，每个类别有6000张图片。训练图像有50000张，测试图像有10000张。\n",
    "https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "\n",
    "将使用Lightning Bolts datamodules加载数据集\n",
    "\n",
    "### 目标\n",
    "\n",
    "在本笔记本中，您将学习如何利用Vertex AI将现有的使用PyTorch Lightining训练的模型示例分布到GPU和多台计算机上进行训练\n",
    "\n",
    "    * 安装和导入库以在本地测试模型训练\n",
    "    * 初始化Vertex AI SDK\n",
    "    * 为训练创建自定义容器\n",
    "    * 创建一个Vertex AI TensorBoard\n",
    "    * 修改代码以传入参数，记录到TensorBoard，并将模型保存到Cloud Storage\n",
    "    * 在单台带有GPU的机器上运行Vertex AI训练任务\n",
    "    * 在多台单GPU附加的机器上运行Vertex AI训练任务\n",
    "    \n",
    "    \n",
    "### 成本\n",
    "\n",
    "本教程使用Google Cloud的计费组件：\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "了解[Vertex AI价格](https://cloud.google.com/vertex-ai/pricing)和[Cloud Storage价格](https://cloud.google.com/storage/pricing)，并使用[Pricing Calculator](https://cloud.google.com/products/calculator/)根据您的预期使用情况生成成本估算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ze4-nDLfK4pw"
   },
   "source": [
    "### 设置您的本地开发环境\n",
    "\n",
    "**如果您正在使用Colab或Google Cloud笔记本**，您的环境已经满足运行本笔记本的所有要求。您可以跳过此步骤。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCuSR8GkAgzl"
   },
   "source": [
    "否则，请确保您的环境符合本笔记本的要求。\n",
    "您需要以下内容：\n",
    "\n",
    "* Google Cloud SDK\n",
    "* Git\n",
    "* Python 3\n",
    "* virtualenv\n",
    "* 在使用 Python 3 的虚拟环境中运行的 Jupyter 笔记本\n",
    "\n",
    "Google Cloud 的 [设置 Python 开发环境指南](https://cloud.google.com/python/setup) 和 [Jupyter 安装指南](https://jupyter.org/install) 提供了满足这些要求的详细说明。以下步骤提供了简化的说明：\n",
    "\n",
    "1. [安装并初始化 Cloud SDK。](https://cloud.google.com/sdk/docs/)\n",
    "2. [安装 Python 3。](https://cloud.google.com/python/setup#installing_python)\n",
    "3. [安装 virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv) 并创建一个使用 Python 3 的虚拟环境。激活虚拟环境。\n",
    "4. 要安装 Jupyter，请在终端 shell 中的命令行中运行 `pip3 install jupyter`。\n",
    "5. 要启动 Jupyter，请在终端 shell 中的命令行中运行 `jupyter notebook`。\n",
    "6. 在 Jupyter Notebook 仪表板中打开此笔记本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "### 安装额外的包\n",
    "\n",
    "在您的笔记本环境中安装未安装的额外包依赖项"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2b4ef9b72d43"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Google Cloud Notebook product has specific requirements\n",
    "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "\n",
    "# Google Cloud Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wyy5Lbnzg5fi"
   },
   "outputs": [],
   "source": [
    "! pip3 install {USER_FLAG} --upgrade \"torch>=1.6, <1.9\"\n",
    "! pip3 install {USER_FLAG} --upgrade \"lightning-bolts\"\n",
    "! pip3 install {USER_FLAG} --upgrade git+https://github.com/PyTorchLightning/pytorch-lightning\n",
    "! pip3 install {USER_FLAG} --upgrade \"torchmetrics>=0.3\"\n",
    "! pip3 install {USER_FLAG} --upgrade \"torchvision\"\n",
    "! pip3 install {USER_FLAG} --upgrade google-cloud-aiplatform\n",
    "! pip3 install {USER_FLAG} --upgrade ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhq5zEbGg0XX"
   },
   "source": [
    "### 重新启动内核\n",
    "\n",
    "在安装完额外的软件包之后，您需要重新启动笔记本内核，以便它能找到这些软件包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EzrelQZ22IZj"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWEdiXsJg0XY"
   },
   "source": [
    "## 在开始之前\n",
    "\n",
    "### 选择 GPU 运行时\n",
    "\n",
    "**确保如果有这个选项的话，在 GPU 运行时中运行这个笔记本。在 Colab 中，选择“运行时 --> 更改运行时类型 > GPU”**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF1j6f9HApxa"
   },
   "source": [
    "### 设置您的Google Cloud项目\n",
    "\n",
    "**无论您使用什么笔记本环境，以下步骤都是必需的。**\n",
    "\n",
    "1. [选择或创建一个Google Cloud项目](https://console.cloud.google.com/cloud-resource-manager)。当您第一次创建账户时，您将获得$300的免费信用，可用于支付计算/存储成本。\n",
    "\n",
    "1. [确保为您的项目启用了计费](https://cloud.google.com/billing/docs/how-to/modify-project)。\n",
    "\n",
    "1. [启用Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com)。{TODO: 更新教程所需的API。编辑API名称，并更新链接以附加API ID，用逗号分隔每个API ID。例如，container.googleapis.com,cloudbuild.googleapis.com}\n",
    "\n",
    "1. 如果您在本地运行此笔记本，您需要安装[Cloud SDK](https://cloud.google.com/sdk)。\n",
    "\n",
    "1. 在下面的单元格中输入您的项目ID。然后运行该单元格，以确保Cloud SDK在本笔记本中的所有命令中使用正确的项目。\n",
    "\n",
    "**注意**：Jupyter会将以`!`开头的行视为shell命令，并将以`$`开头的Python变量插入这些命令中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WReHDGG5g0XY"
   },
   "source": [
    "#### 设置您的项目 ID\n",
    "\n",
    "**如果您不知道您的项目 ID**，您可以使用 `gcloud` 获取您的项目 ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oM1iC_MfAts1"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"\"\n",
    "\n",
    "import os\n",
    "\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJYoRfYng0XZ"
   },
   "source": [
    "否则，在这里设置您的项目ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "riG_qUokg0XZ"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None:\n",
    "    PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_service_account"
   },
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT = \"[your-service-account]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "575f7c610f83"
   },
   "outputs": [],
   "source": [
    "if (\n",
    "    SERVICE_ACCOUNT == \"\"\n",
    "    or SERVICE_ACCOUNT is None\n",
    "    or SERVICE_ACCOUNT == \"[your-service-account]\"\n",
    "):\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = !gcloud auth list 2>/dev/null\n",
    "    SERVICE_ACCOUNT = shell_output[2].strip().replace(\"*\", \"\").replace(\" \", \"\")\n",
    "    print(\"Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06571eb4063b"
   },
   "source": [
    "时间戳\n",
    "\n",
    "如果您在一个现场教程会话中，您可能正在使用一个共享的测试账户或项目。为了避免创建的资源之间的名称冲突，您可以为每个实例会话创建一个时间戳，并将其附加到您在本教程中创建的资源的名称上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "697568e92bd6"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dr--iN2kAylZ"
   },
   "source": [
    "### 验证您的Google Cloud账户\n",
    "\n",
    "**如果您正在使用Google Cloud笔记本**，您的环境已经通过验证。请跳过此步骤。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBCra4QMA2wR"
   },
   "source": [
    "如果您正在使用Colab，请运行下面的单元格，并按照提示进行oAuth账户验证。\n",
    "\n",
    "否则，请按照以下步骤操作：\n",
    "\n",
    "1. 在Cloud控制台中，转到[**创建服务账号密钥**页面](https://console.cloud.google.com/apis/credentials/serviceaccountkey)。\n",
    "\n",
    "2. 点击**创建服务账号**。\n",
    "\n",
    "3. 在**服务账号名称**字段中输入名称，然后点击**创建**。\n",
    "\n",
    "4. 在**授予此服务账号对项目的访问权限**部分，点击**角色**下拉列表。在筛选框中输入\"Vertex AI\"，并选择**Vertex AI管理员**。在筛选框中输入\"存储对象管理员\"，并选择**存储对象管理员**。\n",
    "\n",
    "5. 点击*创建*。包含您密钥的JSON文件将下载到您的本地环境。\n",
    "\n",
    "6.在下面的单元格中将您的服务账号密钥路径输入为`GOOGLE_APPLICATION_CREDENTIALS`变量，并运行该单元格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PyQmSRbKA8r-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "# The Google Cloud Notebook product has specific requirements\n",
    "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "\n",
    "# If on Google Cloud Notebooks, then don't execute this code\n",
    "if not IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\"):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "### 创建一个云存储桶\n",
    "\n",
    "**无论您的笔记本环境如何，都需要执行以下步骤。**\n",
    "\n",
    "\n",
    "使用 Cloud SDK 提交训练作业时，您需要将包含训练代码的 Python 包上传到一个云存储桶中。Vertex AI 将从这个包中运行代码。在本教程中，Vertex AI 还会将作业生成的训练模型保存在同一个存储桶中。通过使用这个模型工件，您可以创建 Vertex AI 模型和端点资源，以便提供在线预测。\n",
    "\n",
    "在下面设置你的云存储桶的名称。它必须在所有云存储桶中是唯一的。\n",
    "\n",
    "您还可以更改 `REGION` 变量，该变量用于本笔记本中的所有操作。我们建议您[选择一个支持 Vertex AI 服务的地区](https://cloud.google.com/vertex-ai/docs/general/locations#available_regions)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "BUCKET_URI = \"gs://[your-bucket-name]\"  # @param {type:\"string\"}\n",
    "REGION = \"[your-region]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cf221059d072"
   },
   "outputs": [],
   "source": [
    "if BUCKET_URI == \"\" or BUCKET_URI is None or BUCKET_URI == \"gs://[your-bucket-name]\":\n",
    "    BUCKET_URI = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP\n",
    "\n",
    "if REGION == \"[your-region]\":\n",
    "    REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EcIXiGsCePi"
   },
   "source": [
    "只有当您的存储桶尚不存在时才运行以下单元格来创建您的云存储桶。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIq7R4HZCfIc"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucvCsknMCims"
   },
   "source": [
    "最后，通过检查云存储桶的内容来验证对其的访问。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vhOb7YnwClBb"
   },
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "导入库并定义常量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRUOFELefqf1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from pl_bolts.datamodules import CIFAR10DataModule\n",
    "from pl_bolts.transforms.dataset_normalizations import cifar10_normalization\n",
    "from pytorch_lightning import LightningModule, Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torchmetrics.functional import accuracy\n",
    "\n",
    "seed_everything(7)\n",
    "\n",
    "PATH_DATASETS = os.environ.get(\"PATH_DATASETS\", \".\")\n",
    "AVAIL_GPUS = min(1, torch.cuda.device_count())\n",
    "BATCH_SIZE = 256 if AVAIL_GPUS else 64\n",
    "NUM_WORKERS = int(os.cpu_count() / 2)\n",
    "\n",
    "print(PATH_DATASETS)\n",
    "print(AVAIL_GPUS)\n",
    "print(BATCH_SIZE)\n",
    "print(NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "定义用于本地测试的训练函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bca47ea0e3c3"
   },
   "outputs": [],
   "source": [
    "train_transforms = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.RandomCrop(32, padding=4),\n",
    "        torchvision.transforms.RandomHorizontalFlip(),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        cifar10_normalization(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_transforms = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        cifar10_normalization(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "cifar10_dm = CIFAR10DataModule(\n",
    "    data_dir=PATH_DATASETS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    train_transforms=train_transforms,\n",
    "    test_transforms=test_transforms,\n",
    "    val_transforms=test_transforms,\n",
    ")\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    model = torchvision.models.resnet18(pretrained=False, num_classes=10)\n",
    "    model.conv1 = nn.Conv2d(\n",
    "        3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
    "    )\n",
    "    model.maxpool = nn.Identity()\n",
    "    return model\n",
    "\n",
    "\n",
    "class LitResnet(LightningModule):\n",
    "    def __init__(self, lr=0.05):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        self.model = create_model()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return F.log_softmax(out, dim=1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def evaluate(self, batch, stage=None):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, y)\n",
    "\n",
    "        if stage:\n",
    "            self.log(f\"{stage}_loss\", loss, prog_bar=True)\n",
    "            self.log(f\"{stage}_acc\", acc, prog_bar=True)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self.evaluate(batch, \"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self.evaluate(batch, \"test\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams.lr,\n",
    "            momentum=0.9,\n",
    "            weight_decay=5e-4,\n",
    "        )\n",
    "        steps_per_epoch = 45000 // BATCH_SIZE\n",
    "        scheduler_dict = {\n",
    "            \"scheduler\": OneCycleLR(\n",
    "                optimizer,\n",
    "                0.1,\n",
    "                epochs=self.trainer.max_epochs,\n",
    "                steps_per_epoch=steps_per_epoch,\n",
    "            ),\n",
    "            \"interval\": \"step\",\n",
    "        }\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler_dict}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71da4176a4cc"
   },
   "source": [
    "### 在本地训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "af65cc43993c"
   },
   "outputs": [],
   "source": [
    "model = LitResnet(lr=0.05)\n",
    "model.datamodule = cifar10_dm\n",
    "\n",
    "trainer = Trainer(\n",
    "    progress_bar_refresh_rate=10,\n",
    "    max_epochs=5,\n",
    "    gpus=AVAIL_GPUS,\n",
    "    logger=TensorBoardLogger(\"lightning_logs/\", name=\"resnet\"),\n",
    "    callbacks=[LearningRateMonitor(logging_interval=\"step\")],\n",
    "    strategy=\"dp\",\n",
    ")\n",
    "\n",
    "trainer.fit(model, cifar10_dm)\n",
    "trainer.test(model, datamodule=cifar10_dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4eb55916219c"
   },
   "source": [
    "使用Vertex AI SDK和自定义容器进行Vertex AI培训."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e61fc5c1b9e0"
   },
   "source": [
    "### 构建自定义容器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "be10c827e494"
   },
   "source": [
    "运行这些步骤一次来设置工件注册表并授权docker使用它"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "667b03930fc3"
   },
   "outputs": [],
   "source": [
    "! gcloud config set project $PROJECT_ID\n",
    "! gcloud services enable artifactregistry.googleapis.com\n",
    "! sudo usermod -a -G docker ${USER}\n",
    "! gcloud auth configure-docker us-central1-docker.pkg.dev --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2647eb53957f"
   },
   "outputs": [],
   "source": [
    "REPOSITORY = \"gpu-training-repository\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "37973b9d3ca3"
   },
   "outputs": [],
   "source": [
    "! gcloud artifacts repositories create $REPOSITORY --repository-format=docker \\\n",
    "--location=$REGION --description=\"Vertex GPU training repository\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2cfd36ceacd"
   },
   "source": [
    "创建一个教练目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "05fa93664880"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.mkdir(\"trainer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "27f1c07650b0"
   },
   "source": [
    "构建容器\n",
    "\n",
    "此代码扩展了原始示例，并添加了参数解析、TensorBoard 日志记录、选择训练策略的能力以及将模型保存到云存储的功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "89f63d109973"
   },
   "outputs": [],
   "source": [
    "%%writefile trainer/task.py\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from pl_bolts.datamodules import CIFAR10DataModule\n",
    "from pl_bolts.transforms.dataset_normalizations import cifar10_normalization\n",
    "from pytorch_lightning import LightningModule, Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.optim.swa_utils import AveragedModel, update_bn\n",
    "from torchmetrics.functional import accuracy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from pl_bolts.datamodules import CIFAR10DataModule\n",
    "from pl_bolts.transforms.dataset_normalizations import cifar10_normalization\n",
    "from pytorch_lightning import LightningModule, Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.optim.swa_utils import AveragedModel, update_bn\n",
    "from torchmetrics.functional import accuracy\n",
    "\n",
    "# Arg parsing and shutil for folder creation\n",
    "import argparse\n",
    "import shutil\n",
    "\n",
    "seed_everything(7)\n",
    "\n",
    "PATH_DATASETS = os.environ.get(\"PATH_DATASETS\", \".\")\n",
    "AVAIL_GPUS = min(1, torch.cuda.device_count())\n",
    "BATCH_SIZE = 256 if AVAIL_GPUS else 64\n",
    "NUM_WORKERS = int(os.cpu_count() / 2)\n",
    "\n",
    "print (PATH_DATASETS)\n",
    "print (AVAIL_GPUS)\n",
    "print (BATCH_SIZE)\n",
    "print (NUM_WORKERS)\n",
    "\n",
    "train_transforms = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.RandomCrop(32, padding=4),\n",
    "        torchvision.transforms.RandomHorizontalFlip(),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        cifar10_normalization(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_transforms = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        cifar10_normalization(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "cifar10_dm = CIFAR10DataModule(\n",
    "    data_dir=PATH_DATASETS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    train_transforms=train_transforms,\n",
    "    test_transforms=test_transforms,\n",
    "    val_transforms=test_transforms,\n",
    ")\n",
    "\n",
    "# Added code to read args\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--epochs', dest='epochs',\n",
    "                        default=10, type=int,\n",
    "                        help='Number of epochs.')\n",
    "    parser.add_argument('--distribute', dest='distribute', type=str, default='dp',\n",
    "                        help='Distributed training strategy.')\n",
    "    parser.add_argument('--num-nodes', dest='num_nodes',\n",
    "                        default=1, type=int,\n",
    "                        help='Number of nodes')\n",
    "    parser.add_argument(\n",
    "          '--model-dir', dest='model_dir', default=os.getenv('AIP_MODEL_DIR'), type=str,\n",
    "          help='a Cloud Storage URI of a directory intended for saving model artifacts')\n",
    "    parser.add_argument(\n",
    "          '--tensorboard-log-dir', dest='tensorboard_log_dir', default=os.getenv('AIP_TENSORBOARD_LOG_DIR'), type=str,\n",
    "          help='a Cloud Storage URI of a directory intended for saving TensorBoard')\n",
    "    parser.add_argument(\n",
    "          '--checkpoint-dir', dest='checkpoint_dir', default=os.getenv('AIP_CHECKPOINT_DIR'), type=str,\n",
    "          help='a Cloud Storage URI of a directory intended for saving checkpoints')\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "# Cunction to make model directory if it doesn't exist\n",
    "def makedirs(model_dir):\n",
    "    if os.path.exists(model_dir) and os.path.isdir(model_dir):\n",
    "        shutil.rmtree(model_dir)\n",
    "    os.makedirs(model_dir)\n",
    "    return\n",
    "\n",
    "def create_model():\n",
    "    model = torchvision.models.resnet18(pretrained=False, num_classes=10)\n",
    "    model.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "    model.maxpool = nn.Identity()\n",
    "    return model\n",
    "\n",
    "class LitResnet(LightningModule):\n",
    "    def __init__(self, lr=0.05):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        self.model = create_model()\n",
    "\n",
    "    # TensorBoard logging at epoch end\n",
    "    def training_epoch_end(self,outputs):\n",
    "        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "\n",
    "        tensorboard_logs = {'loss': avg_loss}\n",
    "\n",
    "        epoch_dictionary={'loss': avg_loss,'log': tensorboard_logs}\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return F.log_softmax(out, dim=1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def evaluate(self, batch, stage=None):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, y)\n",
    "\n",
    "        if stage:\n",
    "            self.log(f\"{stage}_loss\", loss, prog_bar=True)\n",
    "            self.log(f\"{stage}_acc\", acc, prog_bar=True)\n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self.evaluate(batch, \"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self.evaluate(batch, \"test\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams.lr,\n",
    "            momentum=0.9,\n",
    "            weight_decay=5e-4,\n",
    "        )\n",
    "        steps_per_epoch = 45000 // BATCH_SIZE\n",
    "        scheduler_dict = {\n",
    "            \"scheduler\": OneCycleLR(\n",
    "                optimizer,\n",
    "                0.1,\n",
    "                epochs=self.trainer.max_epochs,\n",
    "                steps_per_epoch=steps_per_epoch,\n",
    "            ),\n",
    "            \"interval\": \"step\",\n",
    "        }\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler_dict}\n",
    "\n",
    "def main():   \n",
    "\n",
    "    # Parse args\n",
    "    args = parse_args()\n",
    "    print (f\"Args={args}\")\n",
    "    print (f\"model directory={args.epochs}\")\n",
    "    print (f\"model directory={args.model_dir}\")\n",
    "    print (f\"distribute strategy={args.distribute}\")\n",
    "\n",
    "    # model, ensorboard, and checkpoint directories set\n",
    "    local_model_dir = './tmp/model'\n",
    "    local_tensorboard_log_dir = './tmp/logs'\n",
    "    local_checkpoint_dir = './tmp/checkpoints'\n",
    "\n",
    "    model_dir = args.model_dir or local_model_dir\n",
    "    tensorboard_log_dir = args.tensorboard_log_dir or local_tensorboard_log_dir\n",
    "    checkpoint_dir = args.checkpoint_dir or local_checkpoint_dir\n",
    "\n",
    "    print (\"Model directory\" + model_dir)\n",
    "    print (\"TensorBoard directory\" + tensorboard_log_dir)\n",
    "    print (\"Checkpoint directory\" + checkpoint_dir)\n",
    "\n",
    "    gs_prefix = 'gs://'\n",
    "    gcsfuse_prefix = '/gcs/'\n",
    "    if model_dir and model_dir.startswith(gs_prefix):\n",
    "        model_dir = model_dir.replace(gs_prefix, gcsfuse_prefix)\n",
    "        if not os.path.isdir(model_dir):\n",
    "            os.makedirs(model_dir)\n",
    "    if tensorboard_log_dir and tensorboard_log_dir.startswith(gs_prefix):\n",
    "        tensorboard_log_dir = tensorboard_log_dir.replace(gs_prefix, gcsfuse_prefix)\n",
    "        if not os.path.isdir(tensorboard_log_dir):\n",
    "            os.makedirs(tensorboard_log_dir)\n",
    "    if checkpoint_dir and checkpoint_dir.startswith(gs_prefix):\n",
    "        checkpoint_dir = checkpoint_dir.replace(gs_prefix, gcsfuse_prefix)\n",
    "        if not os.path.isdir(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "\n",
    "    model = LitResnet(lr=0.05)\n",
    "    model.datamodule = cifar10_dm\n",
    "\n",
    "    trainer = Trainer(\n",
    "        progress_bar_refresh_rate=10,\n",
    "        gpus=AVAIL_GPUS, \n",
    "        logger=TensorBoardLogger(tensorboard_log_dir, \"resnet\"), \n",
    "        callbacks=[LearningRateMonitor(logging_interval=\"step\")],\n",
    "        # Changes to use args, change default checkpoint dir, and set number of nodes\n",
    "        max_epochs=args.epochs,\n",
    "        strategy=args.distribute,\n",
    "        default_root_dir=checkpoint_dir,\n",
    "        num_nodes=args.num_nodes,\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, cifar10_dm)\n",
    "    trainer.test(model, datamodule=cifar10_dm)\n",
    "\n",
    "    #Save model step\n",
    "    model_name = \"pylightning_resnet_state_dict.pth\"\n",
    "\n",
    "    model_save_path = os.path.join(model_dir, model_name)\n",
    "    if trainer.global_rank == 0:\n",
    "        makedirs(model_dir)\n",
    "        print(\"Saving model to {}\".format(model_save_path))\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d3682bd84fda"
   },
   "source": [
    "#### 配置容器名称和制品注册表路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c70a580cc6d4"
   },
   "outputs": [],
   "source": [
    "content_name = \"pytorch-lightning-gpu-training\"\n",
    "hostname = f\"{REGION}-docker.pkg.dev\"\n",
    "image_name_train = content_name\n",
    "tag = \"latest\"\n",
    "\n",
    "custom_container_image_uri_train = (\n",
    "    f\"{hostname}/{PROJECT_ID}/{REPOSITORY}/{image_name_train}:{tag}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a45380a286c8"
   },
   "source": [
    "创建 requirements.txt 和 Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1cc1a3438482"
   },
   "outputs": [],
   "source": [
    "%%writefile trainer/requirements.txt\n",
    "torch>=1.6, <1.9\n",
    "lightning-bolts\n",
    "pytorch-lightning>=1.3\n",
    "torchmetrics>=0.3\n",
    "torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "51a958343b27"
   },
   "outputs": [],
   "source": [
    "%%writefile trainer/Dockerfile\n",
    "FROM pytorch/pytorch:1.8.1-cuda11.1-cudnn8-runtime\n",
    "\n",
    "COPY . /trainer\n",
    "\n",
    "WORKDIR /trainer\n",
    "\n",
    "RUN pip install -r requirements.txt\n",
    "\n",
    "ENTRYPOINT [\"python\", \"task.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9a7e8f8f534f"
   },
   "source": [
    "创建一个空的 __init__.py 文件，需要放在容器中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "459b9d5a8965"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "with open(os.path.join(\"trainer\", \"__init__.py\"), \"w\") as fp:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7b36b457aa5d"
   },
   "source": [
    "#### 在容器中构建，本地训练模型，并推送到Artifact Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e58c75158872"
   },
   "outputs": [],
   "source": [
    "! cd trainer && docker build -t $custom_container_image_uri_train -f Dockerfile ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bde55966086a"
   },
   "outputs": [],
   "source": [
    "! docker run --rm $custom_container_image_uri_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "34c407c3be0e"
   },
   "outputs": [],
   "source": [
    "! docker push $custom_container_image_uri_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "526115eabf35"
   },
   "outputs": [],
   "source": [
    "! gcloud artifacts repositories describe $REPOSITORY --location=$REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f026b57d265e"
   },
   "source": [
    "初始化顶点 SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a0deece1086e"
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(\n",
    "    project=PROJECT_ID,\n",
    "    staging_bucket=BUCKET_URI,\n",
    "    location=REGION,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ec68e279ed1"
   },
   "source": [
    "创建一个Vertex AI TensorBoard实例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d945f0e32b02"
   },
   "outputs": [],
   "source": [
    "tensorboard = aiplatform.Tensorboard.create(\n",
    "    display_name=content_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cb13cc82acc5"
   },
   "source": [
    "选项：使用先前创建的Vertex AI TensorBoard实例\n",
    "\n",
    "```\n",
    "tensorboard_name = \"您的TensorBoard资源名称或TensorBoard ID\"\n",
    "tensorboard = aiplatform.Tensorboard(tensorboard_name=tensorboard_name)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0e526882fbda"
   },
   "source": [
    "使用多个GPU运行Vertex AI SDK自定义容器训练作业"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "93e74e00996c"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y-%m-%d-%H%M%S\")\n",
    "print(TIMESTAMP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e38c06b9377f"
   },
   "source": [
    "设置训练的参数。模型/TensorBoard/检查点目录使用 Vertex 默认值。取消注释以设置自己的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8494564137b0"
   },
   "outputs": [],
   "source": [
    "gcs_output_uri_prefix = f\"{BUCKET_URI}/{content_name}-{TIMESTAMP}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6c999fe5df46"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 30\n",
    "TRAIN_STRATEGY = \"dp\"  # Distributed Parallel for single machine multiple GPU\n",
    "MODEL_DIR = f\"{BUCKET_URI}/{content_name}/model\"\n",
    "TB_DIR = f\"{BUCKET_URI}/{content_name}/logs\"\n",
    "CHKPT_DIR = f\"{BUCKET_URI}/{content_name}/checkpoints\"\n",
    "NUM_NODES = 1\n",
    "\n",
    "machine_type = \"n1-standard-4\"\n",
    "accelerator_count = 2\n",
    "accelerator_type = \"NVIDIA_TESLA_V100\"\n",
    "\n",
    "CMDARGS = [\n",
    "    \"--epochs=\" + str(EPOCHS),\n",
    "    \"--distribute=\" + TRAIN_STRATEGY,\n",
    "    \"--num-nodes=\" + str(NUM_NODES),\n",
    "    \"--model-dir=\" + MODEL_DIR,\n",
    "    \"--checkpoint-dir=\" + CHKPT_DIR,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "db9de8eec4c4"
   },
   "outputs": [],
   "source": [
    "custom_container_training_job = aiplatform.CustomContainerTrainingJob(\n",
    "    display_name=content_name + \"-MultGPU-dp-\" + TIMESTAMP,\n",
    "    container_uri=custom_container_image_uri_train,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fa5f67712789"
   },
   "outputs": [],
   "source": [
    "custom_container_training_job.run(\n",
    "    args=CMDARGS,\n",
    "    replica_count=NUM_NODES,\n",
    "    base_output_dir=gcs_output_uri_prefix,\n",
    "    machine_type=machine_type,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    tensorboard=tensorboard.resource_name,\n",
    "    sync=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "29b14f2289f3"
   },
   "outputs": [],
   "source": [
    "print(f\"Custom Training Job Name: {custom_container_training_job.resource_name}\")\n",
    "print(f\"GCS Output URI Prefix: {gcs_output_uri_prefix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e38c06b9377f"
   },
   "source": [
    "删除训练作业"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d681091fd23d"
   },
   "outputs": [],
   "source": [
    "custom_container_training_job.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5f2b541299e9"
   },
   "source": [
    "在多台机器上使用每台1个GPU运行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b15bd8c318d8"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 30\n",
    "TRAIN_STRATEGY = \"ddp\"  # Distributed Parallel for single machine multiple GPU\n",
    "MODEL_DIR = f\"{BUCKET_URI}/{content_name}-ddp/model\"\n",
    "TB_DIR = f\"{BUCKET_URI}/{content_name}-ddp/logs\"\n",
    "CHKPT_DIR = f\"{BUCKET_URI}/{content_name}-ddp/checkpoints\"\n",
    "NUM_NODES = 2\n",
    "\n",
    "machine_type = \"n1-standard-4\"\n",
    "accelerator_count = 1\n",
    "accelerator_type = \"NVIDIA_TESLA_V100\"\n",
    "\n",
    "CMDARGS = [\n",
    "    \"--epochs=\" + str(EPOCHS),\n",
    "    \"--distribute=\" + TRAIN_STRATEGY,\n",
    "    \"--num-nodes=\" + str(NUM_NODES),\n",
    "    \"--model-dir=\" + MODEL_DIR,\n",
    "    \"--checkpoint-dir=\" + CHKPT_DIR,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a4ca2f0e0f00"
   },
   "outputs": [],
   "source": [
    "custom_container_training_job_dist = aiplatform.CustomContainerTrainingJob(\n",
    "    display_name=content_name + \"-MultiCPU-1GPU-ddp-\" + TIMESTAMP,\n",
    "    container_uri=custom_container_image_uri_train,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8fbe3a5549a1"
   },
   "outputs": [],
   "source": [
    "custom_container_training_job_dist.run(\n",
    "    args=CMDARGS,\n",
    "    replica_count=NUM_NODES,\n",
    "    base_output_dir=gcs_output_uri_prefix,\n",
    "    machine_type=machine_type,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    tensorboard=tensorboard.resource_name,\n",
    "    sync=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "29b14f2289f3"
   },
   "outputs": [],
   "source": [
    "print(f\"Custom Training Job Name: {custom_container_training_job_dist.resource_name}\")\n",
    "print(f\"GCS Output URI Prefix: {gcs_output_uri_prefix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "清理\n",
    "\n",
    "要清理此项目中使用的所有Google Cloud资源，您可以[删除用于教程的Google Cloud项目](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects)。\n",
    "\n",
    "否则，您可以删除在本教程中创建的各个资源。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sx_vKniMq9ZX"
   },
   "outputs": [],
   "source": [
    "# Warning: Setting this to true will delete everything in your bucket\n",
    "delete_bucket = False\n",
    "\n",
    "# Delete TensorBoard\n",
    "TB_NAME = tensorboard.resource_name\n",
    "! gcloud beta ai tensorboards delete $TB_NAME --quiet\n",
    "\n",
    "# Delete the training job\n",
    "custom_container_training_job_dist.delete()\n",
    "\n",
    "CONTENT_DIR = f\"{BUCKET_URI}/{content_name}*\"\n",
    "# Delete Cloud Storage objects that were created\n",
    "! gsutil -m rm -r $CONTENT_DIR\n",
    "\n",
    "if delete_bucket and \"BUCKET_URI\" in globals():\n",
    "    ! gsutil -m rm -r $BUCKET_URI"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "sdk_pytorch_lightning_custom_container_training.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
