{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "copyright"
   },
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# Vertex SDK：自动学习表格分类模型用于在线解释\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/tree/master/notebooks/official/automl/sdk_automl_tabular_classification_online_explain.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> 在 Colab 中运行\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/tree/master/notebooks/official/automl/sdk_automl_tabular_classification_online_explain.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"> 查看在 GitHub 上\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/ai/platform/notebooks/deploy-notebook?download_url=https://github.com/GoogleCloudPlatform/vertex-ai-samples/tree/master/notebooks/official/automl/sdk_automl_tabular_classification_online_explain.ipynb\">\n",
    "      在 Google Cloud Notebooks 中打开\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "<br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "overview:automl,xai"
   },
   "source": [
    "## 概述\n",
    "\n",
    "本教程演示了如何使用Vertex SDK创建表格分类模型，并使用Google Cloud的AutoML模型进行在线预测并提供解释。详细步骤请参考[AutoML用户指南](https://cloud.google.com/vertex-ai/docs/start/automl-users)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset:iris,lcn"
   },
   "source": [
    "### 数据集\n",
    "\n",
    "本教程使用的数据集是来自[TensorFlow数据集](https://www.tensorflow.org/datasets/catalog/iris)的[Iris数据集](https://www.tensorflow.org/datasets/catalog/overview)。这个数据集不需要任何特征工程。您在本教程中将使用的数据集版本存储在公共云存储桶中。训练好的模型可以预测三种类别（setosa、virginica、versicolor）中的鸢尾花品种类型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "objective:automl,training,online_prediction,xai"
   },
   "source": [
    "### 目标\n",
    "\n",
    "在本教程中，您将使用Vertex SDK从Python脚本创建一个AutoML表格分类模型，并部署用于在线预测，同时具有可解释性。您也可以选择使用`gcloud`命令行工具或在Cloud Console上在线创建和部署模型。\n",
    "\n",
    "执行的步骤包括：\n",
    "\n",
    "- 创建一个Vertex`Dataset`资源。\n",
    "- 训练模型。\n",
    "- 查看模型评估。\n",
    "- 将`Model`资源部署到服务`Endpoint`资源。\n",
    "- 使用可解释性进行预测请求。\n",
    "- 撤销模型部署。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "costs"
   },
   "source": [
    "### 费用\n",
    "\n",
    "本教程使用Google Cloud的计费组件：\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "了解 [Vertex AI 的定价](https://cloud.google.com/vertex-ai/pricing) 和 [Cloud Storage 的定价](https://cloud.google.com/storage/pricing)，并使用 [定价计算器](https://cloud.google.com/products/calculator/) 根据您的预期使用量生成费用估算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_local"
   },
   "source": [
    "### 设置您的本地开发环境\n",
    "\n",
    "如果您正在使用Colab或谷歌云笔记本，您的环境已经满足运行此笔记本的所有要求。您可以跳过这一步。\n",
    "\n",
    "否则，请确保您的环境满足此笔记本的要求。您需要以下内容：\n",
    "\n",
    "- 云存储 SDK\n",
    "- Git\n",
    "- Python 3\n",
    "- virtualenv\n",
    "- 在使用Python 3的虚拟环境中运行的Jupyter笔记本\n",
    "\n",
    "[设置Python开发环境的云存储指南](https://cloud.google.com/python/setup)和[Jupyter安装指南](https://jupyter.org/install)提供了满足这些要求的详细说明。以下步骤提供了一组简洁的指令：\n",
    "\n",
    "1. [安装和初始化SDK](https://cloud.google.com/sdk/docs/)。\n",
    "\n",
    "2. [安装Python 3](https://cloud.google.com/python/setup#installing_python)。\n",
    "\n",
    "3. [安装virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv)并创建一个使用Python 3的虚拟环境。激活虚拟环境。\n",
    "\n",
    "4. 要安装Jupyter，在终端中运行`pip3 install jupyter`。\n",
    "\n",
    "5. 要启动Jupyter，在终端中运行`jupyter notebook`。\n",
    "\n",
    "6. 在Jupyter Notebook仪表板中打开此笔记本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_aip:mbsdk"
   },
   "source": [
    "## 安装\n",
    "\n",
    "安装最新版本的Python Vertex SDK。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Google Cloud Notebook\n",
    "if os.path.exists(\"/opt/deeplearning/metadata/env_version\"):\n",
    "    USER_FLAG = \"--user\"\n",
    "else:\n",
    "    USER_FLAG = \"\"\n",
    "\n",
    "! pip3 install --upgrade google-cloud-aiplatform $USER_FLAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_storage"
   },
   "source": [
    "安装最新的GA版本*google-cloud-storage*库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_storage"
   },
   "outputs": [],
   "source": [
    "! pip3 install -U google-cloud-storage $USER_FLAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_tabulate"
   },
   "source": [
    "安装最新的*Tabulate*库GA版本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_tabulate"
   },
   "outputs": [],
   "source": [
    "! pip3 install -U tabulate $USER_FLAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_tensorflow"
   },
   "outputs": [],
   "source": [
    "if os.environ[\"IS_TESTING\"]:\n",
    "    ! pip3 install --upgrade tensorflow $USER_FLAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "restart"
   },
   "source": [
    "### 重新启动内核\n",
    "\n",
    "安装了额外的软件包后，您需要重新启动笔记本内核，以便它能够找到这些软件包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "restart"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "before_you_begin:nogpu"
   },
   "source": [
    "开始之前\n",
    "\n",
    "GPU 运行时\n",
    "\n",
    "本教程不需要使用 GPU 运行时。\n",
    "\n",
    "设置您的 Google 云项目\n",
    "\n",
    "**无论您使用何种笔记本环境，以下步骤都是必需的。**\n",
    "\n",
    "1. [选择或创建一个 Google 云项目](https://console.cloud.google.com/cloud-resource-manager)。首次建立帐户时，您将获得 $300 的免费信用额度用于计算和存储成本。\n",
    "\n",
    "2. [确保您的项目已启用计费。](https://cloud.google.com/billing/docs/how-to/modify-project)\n",
    "\n",
    "3. [启用以下 API：Vertex AI API、Compute Engine API 和 Cloud Storage。](https://console.cloud.google.com/flows/enableapi?apiid=ml.googleapis.com,compute_component,storage-component.googleapis.com)\n",
    "\n",
    "4. 如果您在本地运行这个笔记本，您需要安装 [Cloud SDK](https://cloud.google.com/sdk)。\n",
    "\n",
    "5. 在下面的单元格中输入您的项目 ID。然后运行该单元格，确保 Cloud SDK 在本笔记本中的所有命令中使用正确的项目。\n",
    "\n",
    "**注意：** Jupyter 运行以 `!` 为前缀的行作为 shell 命令，并且使用以 `$` 为前缀的 Python 变量插值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_project_id"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "autoset_project_id"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID:\", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_gcloud_project_id"
   },
   "outputs": [],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "#### 地区\n",
    "\n",
    "您还可以更改“REGION”变量，该变量用于本笔记本之余的操作。以下是Vertex AI支持的地区。我们建议您选择离您最近的地区。\n",
    "\n",
    "- 美洲：`us-central1`\n",
    "- 欧洲：`europe-west4`\n",
    "- 亚太：`asia-east1`\n",
    "\n",
    "您可能不会使用多区域存储桶来进行Vertex AI训练。并非所有地区都支持所有Vertex AI服务。\n",
    "\n",
    "了解有关[Vertex AI地区](https://cloud.google.com/vertex-ai/docs/general/locations)的更多信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "region"
   },
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "timestamp"
   },
   "source": [
    "时间戳\n",
    "\n",
    "如果您正在进行实时教程会话，您可能正在使用共享测试账户或项目。为了避免在创建的资源上发生名称冲突，您可以为每个实例会话创建一个时间戳，并将时间戳附加到您在此教程中创建的资源名称上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "timestamp"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcp_authenticate"
   },
   "source": [
    "### 验证您的Google Cloud帐户\n",
    "\n",
    "**如果您正在使用Google Cloud笔记本**，您的环境已经经过验证。跳过此步骤。\n",
    "\n",
    "**如果您正在使用Colab**，运行下面的单元格，并按照提示进行身份验证，通过oAuth验证您的帐户。\n",
    "\n",
    "**否则**，请按照以下步骤操作：\n",
    "\n",
    "在Cloud Console中，转到 [创建服务帐号密钥](https://console.cloud.google.com/apis/credentials/serviceaccountkey) 页面。\n",
    "\n",
    "**点击创建服务帐号**。\n",
    "\n",
    "在**服务帐号名称**字段中输入一个名称，然后点击**创建**。\n",
    "\n",
    "在**授予此服务帐号对项目的访问权限**部分，点击权限下拉列表。在过滤框中输入“Vertex”，然后选择**Vertex管理员**。在过滤框中输入“存储对象管理员”，然后选择**存储对象管理员**。\n",
    "\n",
    "点击创建。一个包含您密钥的JSON文件将下载到您的本地环境。\n",
    "\n",
    "在下面的单元格中将您的服务帐号密钥路径作为GOOGLE_APPLICATION_CREDENTIALS变量输入，并运行该单元格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gcp_authenticate"
   },
   "outputs": [],
   "source": [
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# If on Google Cloud Notebook, then don't execute this code\n",
    "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\"):\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\"):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bucket:mbsdk"
   },
   "source": [
    "### 创建一个云存储桶\n",
    "\n",
    "**无论您使用的是哪种笔记本环境，以下步骤都是必需的。**\n",
    "\n",
    "当您初始化用于 Python 的 Vertex SDK 时，您需要指定一个云存储暂存桶。这个暂存桶是您的数据集和模型资源在各个会话中保留的地方。\n",
    "\n",
    "请在下面设置您的云存储桶的名称。桶的名称必须在所有 Google Cloud 项目中全局唯一，包括您的组织之外的项目。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bucket"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"gs://[your-bucket-name]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "autoset_bucket"
   },
   "outputs": [],
   "source": [
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"gs://[your-bucket-name]\":\n",
    "    BUCKET_NAME = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_bucket"
   },
   "source": [
    "只有在您的存储桶不存在时才能运行以下单元格来创建您的云存储存储桶。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_bucket"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "validate_bucket"
   },
   "source": [
    "最后，通过检查其内容来验证对云存储桶的访问。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "validate_bucket"
   },
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_vars"
   },
   "source": [
    "设定变量\n",
    "\n",
    "接下来，设定一些在教程中使用的变量。\n",
    "导入库并定义常量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk"
   },
   "source": [
    "初始化用于Python的Vertex SDK\n",
    "\n",
    "为您的项目和相应的存储桶初始化Python的Vertex SDK。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "aip.init(project=PROJECT_ID, staging_bucket=BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tutorial_start:automl"
   },
   "source": [
    "# 教程\n",
    "\n",
    "现在您已经准备好开始创建您自己的AutoML表格分类模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_file:u_dataset,csv"
   },
   "source": [
    "云存储训练数据的位置。\n",
    "\n",
    "现在将变量`IMPORT_FILE`设置为云存储中CSV索引文件的位置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_file:iris,csv,lcn"
   },
   "outputs": [],
   "source": [
    "IMPORT_FILE = \"gs://cloud-samples-data/tables/iris_1000.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "quick_peek:tabular"
   },
   "source": [
    "快速查看您的数据\n",
    "\n",
    "本教程使用存储在公共云存储桶中的鸢尾花数据集的一个版本，使用CSV索引文件。\n",
    "\n",
    "首先快速查看数据。通过计算CSV索引文件中的行数(`wc -l`)来计算示例的数量，然后查看前几行。\n",
    "\n",
    "您还需要了解训练过程中标签列的标题名称，该名称保存为`label_column`。对于这个数据集，它是CSV文件中的最后一列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quick_peek:tabular"
   },
   "outputs": [],
   "source": [
    "count = ! gsutil cat $IMPORT_FILE | wc -l\n",
    "print(\"Number of Examples\", int(count[0]))\n",
    "\n",
    "print(\"First 10 rows\")\n",
    "! gsutil cat $IMPORT_FILE | head\n",
    "\n",
    "heading = ! gsutil cat $IMPORT_FILE | head -n1\n",
    "label_column = str(heading).split(\",\")[-1].split(\"'\")[0]\n",
    "print(\"Label Column Name\", label_column)\n",
    "if label_column is None:\n",
    "    raise Exception(\"label column missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_dataset:tabular,lcn"
   },
   "source": [
    "### 创建数据集\n",
    "\n",
    "接下来，使用`TabularDataset`类的`create`方法创建`Dataset`资源，该方法需要以下参数：\n",
    "\n",
    "- `display_name`：`Dataset`资源的可读名称。\n",
    "- `gcs_source`：一个或多个数据集索引文件的列表，用于将数据项导入`Dataset`资源。\n",
    "- `bq_source`：或者，从BigQuery表中导入数据项到`Dataset`资源。\n",
    "\n",
    "这个操作可能需要几分钟时间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_dataset:tabular,lcn"
   },
   "outputs": [],
   "source": [
    "dataset = aip.TabularDataset.create(\n",
    "    display_name=\"Iris\" + \"_\" + TIMESTAMP, gcs_source=[IMPORT_FILE]\n",
    ")\n",
    "\n",
    "print(dataset.resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_automl_pipeline:tabular,lcn"
   },
   "source": [
    "### 创建并运行训练管道\n",
    "\n",
    "要训练一个AutoML模型，您需要执行两个步骤：1）创建一个训练管道，2）运行这个管道。\n",
    "\n",
    "#### 创建训练管道\n",
    "\n",
    "使用 `AutoMLTabularTrainingJob` 类创建一个AutoML训练管道，参数包括：\n",
    "\n",
    "- `display_name`：`TrainingJob`资源的人类可读名称。\n",
    "- `optimization_prediction_type`：为模型训练指定的任务类型。\n",
    "  - `分类`：一个表格分类模型。\n",
    "  - `回归`：一个表格回归模型。\n",
    "- `column_transformations`：（可选）应用于输入列的转换。\n",
    "- `optimization_objective`：要最小化或最大化的优化目标。\n",
    "  - 二元分类：\n",
    "    - `minimize-log-loss`\n",
    "    - `maximize-au-roc`\n",
    "    - `maximize-au-prc`\n",
    "    - `maximize-precision-at-recall`\n",
    "    - `maximize-recall-at-precision`\n",
    "  - 多类分类：\n",
    "    - `minimize-log-loss`\n",
    "  - 回归：\n",
    "    - `minimize-rmse`\n",
    "    - `minimize-mae`\n",
    "    - `minimize-rmsle`\n",
    "\n",
    "实例化的对象是训练管道的DAG（有向无环图）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_automl_pipeline:tabular,lcn"
   },
   "outputs": [],
   "source": [
    "dag = aip.AutoMLTabularTrainingJob(\n",
    "    display_name=\"iris_\" + TIMESTAMP,\n",
    "    optimization_prediction_type=\"classification\",\n",
    "    optimization_objective=\"minimize-log-loss\",\n",
    ")\n",
    "\n",
    "print(dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run_automl_pipeline:tabular"
   },
   "source": [
    "#### 运行训练管道\n",
    "\n",
    "接下来，通过调用方法`run`以以下参数运行DAG来启动训练作业：\n",
    "\n",
    "- `dataset`：要训练模型的`Dataset`资源。\n",
    "- `model_display_name`：训练模型的人类可读名称。\n",
    "- `training_fraction_split`：用于训练的数据集百分比。\n",
    "- `test_fraction_split`：用于测试（留置数据）的数据集百分比。\n",
    "- `validation_fraction_split`：用于验证的数据集百分比。\n",
    "- `target_column`：要作为标签进行训练的列的名称。\n",
    "- `budget_milli_node_hours`：（可选）以毫小时为单位指定的最大训练时间（1000 = 小时）。\n",
    "- `disable_early_stopping`：如果为`True`，服务可能认为无法进一步改善模型目标测量，则在完全使用预算之前可能会完成训练。\n",
    "\n",
    "`run`方法完成后将返回`Model`资源。\n",
    "\n",
    "执行训练管道将需要最多20分钟。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_automl_pipeline:tabular"
   },
   "outputs": [],
   "source": [
    "model = dag.run(\n",
    "    dataset=dataset,\n",
    "    model_display_name=\"iris_\" + TIMESTAMP,\n",
    "    training_fraction_split=0.6,\n",
    "    validation_fraction_split=0.2,\n",
    "    test_fraction_split=0.2,\n",
    "    budget_milli_node_hours=8000,\n",
    "    disable_early_stopping=False,\n",
    "    target_column=label_column,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluate_the_model:mbsdk"
   },
   "source": [
    "## 查看模型评估分数\n",
    "在模型训练完成后，您可以查看其评估分数。\n",
    "\n",
    "首先，您需要获得对新模型的引用。与数据集一样，您可以使用部署模型时创建的模型变量的引用，也可以列出您项目中的所有模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluate_the_model:mbsdk"
   },
   "outputs": [],
   "source": [
    "# Get model resource ID\n",
    "models = aip.Model.list(filter=\"display_name=iris_\" + TIMESTAMP)\n",
    "\n",
    "# Get a reference to the Model Service client\n",
    "client_options = {\"api_endpoint\": f\"{REGION}-aiplatform.googleapis.com\"}\n",
    "model_service_client = aip.gapic.ModelServiceClient(client_options=client_options)\n",
    "\n",
    "model_evaluations = model_service_client.list_model_evaluations(\n",
    "    parent=models[0].resource_name\n",
    ")\n",
    "model_evaluation = list(model_evaluations)[0]\n",
    "print(model_evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "deploy_model:mbsdk,dedicated"
   },
   "source": [
    "部署模型\n",
    "\n",
    "接下来，部署您的模型进行在线预测。要部署模型，您需要调用`deploy`方法，并指定以下参数：\n",
    "\n",
    "- `machine_type`：计算机的类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "deploy_model:mbsdk,dedicated"
   },
   "outputs": [],
   "source": [
    "endpoint = model.deploy(machine_type=\"n1-standard-4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "make_prediction:xai"
   },
   "source": [
    "发送一个带有可解释性的在线预测请求\n",
    "\n",
    "向部署的模型发送一个带有可解释性的在线预测。在这种方法中，预测的响应将包括一个解释，说明特征如何对结果的解释做出贡献。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "make_test_item:automl,online_prediction"
   },
   "source": [
    "制作测试数据项\n",
    "\n",
    "您将使用合成数据作为测试数据项。不要担心我们使用合成数据——我们只是想展示如何进行预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "make_test_item:automl,tabular,iris"
   },
   "outputs": [],
   "source": [
    "INSTANCE = {\n",
    "    \"petal_length\": \"1.4\",\n",
    "    \"petal_width\": \"1.3\",\n",
    "    \"sepal_length\": \"5.1\",\n",
    "    \"sepal_width\": \"2.8\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "explain_request:mbsdk,lcn"
   },
   "source": [
    "现在您的“Model”资源已部署到“Endpoint”资源，您可以通过将预测请求发送到“Endpoint”资源来进行在线解释。\n",
    "\n",
    "#### 请求\n",
    "\n",
    "每个实例的格式为：\n",
    "\n",
    "    [feature_list]\n",
    "\n",
    "由于explain()方法可以接受多个项目（实例），请将您的单个测试项目作为一个测试项目列表发送。\n",
    "\n",
    "#### 响应\n",
    "\n",
    "explain()调用的响应是一个带有以下条目的Python字典：\n",
    "\n",
    "- `ids`：每个预测请求的内部分配的唯一标识符。\n",
    "- `displayNames`：每个类标签的类名。\n",
    "- `confidences`：对于分类，每个类标签的预测置信度，介于0和1之间。\n",
    "- `values`：对于回归，预测值。\n",
    "- `deployed_model_id`：执行预测的部署的“Model”资源的Vertex AI标识符。\n",
    "- `explanations`：特征归因。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "explain_request:mbsdk,lcn"
   },
   "outputs": [],
   "source": [
    "instances_list = [INSTANCE]\n",
    "\n",
    "prediction = endpoint.explain(instances_list)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "understand_explanations"
   },
   "source": [
    "### 理解解释响应\n",
    "\n",
    "首先，您将查看您的模型预测并将其与实际值进行比较。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "understand_explanations:mbsdk,lcn"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    label = np.argmax(prediction[0][0][\"scores\"])\n",
    "    cls = prediction[0][0][\"classes\"][label]\n",
    "    print(\"Predicted Value:\", cls, prediction[0][0][\"scores\"][label])\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "examine_feature_attributions"
   },
   "source": [
    "### 检查特征归因\n",
    "\n",
    "接下来，您将查看此特定示例的特征归因。正面的归因值意味着特定特征使您的模型预测提升了该数量，而负面的归因值则相反。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "examine_feature_attributions:mbsdk,iris"
   },
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "feature_names = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\n",
    "attributions = prediction.explanations[0].attributions[0].feature_attributions\n",
    "\n",
    "rows = []\n",
    "for i, val in enumerate(feature_names):\n",
    "    rows.append([val, INSTANCE[val], attributions[val]])\n",
    "print(tabulate(rows, headers=[\"Feature name\", \"Feature value\", \"Attribution value\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "check_explanations_baselines"
   },
   "source": [
    "### 检查您的解释和基线\n",
    "\n",
    "为了更好地理解您得到的特征归因，您应该将它们与模型的基线进行比较。在大多数情况下，您的归因值的总和加上基线应该非常接近于模型对每个输入的预测值。同时请注意，对于回归模型，从AI解释返回的`baseline_score`将对发送到您的模型的每个示例都是相同的。对于分类模型，每个类别将有自己的基线。\n",
    "\n",
    "在此部分，您将向您的模型发送10个测试示例以进行预测，以便将特征归因与基线进行比较。然后，您将通过`sanity_check_explanations`方法来对每个测试示例的归因进行检查。\n",
    "\n",
    "#### 获取解释"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_explanations_baselines:mbsdk,iris"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Prepare 10 test examples to your model for prediction using a random distribution to generate\n",
    "# test instances\n",
    "instances = []\n",
    "for i in range(10):\n",
    "    pl = str(random.uniform(1.0, 2.0))\n",
    "    pw = str(random.uniform(1.0, 2.0))\n",
    "    sl = str(random.uniform(4.0, 6.0))\n",
    "    sw = str(random.uniform(2.0, 4.0))\n",
    "    instances.append(\n",
    "        {\"petal_length\": pl, \"petal_width\": pw, \"sepal_length\": sl, \"sepal_width\": sw}\n",
    "    )\n",
    "\n",
    "response = endpoint.explain(instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sanity_check_explanations"
   },
   "source": [
    "#### 合理性检查\n",
    "\n",
    "在下面的函数中，您对解释进行了合理性检查。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sanity_check_explanations"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def sanity_check_explanations(\n",
    "    explanation, prediction, mean_tgt_value=None, variance_tgt_value=None\n",
    "):\n",
    "    passed_test = 0\n",
    "    total_test = 1\n",
    "    # `attributions` is a dict where keys are the feature names\n",
    "    # and values are the feature attributions for each feature\n",
    "    baseline_score = explanation.attributions[0].baseline_output_value\n",
    "    print(\"baseline:\", baseline_score)\n",
    "\n",
    "    # Sanity check 1\n",
    "    # The prediction at the input is equal to that at the baseline.\n",
    "    #  Please use a different baseline. Some suggestions are: random input, training\n",
    "    #  set mean.\n",
    "    if abs(prediction - baseline_score) <= 0.05:\n",
    "        print(\"Warning: example score and baseline score are too close.\")\n",
    "        print(\"You might not get attributions.\")\n",
    "    else:\n",
    "        passed_test += 1\n",
    "        print(\"Sanity Check 1: Passed\")\n",
    "\n",
    "    print(passed_test, \" out of \", total_test, \" sanity checks passed.\")\n",
    "\n",
    "\n",
    "i = 0\n",
    "for explanation in response.explanations:\n",
    "    try:\n",
    "        prediction = np.max(response.predictions[i][\"scores\"])\n",
    "    except TypeError:\n",
    "        prediction = np.max(response.predictions[i])\n",
    "    sanity_check_explanations(explanation, prediction)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "undeploy_model:mbsdk"
   },
   "source": [
    "## 下架模型\n",
    "\n",
    "当您完成预测后，您可以从`Endpoint`资源中下架模型。这将取消所有计算资源并停止对部署模型的计费。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "undeploy_model:mbsdk"
   },
   "outputs": [],
   "source": [
    "endpoint.undeploy_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleanup:mbsdk"
   },
   "source": [
    "清理\n",
    "\n",
    "要清理此项目中使用的所有Google Cloud资源，您可以删除用于本教程的[Google Cloud项目](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects)。\n",
    "\n",
    "否则，您可以删除本教程中创建的各个资源：\n",
    "\n",
    "- 数据集\n",
    "- 流水线\n",
    "- 模型\n",
    "- 端点\n",
    "- AutoML训练作业\n",
    "- 批处理作业\n",
    "- 自定义作业\n",
    "- 超参数调整作业\n",
    "- Cloud存储桶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cleanup:mbsdk"
   },
   "outputs": [],
   "source": [
    "delete_all = True\n",
    "\n",
    "if delete_all:\n",
    "    # Delete the dataset using the Vertex dataset object\n",
    "    try:\n",
    "        if \"dataset\" in globals():\n",
    "            dataset.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the model using the Vertex model object\n",
    "    try:\n",
    "        if \"model\" in globals():\n",
    "            model.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the endpoint using the Vertex endpoint object\n",
    "    try:\n",
    "        if \"endpoint\" in globals():\n",
    "            endpoint.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the AutoML or Pipeline trainig job\n",
    "    try:\n",
    "        if \"dag\" in globals():\n",
    "            dag.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the custom trainig job\n",
    "    try:\n",
    "        if \"job\" in globals():\n",
    "            job.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the batch prediction job using the Vertex batch prediction object\n",
    "    try:\n",
    "        if \"batch_predict_job\" in globals():\n",
    "            batch_predict_job.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the hyperparameter tuning job using the Vertex hyperparameter tuning object\n",
    "    try:\n",
    "        if \"hpt_job\" in globals():\n",
    "            hpt_job.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    if \"BUCKET_NAME\" in globals():\n",
    "        ! gsutil rm -r $BUCKET_NAME"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "sdk_automl_tabular_classification_online_explain.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
