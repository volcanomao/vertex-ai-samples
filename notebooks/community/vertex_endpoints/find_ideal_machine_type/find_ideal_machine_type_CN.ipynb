{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zB_PYUGd7-ko"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3bcfa29cc2be"
   },
   "source": [
    "<table align=\"left\">\r\n",
    "  <td>\r\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/master/notebooks/community/vertex_endpoints/find_ideal_machine_type/find_ideal_machine_type.ipynb\"\">\r\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> 在Colab中运行\r\n",
    "    </a>\r\n",
    "  </td>\r\n",
    "  <td>\r\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/master/notebooks/community/vertex_endpoints/find_ideal_machine_type/find_ideal_machine_type.ipynb\">\r\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"> 在GitHub上查看\r\n",
    "    </a>\r\n",
    "  </td>\r\n",
    "  <td>\r\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/notebooks/deploy-notebook?download_url=https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/master/notebooks/community/vertex_endpoints/find_ideal_machine_type/find_ideal_machine_type.ipynb\">\r\n",
    "      <img src=\"https://cloud.google.com/images/products/ai/ai-solutions-icon.svg\" alt=\"Vertex AI Workbench notebook\"> 在Vertex AI Workbench中打开\r\n",
    "    </a>\r\n",
    "  </td>\r\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmDKdOFh8Ko8"
   },
   "source": [
    "确定用于Vertex AI端点的理想机器类型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FTe0sT1p8SHy"
   },
   "source": [
    "## 概述\n",
    "本教程演示了如何根据成本和性能要求确定适合您机器学习模型的理想机器类型。\n",
    "\n",
    "有关最佳实践的更多详细信息，请访问[这里](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#finding_the_ideal_machine_type)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c8OldBdp8VeZ"
   },
   "source": [
    "模型\n",
    "本教程使用的模型是来自[TensorFlow Hub开源模型库](https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4)的`BERT`模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DU7F8BCg8WgZ"
   },
   "source": [
    "## 目标\n",
    "\n",
    "执行的步骤包括：\n",
    "- 创建一个工作台笔记本，使用正在测试的机器类型。\n",
    "- 从 TensorFlow Hub 下载模型。\n",
    "- 创建一个本地模型并部署到本地端点。\n",
    "- 对模型延迟进行基准测试。\n",
    "- 清理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KT8PQtz68aRa"
   },
   "source": [
    "成本\n",
    "本教程使用Google Cloud的可计费组件：\n",
    "- Vertex AI\n",
    "- Cloud Storage\n",
    "\n",
    "了解[Vertex AI价格](https://cloud.google.com/vertex-ai/pricing)和[Cloud Storage价格](https://cloud.google.com/storage/pricing)，并使用[Pricing Calculator](https://cloud.google.com/products/calculator/)根据您的预期使用量生成成本估算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Peg2Bwy_v2fe"
   },
   "source": [
    "## 在开始之前\n",
    "\n",
    "### 设置您的Google Cloud项目\n",
    "\n",
    "**无论您使用什么笔记本环境，都需要按照以下步骤进行操作。**\n",
    "\n",
    "1. [选择或创建一个Google Cloud项目](https://console.cloud.google.com/cloud-resource-manager)。当您第一次创建账户时，您将获得$300的免费信用额用于支付计算/存储成本。\n",
    "\n",
    "2. [确保您的项目已启用计费功能](https://cloud.google.com/billing/docs/how-to/modify-project)。\n",
    "\n",
    "3. [启用Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com)。\n",
    "\n",
    "4. 如果您正在本地运行此笔记本，您需要安装[Cloud SDK](https://cloud.google.com/sdk)。\n",
    "\n",
    "5. 在下面的单元格中输入您的项目ID。然后运行该单元格以确保Cloud SDK在本笔记本中的所有命令中使用正确的项目。\n",
    "\n",
    "**注意**：Jupyter将以`!`为前缀的行视为shell命令，并且将以`$`为前缀的Python变量插入这些命令中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1NRbJL7iwTwm"
   },
   "source": [
    "设置您的项目 ID\n",
    "\n",
    "**如果您不知道您的项目 ID**，您可以尝试使用 `gcloud` 命令获取您的项目 ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jVrtnNtuwOgv"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_ID = \"\"\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Get your Google Cloud project ID from gcloud\n",
    "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yHaNeGniwQwf"
   },
   "source": [
    "否则，请在这里设置您的项目ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7wU5w8-WwakX"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None:\n",
    "    PROJECT_ID = \"\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R3YQn1FNv7I9"
   },
   "source": [
    "时间戳\n",
    "\n",
    "如果您正在参加现场教程会话，可能会使用共享的测试帐户或项目。为了避免在创建的资源上的用户名称冲突，请为每个实例会话创建一个时间戳，并将其附加到您在此教程中创建的资源名称上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TU_3QS-Rwk0v"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VlLFGqlov_Ht"
   },
   "source": [
    "### 验证您的 Google Cloud 账户\n",
    "\n",
    "**如果您正在使用 Google Cloud 笔记本**，您的环境已经通过验证。请跳过此步骤。\n",
    "\n",
    "**如果您正在使用 Colab**，运行下面的单元格并按照提示进行身份验证，通过 oAuth 认证您的账户。\n",
    "\n",
    "**否则**，请按照以下步骤操作：\n",
    "\n",
    "1. 在 Cloud 控制台中，前往 [**创建服务账号密钥** 页面](https://console.cloud.google.com/apis/credentials/serviceaccountkey)。\n",
    "\n",
    "2. 点击 **创建服务账号**。\n",
    "\n",
    "3. 在 **服务账号名称** 栏中输入一个名称，然后点击 **创建**。\n",
    "\n",
    "4.. 在 **授予此服务账号对项目的访问权限** 部分，点击 **角色** 下拉列表。在筛选框中输入 \"Vertex AI\"，并选择 **Vertex AI 管理员**。在筛选框中输入 \"Storage Object Admin\"，并选择 **Storage Object Admin**。\n",
    "\n",
    "5. 点击 **创建**。一个包含您密钥的 JSON 文件将下载到您的本地环境中。\n",
    "\n",
    "6. 在下面的单元格中，将您的服务账号密钥路径作为 `GOOGLE_APPLICATION_CREDENTIALS` 变量输入，并运行该单元格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EBTUnqS1wqHo"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "# The Google Cloud Notebook product has specific requirements\n",
    "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "\n",
    "# If on Google Cloud Notebooks, then don't execute this code\n",
    "if not IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\"):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wPrx2LnU8vFU"
   },
   "source": [
    "### 创建一个云存储桶\n",
    "\n",
    "**无论您使用的是哪种笔记本环境，下面的步骤都是必需的。**\n",
    "\n",
    "首先，您需要将模型文件上传到一个云存储桶中。利用这个模型 artifact，您可以创建 Vertex AI 模型和端点资源，以便提供在线预测。\n",
    "\n",
    "请在下面设置您的云存储桶的名称。它必须在所有云存储桶中是唯一的。\n",
    "\n",
    "您还可以更改`REGION`变量，该变量用于笔记本的其他部分。请确保[选择一个 Vertex AI 服务可用的地区](https://cloud.google.com/vertex-ai/docs/general/locations#available_regions)。您不能使用多区域存储桶进行 Vertex AI 的训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ef0nYMRKxvDV"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"\"  # @param {type:\"string\"}\n",
    "REGION = \"us-central1\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ti_79ErvxxeF"
   },
   "outputs": [],
   "source": [
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"gs://[your-bucket-name]\":\n",
    "    BUCKET_NAME = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ig18nZMtmuGH"
   },
   "outputs": [],
   "source": [
    "print(BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCZRfgA9x0Mz"
   },
   "source": [
    "只有当您的存储桶尚不存在时，运行以下单元格以创建您的云存储存储桶。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gDX_aWtjxzSN"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -p $PROJECT_ID -l $REGION $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n7IDVRdwx4hd"
   },
   "source": [
    "最后，通过检查其内容验证对您的云存储存储桶的访问。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iA6Uqvqxx8Ls"
   },
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cf3978e40b0c"
   },
   "source": [
    "创建工作台笔记本\n",
    "\n",
    "您将使用Google Cloud笔记本在特定的计算机类型上运行负载测试，以便对您的模型在运行在Vertex AI端点时的性能有一个良好的了解。\n",
    "\n",
    "在这里，我们将使用`gcloud`创建笔记本，但您也可以按照[这里](https://cloud.google.com/vertex-ai/docs/workbench/user-managed/create-new#before_you_begin)的说明通过Google云控制台创建它。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "acac1efc5d72"
   },
   "outputs": [],
   "source": [
    "!gcloud notebooks instances create load-test-notebook \\\n",
    "--vm-image-project=\"deeplearning-platform-release\" \\\n",
    "--vm-image-name=\"common-cpu-notebooks-v20221017-debian-10\" \\\n",
    "--machine-type=\"n1-standard-8\" --project=$PROJECT_ID \\\n",
    "--location=us-central1-a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bd08a0c3944e"
   },
   "source": [
    "### 打开工作台笔记本\n",
    "\n",
    "创建笔记本后，打开该笔记本。您将在新创建的笔记本中运行其余的步骤。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55cba005e3fd"
   },
   "source": [
    "### 安装Vegeta\n",
    "\n",
    "Vegeta是一种多功能的HTTP负载测试工具，出于需要以恒定的请求速率测试HTTP服务而构建。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6189cc5f276a"
   },
   "outputs": [],
   "source": [
    "! wget https://github.com/tsenart/vegeta/releases/download/v12.8.4/vegeta_12.8.4_linux_amd64.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b18fb52391c3"
   },
   "outputs": [],
   "source": [
    "! tar -xvf vegeta_12.8.4_linux_amd64.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "308be646d91c"
   },
   "source": [
    "### 安装依赖\n",
    "\n",
    "安装 Python 的依赖项"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1b7e9e3f8985"
   },
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "google-cloud-aiplatform[prediction]>=1.16.0,<2.0.0\n",
    "matplotlib\n",
    "fastapi\n",
    "contexttimer\n",
    "tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b815283c6fcd"
   },
   "outputs": [],
   "source": [
    "%pip install -U --user -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "me1llTRsyImc"
   },
   "source": [
    "### 下载并提取模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_6dmPQqxzGZp"
   },
   "outputs": [],
   "source": [
    "! wget https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4?tf-hub-format=compressed -O bert.tgz\n",
    "! mkdir -p bert_sentence_embedding/00001\n",
    "! tar -xvf bert.tgz -C bert_sentence_embedding/00001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8b21a70079d9"
   },
   "source": [
    "### 在新笔记本中设置存储桶变量\n",
    "\n",
    "您在之前的步骤中创建了一个存储桶。因为您现在正在新的笔记本上工作，您应该重新设置存储桶变量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2efae3889325"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "324fa4e23dab"
   },
   "source": [
    "### 配置\n",
    "\n",
    "为了向端点发送请求，您将创建一个虚拟请求体。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c862a456adb5"
   },
   "outputs": [],
   "source": [
    "# The gcs uri; remember to have a version folder under this link\n",
    "# For example, GCS_URI = \"gs://project/bucket/folder\"\n",
    "# the model should be put in \"gs://project/bucket/folder/1/saved_model.pb\".\n",
    "GCS_URI = f\"gs://{BUCKET_NAME}/bert_sentence_embedding\"\n",
    "REQUEST = \"\"\"\n",
    "{\n",
    "  \"instances\": [\n",
    "    {\n",
    "      \"input_word_ids\": [101, 23784, 11591, 11030, 24340, 21867, 21352, 21455, 20467, 10159, 23804, 10822, 26534, 20355, 14000, 11767, 10131, 28426, 10576, 22469, 22237, 25433, 263, 28636, 12291, 119, 15337, 10171, 25585, 21885, 10263, 13706, 16046, 10112, 18725, 13668, 12208, 10104, 13336, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "      \"input_mask\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "      \"input_type_ids\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8eed897f11df"
   },
   "outputs": [],
   "source": [
    "!echo $GCS_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0e1f4fc0c7b6"
   },
   "source": [
    "### 将模型复制到GCS存储桶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "392ae29f9285"
   },
   "outputs": [],
   "source": [
    "!sudo gsutil cp -r ./bert_sentence_embedding/00001/* $GCS_URI/1/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cf1d35fd7dba"
   },
   "source": [
    "### 记录\n",
    "打开记录以查看模型的日志"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "60d2cd206419"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17fed206bc47"
   },
   "source": [
    "## 顺序请求\n",
    "\n",
    "这个测试可以测试在服务器一次只服务一个请求时的延迟（以及潜在的利用率）。您可以使用这些信息来估算单个副本可以处理多少QPS，作为配置的起点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11a949c70643"
   },
   "source": [
    "将LocalModel进行猴子补丁，以提供更清晰的语法..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0868520f8877"
   },
   "outputs": [],
   "source": [
    "from google.cloud.aiplatform.prediction import LocalModel\n",
    "\n",
    "\n",
    "@classmethod\n",
    "def create_tensorflow2(\n",
    "    cls, version: str, saved_model_path: str, includes_version_subdir: bool = True\n",
    ") -> LocalModel:\n",
    "    version = version.replace(\".\", \"-\")\n",
    "    return cls(\n",
    "        serving_container_image_uri=f\"us-docker.pkg.dev/vertex-ai/prediction/tf2-gpu.{version}:latest\",\n",
    "        serving_container_predict_route=\"/v1/models/default:predict\",\n",
    "        serving_container_health_route=\"/v1/models/default\",\n",
    "        serving_container_ports=[8501],\n",
    "        serving_container_environment_variables={\n",
    "            \"model_name\": \"default\",\n",
    "            \"model_path\": saved_model_path,\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "LocalModel.create_tensorflow2 = create_tensorflow2\n",
    "\n",
    "\n",
    "@classmethod\n",
    "def create_pytorch(cls, version: str) -> LocalModel:\n",
    "    version = version.replace(\".\", \"-\")\n",
    "    return LocalModel(\n",
    "        serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/pytorch-gpu.{version}:latest\",\n",
    "        serving_container_predict_route=\"/predictions/model\",\n",
    "        serving_container_health_route=\"/ping\",\n",
    "        serving_container_ports=[8080],\n",
    "    )\n",
    "\n",
    "\n",
    "LocalModel.create_pytorch = create_pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "792144166f54"
   },
   "source": [
    "创建本地模型并部署到本地端点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5bd8fe59c778"
   },
   "outputs": [],
   "source": [
    "from google.cloud.aiplatform.prediction import LocalModel\n",
    "\n",
    "local_model = LocalModel.create_tensorflow2(version=\"2.7\", saved_model_path=GCS_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d95eab87becb"
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "GPU_COUNT = 1 if os.path.exists(\"/dev/nvidia0\") else None\n",
    "print(GPU_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c4f664318f9a"
   },
   "outputs": [],
   "source": [
    "from contexttimer import Timer\n",
    "\n",
    "with Timer() as timer:\n",
    "    local_endpoint = local_model.deploy_to_local_endpoint(\n",
    "        gpu_count=GPU_COUNT,\n",
    "    )\n",
    "    local_endpoint.serve()\n",
    "\n",
    "# Actual startup time involves more than just loading the container and model, but still\n",
    "# a useful number:\n",
    "print(f\"Startup time: {timer.elapsed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edd0744e993a"
   },
   "source": [
    "发送连续的请求\n",
    "您将向本地端点发送多个请求，并收集延迟度量数据，这将让您很好地了解选择的机器类型在生产环境中模型的性能如何。您将可视化这些结果，并得到平均延迟时间（以毫秒为单位）。\n",
    "\n",
    "由于这是一个变压器模型，它在 CPU 上运行速度较慢，最好使用 GPU 运行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e5ced2a0912c"
   },
   "outputs": [],
   "source": [
    "WARMUP_REQUESTS = 10\n",
    "NUM_REQUESTS = 100\n",
    "PERCENTILE_POINTS = [0, 50, 95, 99, 100]\n",
    "LABELS = [\"min\", \"50\", \"95\", \"99\", \"max\"]\n",
    "\n",
    "import numpy as np\n",
    "from contexttimer import Timer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Send some warm up requests\n",
    "for _ in tqdm(range(WARMUP_REQUESTS), desc=\"Sending warm-up requests\"):\n",
    "    local_endpoint.predict(\n",
    "        request=REQUEST, headers={\"Content-Type\": \"application/json\"}\n",
    "    )\n",
    "\n",
    "# Send sequential requests\n",
    "latencies = []\n",
    "for _ in tqdm(range(NUM_REQUESTS), desc=\"Sending requests\"):\n",
    "    with Timer(factor=1000) as timer:\n",
    "        local_endpoint.predict(\n",
    "            request=REQUEST, headers={\"Content-Type\": \"application/json\"}\n",
    "        )\n",
    "    latencies.append(timer.elapsed)\n",
    "\n",
    "percentiles = np.percentile(latencies, PERCENTILE_POINTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2f833fbfa940"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.hist(latencies, bins=50, density=True)\n",
    "plt.xlabel(\"Latency (ms)\")\n",
    "plt.show()\n",
    "\n",
    "for p, v in zip([\"min\", \"50\", \"95\", \"99\", \"max\"], percentiles):\n",
    "    print(f\"{p}: {v:0.1f}\")\n",
    "\n",
    "print(f\"mean: {np.average(latencies):0.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bcd27ced3eb"
   },
   "source": [
    "发送并发请求\n",
    "\n",
    "上面的练习为每个请求的延迟提供了一个很好的基准，但并不能表明模型在生产环境中处理并发请求时的表现。例如，当机器的资源耗尽时，延迟可能会降低。为了找到一个能够有效处理多个并发请求的理想机器类型，我们将使用 `vegeta`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9fb3ff0c341c"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "REQUEST_FILE = \"request.json\"\n",
    "\n",
    "import json\n",
    "\n",
    "instance = json.loads(REQUEST)[\"instances\"][0]\n",
    "# Row-based encoding\n",
    "with open(REQUEST_FILE, \"w\") as f:\n",
    "    json.dump({\"instances\": [instance] * BATCH_SIZE}, f)\n",
    "\n",
    "# Column-based encoding (more efficient for some models)\n",
    "inputs = {feature: [values] * BATCH_SIZE for feature, values in instance.items()}\n",
    "with open(\"request_cols.json\", \"w\") as f:\n",
    "    json.dump({\"inputs\": inputs}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a010689ecd0d"
   },
   "outputs": [],
   "source": [
    "URL = f\"http://localhost:{local_endpoint.assigned_host_port}{local_endpoint.serving_container_predict_route}\"\n",
    "URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "046bcb10d578"
   },
   "outputs": [],
   "source": [
    "!curl http://localhost:{local_endpoint.assigned_host_port}{local_endpoint.serving_container_health_route}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "00c79b9c8145"
   },
   "outputs": [],
   "source": [
    "!curl -X POST http://localhost:{local_endpoint.assigned_host_port}{local_endpoint.serving_container_predict_route} -d @request.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "33002019b35c"
   },
   "outputs": [],
   "source": [
    "DURATION = \"100s\"\n",
    "\n",
    "! for i in 1 2 3 4; do \\\n",
    "    echo \"POST {URL}\" | \\\n",
    "   ./vegeta attack -header \"Content-Type: application/json\" -body {REQUEST_FILE} -rate ${{i}} -duration {DURATION} | \\\n",
    "   tee report-${{i}}.bin | \\\n",
    "   ./vegeta report --every=60s; \\\n",
    "  done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "564f15310d42"
   },
   "outputs": [],
   "source": [
    "! for f in `ls *.bin`; do \\\n",
    "    ./vegeta report --type=json ${{f}} > ${{f}}.json; \\\n",
    "  done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "506fac26a3fd"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import re\n",
    "\n",
    "throughput, p99, avg = {}, {}, {}\n",
    "for fn in glob.glob(\"report-*.bin.json\"):\n",
    "    with open(fn) as f:\n",
    "        data = json.load(f)\n",
    "    qps = int(re.search(r\"report-(\\d+).bin.json\", fn).group(1))\n",
    "    throughput[qps] = data[\"throughput\"]\n",
    "    p99[qps] = data[\"latencies\"][\"99th\"] / 1000000\n",
    "    avg[qps] = data[\"latencies\"][\"mean\"] / 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1940fa5f23df"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "points = sorted(p99.items(), key=lambda item: item[0])\n",
    "x, y = zip(*points)\n",
    "plt.plot(x, y, \"-o\")\n",
    "plt.xlabel(\"Target QPS\")\n",
    "plt.ylabel(\"P99 Latency (ms)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7a7f0a0ca552"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "points = sorted(throughput.items(), key=lambda item: item[0])\n",
    "x, y = zip(*points)\n",
    "plt.plot(x, y, \"-o\")\n",
    "plt.xlabel(\"Target QPS\")\n",
    "plt.ylabel(\"Actual QPS\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "84886940cac3"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "points = sorted(avg.items(), key=lambda item: item[0])\n",
    "x, y = zip(*points)\n",
    "plt.plot(x, y, \"-o\")\n",
    "plt.xlabel(\"Target QPS\")\n",
    "plt.ylabel(\"Average Latency (ms)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21fe816f4490"
   },
   "source": [
    "我们可以估计单个副本可以处理的并发请求数量：\n",
    "\n",
    "$num\\_concurrent\\_requests = \\frac{qps}{avg\\_latency_{qps}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7319ff6f1223"
   },
   "outputs": [],
   "source": [
    "QPS = 2\n",
    "\n",
    "num_concurrent_requests = QPS / avg[QPS]\n",
    "num_concurrent_requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f61ec29124cd"
   },
   "source": [
    "正如您所见，这种模型在该类型的机器上表现不佳。尝试不同的机器类型配置，或添加GPU，看看结果如何变化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8c83b5cf6819"
   },
   "source": [
    "清理工作\n",
    "\n",
    "要清理此项目中使用的所有Google Cloud资源，您可以删除用于本教程的[Google Cloud项目](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects)。\n",
    "\n",
    "否则，您可以删除本教程中创建的各个资源。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aokup0x_ZiJK"
   },
   "outputs": [],
   "source": [
    "!gsutil rm -r $GCS_URI/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9a52cf30cea2"
   },
   "source": [
    "以下命令将删除用于测试的工作台笔记本实例。在继续之前，请保存您的所有工作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fbb241f715c2"
   },
   "outputs": [],
   "source": [
    "!gcloud notebooks instances delete load-test-notebook"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "find_ideal_machine_type.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
