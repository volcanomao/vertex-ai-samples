{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7a906901030"
   },
   "source": [
    "使用优化的TensorFlow运行时在Vertex AI预测上部署T5x基础。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/vertex_endpoints/optimized_tensorflow_runtime/t5x_base_optimized_online_prediction.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> 在Colab中运行\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/vertex_endpoints/optimized_tensorflow_runtime/t5x_base_optimized_online_prediction.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      在GitHub上查看\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/vertex_endpoints/optimized_tensorflow_runtime/t5x_base_optimized_online_prediction.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      在Vertex AI Workbench中打开\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f53c3b14e636"
   },
   "source": [
    "## 概述\n",
    "\n",
    "在这个示例中，您将学习如何将T5x基础模型部署到Vertex AI Prediction，使用优化的TensorFlow运行时容器。\n",
    "\n",
    "您可以使用MLPerf推理Vertex Prediction基准工具，在优化的TensorFlow运行时容器上评估模型性能，以及不同的优化方法。\n",
    "\n",
    "有关Vertex AI Prediction优化的TensorFlow运行时容器的更多信息，请参阅https://cloud.google.com/vertex-ai/docs/predictions/optimized-tensorflow-runtime。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "830934f1850b"
   },
   "source": [
    "### 目标\n",
    "\n",
    "在本笔记本中，您将学习如何使用优化的TensorFlow运行时部署经过微调的T5x基础模型到Vertex AI预测服务。为了获得最佳性能，您可以使用NVIDIA A100 GPU。\n",
    "\n",
    "您将执行的步骤包括：\n",
    "* 学习如何在Vertex上微调T5x基础模型\n",
    "* 使用优化的TensorFlow运行时容器将T5x基础模型部署到Vertex AI预测，使用不同的优化选项\n",
    "* 对部署的模型进行基准测试并验证其预测结果\n",
    "\n",
    "您可以使用Colab将经过微调的模型部署到Vertex AI预测。但为了获得可靠的基准测试结果，必须在与您的模型相同地区运行的Jupyter VM中运行本指南。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fd44e72fd4bf"
   },
   "source": [
    "### 模型\n",
    "\n",
    "在这个笔记本中，您使用了一个T5x基础模型。\n",
    "\n",
    "T5是一个编码器-解码器模型，在多种任务的混合无监督和有监督任务上进行了预训练，每个任务都被转换为文本到文本的格式。通过将不同的前缀添加到与每个任务相对应的输入中，T5可以直接处理各种任务，例如，用于翻译：将英文翻译为德文：...，用于总结：总结：...。\n",
    "模型可以进一步微调，以用于其未经训练的特定任务。\n",
    "\n",
    "T5X是T5在JAX和Flax中的新的改进实现。\n",
    "在模型经过微调后，可以将其导出为TensorFlow SavedModel格式，然后可以使用优化的TensorFlow运行时在Vertex AI Prediction上使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "费用\n",
    "\n",
    "本教程使用Google Cloud的以下可计费组件：\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "* Cloud TPU（如果您选择在自己上调模型）\n",
    "\n",
    "了解[Vertex AI定价](https://cloud.google.com/vertex-ai/pricing)和[Cloud Storage定价](https://cloud.google.com/storage/pricing)，并使用[Pricing Calculator](https://cloud.google.com/products/calculator/)根据您的预期使用情况生成成本估算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ze4-nDLfK4pw"
   },
   "source": [
    "### 设置您的本地开发环境\n",
    "\n",
    "**如果您正在使用Colab或Vertex AI Workbench笔记本**，您的环境已经满足运行此笔记本的所有要求。您可以跳过这一步。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCuSR8GkAgzl"
   },
   "source": [
    "否则，请确保您的环境符合此笔记本的要求。\n",
    "您需要以下内容：\n",
    "\n",
    "* Google Cloud SDK\n",
    "* Git\n",
    "* Python 3\n",
    "* virtualenv\n",
    "* 在使用Python 3的虚拟环境中运行的Jupyter笔记本\n",
    "\n",
    "Google Cloud指南中的[设置Python开发环境](https://cloud.google.com/python/setup)和[Jupyter安装指南](https://jupyter.org/install)提供了满足这些要求的详细说明。以下步骤提供了一套简要的说明：\n",
    "\n",
    "1. [安装并初始化Cloud SDK。](https://cloud.google.com/sdk/docs/)\n",
    "\n",
    "2. [安装Python 3。](https://cloud.google.com/python/setup#installing_python)\n",
    "\n",
    "3. [安装virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv)并创建一个使用Python 3的虚拟环境。激活虚拟环境。\n",
    "\n",
    "4. 要安装Jupyter，请在终端shell中的命令行上运行`pip3 install jupyter`。\n",
    "\n",
    "5. 要启动Jupyter，请在终端shell中的命令行上运行`jupyter notebook`。\n",
    "\n",
    "6. 在Jupyter Notebook仪表板中打开此笔记本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "安装额外的软件包\n",
    "\n",
    "安装执行此笔记本所需的软件包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2b4ef9b72d43"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Vertex AI Workbench Notebook product has specific requirements\n",
    "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "\n",
    "# Vertex AI Workbench Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\"\n",
    "\n",
    "!pip3 install {USER_FLAG} --upgrade google-cloud-aiplatform google-cloud-storage tensorflow-serving-api -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fd8df302e11"
   },
   "source": [
    "如果您还计划运行MLPerf推理基准测试，您还需要下载和安装额外的依赖项（请参阅https://github.com/tensorflow/tpu/tree/master/models/experimental/inference/load_test＃run-the-benchmark-locally了解详情）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "68c85f558bd4"
   },
   "outputs": [],
   "source": [
    "!pip3 install {USER_FLAG} transformers tf-models-official -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9dba7d2da016"
   },
   "outputs": [],
   "source": [
    "!git clone --recurse-submodules -b r1.0 https://github.com/mlcommons/inference.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cf1b9005a7f5"
   },
   "outputs": [],
   "source": [
    "!cd inference/loadgen && CFLAGS=\"-std=c++14 -O3\" python3 setup.py bdist_wheel && pip3 install {USER_FLAG} --force-reinstall dist/mlperf_loadgen-*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a57f89a5675f"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/tensorflow/tpu.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhq5zEbGg0XX"
   },
   "source": [
    "### 重新启动内核\n",
    "\n",
    "在安装了额外的软件包之后，您必须重新启动笔记本内核，以便它可以找到这些软件包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EzrelQZ22IZj"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWEdiXsJg0XY"
   },
   "source": [
    "在你开始之前"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF1j6f9HApxa"
   },
   "source": [
    "### 设置你的谷歌云项目\n",
    "\n",
    "**以下步骤适用于所有的笔记本环境。**\n",
    "\n",
    "1. [选择或创建一个谷歌云项目](https://console.cloud.google.com/cloud-resource-manager)。当你第一次创建帐户时，将获得300美元的信用用于计算和存储成本。\n",
    "\n",
    "1. [确保你的项目已启用计费功能](https://cloud.google.com/billing/docs/how-to/modify-project)。\n",
    "\n",
    "1. [启用 Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com)。\n",
    "\n",
    "1. 如果你在本地运行这个笔记本，你必须安装 [Cloud SDK](https://cloud.google.com/sdk)。\n",
    "\n",
    "1. 在下面的单元格中输入你的项目 ID。然后运行这个单元格，以确保 Cloud SDK 在这个笔记本中的所有命令中使用正确的项目。\n",
    "\n",
    "**注意**：Jupyter 运行以 `!` 开头的行作为 shell 命令，并将以 `$` 开头的 Python 变量插入这些命令中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WReHDGG5g0XY"
   },
   "source": [
    "设置您的项目ID\n",
    "\n",
    "如果您不知道您的项目ID，您可以尝试使用`gcloud`来获取您的项目ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oM1iC_MfAts1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_ID = \"\"\n",
    "\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJYoRfYng0XZ"
   },
   "source": [
    "否则，在此处设置您的项目 ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "riG_qUokg0XZ"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None:\n",
    "    PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b9a78e10b72b"
   },
   "source": [
    "设置您的地区\n",
    "\n",
    "选择您计划要部署模型的地区。请注意，如果您计划在NVIDIA A100上部署模型，它只在特定地区可用：https://cloud.google.com/vertex-ai/docs/general/locations#region_considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3f22a26ddc83"
   },
   "outputs": [],
   "source": [
    "REGION = \"[your-region]\"  # @param {type:\"string\"}\n",
    "\n",
    "if REGION == \"[your-region]\":\n",
    "    REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dr--iN2kAylZ"
   },
   "source": [
    "### 验证您的Google Cloud帐户\n",
    "\n",
    "**如果您正在使用 Vertex AI Workbench笔记本**，您的环境已经经过验证。请跳过此步骤。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBCra4QMA2wR"
   },
   "source": [
    "如果您正在使用Colab，请运行下面的单元格，并按提示进行oAuth身份验证。\n",
    "\n",
    "否则，请按照以下步骤操作：\n",
    "\n",
    "1. 在Cloud控制台中，转到[创建服务帐户密钥页面](https://console.cloud.google.com/apis/credentials/serviceaccountkey)。\n",
    "\n",
    "2. 点击**创建服务帐户**。\n",
    "\n",
    "3. 在**服务帐户名称**栏中输入名称，然后点击**创建**。\n",
    "\n",
    "4. 在**授予此服务帐户对项目的访问权限**部分，单击**角色**下拉列表。在过滤框中输入\"Vertex AI\"，并选择**Vertex AI管理员**。在过滤框中输入\"Storage Object Admin\"，并选择**Storage Object Admin**。\n",
    "\n",
    "5. 点击**创建**。下载包含您的密钥的JSON文件到您的本地环境。\n",
    "\n",
    "6. 在下面的单元格中将您的服务帐户密钥路径输入为`GOOGLE_APPLICATION_CREDENTIALS`变量，然后运行该单元格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PyQmSRbKA8r-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "# The Vertex AI Workbench Notebook product has specific requirements\n",
    "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "\n",
    "# If on  Vertex AI Workbench Notebooks, then don't execute this code\n",
    "if not IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\"):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fea9a57a59b"
   },
   "source": [
    "# 调整 T5x 基础模型\n",
    "\n",
    "在这个示例中，您使用了经过微调的 T5x 基础模型来进行英语到德语的语言翻译。\n",
    "\n",
    "T5x 是一个基于 JAX 的模型，可以在 Google Cloud TPUs 上进行训练和微调，然后导出为 TensorFlow Saved 模型。\n",
    "\n",
    "要微调模型，请按照 https://github.com/google-research/t5x 上的步骤在 Cloud TPU VM 上微调模型。或者，您可以使用 Vertex Training 服务进行模型的微调，参考 https://github.com/GoogleCloudPlatform/t5x-on-vertex-ai 获取描述如何进行此操作的步骤。\n",
    "\n",
    "要导出微调后的模型，请参考 [导出为 TensorFlow Saved Model](https://github.com/google-research/t5x#exporting-as-tensorflow-saved-model) 部分。\n",
    "\n",
    "为了本指南的目的，您可以使用位于 gs://cloud-samples-data/vertex-ai/model-deployment/models/t5x/base/ 下的可用的微调模型。\n",
    "\n",
    "## 模型算法和权重类型\n",
    "\n",
    "请注意，有两种模型类型，一种是使用 `float32` 权重导出的，另一种是使用 `bfloat16` 权重导出的。\n",
    "\n",
    "`bfloat16` 是 Google Cloud TPUs 的一种原生格式，T5x 模型也默认使用它。\n",
    "NVIDIA A100 GPU 支持 `bfloat16` 算法，并且经过优化的 TensorFlow 运行时可以利用这一优势。\n",
    "如果您计划在不支持 `bfloat16` 的 GPU 上部署模型，比如 NVIDIA T4 或 NVIDIA V100，您需要使用带有 `float32` 权重的模型。幸运的是，经过优化的 TensorFlow 运行时具有一项优化，允许通过指定 `--allow_compression` 选项来在较低精度上运行模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6d793476c569"
   },
   "outputs": [],
   "source": [
    "T5X_BASE_FLOAT32_MODEL_URI = \"gs://cloud-samples-data/vertex-ai/model-deployment/models/t5x/base/saved_model.float32/1\"\n",
    "T5X_BASE_BFLOAT16_MODEL_URI = \"gs://cloud-samples-data/vertex-ai/model-deployment/models/t5x/base/saved_model.bfloat16/1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1d6602e97750"
   },
   "source": [
    "你可以使用`saved_model_cli`工具来观察 TensorFlow 的模型定义。可以忽略与`SentencepieceOp`相关的错误。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7b3cc6c3b0f3"
   },
   "outputs": [],
   "source": [
    "!saved_model_cli show --dir=$T5X_BASE_FLOAT32_MODEL_URI --all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "df9dcbe4727a"
   },
   "source": [
    "将模型部署到Vertex AI端点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8b80237db652"
   },
   "source": [
    "您可以使用[Vertex AI SDK](https://cloud.google.com/python/docs/reference/aiplatform/latest)部署模型，并将其导入到您的笔记本环境中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c369155c05c2"
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b095feae48f0"
   },
   "source": [
    "定义用于部署的节点配置。有关 Vertex AI 预测选项的详细信息，请参阅[配置计算资源](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e3f4d8bde9b5"
   },
   "source": [
    "您将使用优化后的TensorFlow运行时容器部署模型，请查看官方文档中提供的完整容器列表：https://cloud.google.com/vertex-ai/docs/predictions/optimized-tensorflow-runtime#available_container_images。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fa7bfb2b61ee"
   },
   "outputs": [],
   "source": [
    "OPTIMIZED_TF_RUNTIME_IMAGE_URI = (\n",
    "    \"us-docker.pkg.dev/vertex-ai-restricted/prediction/tf_opt-gpu.nightly:latest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3b7e5804f06"
   },
   "source": [
    "您将在NVIDIA T4 GPU上部署T5x模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bd6d54513910"
   },
   "outputs": [],
   "source": [
    "DEPLOY_COMPUTE_T4 = \"n1-standard-8\"\n",
    "DEPLOY_GPU_T4 = \"NVIDIA_TESLA_T4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fdbdd5a570a5"
   },
   "source": [
    "部署带有float32权重和无优化的T5x基本模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6832a146bfaa"
   },
   "outputs": [],
   "source": [
    "t5x_base_float32 = aiplatform.Model.upload(\n",
    "    display_name=\"t5x_base_float32\",\n",
    "    artifact_uri=T5X_BASE_FLOAT32_MODEL_URI,\n",
    "    serving_container_image_uri=OPTIMIZED_TF_RUNTIME_IMAGE_URI,\n",
    "    serving_container_args=[],\n",
    "    location=REGION,\n",
    ")\n",
    "\n",
    "t5x_base_float32_t4_endpoint = t5x_base_float32.deploy(\n",
    "    deployed_model_display_name=\"t5x_base_float32_deployed\",\n",
    "    traffic_split={\"0\": 100},\n",
    "    machine_type=DEPLOY_COMPUTE_T4,\n",
    "    accelerator_type=DEPLOY_GPU_T4,\n",
    "    accelerator_count=1,\n",
    "    min_replica_count=1,\n",
    "    max_replica_count=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ca3fc5656fb5"
   },
   "source": [
    "部署带有float32权重和预编译的T5x基础模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ce09e2c6bc6a"
   },
   "outputs": [],
   "source": [
    "t5x_base_float32_precompiled = aiplatform.Model.upload(\n",
    "    display_name=\"t5x_base_float32_precompiled\",\n",
    "    artifact_uri=T5X_BASE_FLOAT32_MODEL_URI,\n",
    "    serving_container_image_uri=OPTIMIZED_TF_RUNTIME_IMAGE_URI,\n",
    "    serving_container_args=[\"--allow_precompilation\"],\n",
    "    location=REGION,\n",
    ")\n",
    "\n",
    "t5x_base_float32_precompiled_t4_endpoint = t5x_base_float32_precompiled.deploy(\n",
    "    deployed_model_display_name=\"t5x_base_float32_precompiled_deployed\",\n",
    "    traffic_split={\"0\": 100},\n",
    "    machine_type=DEPLOY_COMPUTE_T4,\n",
    "    accelerator_type=DEPLOY_GPU_T4,\n",
    "    accelerator_count=1,\n",
    "    min_replica_count=1,\n",
    "    max_replica_count=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9f077f774408"
   },
   "source": [
    "部署具有float32权重、预编译和压缩的T5x基础模型。模型压缩优化模型，使计算密集型部分以较低的float16精度运行，并利用NVIDIA GPU TensorCores。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dac28ddef3ce"
   },
   "outputs": [],
   "source": [
    "t5x_base_float32_precompiled_mixedprecision = aiplatform.Model.upload(\n",
    "    display_name=\"t5x_base_float32_precompiled_mixedprecision\",\n",
    "    artifact_uri=T5X_BASE_FLOAT32_MODEL_URI,\n",
    "    serving_container_image_uri=OPTIMIZED_TF_RUNTIME_IMAGE_URI,\n",
    "    serving_container_args=[\"--allow_precompilation\", \"--allow_compression\"],\n",
    "    location=REGION,\n",
    ")\n",
    "\n",
    "t5x_base_float32_precompiled_mixedprecision_t4_endpoint = t5x_base_float32_precompiled_mixedprecision.deploy(\n",
    "    deployed_model_display_name=\"t5x_base_float32_precompiled_mixedprecision_deployed\",\n",
    "    traffic_split={\"0\": 100},\n",
    "    machine_type=DEPLOY_COMPUTE_T4,\n",
    "    accelerator_type=DEPLOY_GPU_T4,\n",
    "    accelerator_count=1,\n",
    "    min_replica_count=1,\n",
    "    max_replica_count=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "888aa8507e0c"
   },
   "source": [
    "为了达到最佳性能，您可以在具有对bfloat16算术支持的NVIDIA A100上部署带有bfloat16权重的T5x基本模型。请注意，为了有效利用bfloat16逻辑，模型必须通过预编译部署。由于模型已经以半精度运行，因此不需要模型压缩。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "29bd5ce90f1a"
   },
   "outputs": [],
   "source": [
    "DEPLOY_COMPUTE_A100 = \"a2-highgpu-1g\"\n",
    "DEPLOY_GPU_A100 = \"NVIDIA_TESLA_A100\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cbaedf0b4f1e"
   },
   "outputs": [],
   "source": [
    "t5x_base_float32_a100_endpoint = t5x_base_float32.deploy(\n",
    "    deployed_model_display_name=\"t5x_base_float32_deployed\",\n",
    "    traffic_split={\"0\": 100},\n",
    "    machine_type=DEPLOY_COMPUTE_A100,\n",
    "    accelerator_type=DEPLOY_GPU_A100,\n",
    "    accelerator_count=1,\n",
    "    min_replica_count=1,\n",
    "    max_replica_count=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "80f45a86d18a"
   },
   "outputs": [],
   "source": [
    "t5x_base_bfloat16_precompiled = aiplatform.Model.upload(\n",
    "    display_name=\"t5x_base_bfloat16_precompiled\",\n",
    "    artifact_uri=T5X_BASE_BFLOAT16_MODEL_URI,\n",
    "    serving_container_image_uri=OPTIMIZED_TF_RUNTIME_IMAGE_URI,\n",
    "    serving_container_args=[\"--allow_precompilation\"],\n",
    "    location=REGION,\n",
    ")\n",
    "\n",
    "t5x_base_bfloat16_precompiled_a100_endpoint = t5x_base_bfloat16_precompiled.deploy(\n",
    "    deployed_model_display_name=\"t5x_base_bfloat16_precompiled_deployed\",\n",
    "    traffic_split={\"0\": 100},\n",
    "    machine_type=DEPLOY_COMPUTE_A100,\n",
    "    accelerator_type=DEPLOY_GPU_A100,\n",
    "    accelerator_count=1,\n",
    "    min_replica_count=1,\n",
    "    max_replica_count=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0016bab3f15d"
   },
   "source": [
    "发送预测请求"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ca1dfbcdf81e"
   },
   "source": [
    "您可以直接从每个端点发送请求。T5x模型期望数据是以字典形式存储的，其中包含键为\"text_batch\"（参见上面`saved_model_cli`调用的响应）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9e596e14f2f0"
   },
   "outputs": [],
   "source": [
    "instances = [{\"text_batch\": \"translate English to German: this is good\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ae0e1eef2cb"
   },
   "outputs": [],
   "source": [
    "t5x_base_float32_t4_endpoint.predict(instances=instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f1b562190c2e"
   },
   "outputs": [],
   "source": [
    "t5x_base_float32_precompiled_t4_endpoint.predict(instances=instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ff281732942"
   },
   "outputs": [],
   "source": [
    "t5x_base_float32_precompiled_mixedprecision_t4_endpoint.predict(instances=instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b04cb34b872c"
   },
   "outputs": [],
   "source": [
    "t5x_base_float32_a100_endpoint.predict(instances=instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1f58ddf970a4"
   },
   "outputs": [],
   "source": [
    "t5x_base_bfloat16_precompiled_a100_endpoint.predict(instances=instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddac29c44fab"
   },
   "source": [
    "另外，您还可以发送 POST REST 请求而不使用 SDK。 了解更多关于 https://cloud.google.com/vertex-ai/docs/predictions/online-predictions-custom-models#online_predict_custom_trained-drest。 这种方法稍微更快。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c36d488325dd"
   },
   "source": [
    "## 比较预测\n",
    "\n",
    "为了确保所有模型返回相同的结果，向所有端点发送相同的请求，并比较预测结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3746a32d3e05"
   },
   "outputs": [],
   "source": [
    "# Add your samples here\n",
    "samples = [\n",
    "    \"hello world\",\n",
    "    \"this is a prediction from T5x model\",\n",
    "    \"this is good\",\n",
    "    \"my name is T5x\",\n",
    "]\n",
    "\n",
    "endpoints = {\n",
    "    \"t5x_base_float32_t4\": t5x_base_float32_t4_endpoint,\n",
    "    \"t5x_base_float32_precompiled_t4\": t5x_base_float32_precompiled_t4_endpoint,\n",
    "    \"t5x_base_float32_precompiled_mixedprecision_t4\": t5x_base_float32_precompiled_mixedprecision_t4_endpoint,\n",
    "    \"t5x_base_float32_a100\": t5x_base_float32_a100_endpoint,\n",
    "    \"t5x_base_bfloat16_precompiled_a100\": t5x_base_bfloat16_precompiled_a100_endpoint,\n",
    "}\n",
    "\n",
    "prefix = \"translate English to German: \"\n",
    "\n",
    "for sample in samples:\n",
    "    print(f\"Prediction for: {prefix}{sample}\")\n",
    "    for model_name, endpoint in endpoints.items():\n",
    "        response = endpoint.predict(instances=[{\"text_batch\": f\"{prefix}{sample}\"}])\n",
    "        prediction = response.predictions[0][\"output_0\"][0]\n",
    "        print(f\"Model: {model_name} Prediction: {prediction}\")\n",
    "    print(\"-----------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5ce470e856f"
   },
   "source": [
    "##（可选）比较部署模型的性能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f02ce0cc8d3c"
   },
   "source": [
    "你可以在Colab环境中运行基准测试，为了获得可靠的结果，你应该使用与你的模型相同区域的虚拟机。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c047d35d3ef6"
   },
   "source": [
    "导入用于基准测试模型的辅助函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2a52069756ba"
   },
   "outputs": [],
   "source": [
    "!curl https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/vertex_endpoints/optimized_tensorflow_runtime/benchmark.py -o benchmark.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "92e7ff416f7c"
   },
   "outputs": [],
   "source": [
    "from benchmark import benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "287028072667"
   },
   "source": [
    "这段代码以给定的QPS异步均匀地发送指定数量的请求，然后记录观察到的延迟。接下来，聚合延迟结果并计算百分位数。模型能够处理的实际每秒请求数（actual_qps）是指模型处理发送的请求所需的时间除以请求数。通过为 send_request 和 build_request 函数提供不同的实现，相同的代码可以用于对在本地运行或使用 gRPC 和 REST 协议在 Vertex AI Prediction 上运行的模型进行基准测试。\n",
    "\n",
    "此基准测试的主要目标是测量模型在不同负载下的延迟，以及模型能够处理的最大吞吐量。为了找到最大吞吐量，逐渐增加 QPS 直到 actual_qps 停止增加并且延迟急剧增加。\n",
    "\n",
    "在生产部署中，工作负载并不是均匀的，因此最大模型吞吐量可能较低。目标不是模拟生产工作负载，这个基准测试旨在比较在不同环境中运行相同模型时的延迟和吞吐量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "382954a4d93b"
   },
   "outputs": [],
   "source": [
    "def build_rest_request(row_dict, model_name):\n",
    "    return row_dict\n",
    "\n",
    "\n",
    "def validate_response(response):\n",
    "    assert response\n",
    "    assert len(response.predictions) == 1\n",
    "    assert \"output_0\" in response.predictions[0]\n",
    "    assert response.predictions[0][\"output_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7c4df717c84b"
   },
   "outputs": [],
   "source": [
    "def send_rest_request(request):\n",
    "    response = t5x_base_float32_t4_endpoint.predict(instances=[request])\n",
    "    validate_response(response)\n",
    "\n",
    "\n",
    "t5x_base_float32_t4_results = benchmark(\n",
    "    send_rest_request,\n",
    "    build_rest_request,\n",
    "    \"gs://cloud-samples-data/vertex-ai/model-deployment/models/t5x/requests/requests_100.jsonl\",\n",
    "    [0.5, 0.75, 1, 1.25, 1.5, 1.75, 2.0],\n",
    "    10,\n",
    ")\n",
    "\n",
    "t5x_base_float32_t4_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "01c88cf59367"
   },
   "outputs": [],
   "source": [
    "def send_rest_request(request):\n",
    "    response = t5x_base_float32_precompiled_t4_endpoint.predict(instances=[request])\n",
    "    validate_response(response)\n",
    "\n",
    "\n",
    "t5x_base_float32_precompiled_t4_results = benchmark(\n",
    "    send_rest_request,\n",
    "    build_rest_request,\n",
    "    \"gs://cloud-samples-data/vertex-ai/model-deployment/models/t5x/requests/requests_100.jsonl\",\n",
    "    [0.5, 1, 2, 3, 4, 5],\n",
    "    10,\n",
    ")\n",
    "\n",
    "t5x_base_float32_precompiled_t4_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a4311bebc18d"
   },
   "outputs": [],
   "source": [
    "def send_rest_request(request):\n",
    "    response = t5x_base_float32_precompiled_mixedprecision_t4_endpoint.predict(\n",
    "        instances=[request]\n",
    "    )\n",
    "    validate_response(response)\n",
    "\n",
    "\n",
    "t5x_base_float32_precompiled_mixedprecision_t4_results = benchmark(\n",
    "    send_rest_request,\n",
    "    build_rest_request,\n",
    "    \"gs://cloud-samples-data/vertex-ai/model-deployment/models/t5x/requests/requests_100.jsonl\",\n",
    "    [0.5, 1, 2, 3, 4, 5],\n",
    "    10,\n",
    ")\n",
    "\n",
    "t5x_base_float32_precompiled_mixedprecision_t4_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aeff92ba97ba"
   },
   "outputs": [],
   "source": [
    "def send_rest_request(request):\n",
    "    response = t5x_base_float32_a100_endpoint.predict(instances=[request])\n",
    "    validate_response(response)\n",
    "\n",
    "\n",
    "t5x_base_float32_a100_results = benchmark(\n",
    "    send_rest_request,\n",
    "    build_rest_request,\n",
    "    \"gs://cloud-samples-data/vertex-ai/model-deployment/models/t5x/requests/requests_100.jsonl\",\n",
    "    [0.5, 1, 1.25, 1.5, 1.75, 2.0, 2.25],\n",
    "    10,\n",
    ")\n",
    "\n",
    "t5x_base_float32_a100_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e4a1ba7d2a58"
   },
   "outputs": [],
   "source": [
    "def send_rest_request(request):\n",
    "    response = t5x_base_bfloat16_precompiled_a100_endpoint.predict(instances=[request])\n",
    "    validate_response(response)\n",
    "\n",
    "\n",
    "t5x_base_bfloat16_precompiled_a100_results = benchmark(\n",
    "    send_rest_request,\n",
    "    build_rest_request,\n",
    "    \"gs://cloud-samples-data/vertex-ai/model-deployment/models/t5x/requests/requests_100.jsonl\",\n",
    "    [0.5, 5, 7.5, 10, 12.5, 15],\n",
    "    10,\n",
    ")\n",
    "\n",
    "t5x_base_bfloat16_precompiled_a100_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5f855885e50f"
   },
   "source": [
    "整合并可视化结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1afa1a329b0e"
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def build_graph(x_key, y_key, results_dict, axis, title=\"T5x base model latency\"):\n",
    "    matplotlib.rcParams[\"figure.figsize\"] = [10.0, 7.0]\n",
    "\n",
    "    fig, ax = plt.subplots(facecolor=(1, 1, 1))\n",
    "    ax.set_xlabel(\"QPS\")\n",
    "    ax.set_ylabel(\"Latency(ms)\")\n",
    "    for label, results in results_dict.items():\n",
    "        x = np.array(results[x_key])\n",
    "        y = np.array(results[y_key])\n",
    "        ax.plot(x, y, label=label, marker=\"s\")\n",
    "    ax.grid()\n",
    "    ax.legend()\n",
    "    ax.axis(axis)\n",
    "    ax.set_title(title)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "204d66f54e46"
   },
   "outputs": [],
   "source": [
    "fig = build_graph(\n",
    "    \"actual_qps\",\n",
    "    \"p50\",\n",
    "    {\n",
    "        \"T5x base float32 on T4\": t5x_base_float32_t4_results,\n",
    "        \"T5x base float32 on T4 with precompilation\": t5x_base_float32_precompiled_t4_results,\n",
    "        \"T5x base float32 on T4 with precompilation and compression\": t5x_base_float32_precompiled_mixedprecision_t4_results,\n",
    "        \"T5x base float32 on A100\": t5x_base_float32_a100_results,\n",
    "        \"T5x base bfloat16 on A100 with precompilation\": t5x_base_bfloat16_precompiled_a100_results,\n",
    "    },\n",
    "    (0, 10, 0, 2500),\n",
    "    title=\"T5x base model p50 latency, batch size 1\",\n",
    ")\n",
    "fig.savefig(\"t5x_base_p50_latency.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7251aa3b7721"
   },
   "outputs": [],
   "source": [
    "fig = build_graph(\n",
    "    \"actual_qps\",\n",
    "    \"p99\",\n",
    "    {\n",
    "        \"T5x base float32 on T4\": t5x_base_float32_t4_results,\n",
    "        \"T5x base float32 on T4 with precompilation\": t5x_base_float32_precompiled_t4_results,\n",
    "        \"T5x base float32 on T4 with precompilation and compression\": t5x_base_float32_precompiled_mixedprecision_t4_results,\n",
    "        \"T5x base float32 on A100\": t5x_base_float32_a100_results,\n",
    "        \"T5x base bfloat16 on A100 with precompilation\": t5x_base_bfloat16_precompiled_a100_results,\n",
    "    },\n",
    "    (0, 5, 0, 5500),\n",
    "    title=\"T5x base model p99 latency, batch size 1\",\n",
    ")\n",
    "fig.savefig(\"t5x_base_p99_latency.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8e97703a929"
   },
   "source": [
    "正如您所看到的，Vertex AI Prediction 优化了 TensorFlow 运行时优化，为 T5x 基础模型提供了显著更高的吞吐量和更低的延迟。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06f2711e6221"
   },
   "source": [
    "## （可选）使用MLPerf推断loadgen比较部署模型的性能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adcd7c46de26"
   },
   "source": [
    "MLPerf 推理是一个基准套件，用于衡量系统在各种部署场景中运行模型的速度。MLPerf 现在是衡量模型性能的行业标准方式。您可以在 https://github.com/tensorflow/tpu/tree/master/models/experimental/inference/load_test 上按照说明运行部署模型的 MLPerf 推理基准测试。\n",
    "\n",
    "与以前使用的简单基准测试不同，MLPerf loadgen 使用 Poisson 分布发送请求。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d73bf182f27a"
   },
   "outputs": [],
   "source": [
    "project_id = t5x_base_float32_t4_endpoint.resource_name.split(\"/\")[1]\n",
    "project_id\n",
    "\n",
    "endpoint_id = t5x_base_float32_t4_endpoint.resource_name.split(\"/\")[-1]\n",
    "endpoint_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "51a52c6698de"
   },
   "outputs": [],
   "source": [
    "%cd tpu/models/experimental/inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c4f70d73f164"
   },
   "outputs": [],
   "source": [
    "!python3 -m load_test.examples.loadgen_vertex_main \\\n",
    "  --project_id={project_id} \\\n",
    "  --region={REGION} \\\n",
    "  --endpoint_id={t5x_base_float32_t4_endpoint.resource_name.split(\"/\")[-1]} \\\n",
    "  --dataset=generic_jsonl \\\n",
    "  --data_file=gs://cloud-samples-data/vertex-ai/model-deployment/models/t5x/requests/requests_100.jsonl \\\n",
    "  --api_type=rest \\\n",
    "  --min_query_count=10 \\\n",
    "  --min_duration_ms=10000 \\\n",
    "  --qps=0.5 --qps=1.0 --qps=1.25 --qps=1.5 --qps=1.75 --qps=2.0 \\\n",
    "  --csv_report_filename=\"t5x_base_float32_t4_results.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "456e293c8419"
   },
   "outputs": [],
   "source": [
    "!python3 -m load_test.examples.loadgen_vertex_main \\\n",
    "  --project_id={project_id} \\\n",
    "  --region={REGION} \\\n",
    "  --endpoint_id={t5x_base_float32_precompiled_t4_endpoint.resource_name.split(\"/\")[-1]} \\\n",
    "  --dataset=generic_jsonl \\\n",
    "  --data_file=gs://cloud-samples-data/vertex-ai/model-deployment/models/t5x/requests/requests_100.jsonl \\\n",
    "  --api_type=rest \\\n",
    "  --min_query_count=10 \\\n",
    "  --min_duration_ms=10000 \\\n",
    "  --qps=0.5 --qps=1 --qps=2 --qps=3 --qps=4 --qps=5 \\\n",
    "  --csv_report_filename=\"t5x_base_float32_precompiled_t4_results.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "11b28328eb97"
   },
   "outputs": [],
   "source": [
    "!python3 -m load_test.examples.loadgen_vertex_main \\\n",
    "  --project_id={project_id} \\\n",
    "  --region={REGION} \\\n",
    "  --endpoint_id={t5x_base_float32_precompiled_mixedprecision_t4_endpoint.resource_name.split(\"/\")[-1]} \\\n",
    "  --dataset=generic_jsonl \\\n",
    "  --data_file=gs://cloud-samples-data/vertex-ai/model-deployment/models/t5x/requests/requests_100.jsonl \\\n",
    "  --api_type=rest \\\n",
    "  --min_query_count=10 \\\n",
    "  --min_duration_ms=10000 \\\n",
    "  --qps=0.5 --qps=1 --qps=2 --qps=3 --qps=4 --qps=5 --qps=6 \\\n",
    "  --csv_report_filename=\"t5x_base_float32_precompiled_mixedprecision_t4_results.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "23342227c786"
   },
   "outputs": [],
   "source": [
    "!python3 -m load_test.examples.loadgen_vertex_main \\\n",
    "  --project_id={project_id} \\\n",
    "  --region={REGION} \\\n",
    "  --endpoint_id={t5x_base_float32_a100_endpoint.resource_name.split(\"/\")[-1]} \\\n",
    "  --dataset=generic_jsonl \\\n",
    "  --data_file=gs://cloud-samples-data/vertex-ai/model-deployment/models/t5x/requests/requests_100.jsonl \\\n",
    "  --api_type=rest \\\n",
    "  --min_query_count=10 \\\n",
    "  --min_duration_ms=10000 \\\n",
    "  --qps=0.5 --qps=1.0 --qps=1.25 --qps=1.5 --qps=1.75 --qps=2.0 --qps=2.25 \\\n",
    "  --csv_report_filename=\"t5x_base_float32_a100_results.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d6ad17ddc3d7"
   },
   "outputs": [],
   "source": [
    "!python3 -m load_test.examples.loadgen_vertex_main \\\n",
    "  --project_id={project_id} \\\n",
    "  --region={REGION} \\\n",
    "  --endpoint_id={t5x_base_bfloat16_precompiled_a100_endpoint.resource_name.split(\"/\")[-1]} \\\n",
    "  --dataset=generic_jsonl \\\n",
    "  --data_file=gs://cloud-samples-data/vertex-ai/model-deployment/models/t5x/requests/requests_100.jsonl \\\n",
    "  --api_type=rest \\\n",
    "  --min_query_count=10 \\\n",
    "  --min_duration_ms=10000 \\\n",
    "  --qps=0.5 --qps=5 --qps=7.5 --qps=10 --qps=12.5 --qps=15 \\\n",
    "  --csv_report_filename=\"t5x_base_bfloat16_precompiled_a100_results.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "310d3f7291c4"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "def parse_report_csv(file_name):\n",
    "    with open(file_name, newline=\"\") as f:\n",
    "        reader = csv.reader(f)\n",
    "\n",
    "        d = {}\n",
    "        index_to_key = {}\n",
    "        for row in reader:\n",
    "            if not d:\n",
    "                for index in range(len(row)):\n",
    "                    key = row[index]\n",
    "                    index_to_key[index] = key\n",
    "                    d[key] = []\n",
    "            else:\n",
    "                for index in range(len(row)):\n",
    "                    if index_to_key[index] != \"scenario\":\n",
    "                        d[index_to_key[index]].append(float(row[index]))\n",
    "                    else:\n",
    "                        d[index_to_key[index]].append(row[index])\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "09b0dd57beb1"
   },
   "outputs": [],
   "source": [
    "fig = build_graph(\n",
    "    \"actual_qps\",\n",
    "    \"p50\",\n",
    "    {\n",
    "        \"T5x base float32 on T4\": parse_report_csv(\"t5x_base_float32_t4_results.csv\"),\n",
    "        \"T5x base float32 on T4 with precompilation\": parse_report_csv(\n",
    "            \"t5x_base_float32_precompiled_t4_results.csv\"\n",
    "        ),\n",
    "        \"T5x base float32 on T4 with precompilation and compression\": parse_report_csv(\n",
    "            \"t5x_base_float32_precompiled_mixedprecision_t4_results.csv\"\n",
    "        ),\n",
    "        \"T5x base float32 on A100\": parse_report_csv(\n",
    "            \"t5x_base_float32_a100_results.csv\"\n",
    "        ),\n",
    "        \"T5x base bfloat16 on A100 with precompilation\": parse_report_csv(\n",
    "            \"t5x_base_bfloat16_precompiled_a100_results.csv\"\n",
    "        ),\n",
    "    },\n",
    "    (0, 10, 0, 2500),\n",
    "    title=\"T5x base model p50 latency measured by MLPerf loadgen, batch size 1\",\n",
    ")\n",
    "fig.savefig(\"t5x_base_p50_mlperf_latency.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ffb220d9cd6"
   },
   "outputs": [],
   "source": [
    "fig = build_graph(\n",
    "    \"actual_qps\",\n",
    "    \"p99\",\n",
    "    {\n",
    "        \"T5x base float32 on T4\": parse_report_csv(\"t5x_base_float32_t4_results.csv\"),\n",
    "        \"T5x base float32 on T4 with precompilation\": parse_report_csv(\n",
    "            \"t5x_base_float32_precompiled_t4_results.csv\"\n",
    "        ),\n",
    "        \"T5x base float32 on T4 with precompilation and compression\": parse_report_csv(\n",
    "            \"t5x_base_float32_precompiled_mixedprecision_t4_results.csv\"\n",
    "        ),\n",
    "        \"T5x base float32 on A100\": parse_report_csv(\n",
    "            \"t5x_base_float32_a100_results.csv\"\n",
    "        ),\n",
    "        \"T5x base bfloat16 on A100 with precompilation\": parse_report_csv(\n",
    "            \"t5x_base_bfloat16_precompiled_a100_results.csv\"\n",
    "        ),\n",
    "    },\n",
    "    (0, 10, 0, 3500),\n",
    "    title=\"T5x base model p99 latency measured by MLPerf loadgen, batch size 1\",\n",
    ")\n",
    "fig.savefig(\"t5x_base_p99_mlperf_latency.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f35eff5aea2e"
   },
   "source": [
    "这些结果与使用朴素基准代码获得的结果大致一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c69570acca3d"
   },
   "source": [
    "清理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "df0bd8f8c5f2"
   },
   "source": [
    "完成后，可以安全地移除您创建的端点和部署的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ab6cb1891003"
   },
   "outputs": [],
   "source": [
    "# Undeploy models\n",
    "t5x_base_float32_t4_endpoint.undeploy_all()\n",
    "t5x_base_float32_precompiled_t4_endpoint.undeploy_all()\n",
    "t5x_base_float32_precompiled_mixedprecision_t4_endpoint.undeploy_all()\n",
    "t5x_base_float32_a100_endpoint.undeploy_all()\n",
    "t5x_base_bfloat16_precompiled_a100_endpoint.undeploy_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d1389efff614"
   },
   "outputs": [],
   "source": [
    "# Delete models\n",
    "t5x_base_float32.delete()\n",
    "t5x_base_float32_precompiled.delete()\n",
    "t5x_base_float32_precompiled_mixedprecision.delete()\n",
    "t5x_base_bfloat16_precompiled.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "33f5b617c39b"
   },
   "outputs": [],
   "source": [
    "# Delete endpoints\n",
    "t5x_base_float32_t4_endpoint.delete()\n",
    "t5x_base_float32_precompiled_t4_endpoint.delete()\n",
    "t5x_base_float32_precompiled_mixedprecision_t4_endpoint.delete()\n",
    "t5x_base_float32_a100_endpoint.delete()\n",
    "t5x_base_bfloat16_precompiled_a100_endpoint.delete()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "t5x_base_optimized_online_prediction.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
