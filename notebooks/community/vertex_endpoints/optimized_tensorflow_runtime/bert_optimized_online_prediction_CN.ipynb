{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/vertex_endpoints/optimized_tensorflow_runtime/bert_optimized_online_prediction.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> 在 Colab 中运行\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/vertex_endpoints/optimized_tensorflow_runtime/bert_optimized_online_prediction.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      在 GitHub 上查看\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/vertex_endpoints/optimized_tensorflow_runtime/bert_optimized_online_prediction.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      在 Vertex AI Workbench 中打开\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c214896cc5a2"
   },
   "source": [
    "# 调整 BERT 基础分类模型并将其部署到使用优化 TensorFlow 运行时的 Vertex AI 预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## 概述\n",
    "\n",
    "在这个示例中，您将学习如何微调BERT基础分类模型进行情感分析。\n",
    "\n",
    "然后，您将使用基于开源的TensorFlow 2.7容器和优化的TensorFlow运行时容器将训练好的模型导出到Vertex AI预测服务，并对这些模型进行性能评估并比较它们的预测结果。\n",
    "\n",
    "有关Vertex AI预测优化的TensorFlow运行时容器的其他信息，请参阅https://cloud.google.com/vertex-ai/docs/predictions/optimized-tensorflow-runtime。\n",
    "\n",
    "### 数据集\n",
    "\n",
    "这个笔记本训练一个情感分析模型，根据影评的文本将影评分类为积极或消极。\n",
    "\n",
    "您将使用包含来自互联网电影数据库的50,000部影评的[大型影评数据集](https://ai.stanford.edu/~amaas/data/sentiment/)。\n",
    "\n",
    "### 目标\n",
    "\n",
    "在这个笔记本中，您将学习如何使用优化的TensorFlow运行时将微调的BERT分类模型部署到Vertex AI预测中。接下来，您将比较其性能与基于开源的TensorFlow容器的性能。\n",
    "\n",
    "您执行的步骤包括：\n",
    "* 下载并预处理[大型影评数据集](https://ai.stanford.edu/~amaas/data/sentiment/)\n",
    "* 从TF hub下载BERT基础模型\n",
    "* 微调BERT分类模型\n",
    "* 使用TensorFlow 2.7容器将模型部署到Vertex AI预测\n",
    "* 使用优化的TensorFlow运行时容器将模型部署到Vertex AI预测\n",
    "* 对模型进行基准测试并验证它们的预测结果\n",
    "\n",
    "您可以在Colab上微调BERT模型并将其上传到Vertex AI预测。为了获得可靠的基准测试结果，这个演示必须在与您的模型相同地区运行的Jupyter VM上运行。\n",
    "\n",
    "### 成本\n",
    "\n",
    "本教程使用Google Cloud的以下收费部分：\n",
    "\n",
    "* Vertex AI\n",
    "* 云存储\n",
    "\n",
    "了解[Vertex AI的定价](https://cloud.google.com/vertex-ai/pricing)和[云存储的定价](https://cloud.google.com/storage/pricing)，并使用[定价计算器](https://cloud.google.com/products/calculator/)根据您的预计使用量生成成本估算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ze4-nDLfK4pw"
   },
   "source": [
    "### 设置您的本地开发环境\n",
    "\n",
    "**如果您正在使用Colab或Vertex AI Workbench笔记本**，您的环境已经满足运行此笔记本的所有要求。您可以跳过这一步。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCuSR8GkAgzl"
   },
   "source": [
    "否则，请确保您的环境符合此笔记本的要求。\n",
    "您需要以下内容：\n",
    "\n",
    "* Google Cloud SDK\n",
    "* Git\n",
    "* Python 3\n",
    "* virtualenv\n",
    "* 在使用Python 3的虚拟环境中运行的Jupyter笔记本\n",
    "* 安装了GPU驱动程序和CUDA 11.2\n",
    "\n",
    "Google Cloud指南中的[设置Python开发环境](https://cloud.google.com/python/setup)和[Jupyter安装指南](https://jupyter.org/install)提供了详细的说明来满足这些要求。以下步骤提供了一套简洁的说明：\n",
    "\n",
    "1. [安装并初始化Cloud SDK。](https://cloud.google.com/sdk/docs/)\n",
    "\n",
    "2. [安装Python 3。](https://cloud.google.com/python/setup#installing_python)\n",
    "\n",
    "3. [安装virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv)并创建一个使用Python 3的虚拟环境。激活虚拟环境。\n",
    "\n",
    "4. 要安装Jupyter，请在终端窗口中运行 `pip3 install jupyter`。\n",
    "\n",
    "5. 要启动Jupyter，请在终端窗口中运行 `jupyter notebook`。\n",
    "\n",
    "6. 在Jupyter Notebook Dashboard中打开此笔记本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "### 安装额外的包\n",
    "\n",
    "安装在您的笔记本环境中尚未安装的额外包依赖项，如tensorflow、tensorflow-text、tensorflow serving APIs 和 Vertex AI SDK。请使用每个包的最新稳定版。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2b4ef9b72d43"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Vertex AI Workbench Notebook product has specific requirements\n",
    "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "\n",
    "# Vertex AI Workbench Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wyy5Lbnzg5fi"
   },
   "outputs": [],
   "source": [
    "! pip3 install {USER_FLAG} --upgrade tensorflow==2.7.0 -q\n",
    "! pip3 install {USER_FLAG} --upgrade tensorflow-text==2.7.0 -q\n",
    "! pip3 install {USER_FLAG} --upgrade tensorflow-serving-api==2.7.0 -q\n",
    "! pip3 install {USER_FLAG} --upgrade tf-models-official==2.7.0 -q\n",
    "! pip3 install {USER_FLAG} --upgrade google-cloud-aiplatform -q\n",
    "! pip3 install {USER_FLAG} --upgrade google-cloud-storage -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhq5zEbGg0XX"
   },
   "source": [
    "### 重新启动内核\n",
    "\n",
    "在安装额外的包之后，您必须重新启动笔记本内核，以便它可以找到这些包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EzrelQZ22IZj"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWEdiXsJg0XY"
   },
   "source": [
    "在开始之前"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b37054d4d064"
   },
   "source": [
    "### 选择 GPU 运行时\n",
    "\n",
    "**如果可以的话，请确保使用 GPU 运行时来运行这个笔记本。在 Colab 中，选择 \"运行时 --> 更改运行时类型 > GPU\"**。\n",
    "\n",
    "请注意，为了能够使用 GPU 对模型进行微调，您的虚拟机需要安装 GPU 驱动程序和 CUDA 11.2。您可以使用 [TensorFlow Enterprise 2.7](https://cloud.google.com/tensorflow-enterprise/docs/use-with-notebooks) 用户管理的笔记本实例或在带有 GPU 运行时的 Colab 上进行操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF1j6f9HApxa"
   },
   "source": [
    "### 设置您的Google Cloud项目\n",
    "\n",
    "**以下步骤适用于所有笔记本环境。**\n",
    "\n",
    "1. [选择或创建一个Google Cloud项目](https://console.cloud.google.com/cloud-resource-manager)。当您首次创建帐户时，您将获得$300的信用额度，用于支付计算和存储成本。\n",
    "\n",
    "1. [确保您的项目已启用计费](https://cloud.google.com/billing/docs/how-to/modify-project)。\n",
    "\n",
    "1. [启用Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com)。\n",
    "\n",
    "1. 如果您在本地运行此笔记本，您必须安装[Cloud SDK](https://cloud.google.com/sdk)。\n",
    "\n",
    "1. 在以下单元格中输入您的项目ID。然后运行该单元格，以确保Cloud SDK为本笔记本中的所有命令使用正确的项目。\n",
    "\n",
    "**注意**：Jupyter会运行以`!`为前缀的行作为shell命令，并将以`$`为前缀的Python变量插入这些命令中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WReHDGG5g0XY"
   },
   "source": [
    "设置您的项目ID\n",
    "\n",
    "**如果您不知道您的项目ID**，您可以尝试使用 `gcloud` 获取您的项目ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oM1iC_MfAts1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_ID = \"\"\n",
    "\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJYoRfYng0XZ"
   },
   "source": [
    "否则，请在这里设定您的项目ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "riG_qUokg0XZ"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None:\n",
    "    PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06571eb4063b"
   },
   "source": [
    "时间戳\n",
    "\n",
    "如果您在一个直播教程会话中，可能会使用一个共享的测试账户或项目。为了避免资源名称冲突，为每个实例会话创建一个时间戳，然后将其附加到您在本教程中创建的资源名称中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "697568e92bd6"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dr--iN2kAylZ"
   },
   "source": [
    "### 验证您的 Google Cloud 账户\n",
    "\n",
    "**如果您正在使用 Vertex AI Workbench 笔记本**，您的环境已经经过验证。请跳过此步骤。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBCra4QMA2wR"
   },
   "source": [
    "如果您正在使用Colab，请运行下面的单元格，并按照提示进行oAuth身份验证。\n",
    "\n",
    "**否则**，请按照以下步骤操作：\n",
    "\n",
    "1. 在Cloud控制台中，转到[**创建服务帐号密钥**页面](https://console.cloud.google.com/apis/credentials/serviceaccountkey)。\n",
    "\n",
    "2. 点击**创建服务帐号**。\n",
    "\n",
    "3. 在**服务帐号名称**字段中输入一个名称，然后点击**创建**。\n",
    "\n",
    "4. 在**授予此服务帐号对项目的访问权限**部分，点击**角色**下拉列表。在过滤框中输入“Vertex AI”，并选择**Vertex AI管理员**。在过滤框中输入“存储对象管理员”，并选择**存储对象管理员**。\n",
    "\n",
    "5. 点击**创建**。一个包含您密钥的JSON文件将下载到本地环境中。\n",
    "\n",
    "6. 在以下单元格中将您的服务帐号密钥的路径作为`GOOGLE_APPLICATION_CREDENTIALS`变量输入，然后运行该单元格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PyQmSRbKA8r-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "# The Vertex AI Workbench Notebook product has specific requirements\n",
    "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "\n",
    "# If on  Vertex AI Workbench Notebooks, then don't execute this code\n",
    "if not IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\"):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "### 创建一个云存储存储桶\n",
    "\n",
    "**所有笔记本环境都需要执行以下步骤。**\n",
    "\n",
    "为了让Vertex AI Prediction能够为您的模型提供服务，必须首先将其上传到云存储存储桶中。\n",
    "\n",
    "请在下方设置您的云存储存储桶的名称。它必须是所有云存储存储桶中唯一的。\n",
    "\n",
    "您还可以更改`REGION`变量，该变量在接下来的笔记本中的操作中使用。我们建议您[选择Vertex AI服务可用的区域](https://cloud.google.com/vertex-ai/docs/general/locations#available_regions)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "BUCKET_URI = \"gs://[your-bucket-name]\"  # @param {type:\"string\"}\n",
    "REGION = \"[your-region]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cf221059d072"
   },
   "outputs": [],
   "source": [
    "if BUCKET_URI == \"\" or BUCKET_URI is None or BUCKET_URI == \"gs://[your-bucket-name]\":\n",
    "    BUCKET_URI = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP\n",
    "\n",
    "if REGION == \"[your-region]\":\n",
    "    REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EcIXiGsCePi"
   },
   "source": [
    "只有在您的存储桶不存在时才运行以下单元格以创建您的云存储桶。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIq7R4HZCfIc"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucvCsknMCims"
   },
   "source": [
    "您的云存储桶的最后一步是通过检查其内容来验证对云存储桶的访问权限。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vhOb7YnwClBb"
   },
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "导入库并定义常量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRUOFELefqf1"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import requests as r\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text  # noqa: F401\n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "\n",
    "r.packages.urllib3.disable_warnings()\n",
    "\n",
    "logging = tf.get_logger()\n",
    "logging.propagate = False\n",
    "logging.setLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "08881d110f47"
   },
   "outputs": [],
   "source": [
    "LOCAL_DIRECTORY = \"~/bert_classification\"  # @param {type:\"string\"}\n",
    "LOCAL_DIRECTORY_FULL = os.path.expanduser(LOCAL_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9cbb5302a3c4"
   },
   "source": [
    "下载数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a762c93fb848"
   },
   "source": [
    "从[互联网电影数据库](https://www.imdb.com/)下载[大型电影评论数据集](https://ai.stanford.edu/~amaas/data/sentiment/)。该数据集包含了5万条电影评论的文本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fbc5c536e55e"
   },
   "outputs": [],
   "source": [
    "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "dataset = tf.keras.utils.get_file(\n",
    "    \"aclImdb_v1.tar.gz\", url, untar=True, cache_dir=\".\", cache_subdir=\"\"\n",
    ")\n",
    "\n",
    "dataset_dir = os.path.join(os.path.dirname(dataset), \"aclImdb\")\n",
    "\n",
    "train_dir = os.path.join(dataset_dir, \"train\")\n",
    "\n",
    "# remove unused folders to make it easier to load the data\n",
    "remove_dir = os.path.join(train_dir, \"unsup\")\n",
    "shutil.rmtree(remove_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8b7051896d19"
   },
   "source": [
    "## 预处理数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5e912d5db172"
   },
   "source": [
    "IMDB数据集已经被分为训练集和测试集，但缺少一个验证集。要创建一个验证集，在下一个单元格中，可以使用训练数据的80:20拆分来使用`validation_split`参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b45dfad0d580"
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "batch_size = 32\n",
    "seed = 42\n",
    "\n",
    "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    \"aclImdb/train\",\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=seed,\n",
    ")\n",
    "\n",
    "class_names = raw_train_ds.class_names\n",
    "train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    \"aclImdb/train\",\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=seed,\n",
    ")\n",
    "\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    \"aclImdb/test\", batch_size=batch_size\n",
    ")\n",
    "\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6f104e8e76fe"
   },
   "source": [
    "看一下几篇评论。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ec8a171fa4f7"
   },
   "outputs": [],
   "source": [
    "for text_batch, label_batch in train_ds.take(1):\n",
    "    for i in range(3):\n",
    "        print(f\"Review: {text_batch.numpy()[i]}\")\n",
    "        label = label_batch.numpy()[i]\n",
    "        print(f\"Label : {label} ({class_names[label]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "706c5e78b238"
   },
   "source": [
    "定义BERT基础分类模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d56746554dd6"
   },
   "source": [
    "作为我们模型的基础，您可以从TensorFlow Hub中获取未经大小写处理的BERT-Base模型:\n",
    "https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f4324e29bd66"
   },
   "outputs": [],
   "source": [
    "bert_model_name = \"bert_en_uncased_L-12_H-768_A-12\"\n",
    "tfhub_handle_encoder = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48bc324d317a"
   },
   "source": [
    "为了将文本输入输入到模型中，数据需要使用来自TensorFlow hub的对应BERT预处理器进行预处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3e7fd14957ed"
   },
   "outputs": [],
   "source": [
    "tfhub_handle_preprocess = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "03bbfbecf2f9"
   },
   "outputs": [],
   "source": [
    "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6c4a617d9b23"
   },
   "source": [
    "通过将BERT编码器模型的输出输入到dropout和稠密层中来定义一个分类模型。\n",
    "了解有关[使用BERT对文本进行分类](https://www.tensorflow.org/text/tutorials/classify_text_with_bert)的更多信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "64dc097e0fd5"
   },
   "outputs": [],
   "source": [
    "def build_classifier_model():\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name=\"text\")\n",
    "    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name=\"preprocessing\")\n",
    "    encoder_inputs = preprocessing_layer(text_input)\n",
    "    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name=\"BERT_encoder\")\n",
    "    outputs = encoder(encoder_inputs)\n",
    "    net = outputs[\"pooled_output\"]\n",
    "    net = tf.keras.layers.Dropout(0.1)(net)\n",
    "    net = tf.keras.layers.Dense(1, activation=tf.sigmoid, name=\"classifier\")(net)\n",
    "    return tf.keras.Model(text_input, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2296da5bc1fa"
   },
   "outputs": [],
   "source": [
    "classifier_model = build_classifier_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5baf26b59486"
   },
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1 * num_train_steps)\n",
    "\n",
    "init_lr = 3e-5\n",
    "optimizer = optimization.create_optimizer(\n",
    "    init_lr=init_lr,\n",
    "    num_train_steps=num_train_steps,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    optimizer_type=\"adamw\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "33cac949c431"
   },
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "metrics = tf.metrics.BinaryAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "061751446860"
   },
   "outputs": [],
   "source": [
    "classifier_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0786a8eb8566"
   },
   "source": [
    "微调模型需要一些时间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6abaf7811bc3"
   },
   "outputs": [],
   "source": [
    "print(f\"Training model with {tfhub_handle_encoder}\")\n",
    "history = classifier_model.fit(x=train_ds, validation_data=val_ds, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2b4cf5762fa1"
   },
   "source": [
    "评估模型。预计验证损失约为0.43。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d780483346fd"
   },
   "outputs": [],
   "source": [
    "loss, accuracy = classifier_model.evaluate(test_ds)\n",
    "\n",
    "print(f\"Loss: {loss}\")\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "93b391f42581"
   },
   "source": [
    "导出用于推断的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "07fa7ee3b676"
   },
   "outputs": [],
   "source": [
    "!mkdir -p $LOCAL_DIRECTORY_FULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9298e98110f9"
   },
   "outputs": [],
   "source": [
    "classifier_model.save(LOCAL_DIRECTORY_FULL, include_optimizer=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11cc837b06f9"
   },
   "source": [
    "检查模型签名以查看预测请求应具有哪些字段。\n",
    "\n",
    "请注意，您可能会看到有关缺少'CaseFoldUTF8'操作的堆栈跟踪消息。这是一个关于`saved_model_cli`的已知问题，您可以忽略此消息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "af1ad7b97608"
   },
   "outputs": [],
   "source": [
    "!saved_model_cli show --dir $LOCAL_DIRECTORY_FULL --all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dd4c3d9bf3aa"
   },
   "source": [
    "生成预测请求"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "953eb5e1e86e"
   },
   "source": [
    "现在您可以生成请求发送到我们的模型进行推理。请求以JSON Lines格式生成，每行一个请求。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e07ab9e6213b"
   },
   "outputs": [],
   "source": [
    "!mkdir -p $LOCAL_DIRECTORY_FULL/requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "755b073712eb"
   },
   "outputs": [],
   "source": [
    "def encode(text):\n",
    "    rows = []\n",
    "    for row in text.numpy().tolist():\n",
    "        rows.append(row.decode(\"utf-8\"))\n",
    "\n",
    "    return {\"text\": rows}\n",
    "\n",
    "\n",
    "def export_requests_jsonl(file_name, rows=2, batch_size=32):\n",
    "    with tf.io.gfile.GFile(file_name, mode=\"w\") as f:\n",
    "        for text in test_ds.unbatch().batch(batch_size).take(rows):\n",
    "            d = encode(text[0])\n",
    "            f.write(json.dumps(d))\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ac85dcc6a578"
   },
   "outputs": [],
   "source": [
    "export_requests_jsonl(\n",
    "    os.path.join(LOCAL_DIRECTORY_FULL, \"requests\", \"requests_1_1.jsonl\"),\n",
    "    rows=1,\n",
    "    batch_size=1,\n",
    ")\n",
    "export_requests_jsonl(\n",
    "    os.path.join(LOCAL_DIRECTORY_FULL, \"requests\", \"requests_1_32.jsonl\"),\n",
    "    rows=1,\n",
    "    batch_size=32,\n",
    ")\n",
    "export_requests_jsonl(\n",
    "    os.path.join(LOCAL_DIRECTORY_FULL, \"requests\", \"requests_10_32.jsonl\"),\n",
    "    rows=10,\n",
    "    batch_size=32,\n",
    ")\n",
    "export_requests_jsonl(\n",
    "    os.path.join(LOCAL_DIRECTORY_FULL, \"requests\", \"requests_100_32.jsonl\"),\n",
    "    rows=100,\n",
    "    batch_size=32,\n",
    ")\n",
    "export_requests_jsonl(\n",
    "    os.path.join(LOCAL_DIRECTORY_FULL, \"requests\", \"requests_1000_32.jsonl\"),\n",
    "    rows=1000,\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "291f18b4f085"
   },
   "outputs": [],
   "source": [
    "!cat $LOCAL_DIRECTORY_FULL/requests/requests_1_1.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95a4aacc6246"
   },
   "source": [
    "## （可选）生成热身请求"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bbdded5d2ab"
   },
   "source": [
    "TensorFlow运行时有一些懒加载的组件。延迟初始化可能会导致加载模型后发送的第一个请求的延迟非常高。这种延迟可能比单个推理请求的延迟高几个数量级。\n",
    "\n",
    "有关SavedModel预热的更多信息，请参阅https://www.tensorflow.org/tfx/serving/saved_model_warmup。\n",
    "\n",
    "对于使用优化的TensorFlow运行时的Vertex AI预测，当模型预编译时，每个新批处理大小的第一个请求具有较高的延迟。当`allow_precompilation`标志设置为true时，预编译将启用。\n",
    "\n",
    "为了减少高延迟，需为运行时提供一个用于启动时加载的预热请求。\n",
    "预热文件应包含您预计模型在生产中收到的各种批处理大小。\n",
    "\n",
    "请注意，使用多个批处理大小的预热请求会增加每个节点启动的时间。\n",
    "\n",
    "如果您希望模型接收多个批处理大小，可以使用一组`allowed_batch_sizes`来进行自动服务器端请求批处理。更多信息，请参阅https://www.tensorflow.org/tfx/serving/serving_config#batching_configuration。\n",
    "\n",
    "要为在Vertex AI Prediction上运行的模型启用自动批处理，请将批处理配置放入与saved_model.pb同一GCS目录中的[config/batching_parameters_config](https://cloud.google.com/vertex-ai/docs/training/exporting-model-artifacts#enable_server-side_request_batching_for_tensorflow)文件中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a5d7a5991818"
   },
   "outputs": [],
   "source": [
    "!mkdir -p $LOCAL_DIRECTORY_FULL/assets.extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "136ef90d0320"
   },
   "outputs": [],
   "source": [
    "from tensorflow_serving.apis import predict_pb2, prediction_log_pb2\n",
    "\n",
    "\n",
    "def build_grpc_request(\n",
    "    row_dict, model_name=\"default\", signature_name=\"serving_default\"\n",
    "):\n",
    "    \"\"\"Generate gRPC inference request with payload.\"\"\"\n",
    "\n",
    "    request = predict_pb2.PredictRequest()\n",
    "    request.model_spec.name = model_name\n",
    "    request.model_spec.signature_name = signature_name\n",
    "    for key, value in row_dict.items():\n",
    "        proto = tf.make_tensor_proto(value)\n",
    "        request.inputs[key].CopyFrom(proto)\n",
    "    return request\n",
    "\n",
    "\n",
    "def export_warmup_file(\n",
    "    request_files, export_path, model_name=\"default\", signature_name=\"serving_default\"\n",
    "):\n",
    "    with tf.io.TFRecordWriter(export_path) as writer:\n",
    "        for request_file_path in request_files:\n",
    "            with open(request_file_path) as f:\n",
    "                row_dict = json.loads(f.readline())\n",
    "                request = build_grpc_request(row_dict, model_name, signature_name)\n",
    "            log = prediction_log_pb2.PredictionLog(\n",
    "                predict_log=prediction_log_pb2.PredictLog(request=request)\n",
    "            )\n",
    "            writer.write(log.SerializeToString())\n",
    "\n",
    "\n",
    "export_warmup_file(\n",
    "    [\n",
    "        os.path.join(LOCAL_DIRECTORY_FULL, \"requests\", \"requests_1_1.jsonl\"),\n",
    "        os.path.join(LOCAL_DIRECTORY_FULL, \"requests\", \"requests_1_32.jsonl\"),\n",
    "    ],\n",
    "    os.path.join(LOCAL_DIRECTORY_FULL, \"assets.extra\", \"tf_serving_warmup_requests\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "df9dcbe4727a"
   },
   "source": [
    "将模型部署到Vertex AI 预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8b80237db652"
   },
   "source": [
    "要部署模型到 Vertex AI 预测服务，您必须将其放置在一个 GCS 存储桶中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7K_ebo3ZjeIO"
   },
   "outputs": [],
   "source": [
    "!gsutil rm -r $BUCKET_URI/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "17d915525377"
   },
   "outputs": [],
   "source": [
    "!gsutil cp -r $LOCAL_DIRECTORY_FULL/* $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b89bf1e0ec3b"
   },
   "source": [
    "将Vertex AI Python客户端库导入到您的笔记本环墺中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c369155c05c2"
   },
   "outputs": [],
   "source": [
    "from google.cloud.aiplatform import gapic as aip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b095feae48f0"
   },
   "source": [
    "定义部署时要使用的节点类型。要了解有关 Vertex AI 预测选项的信息，请参见 [配置计算资源](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bd6d54513910"
   },
   "outputs": [],
   "source": [
    "DEPLOY_COMPUTE = \"n1-standard-16\"\n",
    "DEPLOY_GPU = aip.AcceleratorType.NVIDIA_TESLA_T4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83f62a939359"
   },
   "source": [
    "AI 平台的Python客户端库充当客户端/服务器模型。\n",
    "\n",
    "在此示例中，您将使用以下客户端：\n",
    "- 用于管理模型的模型服务。\n",
    "- 用于部署的端点服务。\n",
    "- 用于提供服务的预测服务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8851dabb0473"
   },
   "outputs": [],
   "source": [
    "API_ENDPOINT = f\"{REGION}-aiplatform.googleapis.com\"\n",
    "PARENT = f\"projects/{PROJECT_ID}/locations/{REGION}\"\n",
    "\n",
    "client_options = {\"api_endpoint\": API_ENDPOINT}\n",
    "model_service_client = aip.ModelServiceClient(client_options=client_options)\n",
    "endpoint_service_client = aip.EndpointServiceClient(client_options=client_options)\n",
    "prediction_service_client = aip.PredictionServiceClient(client_options=client_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85076eb8d14e"
   },
   "source": [
    "### 将模型上传到Vertex AI预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0e9070f4be1a"
   },
   "source": [
    "请查看[model_service.upload_model](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform_v1.services.model_service.ModelServiceClient#google_cloud_aiplatform_v1_services_model_service_ModelServiceClient_upload_model)文档以获取详细信息。\n",
    "\n",
    "`artifact_uri`参数应指向一个GCS路径，该路径是您模型中`saved_model.pb`文件所在的位置。\n",
    "\n",
    "`image_uri`指定要使用的docker镜像。在这里，您将使用TF2.7 GPU和Vertex AI Prediction优化的TensorFlow运行时图像上传相同的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dc626262a937"
   },
   "outputs": [],
   "source": [
    "tf27_gpu_model_dict = {\n",
    "    \"display_name\": \"BERT Base TF2.7 GPU model\",\n",
    "    \"artifact_uri\": BUCKET_URI,\n",
    "    \"container_spec\": {\n",
    "        \"image_uri\": \"us-docker.pkg.dev/vertex-ai/prediction/tf2-gpu.2-7:latest\",\n",
    "    },\n",
    "}\n",
    "tf27_gpu_model = (\n",
    "    model_service_client.upload_model(parent=PARENT, model=tf27_gpu_model_dict)\n",
    "    .result(timeout=180)\n",
    "    .model\n",
    ")\n",
    "tf27_gpu_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e9eb58df8087"
   },
   "source": [
    "使用Vertex AI Prediction优化的TensorFlow运行时部署模型时，请使用`us-docker.pkg.dev/vertex-ai-restricted/prediction/tf_opt-gpu.nightly:latest`容器。\n",
    "\n",
    "模型应用了两种优化选项。\n",
    "- *allow_precompilation* - 开启模型预编译以提高性能。请注意，当带有新批量大小的第一个请求到达时，模型预编译将会发生，并且在预编译完成后才会发送该请求的响应。为了减少这种情况，可以指定一个预热文件（请参见本文档早期部分）。模型预编译适用于不同类型的模型，在大多数情况下对性能有积极影响。但是，在生产环境中启用之前，我们建议您在您的模型上尝试一下。\n",
    "- *allow_precision_affecting_optimizations* - 启用影响精度的优化。在一些情况下，这可以使模型运行速度显著加快，但会略微损失一些模型预测的准确性。在使用此优化时，您应该评估对模型精度的影响。\n",
    "\n",
    "有关可用优化的TensorFlow运行时容器和选项列表，请参阅https://cloud.google.com/vertex-ai/docs/predictions/optimized-tensorflow-runtime。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1152b4321e91"
   },
   "outputs": [],
   "source": [
    "tf_opt_gpu_model_dict = {\n",
    "    \"display_name\": \"BERT Base optimized TensorFlow runtime GPU model\",\n",
    "    \"artifact_uri\": BUCKET_URI,\n",
    "    \"container_spec\": {\n",
    "        \"image_uri\": \"us-docker.pkg.dev/vertex-ai-restricted/prediction/tf_opt-gpu.nightly:latest\",\n",
    "        \"args\": [\n",
    "            \"--allow_precompilation=true\",\n",
    "            \"--allow_precision_affecting_optimizations=false\",\n",
    "        ],\n",
    "    },\n",
    "}\n",
    "\n",
    "tf_opt_gpu_model = (\n",
    "    model_service_client.upload_model(parent=PARENT, model=tf_opt_gpu_model_dict)\n",
    "    .result(timeout=180)\n",
    "    .model\n",
    ")\n",
    "tf_opt_gpu_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "610c6d77f5d9"
   },
   "outputs": [],
   "source": [
    "tf_opt_lossy_gpu_model_dict = {\n",
    "    \"display_name\": \"BERT Base optimized TensorFlow runtime GPU model with lossy optimizations\",\n",
    "    \"artifact_uri\": BUCKET_URI,\n",
    "    \"container_spec\": {\n",
    "        \"image_uri\": \"us-docker.pkg.dev/vertex-ai-restricted/prediction/tf_opt-gpu.nightly:latest\",\n",
    "        \"args\": [\n",
    "            \"--allow_precompilation=true\",\n",
    "            \"--allow_precision_affecting_optimizations=true\",\n",
    "        ],\n",
    "    },\n",
    "}\n",
    "\n",
    "tf_opt_lossy_gpu_model = (\n",
    "    model_service_client.upload_model(parent=PARENT, model=tf_opt_lossy_gpu_model_dict)\n",
    "    .result(timeout=180)\n",
    "    .model\n",
    ")\n",
    "tf_opt_lossy_gpu_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d9fb2a4e6c63"
   },
   "source": [
    "列出所有的型号。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "396cc71ebd9e"
   },
   "outputs": [],
   "source": [
    "model_service_client.list_models(parent=PARENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4605d8befbfd"
   },
   "source": [
    "创建端点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ff288387d03"
   },
   "source": [
    "了解有关[endpoint_service.create_endpoint](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform_v1.services.endpoint_service.EndpointServiceClient#google_cloud_aiplatform_v1_services_endpoint_service_EndpointServiceClient_create_endpoint)的更多信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3e9cf3f15310"
   },
   "outputs": [],
   "source": [
    "tf27_gpu_endpoint_dict = {\n",
    "    \"display_name\": \"BERT Base TF2.7 GPU endpoint\",\n",
    "}\n",
    "tf27_gpu_endpoint = (\n",
    "    endpoint_service_client.create_endpoint(\n",
    "        parent=PARENT, endpoint=tf27_gpu_endpoint_dict\n",
    "    )\n",
    "    .result(timeout=300)\n",
    "    .name\n",
    ")\n",
    "tf27_gpu_endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7eb899497ecf"
   },
   "outputs": [],
   "source": [
    "tf_opt_gpu_endpoint_dict = {\n",
    "    \"display_name\": \"BERT Base optimized TensorFlow runtime GPU endpoint\",\n",
    "}\n",
    "tf_opt_gpu_endpoint = (\n",
    "    endpoint_service_client.create_endpoint(\n",
    "        parent=PARENT, endpoint=tf_opt_gpu_endpoint_dict\n",
    "    )\n",
    "    .result(timeout=300)\n",
    "    .name\n",
    ")\n",
    "tf_opt_gpu_endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bf458b8ede48"
   },
   "outputs": [],
   "source": [
    "tf_opt_lossy_gpu_endpoint_dict = {\n",
    "    \"display_name\": \"BERT Base optimized TensorFlow runtime GPU with lossy optimizations endpoint\",\n",
    "}\n",
    "tf_opt_lossy_gpu_endpoint = (\n",
    "    endpoint_service_client.create_endpoint(\n",
    "        parent=PARENT, endpoint=tf_opt_lossy_gpu_endpoint_dict\n",
    "    )\n",
    "    .result(timeout=300)\n",
    "    .name\n",
    ")\n",
    "tf_opt_lossy_gpu_endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6873939a0a1"
   },
   "source": [
    "### 将模型部署到端点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ebb2d55b45e0"
   },
   "source": [
    "了解有关[enpoint_service.deploy_model](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform_v1.services.endpoint_service.EndpointServiceClient#google_cloud_aiplatform_v1_services_endpoint_service_EndpointServiceClient_deploy_model)的更多信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "20f414f9e8be"
   },
   "outputs": [],
   "source": [
    "tf27_gpu_deployed_model_dict = {\n",
    "    \"model\": tf27_gpu_model,\n",
    "    \"display_name\": \"BERT Base TF2.7 GPU deployed model\",\n",
    "    \"dedicated_resources\": {\n",
    "        \"min_replica_count\": 1,\n",
    "        \"max_replica_count\": 1,\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": DEPLOY_COMPUTE,\n",
    "            \"accelerator_type\": DEPLOY_GPU,\n",
    "            \"accelerator_count\": 1,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "tf27_gpu_deployed_model = endpoint_service_client.deploy_model(\n",
    "    endpoint=tf27_gpu_endpoint,\n",
    "    deployed_model=tf27_gpu_deployed_model_dict,\n",
    "    traffic_split={\"0\": 100},\n",
    ").result()\n",
    "tf27_gpu_deployed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bd1358689794"
   },
   "outputs": [],
   "source": [
    "tf_opt_gpu_deployed_model_dict = {\n",
    "    \"model\": tf_opt_gpu_model,\n",
    "    \"display_name\": \"BERT Base optimized TensorFlow runtime GPU model\",\n",
    "    \"dedicated_resources\": {\n",
    "        \"min_replica_count\": 1,\n",
    "        \"max_replica_count\": 1,\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": DEPLOY_COMPUTE,\n",
    "            \"accelerator_type\": DEPLOY_GPU,\n",
    "            \"accelerator_count\": 1,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "tf_opt_gpu_deployed_model = endpoint_service_client.deploy_model(\n",
    "    endpoint=tf_opt_gpu_endpoint,\n",
    "    deployed_model=tf_opt_gpu_deployed_model_dict,\n",
    "    traffic_split={\"0\": 100},\n",
    ").result()\n",
    "tf_opt_gpu_deployed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "25b5541a0e2f"
   },
   "outputs": [],
   "source": [
    "tf_opt_lossy_gpu_deployed_model_dict = {\n",
    "    \"model\": tf_opt_lossy_gpu_model,\n",
    "    \"display_name\": \"BERT Base optimized TensorFlow runtime GPU model with lossy optimizations\",\n",
    "    \"dedicated_resources\": {\n",
    "        \"min_replica_count\": 1,\n",
    "        \"max_replica_count\": 1,\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": DEPLOY_COMPUTE,\n",
    "            \"accelerator_type\": DEPLOY_GPU,\n",
    "            \"accelerator_count\": 1,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "tf_opt_lossy_gpu_deployed_model = endpoint_service_client.deploy_model(\n",
    "    endpoint=tf_opt_lossy_gpu_endpoint,\n",
    "    deployed_model=tf_opt_lossy_gpu_deployed_model_dict,\n",
    "    traffic_split={\"0\": 100},\n",
    ").result()\n",
    "tf_opt_lossy_gpu_deployed_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0016bab3f15d"
   },
   "source": [
    "发送预测请求"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edae193ab9ef"
   },
   "source": [
    "现在您可以使用`prediction_service_client.predict` API向您的模型发送预测请求。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "13a9f2499fd9"
   },
   "outputs": [],
   "source": [
    "prediction_service_client.predict(\n",
    "    endpoint=tf27_gpu_endpoint,\n",
    "    instances=[\"This was the best movie ever\", \"Movie was boring\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ca1dfbcdf81e"
   },
   "source": [
    "或者，您可以在不使用SDK的情况下发送POST REST请求。了解更多信息，请访问https://cloud.google.com/vertex-ai/docs/predictions/online-predictions-custom-models#online_predict_custom_trained-drest。这种方法稍微更快。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ff281732942"
   },
   "outputs": [],
   "source": [
    "def get_headers():\n",
    "    gcloud_access_token = (\n",
    "        subprocess.check_output(\"gcloud auth print-access-token\".split(\" \"))\n",
    "        .decode()\n",
    "        .rstrip(\"\\n\")\n",
    "    )\n",
    "    return {\"authorization\": \"Bearer \" + gcloud_access_token}\n",
    "\n",
    "\n",
    "def send_post_request(uri, request_dict):\n",
    "    return r.post(\n",
    "        uri, data=json.dumps(request_dict), headers=get_headers(), verify=False\n",
    "    )\n",
    "\n",
    "\n",
    "uri = f\"https://{REGION}-aiplatform.googleapis.com/v1/{tf27_gpu_endpoint}:predict\"\n",
    "print(uri)\n",
    "\n",
    "request = {\n",
    "    \"instances\": [\n",
    "        {\"text\": \"This was the best movie ever\"},\n",
    "        {\"text\": \"Movie was boring\"},\n",
    "    ]\n",
    "}\n",
    "response = send_post_request(uri, request)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5ce470e856f"
   },
   "source": [
    "##（可选） 部署模型进行基准测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f02ce0cc8d3c"
   },
   "source": [
    "您可以在Colab环境中运行基准测试，为了获得可靠的结果，您应该使用与您的模型相同区域的虚拟机。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5d59163882bc"
   },
   "source": [
    "导入用于基准测试模型的帮助函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2f213c470f4f"
   },
   "outputs": [],
   "source": [
    "!curl https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/vertex_endpoints/optimized_tensorflow_runtime/benchmark.py -o benchmark.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a54efa306ae8"
   },
   "outputs": [],
   "source": [
    "from benchmark import benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "15e169255bbe"
   },
   "source": [
    "该代码以给定的QPS异步且均匀地发送指定数量的请求，然后记录观察到的延迟。接下来，延迟结果被合并并计算百分位数。\n",
    "模型可以处理的`actual_qps` 是通过模型处理发送的请求所需的时间除以请求数来计算的。\n",
    "通过为`send_request` 和 `build_request` 函数提供不同的实现，可以使用相同的代码对在本地或在 Vertex AI Prediction 上使用 gRPC 和 REST 协议运行的模型进行基准测试。\n",
    "\n",
    "这个基准测试的主要目标是在不同负载下测量模型延迟和模型可以处理的最大吞吐量。为了找到最大吞吐量，逐渐增加 QPS 直到`actual_qps` 不再增加而延迟急剧增加为止。\n",
    "\n",
    "在生产部署中，工作负载并不是均匀的，因此最大模型吞吐量可能会较低。\n",
    "我们并不打算在这里模拟生产工作负载。这个基准测试旨在比较在不同环境下运行相同模型的延迟和吞吐量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3b1efbe01d49"
   },
   "outputs": [],
   "source": [
    "def build_rest_request(row_dict, model_name):\n",
    "    payload = json.dumps({\"instances\": row_dict})\n",
    "    return payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "47bf78949b57"
   },
   "outputs": [],
   "source": [
    "headers = get_headers()\n",
    "\n",
    "\n",
    "def send_rest_request(request):\n",
    "    res = r.post(\n",
    "        f\"https://{REGION}-aiplatform.googleapis.com/v1/{tf27_gpu_endpoint}:predict\",\n",
    "        data=request,\n",
    "        headers=headers,\n",
    "        verify=False,\n",
    "    )\n",
    "    assert res.status_code == 200\n",
    "    return res\n",
    "\n",
    "\n",
    "tf27_gpu_results = benchmark(\n",
    "    send_rest_request,\n",
    "    build_rest_request,\n",
    "    f\"{LOCAL_DIRECTORY_FULL}/requests/requests_10_32.jsonl\",\n",
    "    [1, 2, 3, 4, 5],\n",
    "    5,\n",
    ")\n",
    "\n",
    "tf27_gpu_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "41de13a31620"
   },
   "outputs": [],
   "source": [
    "headers = get_headers()\n",
    "\n",
    "\n",
    "def send_rest_request(request):\n",
    "    res = r.post(\n",
    "        f\"https://{REGION}-aiplatform.googleapis.com/v1/{tf_opt_gpu_endpoint}:predict\",\n",
    "        data=request,\n",
    "        headers=headers,\n",
    "        verify=False,\n",
    "    )\n",
    "    assert res.status_code == 200\n",
    "    return res\n",
    "\n",
    "\n",
    "tf_opt_gpu_results = benchmark(\n",
    "    send_rest_request,\n",
    "    build_rest_request,\n",
    "    f\"{LOCAL_DIRECTORY_FULL}/requests/requests_10_32.jsonl\",\n",
    "    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    5,\n",
    ")\n",
    "\n",
    "tf_opt_gpu_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "83378b26638a"
   },
   "outputs": [],
   "source": [
    "headers = get_headers()\n",
    "\n",
    "\n",
    "def send_rest_request(request):\n",
    "    res = r.post(\n",
    "        f\"https://{REGION}-aiplatform.googleapis.com/v1/{tf_opt_lossy_gpu_endpoint}:predict\",\n",
    "        data=request,\n",
    "        headers=headers,\n",
    "        verify=False,\n",
    "    )\n",
    "    assert res.status_code == 200\n",
    "    return res\n",
    "\n",
    "\n",
    "tf_opt_lossy_gpu_results = benchmark(\n",
    "    send_rest_request,\n",
    "    build_rest_request,\n",
    "    f\"{LOCAL_DIRECTORY_FULL}/requests/requests_10_32.jsonl\",\n",
    "    [1, 5, 10, 15, 20, 21, 22, 23, 24, 25],\n",
    "    5,\n",
    ")\n",
    "\n",
    "tf_opt_lossy_gpu_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5f855885e50f"
   },
   "source": [
    "合并和可视化结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1afa1a329b0e"
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def build_graph(x_key, y_key, results_dict, axis):\n",
    "    matplotlib.rcParams[\"figure.figsize\"] = [10.0, 7.0]\n",
    "\n",
    "    fig, ax = plt.subplots(facecolor=(1, 1, 1))\n",
    "    ax.set_xlabel(\"QPS\")\n",
    "    ax.set_ylabel(\"Latency(ms)\")\n",
    "    for title, results in results_dict.items():\n",
    "        x = np.array(results[x_key])\n",
    "        y = np.array(results[y_key])\n",
    "        ax.plot(x, y, label=title)\n",
    "    ax.legend()\n",
    "    ax.axis(axis)\n",
    "    ax.set_title(f\"BERT base model {y_key} latency, batch size 32\")\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "204d66f54e46"
   },
   "outputs": [],
   "source": [
    "fig = build_graph(\n",
    "    \"actual_qps\",\n",
    "    \"p50\",\n",
    "    {\n",
    "        \"TF2.7 GPU\": tf27_gpu_results,\n",
    "        \"TF opt GPU\": tf_opt_gpu_results,\n",
    "        \"TF opt GPU lossy\": tf_opt_lossy_gpu_results,\n",
    "    },\n",
    "    (0, 14, 0, 1000),\n",
    ")\n",
    "fig.savefig(\"bert_p50_latency_32.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7251aa3b7721"
   },
   "outputs": [],
   "source": [
    "fig = build_graph(\n",
    "    \"actual_qps\",\n",
    "    \"p99\",\n",
    "    {\n",
    "        \"TF2.7 GPU\": tf27_gpu_results,\n",
    "        \"TF opt GPU\": tf_opt_gpu_results,\n",
    "        \"TF opt GPU lossy\": tf_opt_lossy_gpu_results,\n",
    "    },\n",
    "    (0, 14, 0, 1000),\n",
    ")\n",
    "fig.savefig(\"bert_p99_latency_32.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8e97703a929"
   },
   "source": [
    "您可以看到，Vertex AI Prediction 优化的 TensorFlow 运行时与 TensorFlow 2.7 相比，具有明显更高的吞吐量和更低的延迟。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06f2711e6221"
   },
   "source": [
    "## （可选）使用MLPerf推理负载生成器比较部署模型的性能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adcd7c46de26"
   },
   "source": [
    "MLPerf推理是一个基准套件，用于测量系统在各种部署场景中可以运行模型的速度。MLPerf现在是衡量模型性能的行业标准方式。您可以按照 https://github.com/tensorflow/tpu/tree/master/models/experimental/inference/load_test 上的说明来运行已部署模型的MLPerf推理基准测试。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45bb8944198c"
   },
   "source": [
    "（可选）比较预测结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f45ccc0dd077"
   },
   "source": [
    "在这个示例中，使用了优化的TensorFlow运行时进行顶点预测，并将`allow_precision_affecting_optimizations`标志设置为`true`，以获得额外的加速。现在让我们检查这些优化如何影响预测结果。\n",
    "\n",
    "比较在优化的TensorFlow运行时和TF2.7上运行的模型对32,000个请求的预测结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "47c82c4797dc"
   },
   "outputs": [],
   "source": [
    "def get_predictions(endpoint, requests_file_path):\n",
    "    responses = []\n",
    "\n",
    "    with tf.io.gfile.GFile(requests_file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            row_dict = json.loads(line)\n",
    "            response = prediction_service_client.predict(\n",
    "                endpoint=endpoint,\n",
    "                instances=row_dict[\"text\"],\n",
    "            )\n",
    "            for prediction in response.predictions:\n",
    "                responses.append(prediction[0])\n",
    "\n",
    "    return np.array(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "130c95fb82fa"
   },
   "outputs": [],
   "source": [
    "tf27_gpu_predictions = get_predictions(\n",
    "    tf27_gpu_endpoint, f\"{LOCAL_DIRECTORY_FULL}/requests/requests_1000_32.jsonl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5bd0c12a7ed0"
   },
   "outputs": [],
   "source": [
    "tf_opt_lossy_gpu_predictions = get_predictions(\n",
    "    tf_opt_lossy_gpu_endpoint, f\"{LOCAL_DIRECTORY_FULL}/requests/requests_1000_32.jsonl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1f60d1e7035e"
   },
   "outputs": [],
   "source": [
    "np.average(tf_opt_lossy_gpu_predictions - tf27_gpu_predictions) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "61a550dc2270"
   },
   "outputs": [],
   "source": [
    "np.max(np.abs(tf_opt_lossy_gpu_predictions - tf27_gpu_predictions)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fd7a55926226"
   },
   "source": [
    "你可以看到，平均结果在少于0.01%时是不同的。在最坏的情况下，差异小于1%。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c69570acca3d"
   },
   "source": [
    "清理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "df0bd8f8c5f2"
   },
   "source": [
    "完成后，可以安全地移除您创建的端点和部署的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18074be6b41c"
   },
   "outputs": [],
   "source": [
    "def cleanup(endpoint, model_name, deployed_model_id):\n",
    "    response = endpoint_service_client.undeploy_model(\n",
    "        endpoint=endpoint, deployed_model_id=deployed_model_id\n",
    "    )\n",
    "    print(\"running undeploy_model operation:\", response.operation.name)\n",
    "    print(response.result())\n",
    "\n",
    "    response = endpoint_service_client.delete_endpoint(name=endpoint)\n",
    "    print(\"running delete_endpoint operation:\", response.operation.name)\n",
    "    print(response.result())\n",
    "\n",
    "    response = model_service_client.delete_model(name=model_name)\n",
    "    print(\"running delete_model operation:\", response.operation.name)\n",
    "    print(response.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ab6cb1891003"
   },
   "outputs": [],
   "source": [
    "cleanup(tf27_gpu_endpoint, tf27_gpu_model, tf27_gpu_deployed_model.deployed_model.id)\n",
    "cleanup(\n",
    "    tf_opt_gpu_endpoint, tf_opt_gpu_model, tf_opt_gpu_deployed_model.deployed_model.id\n",
    ")\n",
    "cleanup(\n",
    "    tf_opt_lossy_gpu_endpoint,\n",
    "    tf_opt_lossy_gpu_model,\n",
    "    tf_opt_lossy_gpu_deployed_model.deployed_model.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2e0c5c6f2e9"
   },
   "source": [
    "你现在也可以从GCS存储桶中删除模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "45026d023cae"
   },
   "outputs": [],
   "source": [
    "# Set this to true only if you'd like to delete your bucket\n",
    "delete_bucket = False\n",
    "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
    "    !gsutil rm -r $BUCKET_URI"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "bert_optimized_online_prediction.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
