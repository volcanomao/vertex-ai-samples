{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title:generic,gcp"
   },
   "source": [
    "# 在GCP上进行端到端的机器学习：MLOps阶段6：使用FastAPI和Vertex AI Prediction开始\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage6/get_started_with_fastapi.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> 在Colab中运行\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage6/get_started_with_fastapi.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      在GitHub上查看\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/ml_ops/stage6/get_started_with_fastapi.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      在Vertex AI Workbench中打开\n",
    "    </a>\n",
    "  </td>        \n",
    "</table>\n",
    "<br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## 概述\n",
    "\n",
    "本教程演示了如何使用使用`FastAPI`构建的自定义服务二进制文件从`Vertex AI Endpoint`中提供预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9402cfbdc2d"
   },
   "source": [
    "### 目标\n",
    "\n",
    "在本教程中，您将学习如何使用`Vertex AI Prediction`在`Vertex AI Endpoint`上使用`FastAPI`自定义服务二进制文件。\n",
    "\n",
    "本教程使用以下谷歌云ML服务和资源：\n",
    "\n",
    "- `Vertex AI Prediction`\n",
    "- `Vertex AI Models`\n",
    "- `Vertex AI Endpoints`\n",
    "\n",
    "执行的步骤包括：\n",
    "\n",
    "- 从TensorFlow Hub下载预训练的图像分类模型。\n",
    "- 创建一个用于接收压缩图像数据并输出解压缩后的预处理数据供模型输入的服务函数。\n",
    "- 将TensorFlow Hub模型和服务函数上传为`Vertex AI Model`资源。\n",
    "- 创建一个`Endpoint`资源。\n",
    "- 使用`FastAPI`自定义服务二进制部署`Model`资源到`Endpoint`资源。\n",
    "- 对部署到`Endpoint`资源的`Model`资源实例进行在线预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset:iris,lcn"
   },
   "source": [
    "数据集\n",
    "\n",
    "此教程使用了来自TensorFlow Hub的预训练图像分类模型，该模型是在ImageNet数据集上训练的。\n",
    "\n",
    "了解更多关于[ResNet V2预训练模型](https://tfhub.dev/google/imagenet/resnet_v2_101/classification/5)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "costs"
   },
   "source": [
    "### 费用\n",
    "\n",
    "本教程使用 Google Cloud 的计费组件：\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "了解 [Vertex AI 价格](https://cloud.google.com/vertex-ai/pricing) 和 [Cloud Storage 价格](https://cloud.google.com/storage/pricing)，并使用 [定价计算器](https://cloud.google.com/products/calculator/) 根据您的预期使用情况生成费用估算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_aip"
   },
   "source": [
    "## 安装\n",
    "\n",
    "安装以下软件包以执行该笔记本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_aip"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Vertex AI Workbench Notebook product has specific requirements\n",
    "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\")\n",
    "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
    "    \"/opt/deeplearning/metadata/env_version\"\n",
    ")\n",
    "\n",
    "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_WORKBENCH_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\"\n",
    "\n",
    "! pip3 install --upgrade google-cloud-aiplatform $USER_FLAG -q\n",
    "! pip3 install --upgrade google-cloud-pipeline-components $USER_FLAG -q\n",
    "! pip3 install tensorflow-hub $USER_FLAG -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhq5zEbGg0XX"
   },
   "source": [
    "### 重新启动内核\n",
    "\n",
    "安装完额外的包之后，您需要重新启动笔记本内核，以便它能找到这些包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EzrelQZ22IZj"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "before_you_begin"
   },
   "source": [
    "## 开始之前\n",
    "\n",
    "### GPU运行时\n",
    "\n",
    "*如果可以的话，请确保在GPU运行时中运行此笔记本。在Colab中，选择* **运行时 > 更改运行时类型 > GPU**\n",
    "\n",
    "### 设置您的谷歌云项目\n",
    "\n",
    "**以下步骤是必需的，无论您的笔记本环境如何。**\n",
    "\n",
    "1. [选择或创建谷歌云项目](https://console.cloud.google.com/cloud-resource-manager)。当您首次创建帐户时，您将获得300美元的免费信用额度来支付计算/存储成本。\n",
    "\n",
    "2. [确保为您的项目启用了结算。](https://cloud.google.com/billing/docs/how-to/modify-project)\n",
    "\n",
    "3. [启用以下API：Vertex AI APIs，Compute Engine APIs和Cloud Storage。](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,compute_component,storage-component.googleapis.com)\n",
    "\n",
    "4. 如果您在本地运行此笔记本，您需要安装[Cloud SDK](https://cloud.google.com/sdk)。\n",
    "\n",
    "5. 在下面的单元格中输入您的项目ID。然后运行该单元格，以确保Cloud SDK在此笔记本中的所有命令中使用正确的项目。\n",
    "\n",
    "**注意**：Jupyter会将以`!`为前缀的行视为shell命令，并对以`$`为前缀的Python变量进行插值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "project_id"
   },
   "source": [
    "设置您的项目ID\n",
    "\n",
    "**如果您不知道您的项目ID**，您可以使用`gcloud`获取您的项目ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_project_id"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "autoset_project_id"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID:\", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_gcloud_project_id"
   },
   "outputs": [],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "#### 区域\n",
    "\n",
    "您还可以更改用于此笔记本其余部分操作的 `REGION` 变量。以下是 Vertex AI 支持的区域。我们建议您选择距离您最近的区域。\n",
    "\n",
    "- 美洲：`us-central1`\n",
    "- 欧洲：`europe-west4`\n",
    "- 亚太：`asia-east1`\n",
    "\n",
    "您可能不能使用多区域存储桶来训练 Vertex AI。并非所有区域都支持所有 Vertex AI 服务。\n",
    "\n",
    "了解更多有关 [Vertex AI 区域](https://cloud.google.com/vertex-ai/docs/general/locations)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "region"
   },
   "outputs": [],
   "source": [
    "REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
    "\n",
    "if REGION == \"[your-region]\":\n",
    "    REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "timestamp"
   },
   "source": [
    "时间戳\n",
    "\n",
    "如果您正在进行实时教程会话，您可能正在使用一个共享的测试账户或项目。为了避免用户之间在创建资源时发生名称冲突，您为每个实例会话创建一个时间戳，并将时间戳附加到您在本教程中创建的资源的名称中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "timestamp"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcp_authenticate"
   },
   "source": [
    "### 验证您的 Google Cloud 帐户\n",
    "\n",
    "**如果您正在使用 Vertex AI Workbench Notebooks**，您的环境已经通过验证。请跳过这一步。\n",
    "\n",
    "**如果您正在使用 Colab**，运行下面的单元格，并按照提示进行身份验证以通过 oAuth 认证您的帐户。\n",
    "\n",
    "**否则**，请按照以下步骤操作：\n",
    "\n",
    "在 Cloud Console 中，转到 [创建服务帐号密钥](https://console.cloud.google.com/apis/credentials/serviceaccountkey) 页面。\n",
    "\n",
    "**单击创建服务帐号**。\n",
    "\n",
    "在**服务帐号名称**字段中输入一个名称，并单击**创建**。\n",
    "\n",
    "在**将此服务帐号授予项目访问权限**部分，单击角色下拉列表。在过滤框中键入 \"Vertex\"，并选择**Vertex 管理员**。在过滤框中键入 \"Storage Object Admin\"，并选择**存储对象管理员**。\n",
    "\n",
    "单击创建。包含您密钥的 JSON 文件将下载到您的本地环境。\n",
    "\n",
    "在下面的单元格中，将您的服务帐号密钥路径输入为 GOOGLE_APPLICATION_CREDENTIALS 变量，并运行该单元格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gcp_authenticate"
   },
   "outputs": [],
   "source": [
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# If on Vertex AI Workbench, then don't execute this code\n",
    "IS_COLAB = False\n",
    "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\") and not os.getenv(\n",
    "    \"DL_ANACONDA_HOME\"\n",
    "):\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        IS_COLAB = True\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\"):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bucket:mbsdk"
   },
   "source": [
    "### 创建一个云存储桶\n",
    "\n",
    "**无论您使用的是哪种笔记本环境，都需要按照以下步骤进行。**\n",
    "\n",
    "当您为 Python 初始化 Vertex AI SDK 时，您需要指定一个云存储暂存桶。这个暂存桶是您的数据集和模型资源在会话之间保留的地方。\n",
    "\n",
    "请在下面设置您的云存储桶的名称。桶的名称必须在所有谷歌云项目中全局唯一，包括您组织外的项目。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bucket"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"[your-bucket-name]\"  # @param {type:\"string\"}\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "autoset_bucket"
   },
   "outputs": [],
   "source": [
    "if BUCKET_URI == \"\" or BUCKET_URI is None or BUCKET_URI == \"gs://[your-bucket-name]\":\n",
    "    BUCKET_NAME = PROJECT_ID + \"aip-\" + TIMESTAMP\n",
    "    BUCKET_URI = \"gs://\" + BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_bucket"
   },
   "source": [
    "只有当您的存储桶还不存在时：运行以下单元格以创建您的云存储桶。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_bucket"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "validate_bucket"
   },
   "source": [
    "最后，通过检查云存储桶的内容来验证访问权限。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "validate_bucket"
   },
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_vars"
   },
   "source": [
    "### 设置变量\n",
    "\n",
    "接下来，设置一些在教程中使用的变量。\n",
    "\n",
    "### 导入库并定义常量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aip\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk"
   },
   "source": [
    "### 初始化用于Python的Vertex AI SDK\n",
    "\n",
    "为您的项目和对应的存储桶初始化用于Python的Vertex AI SDK。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "aip.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "accelerators:training,cpu,prediction,cpu,mbsdk"
   },
   "source": [
    "设置硬件加速器\n",
    "\n",
    "您可以为训练和预测设置硬件加速器。\n",
    "\n",
    "设置变量`DEPLOY_GPU/DEPLOY_NGPU`以使用支持GPU的容器映像，并指定分配给虚拟机实例的GPU数量。例如，要使用一个支持GPU的容器映像，并且为每个虚拟机分配4个Nvidia Telsa K80 GPU，则需要指定：\n",
    "\n",
    "    (aip.AcceleratorType.NVIDIA_TESLA_K80, 4)\n",
    "\n",
    "否则，指定`(None, None)`以使用一个在CPU上运行的容器映像。\n",
    "\n",
    "了解更多关于[您位置的硬件加速器支持的信息](https://cloud.google.com/vertex-ai/docs/general/locations#accelerators)。\n",
    "\n",
    "*注意*：在2.3之前的TF版本中，GPU支持将无法在本教程中加载自定义模型。这是一个已知问题，在TF 2.3中已修复。这是由于在服务函数中生成的静态图操作造成的。如果在您自己的自定义模型上遇到此问题，请使用带有GPU支持的TF 2.3容器映像。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "accelerators:training,cpu,prediction,cpu,mbsdk"
   },
   "outputs": [],
   "source": [
    "if os.getenv(\"IS_TESTING_DEPLOY_GPU\"):\n",
    "    DEPLOY_GPU, DEPLOY_NGPU = (\n",
    "        aip.gapic.AcceleratorType.NVIDIA_TESLA_K80,\n",
    "        int(os.getenv(\"IS_TESTING_DEPLOY_GPU\")),\n",
    "    )\n",
    "else:\n",
    "    DEPLOY_GPU, DEPLOY_NGPU = (None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "machine:training"
   },
   "source": [
    "#### 设置机器类型\n",
    "\n",
    "接下来，设置用于预测的机器类型。\n",
    "\n",
    "- 将变量`DEPLOY_COMPUTE`设置为配置用于预测的虚拟机的计算资源。\n",
    " - `机器类型`\n",
    "     - `n1-standard`：每个vCPU 3.75GB内存。\n",
    "     - `n1-highmem`：每个vCPU 6.5GB内存\n",
    "     - `n1-highcpu`：每个vCPU 0.9GB内存\n",
    " - `vCPUs`：数量为\\[2, 4, 8, 16, 32, 64, 96 \\]\n",
    "\n",
    "*注意：您也可以使用n2和e2机器类型进行训练和部署，但它们不支持GPU。*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "machine:training"
   },
   "outputs": [],
   "source": [
    "if os.getenv(\"IS_TESTING_DEPLOY_MACHINE\"):\n",
    "    MACHINE_TYPE = os.getenv(\"IS_TESTING_DEPLOY_MACHINE\")\n",
    "else:\n",
    "    MACHINE_TYPE = \"n1-standard\"\n",
    "\n",
    "VCPU = \"4\"\n",
    "DEPLOY_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
    "print(\"Train machine type\", DEPLOY_COMPUTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gar_enable_api"
   },
   "source": [
    "### 启用Artifact Registry API\n",
    "\n",
    "您必须为您的项目启用Artifact Registry API服务。\n",
    "\n",
    "了解更多关于[启用服务](https://cloud.google.com/artifact-registry/docs/enable-service)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gar_enable_api"
   },
   "outputs": [],
   "source": [
    "! gcloud services enable artifactregistry.googleapis.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gar_create_repo"
   },
   "source": [
    "## 创建一个私有的 Docker 仓库\n",
    "\n",
    "您的第一步是在 Google Artifact Registry 中创建您自己的 Docker 仓库。\n",
    "\n",
    "1. 运行 `gcloud artifacts repositories create` 命令，在您的区域内创建一个新的 Docker 仓库，描述为 \"docker 仓库\"。\n",
    "\n",
    "2. 运行 `gcloud artifacts repositories list` 命令，验证您的仓库是否已创建。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gar_create_repo"
   },
   "outputs": [],
   "source": [
    "PRIVATE_REPO = \"my-docker-repo\"\n",
    "\n",
    "! gcloud artifacts repositories create {PRIVATE_REPO} --repository-format=docker --location={REGION} --description=\"Docker repository\"\n",
    "\n",
    "! gcloud artifacts repositories list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gar_auth"
   },
   "source": [
    "### 配置身份验证到您的私有仓库\n",
    "\n",
    "在推送或拉取容器镜像之前，配置Docker以使用`gcloud`命令行工具来验证请求到您地区的`Artifact Registry`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gar_auth"
   },
   "outputs": [],
   "source": [
    "! gcloud auth configure-docker {REGION}-docker.pkg.dev --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8128b8ff025"
   },
   "source": [
    "## 从 TensorFlow Hub 获取预训练模型\n",
    "\n",
    "为了演示目的，本教程使用了从 TensorFlow Hub(TFHub) 获取的预训练模型，然后将其上传到 `Vertex AI Model` 资源。一旦您有了一个 `Vertex AI Model` 资源，该模型就可以部署到一个 `Vertex AI Endpoint` 资源中。\n",
    "\n",
    "### 下载预训练模型\n",
    "\n",
    "首先，您需要从 TensorFlow Hub 下载预训练模型。该模型将作为一个 TF.Keras 层进行下载。在这个示例中，为了完成模型的构建，您将创建一个带有下载的 TFHub 模型作为一层的 `Sequential()` 模型，并指定模型的输入形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c55fa4c826f7"
   },
   "outputs": [],
   "source": [
    "tfhub_model = tf.keras.Sequential(\n",
    "    [hub.KerasLayer(\"https://tfhub.dev/google/imagenet/resnet_v2_101/classification/5\")]\n",
    ")\n",
    "\n",
    "tfhub_model.build([None, 224, 224, 3])\n",
    "\n",
    "tfhub_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63de49055083"
   },
   "source": [
    "###保存模型工件\n",
    "\n",
    "此时，模型位于内存中。接下来，您将模型工件保存到云存储位置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "64618c713db9"
   },
   "outputs": [],
   "source": [
    "MODEL_DIR = BUCKET_URI + \"/model\"\n",
    "tfhub_model.save(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "how_serving_function_works"
   },
   "source": [
    "## 上传模型以提供服务\n",
    "\n",
    "接下来，您将从自定义作业上传您的 TF.Keras 模型到 Vertex `Model` 服务，这将为您的自定义模型创建一个 Vertex `Model` 资源。在上传过程中，您需要定义一个服务函数，将数据转换为模型期望的格式。如果您将编码数据发送给 Vertex AI，您的服务函数将确保在将数据作为输入传递给模型之前，在模型服务器上对数据进行解码。\n",
    "\n",
    "### 服务函数如何工作\n",
    "\n",
    "当您向在线预测服务器发送请求时，请求会被一个 HTTP 服务器接收。HTTP 服务器会从 HTTP 请求内容体中提取预测请求。提取的预测请求会被转发给服务函数。对于 Google 预构建的预测容器，请求内容会以 `tf.string` 的形式传递给服务函数。\n",
    "\n",
    "服务函数包括两个部分：\n",
    "\n",
    "- `预处理函数`：\n",
    "  - 将输入 (`tf.string`) 转换为底层模型的输入形状和数据类型（动态图）。\n",
    "  - 执行与训练底层模型期间相同的数据预处理 -- 例如，标准化，缩放等。\n",
    "- `后处理函数`:\n",
    "  - 将模型输出转换为接收应用程序所期望的格式 -- 如，压缩输出。\n",
    "  - 为接收应用程序打包输出 -- 例如，添加标题，构建 JSON 对象等。\n",
    "\n",
    "预处理和后处理函数都会转换为静态图，然后与模型融合。底层模型的输出会被传递给后处理函数。后处理函数会将转换/打包后的输出传递回HTTP服务器。HTTP 服务器会将输出作为 HTTP 响应内容返回。\n",
    "\n",
    "在为 TF.Keras 模型构建服务函数时，您需要考虑一个因素，即它们运行为静态图。这意味着您不能使用需要动态图的 TF 图操作。如果您这样做，您将在服务函数的编译过程中收到错误提示，指出您正在使用不受支持的 EagerTensor。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "serving_function_image:post"
   },
   "source": [
    "### 图像数据的服务功能\n",
    "\n",
    "#### 预处理\n",
    "\n",
    "为了将图像传递给预测服务，您需要将经过压缩的（例如JPEG）图像字节编码为Base 64 — 这样可以在通过网络传输二进制数据时保证内容不被修改。由于部署的模型期望输入数据为原始（未压缩）字节，您需要确保将Base 64编码的数据转换回原始字节，然后对其进行预处理以匹配模型输入的要求，然后将其作为输入传递给部署的模型。\n",
    "\n",
    "为了解决这个问题，您需要定义一个服务函数（`serving_fn`）并将其附加到模型作为预处理步骤。添加一个`@tf.function`装饰器，这样服务函数就会与底层模型融合在一起（而不是在CPU上游运行）。\n",
    "\n",
    "当您发送预测或解释请求时，请求的内容会被解码为一个Tensorflow字符串（`tf.string`），然后传递给服务函数（`serving_fn`）。服务函数会将`tf.string`预处理成原始（未压缩）的numpy字节（`preprocess_fn`），以匹配模型的输入要求：\n",
    "\n",
    "- `io.decode_jpeg` - 解压缩JPG图像，返回一个具有三个通道（RGB）的Tensorflow张量。\n",
    "- `image.convert_image_dtype` - 将整数像素值转换为浮点32，并重新调整像素数据在0到1之间。\n",
    "- `image.resize` - 调整图像大小以匹配模型的输入形状。\n",
    "\n",
    "在这一点上，数据可以通过一个具体函数传递到模型（`m_call`）。服务函数是一个静态图，而模型是一个动态图。具体函数执行将输入数据从服务函数传递到模型的任务，并将模型的预测结果从模型传递回服务函数的任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "serving_function_image"
   },
   "outputs": [],
   "source": [
    "CONCRETE_INPUT = \"numpy_inputs\"\n",
    "\n",
    "\n",
    "def _preprocess(bytes_input):\n",
    "    decoded = tf.io.decode_jpeg(bytes_input, channels=3)\n",
    "    decoded = tf.image.convert_image_dtype(decoded, tf.float32)\n",
    "    resized = tf.image.resize(decoded, size=(224, 224))\n",
    "    return resized\n",
    "\n",
    "\n",
    "@tf.function(input_signature=[tf.TensorSpec([None], tf.string)])\n",
    "def preprocess_fn(bytes_inputs):\n",
    "    decoded_images = tf.map_fn(\n",
    "        _preprocess, bytes_inputs, dtype=tf.float32, back_prop=False\n",
    "    )\n",
    "    return {\n",
    "        CONCRETE_INPUT: decoded_images\n",
    "    }  # User needs to make sure the key matches model's input\n",
    "\n",
    "\n",
    "@tf.function(input_signature=[tf.TensorSpec([None], tf.string)])\n",
    "def serving_fn(bytes_inputs):\n",
    "    images = preprocess_fn(bytes_inputs)\n",
    "    prob = m_call(**images)\n",
    "    return prob\n",
    "\n",
    "\n",
    "m_call = tf.function(tfhub_model.call).get_concrete_function(\n",
    "    [tf.TensorSpec(shape=[None, 224, 224, 3], dtype=tf.float32, name=CONCRETE_INPUT)]\n",
    ")\n",
    "\n",
    "tf.saved_model.save(tfhub_model, MODEL_DIR, signatures={\"serving_default\": serving_fn})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "serving_function_signature:image"
   },
   "source": [
    "获取服务函数签名\n",
    "\n",
    "您可以通过重新加载模型到内存中，并查询每个层对应的签名来获取模型的输入和输出层的签名。\n",
    "\n",
    "为了您的目的，您需要服务函数的签名。为什么？因为当我们将数据作为HTTP请求包发送进行预测时，图像数据是base64编码的，而我们的TF.Keras模型需要numpy输入。您的服务函数将会将数据从base64转换为一个numpy数组。\n",
    "\n",
    "在发出预测请求时，您需要将请求路由到服务函数而不是模型，因此您需要知道服务函数的输入层名称 -- 这将在您进行预测请求时使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "serving_function_signature:image"
   },
   "outputs": [],
   "source": [
    "loaded = tf.saved_model.load(MODEL_DIR)\n",
    "\n",
    "serving_input = list(\n",
    "    loaded.signatures[\"serving_default\"].structured_input_signature[1].keys()\n",
    ")[0]\n",
    "print(\"Serving function input:\", serving_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52886f5f9fe1"
   },
   "source": [
    "使用FastAPI构建一个HTTP服务器\n",
    "\n",
    "您的自定义容器映像要求容器必须运行一个HTTP服务器。容器必须监听并响应活跃性检查、健康检查和预测请求。\n",
    "\n",
    "在本教程中，您将使用FastAPI来实现HTTP服务器。HTTP服务器必须在0.0.0.0上监听请求。\n",
    "\n",
    "了解更多关于[FastAPI](https://fastapi.tiangolo.com/)的信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "771a6d897f6e"
   },
   "outputs": [],
   "source": [
    "# Make folder for Python training script\n",
    "! rm -rf serve\n",
    "! mkdir serve\n",
    "\n",
    "# Make the predictor subfolder\n",
    "! mkdir serve/app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8baa8361ecb4"
   },
   "source": [
    "为服务器创建要求文件\n",
    "\n",
    "接下来为服务器环境创建`requirements.txt`文件，该文件指定了需要在服务容器上安装的Python包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9507f0514120"
   },
   "outputs": [],
   "source": [
    "%%writefile serve/requirements.txt\n",
    "\n",
    "numpy~=1.20\n",
    "tensorflow>=2.5\n",
    "google-cloud-storage>=1.26.0,<2.0.0dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07c16e1003ef"
   },
   "source": [
    "### 编写 FastAPI 服务脚本\n",
    "\n",
    "接下来，您可以使用 `FastAPI` 编写 HTTP 服务器的服务脚本，具体如下：\n",
    "\n",
    "- `app`：实例化一个 `FastAPI` 应用程序\n",
    "- `health()`：定义对健康请求的响应。\n",
    "    - 返回状态码 200\n",
    "- `predict()`：定义对预测请求的响应。\n",
    "    - `body = await request.json()`：异步等待 HTTP 请求。\n",
    "    - `instances = body[\"instances\"]`：预测请求的内容。\n",
    "    - `outputs = model.predict(inputs)`：调用模型进行预测。\n",
    "    - `return {\"predictions\": ... }`：在响应体中返回格式化的预测结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2d99735d4787"
   },
   "outputs": [],
   "source": [
    "%%writefile serve/app/main.py\n",
    "from fastapi.logger import logger\n",
    "from fastapi import FastAPI, Request\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import logging\n",
    "\n",
    "gunicorn_logger = logging.getLogger('gunicorn.error')\n",
    "logger.handlers = gunicorn_logger.handlers\n",
    "\n",
    "if __name__ != \"main\":\n",
    "    logger.setLevel(gunicorn_logger.level)\n",
    "else:\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "logger.info(\"Loading model\")\n",
    "model = tf.saved_model.load('/model')\n",
    "\n",
    "@app.get(os.environ['AIP_HEALTH_ROUTE'], status_code=200)\n",
    "def health():\n",
    "    \"\"\" health check to ensure HTTP server is ready to handle \n",
    "        prediction requests\n",
    "    \"\"\"\n",
    "    return {\"status\": \"healthy\"}\n",
    "\n",
    "\n",
    "@app.post(os.environ['AIP_PREDICT_ROUTE'])\n",
    "async def predict(request: Request):\n",
    "    body = await request.json()\n",
    "    instances = body[\"instances\"]\n",
    "    inputs = []\n",
    "    for instance in instances:\n",
    "        inputs.append(instance['bytes_inputs']['b64'])\n",
    "       \n",
    "    # unfinished, returns Internal Server error\n",
    "    #outputs = model.predict(inputs)\n",
    "    #logger.info(f\"Outputs {outputs}\")\n",
    "    #return {\"predictions\": [class_num for class_num in np.argmax(outputs, axis=1)]}\n",
    "    return {'test': 'to-be-finished'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7d79a0c6daf0"
   },
   "source": [
    "### 添加预启动脚本\n",
    "\n",
    "FastAPI将在启动服务器之前执行此脚本。为了在与Vertex AI期望的端口相同的端口上运行FastAPI，将PORT环境变量设置为等于`AIP_HTTP_PORT`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7d80c7c9d6de"
   },
   "outputs": [],
   "source": [
    "%%writefile serve/app/prestart.sh\n",
    "\n",
    "#!/bin/bash\n",
    "export PORT=$AIP_HTTP_PORT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94381beddf9e"
   },
   "source": [
    "创建Dockerfile，使用tiangolo/uvicorn-gunicorn-fastapi作为基础镜像。这将自动使用Gunicorn和Uvicorn为您运行FastAPI。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "653b62d8aae1"
   },
   "outputs": [],
   "source": [
    "%%bash -s $MODEL_DIR \n",
    "\n",
    "MODEL_DIR=$1\n",
    "\n",
    "mkdir -p ./serve/model/\n",
    "gsutil cp -r ${MODEL_DIR} ./serve/model/ \n",
    "\n",
    "cat > ./serve/Dockerfile <<EOF\n",
    "FROM tiangolo/uvicorn-gunicorn-fastapi:python3.7\n",
    "\n",
    "COPY ./app /app\n",
    "COPY ./model /\n",
    "COPY requirements.txt requirements.txt\n",
    "\n",
    "RUN pip install -r requirements.txt\n",
    "\n",
    "EXPOSE 7080\n",
    "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"7080\"]\n",
    "\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "container:tfserving"
   },
   "source": [
    "#### 用于提供服务的容器（Docker）镜像\n",
    "\n",
    "设置用于提供预测的FastAPI Serving Docker容器镜像。\n",
    "\n",
    "1. 从Docker Hub拉取相应的用于TF Serving的CPU或GPU Docker镜像。\n",
    "2. 为图像创建一个标记，用于在Artifact Registry中注册图像。\n",
    "3. 使用Artifact Registry注册图像。\n",
    "\n",
    "了解更多关于[使用Docker部署FastAPI](https://fastapi.tiangolo.com/deployment/docker/)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "container:tfserving"
   },
   "outputs": [],
   "source": [
    "SUDO = \"\"  # \"sudo\"\n",
    "\n",
    "DEPLOY_IMAGE = (\n",
    "    f\"{REGION}-docker.pkg.dev/\" + PROJECT_ID + f\"/{PRIVATE_REPO}\" + \"/fastapi\"\n",
    ")\n",
    "\n",
    "! {SUDO} docker build serve -t $DEPLOY_IMAGE\n",
    "! {SUDO} docker push $DEPLOY_IMAGE\n",
    "\n",
    "print(\"Deployment:\", DEPLOY_IMAGE, DEPLOY_GPU, DEPLOY_NGPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b35aa224e61b"
   },
   "source": [
    "### 在本地运行和测试容器（可选）\n",
    "\n",
    "以分离模式在本地运行容器并提供容器所需的环境变量。这些环境变量将在部署后由Vertex AI Prediction服务提供给容器。测试/health和/predict路径，然后停止正在运行的镜像。\n",
    "\n",
    "在将容器镜像推送到Artifact Registry以与Vertex AI Predictions一起使用之前，您可以在本地环境中将其作为容器运行，以验证服务器是否按预期工作。\n",
    "\n",
    "要在本地将容器镜像作为容器运行，请运行以下命令："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ae63faa302ac"
   },
   "outputs": [],
   "source": [
    "! {SUDO} docker rm local-fastapi 2>/dev/null\n",
    "! {SUDO} docker run -t -d --rm -p 7080:7080 \\\n",
    "    --name=local-fastapi \\\n",
    "    -e AIP_HTTP_PORT=7080 \\\n",
    "    -e AIP_HEALTH_ROUTE=/health \\\n",
    "    -e AIP_PREDICT_ROUTE=/predict \\\n",
    "    -e AIP_STORAGE_URI={MODEL_DIR} \\\n",
    "    -e AIP_MODEL_DIR={MODEL_DIR} \\\n",
    "    {DEPLOY_IMAGE}\n",
    "! {SUDO} docker container ls\n",
    "! sleep 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cf672aa8c42d"
   },
   "source": [
    "要发送容器服务器的健康检查，请运行以下命令。输出应为{\"status\": \"healthy\"}。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2a96aedf4f3b"
   },
   "outputs": [],
   "source": [
    "! curl http://localhost:7080/health"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "daf4494bd342"
   },
   "source": [
    "### 为预测准备测试数据\n",
    "\n",
    "接下来，您将把一个压缩的JPEG图片加载到内存中，然后进行base64编码。为了演示目的，您将使用来自鲜花数据集的一张图片。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "486dc48ce249"
   },
   "outputs": [],
   "source": [
    "! gsutil cp gs://cloud-ml-data/img/flower_photos/daisy/100080576_f52e8ee070_n.jpg test.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d9cde1862983"
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "with open(\"test.jpg\", \"rb\") as f:\n",
    "    data = f.read()\n",
    "b64str = base64.b64encode(data).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "425109c142ae"
   },
   "source": [
    "### 进行本地预测\n",
    "\n",
    "接下来，通过进行本地预测请求来测试容器。\n",
    "\n",
    "**待办事项**：请注意，服务应用程序中的预测路由是不完整的，并且只返回Base64编码的字符串，而不会调用模型。请查看代码中的注释。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "075f347ca52d"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "instance = {serving_input: {\"b64\": b64str}}\n",
    "instances = {\"instances\": [instance]}\n",
    "s = json.dumps(instances)\n",
    "with open(\"serve/instances.json\", \"w\") as f:\n",
    "    f.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "47039c1f8bdd"
   },
   "outputs": [],
   "source": [
    "! curl -X POST \\\n",
    "  -d @serve/instances.json \\\n",
    "  -H \"Content-Type: application/json; charset=utf-8\" \\\n",
    "  http://localhost:7080/predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8c1b3f4e183e"
   },
   "source": [
    "要停止容器，运行以下命令："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "109f41d7d2cd"
   },
   "outputs": [],
   "source": [
    "! {SUDO} docker stop local-fastapi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8ce91147c93"
   },
   "source": [
    "### 将TensorFlow Hub模型上传到`Vertex AI模型`资源\n",
    "\n",
    "最后，您将TFHub模型和服务功能中的模型工件上传到`Vertex AI模型`资源中。由于您正在使用非Google预构建的服务二进制文件 - 例如TensorFlow Serving，您需要指定以下额外的服务配置设置：\n",
    "\n",
    "- `serving_container_health_route`：服务用于定期ping以验证服务二进制文件正在运行的URL。\n",
    "- `serving_container_predict_route`：用于将基于REST的预测请求路由到服务的URL。\n",
    "- `serving_container_ports`：HTTP服务器用于监听请求的端口列表。\n",
    "\n",
    "将模型上传到Vertex AI模型资源将返回一个长时间运行的操作，因为这可能需要一些时间。\n",
    "\n",
    "*注意：*当您将模型工件上传到`Vertex AI模型`资源时，您需要指定相应的部署容器镜像。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ad61e1429512"
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"example_\" + TIMESTAMP\n",
    "\n",
    "model = aip.Model.upload(\n",
    "    display_name=\"example_\" + TIMESTAMP,\n",
    "    artifact_uri=MODEL_DIR,\n",
    "    serving_container_image_uri=DEPLOY_IMAGE,\n",
    "    serving_container_health_route=\"/ping\",\n",
    "    serving_container_predict_route=\"/predict\",\n",
    "    serving_container_ports=[7080],\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "628de0914ba1"
   },
   "source": [
    "创建`Endpoint`资源\n",
    "\n",
    "您可以使用`Endpoint.create()`方法创建`Endpoint`资源。至少，您需要为端点指定显示名称。您还可以选择指定项目和位置（区域）；否则设置将继承您使用`init()`方法初始化Vertex AI SDK时设置的值。\n",
    "\n",
    "在此示例中，指定了以下参数：\n",
    "\n",
    "- `display_name`：`Endpoint`资源的人类可读名称。\n",
    "- `project`：您的项目ID。\n",
    "- `location`：您的区域。\n",
    "- `labels`：（可选）`Endpoint`的用户定义的元数据，以键/值对的形式。\n",
    "\n",
    "此方法返回一个`Endpoint`对象。\n",
    "\n",
    "了解更多关于[Vertex AI Endpoints](https://cloud.google.com/vertex-ai/docs/predictions/deploy-model-api)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ea443f9593b"
   },
   "outputs": [],
   "source": [
    "endpoint = aip.Endpoint.create(\n",
    "    display_name=\"example_\" + TIMESTAMP,\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    labels={\"your_key\": \"your_value\"},\n",
    ")\n",
    "\n",
    "print(endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ca3fa3f6a894"
   },
   "source": [
    "## 部署 `Model` 资源到一个 `Endpoint` 资源。\n",
    "\n",
    "您可以将一个或多个 `Vertex AI Model` 资源实例部署到同一个 Endpoint 中。每个部署的 `Vertex AI Model` 资源都将拥有自己的用于提供二进制文件的部署容器。\n",
    "\n",
    "*注意:* 在之前的步骤中，您已经为 TFHub 模型指定了部署容器，讲上传模型工件到 `Vertex AI Model` 资源。\n",
    "\n",
    "在下一个示例中，您将部署 `Vertex AI Model` 资源到一个 `Vertex AI Endpoint` 资源。`Vertex AI Model` 资源已经为它定义了部署容器镜像。要部署，您需要指定以下额外的配置设置:\n",
    "\n",
    "- 机器类型。\n",
    "- （如果有）GPU 的类型和数量。\n",
    "- VM 实例的静态、手动或自动缩放。\n",
    "\n",
    "在此示例中，您将使用最少量的指定参数来部署模型，如下所示:\n",
    "\n",
    "- `model`: `Model` 资源。\n",
    "- `deployed_model_displayed_name`: 部署模型实例的可读名称。\n",
    "- `machine_type`: 每个 VM 实例的机器类型。\n",
    "\n",
    "根据资源供应需求，此过程可能需要几分钟。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4e93b034a72f"
   },
   "outputs": [],
   "source": [
    "response = endpoint.deploy(\n",
    "    model=model,\n",
    "    deployed_model_display_name=\"example_\" + TIMESTAMP,\n",
    "    machine_type=DEPLOY_COMPUTE,\n",
    ")\n",
    "\n",
    "print(endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "predict_request:mbsdk,custom,icn"
   },
   "source": [
    "### 进行预测\n",
    "\n",
    "现在您的`Model`资源已部署到`Endpoint`资源，您可以通过向Endpoint资源发送预测请求来进行在线预测。\n",
    "\n",
    "#### 请求\n",
    "\n",
    "在这个例子中，由于您的测试项目位于云存储桶中，您可以使用`tf.io.gfile.Gfile()`打开并读取图像的内容。为了将测试数据传递给预测服务，您需要将字节编码为base64 -- 这样在通过网络传输二进制数据时内容就不会被修改。\n",
    "\n",
    "每个实例的格式为：\n",
    "\n",
    "    { serving_input: { 'b64': base64编码的字节 } }\n",
    "\n",
    "由于`predict()`方法可以接受多个项目（实例），请将您的单个测试项目作为一个测试项目的列表发送。\n",
    "\n",
    "#### 响应\n",
    "\n",
    "从`predict()`调用的响应是一个Python字典，包含以下条目：\n",
    "\n",
    "- `ids`：每个预测请求的内部分配的唯一标识符。\n",
    "- `predictions`：每个类别标签的预测置信度，介于0到1之间。\n",
    "- `deployed_model_id`：执行预测的部署的`Model`资源的Vertex AI标识符。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "predict_request:mbsdk,custom,icn"
   },
   "outputs": [],
   "source": [
    "# The format of each instance should conform to the deployed model's prediction input schema.\n",
    "instances = [{serving_input: {\"b64\": b64str}}]\n",
    "\n",
    "prediction = endpoint.predict(instances=instances)\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "清理工作\n",
    "\n",
    "要清理此项目中使用的所有Google Cloud资源，您可以删除用于此教程的[Google Cloud项目](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects)。\n",
    "\n",
    "否则，您可以删除在此教程中创建的各个资源。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sx_vKniMq9ZX"
   },
   "outputs": [],
   "source": [
    "! rm -rf serve test.jpg\n",
    "\n",
    "delete_bucket = False\n",
    "delete_model = True\n",
    "delete_endpoint = True\n",
    "\n",
    "if delete_endpoint:\n",
    "    try:\n",
    "        endpoint.undeploy_all()\n",
    "        endpoint.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "if delete_model:\n",
    "    try:\n",
    "        model.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
    "    ! gsutil rm -rf {BUCKET_URI}"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "get_started_with_fastapi.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
