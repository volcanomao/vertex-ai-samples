{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "copyright"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# GCP上的端到端机器学习：MLOps阶段6：开始使用Vertex AI批量预测为AutoML文本模型\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage6/get_started_with_automl_text_model_batch.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> 在Colab中运行\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage6/get_started_with_automl_text_model_batch.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      在GitHub上查看\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/ml_ops/stage6/get_started_with_automl_text_model_batch.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      在Vertex AI Workbench中打开\n",
    "    </a>\n",
    "  </td>        \n",
    "</table>\n",
    "<br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "overview:automl"
   },
   "source": [
    "## 概述\n",
    "\n",
    "本教程演示了如何从 `AutoML` 文本模型进行批量预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "objective:automl,training,batch_prediction"
   },
   "source": [
    "### 目标\n",
    "\n",
    "在本教程中，您将学习如何使用`Vertex AI Batch Prediction`与一个`AutoML`文本模型。\n",
    "\n",
    "本教程使用以下谷歌云机器学习服务和资源：\n",
    "\n",
    "- `Vertex AutoML`\n",
    "- `Vertex AI Batch Prediction`\n",
    "- `Vertex AI Models`\n",
    "\n",
    "所执行的步骤包括：\n",
    "\n",
    "- 创建一个Vertex `Dataset`资源。\n",
    "- 训练一个`AutoML`模型。\n",
    "- 使用JSONL输入进行批量预测。\n",
    "\n",
    "使用批量预测和在线预测之间有一个关键区别：\n",
    "\n",
    "* 预测服务：对整个实例集（即一个或多个数据项）进行按需预测，并实时返回结果。\n",
    "\n",
    "* 批量预测服务：对整个实例集进行排队（批量）预测，后台运行，并在准备就绪时将结果存储在Cloud Storage存储桶中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset:happydb,tcn"
   },
   "source": [
    "数据集\n",
    "\n",
    "本教程使用的数据集是来自Kaggle数据集的[幸福时刻数据集](https://www.kaggle.com/ritresearch/happydb)。您在本教程中使用的数据集版本存储在公共云存储桶中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "costs"
   },
   "source": [
    "### 成本\n",
    "\n",
    "本教程使用Google Cloud的计费组件：\n",
    "\n",
    "* Vertex AI\n",
    "* 云存储\n",
    "\n",
    "了解[Vertex AI\n",
    "定价](https://cloud.google.com/vertex-ai/pricing)和[云存储\n",
    "定价](https://cloud.google.com/storage/pricing)，并使用[Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "根据您预计的使用情况生成成本估算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_local"
   },
   "source": [
    "### 设置本地开发环境\n",
    "\n",
    "如果您正在使用Colab或Vertex AI Workbench笔记本，您的环境已经满足运行此笔记本的所有要求。您可以跳过这一步。\n",
    "\n",
    "否则，请确保您的环境满足此笔记本的要求。您需要以下内容：\n",
    "\n",
    "- 云存储SDK\n",
    "- Git\n",
    "- Python 3\n",
    "- virtualenv\n",
    "- 在使用Python 3的虚拟环境中运行的Jupyter笔记本\n",
    "\n",
    "云存储指南中提供了关于[设置Python开发环境](https://cloud.google.com/python/setup)和[Jupyter安装指南](https://jupyter.org/install)的详细说明来满足这些要求。以下步骤提供了一组简洁的说明：\n",
    "\n",
    "1. [安装并初始化SDK](https://cloud.google.com/sdk/docs/)。\n",
    "\n",
    "2. [安装Python 3](https://cloud.google.com/python/setup#installing_python)。\n",
    "\n",
    "3. [安装virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv)并创建一个使用Python 3的虚拟环境。激活虚拟环境。\n",
    "\n",
    "4. 要安装Jupyter，请在终端shell中运行`pip3 install jupyter`。\n",
    "\n",
    "5. 要启动Jupyter，请在终端shell中运行`jupyter notebook`。\n",
    "\n",
    "6. 在Jupyter Notebook仪表板中打开此笔记本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_aip:mbsdk"
   },
   "source": [
    "## 安装\n",
    "\n",
    "安装以下软件包以执行此笔记本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Vertex AI Workbench Notebook product has specific requirements\n",
    "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\")\n",
    "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
    "    \"/opt/deeplearning/metadata/env_version\"\n",
    ")\n",
    "\n",
    "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_WORKBENCH_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\"\n",
    "\n",
    "! pip3 install --upgrade google-cloud-aiplatform $USER_FLAG -q\n",
    "! pip3 install tensorflow $USER_FLAG -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "restart"
   },
   "source": [
    "重启内核\n",
    "\n",
    "一旦您安装了额外的包，您需要重新启动笔记本的内核，以便它可以找到这些包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "restart"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "before_you_begin:nogpu"
   },
   "source": [
    "## 开始之前\n",
    "\n",
    "### GPU 运行时\n",
    "\n",
    "本教程不需要 GPU 运行时。\n",
    "\n",
    "### 设置您的 Google Cloud 项目\n",
    "\n",
    "**无论您的笔记本环境如何，以下步骤都是必需的。**\n",
    "\n",
    "1. [选择或创建一个 Google Cloud 项目](https://console.cloud.google.com/cloud-resource-manager)。当您首次创建帐户时，您将获得$300 的免费信用，可用于支付计算和存储成本。\n",
    "\n",
    "2. [确保您的项目已启用计费功能。](https://cloud.google.com/billing/docs/how-to/modify-project)\n",
    "\n",
    "3. [启用以下 API：Vertex AI APIs、Compute Engine APIs 和 Cloud Storage。](https://console.cloud.google.com/flows/enableapi?apiid=ml.googleapis.com,compute_component,storage-component.googleapis.com)\n",
    "\n",
    "4. 如果您在本地运行此笔记本，您需要安装 [Cloud SDK](https://cloud.google.com/sdk)。\n",
    "\n",
    "5. 在下面的单元格中输入您的项目 ID。然后运行该单元格，以确保 Cloud SDK 在本笔记本中执行所有命令时使用正确的项目。\n",
    "\n",
    "**注意**: Jupyter 会将以 `!` 为前缀的行视为 shell 命令，并会内插以 `$` 为前缀的 Python 变量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "project_id"
   },
   "source": [
    "#### 设置您的项目ID\n",
    "\n",
    "**如果您不知道您的项目ID**，您可以使用`gcloud`获取您的项目ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_project_id"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "autoset_project_id"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID:\", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_gcloud_project_id"
   },
   "outputs": [],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "#### 区域\n",
    "\n",
    "您还可以更改 `REGION` 变量，该变量用于笔记本的其余部分操作。以下是 Vertex AI 支持的区域。我们建议您选择离您最近的区域。\n",
    "\n",
    "- 美洲：`us-central1`\n",
    "- 欧洲：`europe-west4`\n",
    "- 亚太：`asia-east1`\n",
    "\n",
    "您可能无法使用多区域存储桶进行 Vertex AI 训练。并非所有区域都支持所有 Vertex AI 服务。\n",
    "\n",
    "了解更多关于 [Vertex AI 区域](https://cloud.google.com/vertex-ai/docs/general/locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "region"
   },
   "outputs": [],
   "source": [
    "REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
    "\n",
    "if REGION == \"[your-region]\":\n",
    "    REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "timestamp"
   },
   "source": [
    "UUID\n",
    "\n",
    "如果您正在进行实时教程会话，可能正在使用共享测试账户或项目。为了避免资源创建时用户之间发生名称冲突，您可以为每个实例会话创建一个UUID，并将其附加到本教程中创建的资源名称。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "timestamp"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "\n",
    "# Generate a uuid of a specifed length(default=8)\n",
    "def generate_uuid(length: int = 8) -> str:\n",
    "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
    "\n",
    "\n",
    "UUID = generate_uuid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcp_authenticate"
   },
   "source": [
    "### 验证您的Google Cloud账户\n",
    "\n",
    "**如果您正在使用Vertex AI Workbench Notebooks**，您的环境已经通过身份验证。\n",
    "\n",
    "**如果您正在使用Colab**，运行下面的单元格，并按照提示进行身份验证。\n",
    "\n",
    "**否则**，请按照以下步骤操作：\n",
    "\n",
    "在Cloud控制台中，转到[创建服务帐号密钥](https://console.cloud.google.com/apis/credentials/serviceaccountkey)页面。\n",
    "\n",
    "**点击创建服务帐号**。\n",
    "\n",
    "在**服务帐号名称**字段中输入名称，然后点击**创建**。\n",
    "\n",
    "在**授予此服务帐号对项目的访问权限**部分，点击角色下拉列表。在过滤框中键入“Vertex”，然后选择**Vertex管理员**。在过滤框中键入“Storage Object Admin”，然后选择**存储对象管理员**。\n",
    "\n",
    "点击创建。一个包含您的密钥的JSON文件将下载到您的本地环境。\n",
    "\n",
    "将服务帐号密钥的路径输入到下面的单元格中作为GOOGLE_APPLICATION_CREDENTIALS变量，并运行该单元格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gcp_authenticate"
   },
   "outputs": [],
   "source": [
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# If on Vertex AI Workbench, then don't execute this code\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\") and not os.getenv(\n",
    "    \"DL_ANACONDA_HOME\"\n",
    "):\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\"):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bucket:mbsdk"
   },
   "source": [
    "### 创建一个云存储桶\n",
    "\n",
    "**无论您使用哪种笔记本环境，以下步骤都是必须的。**\n",
    "\n",
    "当您为 Python 初始化 Vertex AI SDK 时，您需要指定一个云存储暂存桶。这个暂存桶是您的数据集和模型资源在不同会话间保留的地方。\n",
    "\n",
    "在下面设置您的云存储桶的名称。存储桶的名称必须在所有谷歌云项目中全局唯一，包括您组织之外的项目。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bucket"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"[your-bucket-name]\"  # @param {type:\"string\"}\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "autoset_bucket"
   },
   "outputs": [],
   "source": [
    "if BUCKET_URI == \"\" or BUCKET_URI is None or BUCKET_URI == \"gs://[your-bucket-name]\":\n",
    "    BUCKET_NAME = PROJECT_ID + \"aip-\" + UUID\n",
    "    BUCKET_URI = \"gs://\" + BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_bucket"
   },
   "source": [
    "只有当您的存储桶尚不存在时：运行以下单元格以创建您的云存储存储桶。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_bucket"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "validate_bucket"
   },
   "source": [
    "最后，通过检查Cloud Storage存储桶的内容来验证访问权限。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "validate_bucket"
   },
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_vars"
   },
   "source": [
    "### 设置变量\n",
    "\n",
    "接下来，设置一些在教程中使用的变量。\n",
    "### 导入库和定义常量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk"
   },
   "source": [
    "### 初始化用于Python的Vertex AI SDK\n",
    "\n",
    "为您的项目和相应的存储桶初始化用于Python的Vertex AI SDK。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tutorial_start:automl"
   },
   "source": [
    "# 教程\n",
    "\n",
    "现在你准备好开始创建你自己的AutoML文本分类模型了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_file:u_dataset,csv"
   },
   "source": [
    "云存储训练数据的位置。\n",
    "\n",
    "现在将变量`IMPORT_FILE`设置为在云存储中CSV索引文件的位置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_file:happydb,csv,tcn"
   },
   "outputs": [],
   "source": [
    "IMPORT_FILE = \"gs://cloud-ml-data/NL-classification/happiness.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "quick_peek:csv"
   },
   "source": [
    "快速查看您的数据\n",
    "\n",
    "本教程使用存储在公共云存储桶中的Happy Moments数据集的一个版本，使用CSV索引文件。\n",
    "\n",
    "首先快速查看数据。通过计算CSV索引文件中的行数（`wc -l`）来计数示例的数量，然后查看前几行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quick_peek:csv"
   },
   "outputs": [],
   "source": [
    "if \"IMPORT_FILES\" in globals():\n",
    "    FILE = IMPORT_FILES[0]\n",
    "else:\n",
    "    FILE = IMPORT_FILE\n",
    "\n",
    "count = ! gsutil cat $FILE | wc -l\n",
    "print(\"Number of Examples\", int(count[0]))\n",
    "\n",
    "print(\"First 10 rows\")\n",
    "! gsutil cat $FILE | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_dataset:text,tcn"
   },
   "source": [
    "### 创建数据集\n",
    "\n",
    "接下来，使用`TextDataset`类的`create`方法创建`Dataset`资源，该方法接受以下参数：\n",
    "\n",
    "- `display_name`：`Dataset`资源的可读名称。\n",
    "- `gcs_source`：要将数据项导入`Dataset`资源的一个或多个数据集索引文件列表。\n",
    "- `import_schema_uri`：数据项的数据标记模式。\n",
    "\n",
    "此操作可能需要几分钟时间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_dataset:text,tcn"
   },
   "outputs": [],
   "source": [
    "dataset = aiplatform.TextDataset.create(\n",
    "    display_name=\"Happy Moments\" + \"_\" + UUID,\n",
    "    gcs_source=[IMPORT_FILE],\n",
    "    import_schema_uri=aiplatform.schema.dataset.ioformat.text.single_label_classification,\n",
    ")\n",
    "\n",
    "print(dataset.resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_automl_pipeline:text,tcn"
   },
   "source": [
    "### 创建并运行训练流水线\n",
    "\n",
    "要训练一个AutoML模型，您需要执行两个步骤：1）创建一个训练流水线，2）运行这个流水线。\n",
    "\n",
    "#### 创建训练流水线\n",
    "\n",
    "使用`AutoMLTextTrainingJob`类创建一个AutoML训练流水线，需要以下参数：\n",
    "\n",
    "- `display_name`：`TrainingJob`资源的可读名称。\n",
    "- `prediction_type`：训练模型的任务类型。\n",
    "  - `classification`：文本分类模型。\n",
    "  - `sentiment`：文本情感分析模型。\n",
    "  - `extraction`：文本实体抽取模型。\n",
    "- `multi_label`：如果是分类任务，是单标签（False）还是多标签（True）。\n",
    "- `sentiment_max`：如果是情感分析任务，情感值的最大值。\n",
    "\n",
    "实例化的对象是训练流水线的有向无环图（DAG）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_automl_pipeline:text,tcn"
   },
   "outputs": [],
   "source": [
    "dag = aiplatform.AutoMLTextTrainingJob(\n",
    "    display_name=\"happydb_\" + UUID, prediction_type=\"classification\"\n",
    ")\n",
    "\n",
    "print(dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run_automl_pipeline:text"
   },
   "source": [
    "#### 运行训练管道\n",
    "\n",
    "接下来，通过调用`run`方法运行DAG来开始训练作业，参数如下：\n",
    "\n",
    "- `dataset`：用于训练模型的`Dataset`资源。\n",
    "- `model_display_name`：训练模型的人类可读名称。\n",
    "- `training_fraction_split`：用于训练的数据集百分比。\n",
    "- `test_fraction_split`：用于测试（留出数据）的数据集百分比。\n",
    "- `validation_fraction_split`：用于验证的数据集百分比。\n",
    "\n",
    "当`run`方法完成时，将返回`Model`资源。\n",
    "\n",
    "训练管道的执行将持续最多4小时。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_automl_pipeline:text"
   },
   "outputs": [],
   "source": [
    "model = dag.run(\n",
    "    dataset=dataset,\n",
    "    model_display_name=\"happydb_\" + UUID,\n",
    "    training_fraction_split=0.8,\n",
    "    validation_fraction_split=0.1,\n",
    "    test_fraction_split=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11e16f54bc90"
   },
   "source": [
    "## 批处理预测简介\n",
    "\n",
    "批处理预测提供了离线批量处理大量预测请求的能力。资源仅在批处理过程中预留，并且在批处理请求完成后取消预留。结果存储在云存储中，与在线预测不同，在在线预测中结果作为HTTP响应数据包返回。\n",
    "\n",
    "批处理作业的输入格式取决于您的模型服务器支持的格式。首先，您的模型服务器中的Web服务器必须支持JSONL格式，该Web服务器将其转换为模型输入界面或服务函数界面直接支持的格式。对于批处理预测，这个JSONL格式被称为`pivot`格式。\n",
    "\n",
    "### 批处理预测作业的输入格式\n",
    "\n",
    "批处理服务器接受以下AutoML文本模型的输入格式：\n",
    "\n",
    "- JSONL\n",
    "\n",
    "### 批处理预测作业的输出格式\n",
    "\n",
    "批处理服务器接受以下AutoML文本模型的输出格式：\n",
    "\n",
    "- JSONL\n",
    "\n",
    "\n",
    "### 枢轴格式\n",
    "\n",
    "批处理服务器将输入格式转换为`pivot`（JSONL）格式，如下所示：\n",
    "\n",
    "**JSONL**\n",
    "\n",
    "每个输入行（请求）应包含一个且仅一个有效的json值。\n",
    "\n",
    "    {\"values\": [1, 2, 3, 4], \"key\": 1}\n",
    "    {\"values\": [5, 6, 7, 8], \"key\": 2}\n",
    "\n",
    "批处理服务器生成具有相同格式的枢轴数据。然后将生成的枢轴数据包装成一个有效载荷请求：\n",
    "\n",
    "    {\"instances\": [\n",
    "      {\"values\": [1, 2, 3, 4], \"key\": 1},\n",
    "      {\"values\": [5, 6, 7, 8], \"key\": 2}\n",
    "    ]}\n",
    "\n",
    "**CSV**\n",
    "\n",
    "第一行中的CSV标题将始终被忽略。必须明确将字符串字段用双引号括起来，否则该行将被丢弃，并且会输出解析错误消息到错误文件。非引号的值总是以浮点数传输。\n",
    "\n",
    "    col1,col2,col3\n",
    "    1,3,\"cat1\"\n",
    "    2,4,\"cat2\"\n",
    "\n",
    "批处理服务器会将每个输入行（请求）转换为JSON数组。\n",
    "\n",
    "    {\"instances\": [\n",
    "     [1.0,3.0,\"cat1\"],\n",
    "     [2.0,4.0,\"cat2\"]\n",
    "    ]}\n",
    "    \n",
    "**BigQuery**\n",
    "\n",
    "每行都会被转换为JSON数组。例如：\n",
    "\n",
    "    [1.0,3.0,\"cat1\"]\n",
    "    [2.0,4.0,\"cat2\"]\n",
    "    \n",
    "批处理服务器生成具有相同格式的枢轴数据。然后将生成的枢轴数据包装成一个有效载荷请求：\n",
    "\n",
    "    {\"instances\": [\n",
    "     [1.0,3.0,\"cat1\"],\n",
    "     [2.0,4.0,\"cat2\"]\n",
    "    ]}\n",
    "\n",
    "**TFRecords**\n",
    "\n",
    "TFRecord文件中的实例由apache_beam.io.tfrecordio模块作为二进制读取。然后将二进制对象序列化为ASCII字符串。预测服务器负责知道恢复实例的解码器。\n",
    "\n",
    "    {\"instances\": [\n",
    "     {\"b64\",\"b64EncodedASCIIString\"},\n",
    "     {\"b64\",\"b64EncodedASCIIString\"}\n",
    "    ]}\n",
    "\n",
    "**文件列表**\n",
    "\n",
    "文件列表格式包含文件列表。在“FileList”文件中的每一行都指定一个单独的文件路径，指定为云存储位置。\n",
    "\n",
    "    gs://my-bucket/file1.txt\n",
    "    gs://my-bucket/file2.txt\n",
    "\n",
    "批处理服务器读取文件为二进制。然后将二进制对象序列化为ASCII字符串。\n",
    "\n",
    "    {\"instances\": [\n",
    "     {\"b64\",\"b64EncodedASCIIString\"},\n",
    "     {\"b64\",\"b64EncodedASCIIString\"}\n",
    "    ]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "make_batch_file:automl,tabular,alt"
   },
   "source": [
    "### 使用JSONL输入创建批量输入文件\n",
    "\n",
    "现在创建一个批量输入文件，将其存储在您的本地云存储桶中。在这个例子中，您可以使用JSONL格式作为输入和输出。对于输入，JSONL文件的布局如下：\n",
    "\n",
    "    {'content': '[your-bucket]/file1.txt', 'mime_type': 'text'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "get_test_items:batch_prediction"
   },
   "source": [
    "### 获取测试项\n",
    "\n",
    "现在对您的 Vertex AI 模型进行批量预测。您将使用数据集中的任意示例作为测试项。不必担心这些示例很可能已经在训练模型时使用过 — 我们只是想演示如何进行预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "get_test_items:automl,tcn,csv"
   },
   "outputs": [],
   "source": [
    "test_items = ! gsutil cat $IMPORT_FILE | head -n2\n",
    "if len(test_items[0]) == 3:\n",
    "    _, test_item_1, test_label_1 = str(test_items[0]).split(\",\")\n",
    "    _, test_item_2, test_label_2 = str(test_items[1]).split(\",\")\n",
    "else:\n",
    "    test_item_1, test_label_1 = str(test_items[0]).split(\",\")\n",
    "    test_item_2, test_label_2 = str(test_items[1]).split(\",\")\n",
    "\n",
    "print(test_item_1, test_label_1)\n",
    "print(test_item_2, test_label_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "make_batch_file:automl,text"
   },
   "source": [
    "### 创建批量输入文件\n",
    "\n",
    "现在创建一个批量输入文件，您将存储在本地云存储存储桶中。批量输入文件只能是JSONL格式。对于JSONL文件，您为每个数据项（实例）在每一行上创建一个字典条目。该字典包含键/值对：\n",
    "\n",
    "- `content`：文本项文件的云存储路径。\n",
    "- `mime_type`：内容类型。在我们的例子中，它是一个`text`文件。\n",
    "\n",
    "例如：\n",
    "\n",
    "{'content': '[your-bucket]/file1.txt', 'mime_type': 'text'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "make_batch_file:automl,text"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "gcs_test_item_1 = BUCKET_URI + \"/test1.txt\"\n",
    "with open(\"test1.txt\", \"w\") as f:\n",
    "    f.write(test_item_1 + \"\\n\")\n",
    "gcs_test_item_2 = BUCKET_URI + \"/test2.txt\"\n",
    "with open(\"test2.txt\", \"w\") as f:\n",
    "    f.write(test_item_2 + \"\\n\")\n",
    "\n",
    "! gsutil cp test1.txt $gcs_test_item_1\n",
    "! gsutil cp test2.txt $gcs_test_item_2\n",
    "\n",
    "gcs_input_uri = BUCKET_URI + \"/test.jsonl\"\n",
    "with open(\"test.jsonl\", \"w\") as f:\n",
    "    data = {\"content\": gcs_test_item_1, \"mime_type\": \"text/plain\"}\n",
    "    f.write(json.dumps(data) + \"\\n\")\n",
    "    data = {\"content\": gcs_test_item_2, \"mime_type\": \"text/plain\"}\n",
    "    f.write(json.dumps(data) + \"\\n\")\n",
    "\n",
    "print(gcs_input_uri)\n",
    "! gsutil cp test.jsonl $gcs_input_uri\n",
    "! gsutil cat $gcs_input_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "batch_request:mbsdk"
   },
   "source": [
    "### 进行批处理预测请求\n",
    "\n",
    "现在您的模型资源已经训练完成，您可以通过调用batch_predict()方法进行批处理预测，使用以下参数：\n",
    "\n",
    "- `job_display_name`: 批处理预测任务的可读名称。\n",
    "- `instances_format`: 批处理预测请求文件的格式： \"jsonl\", \"csv\", \"bigquery\", \"tf-record\", \"tf-record-gzip\" 或 \"file-list\"\n",
    "- `prediction_format`: 批处理预测响应文件的格式： \"jsonl\", \"csv\", \"bigquery\", \"tf-record\", \"tf-record-gzip\" 或 \"file-list\"\n",
    "- `gcs_source`: 一个或多个批处理请求输入文件的列表。\n",
    "- `gcs_destination_prefix`: 存储批处理预测结果的云存储位置。\n",
    "- `sync`: 如果设置为True，则调用将阻塞，等待异步批处理任务完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "batch_request:mbsdk"
   },
   "outputs": [],
   "source": [
    "batch_predict_job = model.batch_predict(\n",
    "    job_display_name=\"happydb_\" + UUID,\n",
    "    instances_format=\"jsonl\",\n",
    "    predictions_format=\"jsonl\",\n",
    "    gcs_source=gcs_input_uri,\n",
    "    gcs_destination_prefix=BUCKET_URI,\n",
    "    sync=False,\n",
    ")\n",
    "\n",
    "print(batch_predict_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "batch_request_wait:mbsdk"
   },
   "source": [
    "### 等待批量预测作业完成\n",
    "\n",
    "接下来，等待批量作业完成。或者，可以在`batch_predict()`方法中将参数`sync`设置为`True`，以阻塞直到批量预测作业完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "batch_request_wait:mbsdk"
   },
   "outputs": [],
   "source": [
    "batch_predict_job.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "batch_request_wait:mbsdk"
   },
   "outputs": [],
   "source": [
    "batch_predict_job.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "get_batch_prediction:mbsdk,tcn"
   },
   "source": [
    "### 获取预测结果\n",
    "\n",
    "接下来，从已完成的批量预测作业中获取结果。\n",
    "\n",
    "结果被写入您在批量预测请求中指定的 Cloud Storage 输出存储桶中。您可以调用 `iter_outputs()` 方法来获取生成的与结果相关的每个 Cloud Storage 文件的列表。每个文件以 JSON 格式包含一个或多个预测请求：\n",
    "\n",
    "- `content`：预测请求。\n",
    "- `prediction`：预测响应。\n",
    " - `ids`：每个预测请求的内部分配的唯一标识符。\n",
    " - `displayNames`：每个类别标签的类名。\n",
    " - `confidences`：每个类别标签的预测置信度，介于 0 和 1 之间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "get_batch_prediction:mbsdk,tcn"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "bp_iter_outputs = batch_predict_job.iter_outputs()\n",
    "\n",
    "prediction_results = list()\n",
    "for blob in bp_iter_outputs:\n",
    "    if blob.name.split(\"/\")[-1].startswith(\"prediction\"):\n",
    "        prediction_results.append(blob.name)\n",
    "\n",
    "tags = list()\n",
    "for prediction_result in prediction_results:\n",
    "    gfile_name = f\"gs://{bp_iter_outputs.bucket.name}/{prediction_result}\"\n",
    "    with tf.io.gfile.GFile(name=gfile_name, mode=\"r\") as gfile:\n",
    "        for line in gfile.readlines():\n",
    "            line = json.loads(line)\n",
    "            print(line)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleanup:mbsdk"
   },
   "source": [
    "清理\n",
    "\n",
    "要清理此项目中使用的所有Google Cloud资源，您可以删除用于本教程的[Google Cloud项目](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects)。\n",
    "\n",
    "否则，您可以删除本教程中创建的各个资源。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cleanup:mbsdk"
   },
   "outputs": [],
   "source": [
    "delete_bucket = False\n",
    "delete_model = True\n",
    "delete_batch_job = True\n",
    "\n",
    "if delete_model:\n",
    "    try:\n",
    "        model.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "if delete_batch_job:\n",
    "    batch_predict_job.delete()\n",
    "\n",
    "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
    "    ! gsutil rm -rf {BUCKET_URI}"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "get_started_with_automl_text_model_batch.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
