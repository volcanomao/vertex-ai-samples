{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "copyright"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "在GCP上的E2E ML：MLOps阶段6：开始使用Vertex AI批量预测和可解释的AI进行AutoML表格模型\n",
    "\n",
    "在Colab中运行\n",
    "在GitHub上查看\n",
    "在Vertex AI Workbench中打开"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "overview:automl"
   },
   "source": [
    "## 概述\n",
    "\n",
    "本教程演示了如何使用Vertex AI SDK创建表格二元分类模型，并使用[AutoML](https://cloud.google.com/vertex-ai/docs/start/automl-users)模型进行可解释的AI批量预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "objective:automl,training,batch_prediction"
   },
   "source": [
    "### 目标\n",
    "\n",
    "在本教程中，您将从Python脚本中创建一个AutoML表格二元分类模型，然后使用Vertex AI SDK进行批量预测，同时使用可解释AI。您还可以选择使用`gcloud`命令行工具或在线使用Cloud Console来创建和部署模型。\n",
    "\n",
    "本教程使用以下Google Cloud ML服务和资源：\n",
    "\n",
    "- `Vertex AutoML`\n",
    "- `Vertex AI Batch Prediction`\n",
    "- `Vertex AI Datasets`\n",
    "- `Vertex AI Models`\n",
    "- `BigQuery`\n",
    "\n",
    "执行的步骤包括：\n",
    "\n",
    "- 创建一个Vertex `Dataset`资源。\n",
    "- 训练一个`AutoML`表格模型。\n",
    "- 使用CSV输入进行批量预测。\n",
    "- 使用JSONL对象输入进行批量预测。\n",
    "- 使用JSONL列表输入进行批量预测。\n",
    "- 使用BigQuery表输入进行批量预测。\n",
    "- 使用解释进行批量预测。\n",
    "\n",
    "使用批量预测和使用在线预测之间有一个关键区别：\n",
    "\n",
    "* 预测服务：对整个实例集合（即一个或多个数据项）进行即时预测并实时返回结果。\n",
    "\n",
    "* 批量预测服务：对整个实例集合进行排队（批处理）预测，并在准备就绪时将结果存储在Cloud Storage存储桶中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset:bank,lbn"
   },
   "source": [
    "数据集\n",
    "\n",
    "这篇教程使用的数据集是[银行营销](https://pantheon.corp.google.com/storage/browser/_details/cloud-ml-tables-data/bank-marketing.csv)。这个数据集不需要任何特征工程。你在这篇教程中使用的数据集版本存储在一个公共云存储桶中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "costs"
   },
   "source": [
    "### 费用\n",
    "\n",
    "本教程使用Google Cloud的计费组件：\n",
    "\n",
    "- Vertex AI\n",
    "- Cloud Storage\n",
    "- BigQuery\n",
    "\n",
    "了解[Vertex AI定价](https://cloud.google.com/vertex-ai/pricing)、[Cloud Storage定价](https://cloud.google.com/storage/pricing)和[BigQuery定价](https://cloud.google.com/bigquery/pricing)，并使用[Pricing Calculator](https://cloud.google.com/products/calculator/)根据您的预期使用情况生成成本估算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_local"
   },
   "source": [
    "### 设置本地开发环境\n",
    "\n",
    "如果您正在使用Colab或Vertex AI Workbench笔记本，则您的环境已经满足运行此笔记本的所有要求。您可以跳过此步骤。\n",
    "\n",
    "否则，请确保您的环境满足此笔记本的要求。您需要以下内容：\n",
    "\n",
    "- 云存储SDK\n",
    "- Git\n",
    "- Python 3\n",
    "- virtualenv\n",
    "- 在Python 3虚拟环境中运行的Jupyter笔记本\n",
    "\n",
    "[设置Python开发环境](https://cloud.google.com/python/setup)和[Jupyter安装指南](https://jupyter.org/install)提供了满足这些要求的详细说明。以下步骤提供了一套简明的指令：\n",
    "\n",
    "1. [安装并初始化SDK](https://cloud.google.com/sdk/docs/)。\n",
    "\n",
    "2. [安装Python 3](https://cloud.google.com/python/setup#installing_python)。\n",
    "\n",
    "3. [安装virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv)并创建一个使用Python 3的虚拟环境。激活虚拟环境。\n",
    "\n",
    "4. 要安装Jupyter，可以在终端shell中运行`pip3 install jupyter`。\n",
    "\n",
    "5. 要启动Jupyter，可以在终端shell中运行`jupyter notebook`。\n",
    "\n",
    "6. 在Jupyter Notebook仪表板中打开这个笔记本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_aip:mbsdk"
   },
   "source": [
    "## 安装\n",
    "\n",
    "安装以下软件包以执行此笔记本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Vertex AI Workbench Notebook product has specific requirements\n",
    "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\")\n",
    "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
    "    \"/opt/deeplearning/metadata/env_version\"\n",
    ")\n",
    "\n",
    "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_WORKBENCH_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\"\n",
    "\n",
    "! pip3 install --upgrade google-cloud-aiplatform $USER_FLAG -q\n",
    "! pip3 install {USER_FLAG} --quiet --upgrade google-cloud-bigquery\n",
    "! pip3 install tensorflow $USER_FLAG -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "restart"
   },
   "source": [
    "重新启动内核\n",
    "\n",
    "安装了额外的软件包后，需要重新启动笔记本内核，以便它能找到这些软件包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "restart"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "before_you_begin:nogpu"
   },
   "source": [
    "在您开始之前\n",
    "\n",
    "GPU运行时\n",
    "\n",
    "本教程不需要GPU运行时。\n",
    "\n",
    "设置您的Google Cloud项目\n",
    "\n",
    "**无论您使用的是笔记本环境，以下步骤都是必需的。**\n",
    "\n",
    "1. [选择或创建Google Cloud项目](https://console.cloud.google.com/cloud-resource-manager)。当您首次创建帐户时，您将获得$300的免费信用额度用于计算/存储成本。\n",
    "\n",
    "2. [确保为您的项目启用了计费。](https://cloud.google.com/billing/docs/how-to/modify-project)\n",
    "\n",
    "3. [启用以下API：Vertex AI APIs, Compute Engine APIs和Cloud Storage。](https://console.cloud.google.com/flows/enableapi?apiid=ml.googleapis.com,compute_component,storage-component.googleapis.com)\n",
    "\n",
    "4. 如果您在本地运行此笔记本，您需要安装[Cloud SDK](https://cloud.google.com/sdk)。\n",
    "\n",
    "5. 在下面的单元格中输入您的项目ID。然后运行该单元格，确保Cloud SDK在本笔记本中的所有命令中使用正确的项目。\n",
    "\n",
    "**注意**: Jupyter会将以`!`为前缀的行作为shell命令运行，并会替换以`$`为前缀的Python变量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "project_id"
   },
   "source": [
    "#### 设置您的项目ID\n",
    "\n",
    "**如果您不知道您的项目ID**，您可以使用`gcloud`来获取您的项目ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_project_id"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "autoset_project_id"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID:\", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_gcloud_project_id"
   },
   "outputs": [],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "#### 区域\n",
    "\n",
    "您还可以更改`REGION`变量，该变量用于笔记本的其余操作。以下是Vertex AI支持的区域。我们建议您选择离您最近的区域。\n",
    "\n",
    "- 美洲：`us-central1`\n",
    "- 欧洲：`europe-west4`\n",
    "- 亚太：`asia-east1`\n",
    "\n",
    "您可能无法使用多区域存储桶来训练Vertex AI。并非所有区域都支持所有Vertex AI服务。\n",
    "\n",
    "了解更多关于[Vertex AI区域](https://cloud.google.com/vertex-ai/docs/general/locations)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "region"
   },
   "outputs": [],
   "source": [
    "REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
    "\n",
    "if REGION == \"[your-region]\":\n",
    "    REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "timestamp"
   },
   "source": [
    "UUID\n",
    "\n",
    "如果您正在进行实时教程会话，您可能正在使用共享的测试帐户或项目。为了避免在创建的资源上发生名称冲突，您需要为每个实例会话创建一个uuid，并将其附加到您在本教程中创建的资源名称上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "timestamp"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "\n",
    "# Generate a uuid of a specifed length(default=8)\n",
    "def generate_uuid(length: int = 8) -> str:\n",
    "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
    "\n",
    "\n",
    "UUID = generate_uuid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcp_authenticate"
   },
   "source": [
    "### 认证您的Google Cloud账户\n",
    "\n",
    "**如果您正在使用Vertex AI Workbench笔记本**，您的环境已经经过了身份验证。\n",
    "\n",
    "**如果您正在使用Colab**，运行下方的单元格，并按提示进行操作通过oAuth进行账户认证。\n",
    "\n",
    "**否则**，请按照以下步骤进行：\n",
    "\n",
    "在Cloud控制台上，进入[创建服务账户密钥](https://console.cloud.google.com/apis/credentials/serviceaccountkey)页面。\n",
    "\n",
    "**点击创建服务账户**。\n",
    "\n",
    "在**服务账户名称**字段中输入名称，然后点击**创建**。\n",
    "\n",
    "在**授予该服务账户对项目的访问权限**部分，点击角色下拉列表。在过滤框中输入\"Vertex\"，选择**Vertex管理员**。在过滤框中输入\"Storage Object Admin\"，选择**存储对象管理员**。\n",
    "\n",
    "点击创建。一个包含您密钥的JSON文件会下载到您的本地环境中。\n",
    "\n",
    "将您的服务账户密钥路径输入到下方的单元格作为GOOGLE_APPLICATION_CREDENTIALS变量，并运行该单元格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gcp_authenticate"
   },
   "outputs": [],
   "source": [
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# If on Vertex AI Workbench, then don't execute this code\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\") and not os.getenv(\n",
    "    \"DL_ANACONDA_HOME\"\n",
    "):\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\"):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bucket:mbsdk"
   },
   "source": [
    "### 创建一个云存储桶\n",
    "\n",
    "**无论您使用的笔记本环境是什么，都需要执行以下步骤。**\n",
    "\n",
    "当您初始化用于 Python 的 Vertex AI SDK 时，您需要指定一个云存储暂存桶。暂存桶是您的数据集和模型资源相关的所有数据在会话之间保留的地方。\n",
    "\n",
    "在下面设置您的云存储桶的名称。存储桶的名称必须在所有 Google Cloud 项目中全局唯一，包括您组织之外的项目。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bucket"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"[your-bucket-name]\"  # @param {type:\"string\"}\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "autoset_bucket"
   },
   "outputs": [],
   "source": [
    "if BUCKET_URI == \"\" or BUCKET_URI is None or BUCKET_URI == \"gs://[your-bucket-name]\":\n",
    "    BUCKET_NAME = PROJECT_ID + \"aip-\" + UUID\n",
    "    BUCKET_URI = \"gs://\" + BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_bucket"
   },
   "source": [
    "只有当您的存储桶尚不存在时：运行以下单元格以创建您的Cloud Storage存储桶。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_bucket"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "validate_bucket"
   },
   "source": [
    "最后，通过检查 Cloud Storage 存储桶的内容来验证对其的访问权限。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "validate_bucket"
   },
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_vars"
   },
   "source": [
    "### 设置变量\n",
    "\n",
    "接下来，设置一些在教程中使用的变量。\n",
    "### 导入库并定义常量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aiplatform\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk"
   },
   "source": [
    "初始化Python的Vertex AI SDK\n",
    "\n",
    "为您的项目和相应的存储桶初始化Python的Vertex AI SDK。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_bq"
   },
   "source": [
    "### 创建 BigQuery 客户端\n",
    "\n",
    "在本教程中，您将使用与预训练模型训练时使用的相同公共 BigQuery 表中的数据。您将创建一个客户端接口，然后用它来访问数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_bq"
   },
   "outputs": [],
   "source": [
    "bqclient = bigquery.Client(project=PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tutorial_start:automl"
   },
   "source": [
    "现在您准备好开始创建自己的AutoML表格二分类模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_file:u_dataset,csv"
   },
   "source": [
    "#### 云存储训练数据的位置。\n",
    "\n",
    "现在将变量`IMPORT_FILE`设置为云存储中CSV索引文件的位置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_file:bank,csv,lbn"
   },
   "outputs": [],
   "source": [
    "IMPORT_FILE = \"gs://cloud-ml-tables-data/bank-marketing.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "quick_peek:tabular"
   },
   "source": [
    "快速查看您的数据\n",
    "\n",
    "本教程使用存储在公共云存储桶中的 Bank Marketing 数据集的一个版本，使用 CSV 索引文件。\n",
    "\n",
    "首先快速查看数据。您可以通过统计 CSV 索引文件中的行数（`wc -l`）来计算示例的数量，然后查看前几行。\n",
    "\n",
    "您还需要知道训练目的，标签列的标题名称，这个名称保存为`label_column`。对于这个数据集，它是 CSV 文件中的最后一列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quick_peek:tabular"
   },
   "outputs": [],
   "source": [
    "count = ! gsutil cat $IMPORT_FILE | wc -l\n",
    "print(\"Number of Examples\", int(count[0]))\n",
    "\n",
    "print(\"First 10 rows\")\n",
    "! gsutil cat $IMPORT_FILE | head\n",
    "\n",
    "heading = ! gsutil cat $IMPORT_FILE | head -n1\n",
    "label_column = str(heading).split(\",\")[-1].split(\"'\")[0]\n",
    "print(\"Label Column Name\", label_column)\n",
    "if label_column is None:\n",
    "    raise Exception(\"label column missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_dataset:tabular,lbn"
   },
   "source": [
    "### 创建数据集\n",
    "\n",
    "接下来，使用`TabularDataset`类的`create`方法创建`Dataset`资源，需要传入以下参数：\n",
    "\n",
    "- `display_name`：`Dataset`资源的可读名称。\n",
    "- `gcs_source`：要将数据项导入`Dataset`资源的一个或多个数据集索引文件列表。\n",
    "- `bq_source`：或者，从BigQuery表中导入数据项到`Dataset`资源中。\n",
    "\n",
    "该操作可能需要几分钟的时间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_dataset:tabular,lbn"
   },
   "outputs": [],
   "source": [
    "dataset = aiplatform.TabularDataset.create(\n",
    "    display_name=\"Bank Marketing\" + \"_\" + UUID, gcs_source=[IMPORT_FILE]\n",
    ")\n",
    "\n",
    "print(dataset.resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_automl_pipeline:tabular,lbn"
   },
   "source": [
    "### 创建和运行训练流程\n",
    "\n",
    "要训练一个AutoML模型，您需要执行两个步骤：1）创建一个训练流程，2）运行该流程。\n",
    "\n",
    "#### 创建训练流程\n",
    "\n",
    "使用`AutoMLTabularTrainingJob`类创建一个AutoML训练流程，具有以下参数：\n",
    "\n",
    "- `display_name`：`TrainingJob`资源的可读名称。\n",
    "- `optimization_prediction_type`：要为模型训练的任务类型。\n",
    "  - `classification`：一个表格分类模型。\n",
    "  - `regression`：一个表格回归模型。\n",
    "- `column_transformations`：（可选）要应用于输入列的转换。\n",
    "- `optimization_objective`：要最小化或最大化的优化目标。\n",
    "  - 二元分类：\n",
    "    - `minimize-log-loss`\n",
    "    - `maximize-au-roc`\n",
    "    - `maximize-au-prc`\n",
    "    - `maximize-precision-at-recall`\n",
    "    - `maximize-recall-at-precision`\n",
    "  - 多类别分类：\n",
    "    - `minimize-log-loss`\n",
    "  - 回归：\n",
    "    - `minimize-rmse`\n",
    "    - `minimize-mae`\n",
    "    - `minimize-rmsle`\n",
    "\n",
    "实例化的对象是用于训练流程的有向无环图（DAG）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_automl_pipeline:tabular,lbn"
   },
   "outputs": [],
   "source": [
    "dag = aiplatform.AutoMLTabularTrainingJob(\n",
    "    display_name=\"bank_\" + UUID,\n",
    "    optimization_prediction_type=\"classification\",\n",
    "    optimization_objective=\"minimize-log-loss\",\n",
    ")\n",
    "\n",
    "print(dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run_automl_pipeline:tabular"
   },
   "source": [
    "#### 运行训练管道\n",
    "\n",
    "接下来，通过调用`run`方法并传入以下参数来运行DAG以启动训练作业：\n",
    "\n",
    "- `dataset`：要训练模型的`Dataset`资源。\n",
    "- `model_display_name`：训练模型的可读名称。\n",
    "- `training_fraction_split`：用于训练的数据集百分比。\n",
    "- `test_fraction_split`：用于测试（留出数据）的数据集百分比。\n",
    "- `validation_fraction_split`：用于验证的数据集百分比。\n",
    "- `target_column`：要作为标签进行训练的列名。\n",
    "- `budget_milli_node_hours`：（可选）以毫小时为单位指定的最大训练时间（1000 = 小时）。\n",
    "- `disable_early_stopping`：如果为`True`，则服务可能会在达到整个预算之前完成训练，如果认为无法进一步改善模型目标测量。\n",
    "\n",
    "完成`run`方法后会返回`Model`资源。\n",
    "\n",
    "训练管道的执行可能需要长达 8 小时。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_automl_pipeline:tabular"
   },
   "outputs": [],
   "source": [
    "model = dag.run(\n",
    "    dataset=dataset,\n",
    "    model_display_name=\"bank_\" + UUID,\n",
    "    training_fraction_split=0.6,\n",
    "    validation_fraction_split=0.2,\n",
    "    test_fraction_split=0.2,\n",
    "    budget_milli_node_hours=8000,\n",
    "    disable_early_stopping=False,\n",
    "    target_column=label_column,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11e16f54bc90"
   },
   "source": [
    "## 批量预测介绍\n",
    "\n",
    "批量预测提供了对大量预测请求进行离线批处理的能力。资源只在批处理过程中被分配，然后在批请求完成时取消分配。结果存储在云存储中，与在线预测不同，在在线预测中，结果将作为HTTP响应数据包返回。\n",
    "\n",
    "批处理作业的输入格式取决于您的模型服务器支持的格式。首要的是，您的模型服务器中的Web服务器必须支持JSONL格式，该Web服务器将其转换为模型输入接口或服务函数接口直接支持的格式。对于批处理预测，该JSONL格式被称为“枢纽”格式。\n",
    "\n",
    "### 批量预测作业的输入格式\n",
    "\n",
    "批处理服务器接受以下输入格式的AutoML表格模型：\n",
    "\n",
    "- JSONL\n",
    "- CSV\n",
    "- BigQuery 表\n",
    "\n",
    "### 批量预测作业的输出格式\n",
    "\n",
    "批处理服务器接受以下AutoML表格模型的输出格式：\n",
    "\n",
    "- JSONL\n",
    "- CSV\n",
    "- BigQuery 表\n",
    "\n",
    "### 枢纽格式\n",
    "\n",
    "批处理服务器将输入格式转换为“枢纽”（JSONL）格式，如下所示：\n",
    "\n",
    "**JSONL**\n",
    "\n",
    "每个输入行（请求）应包含一个且仅一个有效的json值。\n",
    "\n",
    "    {\"values\": [1, 2, 3, 4], \"key\": 1}\n",
    "    {\"values\": [5, 6, 7, 8], \"key\": 2}\n",
    "\n",
    "批处理服务器使用相同格式生成枢纽数据。生成的枢纽数据随后被包装到一个载荷请求中：\n",
    "\n",
    "    {\"instances\": [\n",
    "      {\"values\": [1, 2, 3, 4], \"key\": 1},\n",
    "      {\"values\": [5, 6, 7, 8], \"key\": 2}\n",
    "    ]}\n",
    "\n",
    "**CSV**\n",
    "\n",
    "第一行中的CSV标题总是被忽略。字符串字段需要显式地用双引号括起来，否则该行将被丢弃，并且会输出解析错误信息到错误文件。非引号的值总是被传输为浮点数。\n",
    "\n",
    "    col1,col2,col3\n",
    "    1,3,\"cat1\"\n",
    "    2,4,\"cat2\"\n",
    "\n",
    "批处理服务器将每个输入行（请求）转换为JSON数组。\n",
    "\n",
    "    {\"instances\": [\n",
    "     [1.0,3.0,\"cat1\"],\n",
    "     [2.0,4.0,\"cat2\"]\n",
    "    ]}\n",
    "    \n",
    "**BigQuery**\n",
    "\n",
    "每行都转换为JSON数组。例如：\n",
    "\n",
    "    [1.0,3.0,\"cat1\"]\n",
    "    [2.0,4.0,\"cat2\"]\n",
    "    \n",
    "批处理服务器使用相同格式生成枢纽数据。生成的枢纽数据随后被包装到一个载荷请求中：\n",
    "\n",
    "    {\"instances\": [\n",
    "     [1.0,3.0,\"cat1\"],\n",
    "     [2.0,4.0,\"cat2\"]\n",
    "    ]}\n",
    "\n",
    "**TFRecords**\n",
    "\n",
    "TFRecord文件中的实例由apache_beam.io.tfrecordio模块以二进制形式读取。然后将二进制对象序列化为ASCII字符串。预测服务器负责知道解码器以还原实例。\n",
    "\n",
    "    {\"instances\": [\n",
    "     {\"b64\",\"b64编码的ASCII字符串\"},\n",
    "     {\"b64\",\"b64编码的ASCII字符串\"}\n",
    "    ]}\n",
    "\n",
    "**文件列表**\n",
    "\n",
    "FileList格式包含文件列表。在“FileList”文件中，每一行指定一个文件路径，指定为云存储位置。\n",
    "\n",
    "    gs://my-bucket/file1.txt\n",
    "    gs://my-bucket/file2.txt\n",
    "\n",
    "批处理服务器将文件读取为二进制文件。二进制对象被序列化为ASCII字符串。\n",
    "\n",
    "    {\"instances\": [\n",
    "     {\"b64\",\"b64编码的ASCII字符串\"},\n",
    "     {\"b64\",\"b64编码的ASCII字符串\"}\n",
    "    ]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "make_batch_file:automl,tabular,alt"
   },
   "source": [
    "### 使用CSV输入制作批量输入文件\n",
    "\n",
    "现在创建一个批量输入文件，并将其存储在本地的云存储桶中。在这个例子中，您可以使用CSV格式来进行输入和输出。对于输入文件，CSV文件的布局如下：\n",
    "\n",
    "- 第一行是带有特征（字段）名称的标题。\n",
    "- 每个剩余的行都是一个独立的预测请求，具有相应的特征值。\n",
    "\n",
    "例如：\n",
    "\n",
    "    \"feature_1\", \"feature_2\". ...\n",
    "    value_1, value_2, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "make_test_items:automl,batch_prediction"
   },
   "source": [
    "制作测试项目\n",
    "\n",
    "您可以使用合成数据作为测试数据项目。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "make_batch_file:automl,tabular,alt"
   },
   "outputs": [],
   "source": [
    "! gsutil cat $IMPORT_FILE | head -n 1 > tmp.csv\n",
    "! gsutil cat $IMPORT_FILE | tail -n 10 >> tmp.csv\n",
    "\n",
    "! cut -d, -f1-16 tmp.csv > batch.csv\n",
    "\n",
    "gcs_input_uri = BUCKET_URI + \"/test.csv\"\n",
    "\n",
    "! gsutil cp batch.csv $gcs_input_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "batch_request:mbsdk,both_csv"
   },
   "source": [
    "### 发出批量预测请求\n",
    "\n",
    "现在您的模型资源已经训练好了，您可以通过调用batch_predict()方法来进行批量预测，以下是参数：\n",
    "\n",
    "- `job_display_name`：批量预测作业的人类可读名称。\n",
    "- `gcs_source`：一个或多个批请求输入文件的列表。\n",
    "- `gcs_destination_prefix`：用于存储批量预测结果的Cloud Storage位置。\n",
    "- `instances_format`：输入实例的格式，可以是'csv'或'jsonl'。默认为'jsonl'。\n",
    "- `predictions_format`：输出预测的格式，可以是'csv'或'jsonl'。默认为'jsonl'。\n",
    "- `sync`：如果设置为True，则调用在等待异步批处理作业完成时会阻塞。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "batch_request:mbsdk,both_csv"
   },
   "outputs": [],
   "source": [
    "batch_predict_job = model.batch_predict(\n",
    "    job_display_name=\"bank_\" + UUID,\n",
    "    gcs_source=gcs_input_uri,\n",
    "    gcs_destination_prefix=BUCKET_URI,\n",
    "    instances_format=\"csv\",\n",
    "    predictions_format=\"csv\",\n",
    "    sync=False,\n",
    ")\n",
    "\n",
    "print(batch_predict_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "batch_request_wait:mbsdk"
   },
   "source": [
    "等待批量预测任务完成\n",
    "\n",
    "接下来，等待批量任务完成。或者，可以在`batch_predict()`方法中将参数`sync`设置为`True`，以阻塞直到批量预测任务完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "batch_request_wait:mbsdk"
   },
   "outputs": [],
   "source": [
    "batch_predict_job.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "get_batch_prediction:mbsdk,lbn"
   },
   "source": [
    "获取预测结果\n",
    "\n",
    "接下来，从已完成的批量预测作业中获取结果。\n",
    "\n",
    "结果将写入您在批量预测请求中指定的云存储输出存储桶。您可以调用iter_outputs()方法获取生成的结果的每个云存储文件的列表。每个文件都以CSV格式包含一个或多个预测请求：\n",
    "\n",
    "- CSV标题 + 预测标签\n",
    "- CSV行 + 预测，每个预测请求一个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "get_batch_prediction:mbsdk,lbn"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "bp_iter_outputs = batch_predict_job.iter_outputs()\n",
    "\n",
    "prediction_results = list()\n",
    "for blob in bp_iter_outputs:\n",
    "    if blob.name.split(\"/\")[-1].startswith(\"prediction\"):\n",
    "        prediction_results.append(blob.name)\n",
    "\n",
    "tags = list()\n",
    "for prediction_result in prediction_results:\n",
    "    gfile_name = f\"gs://{bp_iter_outputs.bucket.name}/{prediction_result}\"\n",
    "    with tf.io.gfile.GFile(name=gfile_name, mode=\"r\") as gfile:\n",
    "        for line in gfile.readlines():\n",
    "            print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12ed21da6f1f"
   },
   "source": [
    "#### 删除批量预测作业\n",
    "\n",
    "您可以使用`delete（）`方法删除您的`Vertex AI批量预测`作业。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "286a90d9b6e7"
   },
   "outputs": [],
   "source": [
    "batch_predict_job.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "make_batch_file:automl,tabular,alt"
   },
   "source": [
    "使用JSONL输入制作批量输入文件\n",
    "\n",
    "现在制作一个批量输入文件，将其存储在您的本地云存储桶中。在这个例子中，您可以使用JSONL格式作为输入和输出。对于输入，JSONL文件的布局如下：\n",
    "\n",
    "{\"feature1\": \"value1\", \"feature2\": \"value2\", ..., \"featureN\": \"valueN\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "make_test_items:automl,batch_prediction"
   },
   "source": [
    "制作测试项目\n",
    "\n",
    "您使用合成数据作为测试数据项目。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0af33545ea14"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "INSTANCE_1 = {\n",
    "    \"Age\": \"58\",\n",
    "    \"Job\": \"management\",\n",
    "    \"MaritalStatus\": \"married\",\n",
    "    \"Education\": \"tertiary\",\n",
    "    \"Default\": \"no\",\n",
    "    \"Balance\": \"2143\",\n",
    "    \"Housing\": \"yes\",\n",
    "    \"Loan\": \"no\",\n",
    "    \"Contact\": \"unknown\",\n",
    "    \"Day\": \"5\",\n",
    "    \"Month\": \"may\",\n",
    "    \"Duration\": \"261\",\n",
    "    \"Campaign\": \"1\",\n",
    "    \"PDays\": \"-1\",\n",
    "    \"Previous\": \"0\",\n",
    "    \"POutcome\": \"unknown\",\n",
    "}\n",
    "\n",
    "gcs_input_uri = BUCKET_URI + \"/test.jsonl\"\n",
    "with tf.io.gfile.GFile(gcs_input_uri, \"w\") as f:\n",
    "    f.write(json.dumps(INSTANCE_1) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "batch_request:mbsdk,both_csv"
   },
   "source": [
    "### 发起批量预测请求\n",
    "\n",
    "现在您的模型资源已经训练完成，您可以通过调用 batch_predict() 方法进行批量预测，使用以下参数：\n",
    "\n",
    "- 'job_display_name'：批量预测作业的可读名称。\n",
    "- 'gcs_source'：一个或多个批量请求输入文件的列表。\n",
    "- 'gcs_destination_prefix'：用于存储批量预测结果的云存储位置。\n",
    "- 'instances_format'：输入实例的格式，可以是'csv'或'jsonl'。默认为'jsonl'。\n",
    "- 'predictions_format'：输出预测的格式，可以是'csv'或'jsonl'。默认为'jsonl'。\n",
    "- 'sync'：如果设置为True，则调用将在等待异步批量作业完成时阻塞。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "batch_request:mbsdk,both_csv"
   },
   "outputs": [],
   "source": [
    "batch_predict_job = model.batch_predict(\n",
    "    job_display_name=\"bank_\" + UUID,\n",
    "    gcs_source=gcs_input_uri,\n",
    "    gcs_destination_prefix=BUCKET_URI,\n",
    "    instances_format=\"jsonl\",\n",
    "    predictions_format=\"jsonl\",\n",
    "    sync=False,\n",
    ")\n",
    "\n",
    "print(batch_predict_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "batch_request_wait:mbsdk"
   },
   "source": [
    "### 等待批量预测作业完成\n",
    "\n",
    "接下来，等待批处理作业完成。或者，可以在`batch_predict()`方法中将参数`sync`设置为`True`，以阻塞直到批量预测作业完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "batch_request_wait:mbsdk"
   },
   "outputs": [],
   "source": [
    "batch_predict_job.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "get_batch_prediction:mbsdk,lbn"
   },
   "source": [
    "### 获取预测结果\n",
    "\n",
    "接下来，从完成的批量预测作业中获取结果。\n",
    "\n",
    "结果将写入您在批量预测请求中指定的Cloud Storage输出存储桶。您可以调用iter_outputs()方法获取每个用于存储结果的Cloud Storage文件的列表。每个文件以JSONL格式包含一个或多个预测请求：\n",
    "\n",
    "    {\"instance\": {\"feature1\": \"value1\", ..., \"featureN\": \"valueN\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "get_batch_prediction:mbsdk,lbn"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "bp_iter_outputs = batch_predict_job.iter_outputs()\n",
    "\n",
    "prediction_results = list()\n",
    "for blob in bp_iter_outputs:\n",
    "    if blob.name.split(\"/\")[-1].startswith(\"prediction\"):\n",
    "        prediction_results.append(blob.name)\n",
    "\n",
    "tags = list()\n",
    "for prediction_result in prediction_results:\n",
    "    gfile_name = f\"gs://{bp_iter_outputs.bucket.name}/{prediction_result}\"\n",
    "    with tf.io.gfile.GFile(name=gfile_name, mode=\"r\") as gfile:\n",
    "        for line in gfile.readlines():\n",
    "            print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12ed21da6f1f"
   },
   "source": [
    "#### 删除批量预测作业\n",
    "\n",
    "您可以使用`delete（）`方法删除您的`Vertex AI Batch Prediction`作业。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "286a90d9b6e7"
   },
   "outputs": [],
   "source": [
    "batch_predict_job.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "batch_explain_request:mbsdk,csv"
   },
   "source": [
    "### 发起批处理解释请求\n",
    "\n",
    "现在您的模型资源已经训练完成，您可以通过调用`batch_predict()`方法进行批处理预测，带有以下参数：\n",
    "\n",
    "- `job_display_name`：批处理预测作业的可读名称。\n",
    "- `gcs_source`：一个或多个批处理请求输入文件的列表。\n",
    "- `gcs_destination_prefix`：用于存储批处理预测结果的云存储位置。\n",
    "- `instances_format`：输入实例的格式，可以选择'csv'或'jsonl'。默认值为'jsonl'。\n",
    "- `predictions_format`：输出预测结果的格式，可以选择'csv'或'jsonl'。默认值为'jsonl'。\n",
    "- `generate_explanations`：设置为`True`以生成解释。\n",
    "- `sync`：如果设置为True，则调用将在等待异步批处理作业完成时阻塞。\n",
    "\n",
    "对于解释部分，仅支持'jsonl'和'bigquery'输出格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "batch_request:mbsdk,both_csv"
   },
   "outputs": [],
   "source": [
    "batch_predict_job = model.batch_predict(\n",
    "    job_display_name=\"bank_\" + UUID,\n",
    "    gcs_source=gcs_input_uri,\n",
    "    gcs_destination_prefix=BUCKET_URI,\n",
    "    instances_format=\"jsonl\",\n",
    "    predictions_format=\"jsonl\",\n",
    "    generate_explanation=True,\n",
    "    sync=False,\n",
    ")\n",
    "\n",
    "print(batch_predict_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "batch_request_wait:mbsdk"
   },
   "source": [
    "### 等待批量预测任务完成\n",
    "\n",
    "接下来，等待批量任务完成。或者，可以在`batch_predict()`方法中将参数`sync`设置为`True`，以阻塞直到批量预测任务完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "batch_request_wait:mbsdk"
   },
   "outputs": [],
   "source": [
    "batch_predict_job.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "get_batch_explanation:mbsdk,lbn"
   },
   "source": [
    "### 获取解释\n",
    "\n",
    "接下来，从完成的批量预测作业中获取解释结果。\n",
    "\n",
    "结果被写入到您在批量预测请求中指定的Cloud Storage输出桶中。您调用iter_outputs()方法来获取包含结果的每个Cloud Storage文件的列表。每个文件以JSONL格式包含一个或多个解释请求：\n",
    "\n",
    "- instance: 用于预测的特征值。\n",
    "- prediction: 预测值。\n",
    "- explanation: 每个特征值的归因。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "get_batch_explanation:mbsdk,lbn"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "bp_iter_outputs = batch_predict_job.iter_outputs()\n",
    "\n",
    "explanation_results = list()\n",
    "for blob in bp_iter_outputs:\n",
    "    if blob.name.split(\"/\")[-1].startswith(\"explanation\"):\n",
    "        explanation_results.append(blob.name)\n",
    "\n",
    "tags = list()\n",
    "for explanation_result in explanation_results:\n",
    "    gfile_name = f\"gs://{bp_iter_outputs.bucket.name}/{explanation_result}\"\n",
    "    with tf.io.gfile.GFile(name=gfile_name, mode=\"r\") as gfile:\n",
    "        for line in gfile.readlines():\n",
    "            print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "make_batch_file:automl,tabular,alt"
   },
   "source": [
    "使用JSONL输入（列表）创建批处理输入文件\n",
    "\n",
    "或者，您可以将JSONL格式指定为列表（与对象相对）。在这种情况下，值必须按照为训练模型指定的相同列表顺序排列。\n",
    "\n",
    "JSONL文件的布局如下：\n",
    "\n",
    "[\"值1\", \"值2\", ..., \"值N\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "make_test_items:automl,batch_prediction"
   },
   "source": [
    "制作测试项\n",
    "\n",
    "您可以使用合成数据作为测试数据项。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "25064e9dcc47"
   },
   "outputs": [],
   "source": [
    "INSTANCE_1 = [\n",
    "    58,\n",
    "    \"management\",\n",
    "    \"married\",\n",
    "    \"tertiary\",\n",
    "    \"no\",\n",
    "    2143,\n",
    "    \"yes\",\n",
    "    \"no\",\n",
    "    \"unknown\",\n",
    "    5,\n",
    "    \"may\",\n",
    "    261,\n",
    "    1,\n",
    "    -1,\n",
    "    0,\n",
    "    \"unknown\",\n",
    "]\n",
    "\n",
    "gcs_input_uri = BUCKET_URI + \"/test.jsonl\"\n",
    "with tf.io.gfile.GFile(gcs_input_uri, \"w\") as f:\n",
    "    f.write(json.dumps(INSTANCE_1) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "batch_request:mbsdk,both_csv"
   },
   "source": [
    "### 进行批量预测请求\n",
    "\n",
    "现在您的模型资源已经训练完成，您可以通过调用 batch_predict() 方法来进行批量预测，使用以下参数：\n",
    "\n",
    "- `job_display_name`：批量预测作业的人类可读名称。\n",
    "- `gcs_source`：一个或多个批量请求输入文件的列表。\n",
    "- `gcs_destination_prefix`：用于存储批量预测结果的云存储位置。\n",
    "- `instances_format`：输入实例的格式，可以是'csv'或'jsonl'。默认为'jsonl'。\n",
    "- `predictions_format`：输出预测的格式，可以是'csv'或'jsonl'。默认为'jsonl'。\n",
    "- `sync`：如果设置为True，则调用将在等待异步批处理作业完成时阻塞。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "batch_request:mbsdk,both_csv"
   },
   "outputs": [],
   "source": [
    "batch_predict_job = model.batch_predict(\n",
    "    job_display_name=\"bank_\" + UUID,\n",
    "    gcs_source=gcs_input_uri,\n",
    "    gcs_destination_prefix=BUCKET_URI,\n",
    "    instances_format=\"jsonl\",\n",
    "    predictions_format=\"jsonl\",\n",
    "    sync=False,\n",
    ")\n",
    "\n",
    "print(batch_predict_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "batch_request_wait:mbsdk"
   },
   "source": [
    "### 等待批处理预测作业完成\n",
    "\n",
    "接下来，等待批处理作业完成。或者，可以在`batch_predict()`方法中将参数`sync`设置为`True`，以阻塞直到批处理预测作业完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "batch_request_wait:mbsdk"
   },
   "outputs": [],
   "source": [
    "batch_predict_job.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "get_batch_prediction:mbsdk,lbn"
   },
   "source": [
    "获取预测结果\n",
    "\n",
    "接下来，从已完成的批量预测任务中获取结果。\n",
    "\n",
    "结果将被写入您在批量预测请求中指定的云存储输出存储桶中。您可以调用 iter_outputs() 方法获得包含结果的每个云存储文件的列表。每个文件以 JSONL 格式包含一个或多个预测请求：\n",
    "\n",
    "    {\"instance\": {\"feature1\": \"value1\", ..., \"featureN\": \"valueN\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "get_batch_prediction:mbsdk,lbn"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "bp_iter_outputs = batch_predict_job.iter_outputs()\n",
    "\n",
    "prediction_results = list()\n",
    "for blob in bp_iter_outputs:\n",
    "    if blob.name.split(\"/\")[-1].startswith(\"prediction\"):\n",
    "        prediction_results.append(blob.name)\n",
    "\n",
    "tags = list()\n",
    "for prediction_result in prediction_results:\n",
    "    gfile_name = f\"gs://{bp_iter_outputs.bucket.name}/{prediction_result}\"\n",
    "    with tf.io.gfile.GFile(name=gfile_name, mode=\"r\") as gfile:\n",
    "        for line in gfile.readlines():\n",
    "            print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12ed21da6f1f"
   },
   "source": [
    "#### 删除批量预测作业\n",
    "\n",
    "您可以使用`delete()`方法删除`Vertex AI Batch Prediction`作业。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "286a90d9b6e7"
   },
   "outputs": [],
   "source": [
    "batch_predict_job.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0401e697ffa7"
   },
   "source": [
    "使用BigQuery输入格式执行批量预测\n",
    "\n",
    "接下来，您将执行相同的批处理作业，只不过输入格式为BigQuery表。当输入格式为BigQuery表时，输出格式也必须为BigQuery表。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "csv_to_bq"
   },
   "source": [
    "### 从CSV文件创建一个BigQuery数据集\n",
    "\n",
    "您可以使用BigQuery的`create_dataset()`和`load_table_from_uri()`方法从CSV文件创建一个BigQuery数据集，步骤如下：\n",
    "\n",
    "- `create_dataset()`: 创建一个空的BigQuery数据集，具有以下参数：\n",
    "  - `dataset_ref`: 从数据集ID创建的`DatasetReference`，例如samples。\n",
    "- `load_table_from_uri()`: 将一个或多个CSV文件加载到相应数据集中的表中，具有以下参数：\n",
    "  - `url`: 存储在云存储中的一个或多个CVS文件的集合。\n",
    "  - `table`: 表的`TableReference`。\n",
    "  - `job_config`: 加载CSV数据的规格。\n",
    "\n",
    "了解更多关于[将CSV数据导入到BigQuery](https://www.tensorflow.org/io/tutorials/bigquery#import_census_data_into_bigquery)的信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a61e4f0c0f8f"
   },
   "outputs": [],
   "source": [
    "# Create test data w/o the label\n",
    "output = ! gsutil cat $IMPORT_FILE | head -n 10\n",
    "rows = []\n",
    "for i in range(0, 10):\n",
    "    rows.append(output[i][:-2])\n",
    "\n",
    "with tf.io.gfile.GFile(BUCKET_URI + \"/test.csv\", \"w\") as f:\n",
    "    for row in rows:\n",
    "        f.write(row + \"\\n\")\n",
    "\n",
    "LOCATION = \"us\"\n",
    "\n",
    "CSV_SCHEMA = [\n",
    "    bigquery.SchemaField(\"Age\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"Job\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"MaritalStatus\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"Education\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"Default\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"Balance\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"Housing\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"Loan\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"Contact\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"Day\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"Month\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"Duration\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"Campaign\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"PDays\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"Previous\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"POutcome\", \"STRING\"),\n",
    "]\n",
    "\n",
    "DATASET_ID = \"batch\"\n",
    "TABLE_ID = \"slice1\"\n",
    "\n",
    "\n",
    "def create_bigquery_dataset(dataset_id):\n",
    "    dataset = bigquery.Dataset(\n",
    "        bigquery.dataset.DatasetReference(PROJECT_ID, dataset_id)\n",
    "    )\n",
    "    dataset.location = \"us\"\n",
    "\n",
    "    try:\n",
    "        dataset = bqclient.create_dataset(dataset)  # API request\n",
    "        return True\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "        if err.code != 409:  # http_client.CONFLICT\n",
    "            raise\n",
    "    return False\n",
    "\n",
    "\n",
    "def load_data_into_bigquery(url, dataset_id, table_id):\n",
    "    create_bigquery_dataset(dataset_id)\n",
    "    dataset = bqclient.dataset(dataset_id)\n",
    "    table = dataset.table(table_id)\n",
    "\n",
    "    job_config = bigquery.LoadJobConfig()\n",
    "    job_config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE\n",
    "    job_config.source_format = bigquery.SourceFormat.CSV\n",
    "    job_config.schema = CSV_SCHEMA\n",
    "    job_config.skip_leading_rows = 1  # heading\n",
    "\n",
    "    load_job = bqclient.load_table_from_uri(url, table, job_config=job_config)\n",
    "    print(\"Starting job {}\".format(load_job.job_id))\n",
    "\n",
    "    load_job.result()  # Waits for table load to complete.\n",
    "    print(\"Job finished.\")\n",
    "\n",
    "    destination_table = bqclient.get_table(table)\n",
    "    print(\"Loaded {} rows.\".format(destination_table.num_rows))\n",
    "\n",
    "    return destination_table\n",
    "\n",
    "\n",
    "bq_table = load_data_into_bigquery(BUCKET_URI + \"/test.csv\", DATASET_ID, TABLE_ID)\n",
    "print(bq_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "send_prediction_request:image"
   },
   "source": [
    "发送批量预测请求\n",
    "\n",
    "再次，使用`batch_predict()`方法发送批量预测请求，但是参数有以下更改：\n",
    "\n",
    "- `instances_format`：设置为'bigquery'\n",
    "- `predictions_format`：设置为'bigquery'\n",
    "- 使用`bigquery_source`代替`gcs_source`。\n",
    "- 使用`bigquery_destination_prefix`代替`gcs_destination_prefix`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1cf1076178fc"
   },
   "outputs": [],
   "source": [
    "# The name of the job\n",
    "BATCH_PREDICTION_JOB_NAME = \"bank-\" + UUID\n",
    "\n",
    "# Folder in the bucket to write results to\n",
    "DESTINATION_FOLDER = \"batch_prediction_results_bq\"\n",
    "\n",
    "# The Cloud Storage bucket to upload results to\n",
    "BATCH_PREDICTION_GCS_DEST_PREFIX = BUCKET_URI + \"/\" + DESTINATION_FOLDER\n",
    "\n",
    "BQ_TABLE_SOURCE = f\"bq://{PROJECT_ID}.{bq_table.dataset_id}.{bq_table.table_id}\"\n",
    "\n",
    "# Make SDK batch_predict method call\n",
    "batch_prediction_job = model.batch_predict(\n",
    "    instances_format=\"bigquery\",\n",
    "    predictions_format=\"bigquery\",\n",
    "    job_display_name=BATCH_PREDICTION_JOB_NAME,\n",
    "    bigquery_source=BQ_TABLE_SOURCE,\n",
    "    bigquery_destination_prefix=f\"bq://{PROJECT_ID}.batch\",\n",
    "    model_parameters=None,\n",
    "    sync=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "batch_request_wait:mbsdk"
   },
   "source": [
    "### 等待批量预测任务完成\n",
    "\n",
    "接下来，等待批量任务完成。或者，可以在`batch_predict()`方法中将参数`sync`设置为`True`，以阻塞直到批量预测任务完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "batch_request_wait:mbsdk"
   },
   "outputs": [],
   "source": [
    "batch_predict_job.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "get_batch_prediction:mbsdk,custom,icn"
   },
   "source": [
    "### 获取预测结果\n",
    "\n",
    "接下来，从已完成的批处理预测作业中获取结果。\n",
    "\n",
    "结果将写入到一个 BigQuery 表中，该表位于您指定的 BigQuery 数据集路径下作为目的地。批处理服务器创建表，表位置由以下参数指定：\n",
    "\n",
    "`batch_prediction_job.output_info.bigquery_output_dataset`：项目和数据集组件。\n",
    "`batch_prediction_job.output_info.bigquery_output_table`：表组件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6833bef283b7"
   },
   "outputs": [],
   "source": [
    "BQ_RESULTS_TABLE = (\n",
    "    batch_prediction_job.output_info.bigquery_output_dataset\n",
    "    + \".\"\n",
    "    + batch_prediction_job.output_info.bigquery_output_table\n",
    ")\n",
    "\n",
    "print(BQ_RESULTS_TABLE)\n",
    "\n",
    "table = bigquery.TableReference.from_string(BQ_RESULTS_TABLE[5:])\n",
    "\n",
    "rows = bqclient.list_rows(table, max_results=10)\n",
    "\n",
    "for row in rows:\n",
    "    print(row)\n",
    "    for key, value in row.items():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12ed21da6f1f"
   },
   "source": [
    "您可以使用`delete()`方法删除您的`Vertex AI批量预测`作业。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "286a90d9b6e7"
   },
   "outputs": [],
   "source": [
    "batch_predict_job.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "992a2d57c0fc"
   },
   "source": [
    "删除模型\n",
    "\n",
    "接下来，您可以使用`delete()`方法删除`顶点AI模型`资源。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "27f9b3beec3f"
   },
   "outputs": [],
   "source": [
    "model.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "beebda94c217"
   },
   "source": [
    "删除数据集\n",
    "\n",
    "接下来，您可以使用`delete()`方法删除`Vertex AI数据集`资源。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9e028ffc4ada"
   },
   "outputs": [],
   "source": [
    "dataset.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleanup:mbsdk"
   },
   "source": [
    "清理\n",
    "\n",
    "要清理此项目中使用的所有Google Cloud资源，您可以删除用于教程的[Google Cloud 项目](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects)。\n",
    "\n",
    "否则，您可以删除在此教程中创建的各个资源。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cleanup:mbsdk"
   },
   "outputs": [],
   "source": [
    "delete_bucket = False\n",
    "\n",
    "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
    "    ! gsutil rm -rf {BUCKET_URI}"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "get_started_with_automl_tabular_model_batch.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
