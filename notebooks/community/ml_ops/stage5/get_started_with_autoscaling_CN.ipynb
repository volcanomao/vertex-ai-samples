{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "copyright"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title:generic,gcp"
   },
   "source": [
    "# GCP上的E2E ML: MLOps阶段5：部署：开始配置Vertex AI端点部署的自动缩放\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage5/get_started_with_autoscaling.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      在GitHub上查看\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "        <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage5/get_started_with_autoscaling.ipynb\">\n",
    "        <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> 在Colab中运行\n",
    "        </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/ml_ops/stage5/get_started_with_autoscaling.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      在Vertex AI Workbench中打开\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "<br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "overview:mlops"
   },
   "source": [
    "## 概述\n",
    "\n",
    "本教程演示了如何在Google Cloud上使用Vertex AI进行端到端的MLOps生产。本教程涵盖阶段5：部署：使用自动扩展进行部署的入门。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "objective:mlops,stage2,get_started_automl_training"
   },
   "source": [
    "### 目标\n",
    "\n",
    "在本教程中，您将学习如何在部署`Model`资源到`Endpoint`资源时使用微调控制自动缩放配置。\n",
    "\n",
    "本教程使用以下Google Cloud ML服务：\n",
    "\n",
    "- `Vertex ML Prediction`\n",
    "\n",
    "执行的步骤包括：\n",
    "\n",
    "- 从TensorFlow Hub下载一个预训练的图像分类模型。\n",
    "- 将预训练模型上传为`Model`资源。\n",
    "- 创建一个`Endpoint`资源。\n",
    "- 为不缩放（单节点）部署`Model`资源。\n",
    "- 为手动缩放部署`Model`资源。\n",
    "- 为自动缩放部署`Model`资源。\n",
    "- 微调CPU利用率的缩放阈值。\n",
    "- 微调GPU利用率的缩放阈值。\n",
    "- 将CPU和GPU模型实例的混合部署与自动缩放到一个`Endpoint`资源中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset:flowers,icn"
   },
   "source": [
    "数据集\n",
    "\n",
    "本教程使用的是来自TensorFlow Hub的一个预训练的图像分类模型，该模型是在ImageNet数据集上训练的。\n",
    "\n",
    "了解更多关于[ResNet V2预训练模型](https://tfhub.dev/google/imagenet/resnet_v2_101/classification/5)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fb3451ce8e47"
   },
   "source": [
    "### 费用\n",
    "本教程使用 Google Cloud 的可计费组件：\n",
    "\n",
    "- Vertex AI\n",
    "- 云存储\n",
    "\n",
    "了解 [Vertex AI 定价](https://cloud.google.com/vertex-ai/pricing) 和 [云存储定价](https://cloud.google.com/storage/pricing)，并使用 [定价计算器](https://cloud.google.com/products/calculator/) 根据您预计的使用量生成一个费用估算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_mlops"
   },
   "source": [
    "安装执行此笔记本所需的软件包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_mlops"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Vertex AI Workbench Notebook product has specific requirements\n",
    "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\")\n",
    "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
    "    \"/opt/deeplearning/metadata/env_version\"\n",
    ")\n",
    "\n",
    "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_WORKBENCH_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\"\n",
    "\n",
    "# Install the packages\n",
    "\n",
    "! pip3 install --upgrade google-cloud-aiplatform $USER_FLAG -q\n",
    "! pip3 install --upgrade google-cloud-storage $USER_FLAG -q\n",
    "! pip3 install tensorflow-hub $USER_FLAG -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "restart"
   },
   "source": [
    "### 重启内核\n",
    "\n",
    "安装了额外的包之后，您需要重启笔记本内核以便它能找到这些包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "restart"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "project_id"
   },
   "source": [
    "## 在开始之前\n",
    "\n",
    "### 设置您的 Google Cloud 项目\n",
    "\n",
    "**无论您使用什么笔记本环境，下面的步骤都是必须的。**\n",
    "\n",
    "1. [选择或创建一个 Google Cloud 项目](https://console.cloud.google.com/cloud-resource-manager)。当您第一次创建账户时，您会获得一个可用于支付计算/存储成本的300美元的免费信用额度。\n",
    "\n",
    "2. [确保您的项目已启用计费](https://cloud.google.com/billing/docs/how-to/modify-project)。\n",
    "\n",
    "3. [启用 Vertex AI、Compute Engine 和 Cloud Storage API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,compute_component,storage_component)。\n",
    "\n",
    "4. 如果您正在本地运行这个笔记本，您需要安装 [Cloud SDK](https://cloud.google.com/sdk)。\n",
    "\n",
    "5. 在下面的单元格中输入您的项目 ID。然后运行单元格，确保\n",
    "Cloud SDK 使用正确的项目来运行本笔记本中的所有命令。\n",
    "\n",
    "**注意**：Jupyter以`!`为前缀运行的行作为shell命令，并将以`$`为前缀的Python变量插入这些命令中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56d591439df1"
   },
   "source": [
    "设置您的项目ID\n",
    "\n",
    "**如果您不知道您的项目ID**，您可以使用`gcloud`来获取您的项目ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_project_id"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "autoset_project_id"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID:\", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_gcloud_project_id"
   },
   "outputs": [],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "区域\n",
    "\n",
    "您还可以更改“REGION”变量，该变量用于本笔记本的其余操作。下面是 Vertex AI 支持的区域。我们建议您选择离您最近的区域。\n",
    "\n",
    "- 美洲：`us-central1`\n",
    "- 欧洲：`europe-west4`\n",
    "- 亚太地区：`asia-east1`\n",
    "\n",
    "您不可以使用一个多区域存储桶来训练 Vertex AI。并非所有区域都支持所有 Vertex AI 服务。\n",
    "\n",
    "了解更多关于 [Vertex AI 区域](https://cloud.google.com/vertex-ai/docs/general/locations)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "region"
   },
   "outputs": [],
   "source": [
    "REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
    "\n",
    "if REGION == \"[your-region]\":\n",
    "    REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "timestamp"
   },
   "source": [
    "时间戳\n",
    "\n",
    "如果您正在参加现场教程会话，您可能正在使用共享测试帐户或项目。为了避免在创建的资源上出现名称冲突，您需要为每个实例会话创建一个时间戳，并将该时间戳附加到您在此教程中创建的资源名称上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "timestamp"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ffa6b6c7cdb"
   },
   "source": [
    "###验证您的谷歌云帐户\n",
    "\n",
    "**如果您使用Vertex AI Workbench Notebooks**，则您的环境已经通过身份验证。跳过此步骤。\n",
    "\n",
    "**如果您使用Colab**，请运行下面的单元格，并按提示通过oAuth验证您的帐户。\n",
    "\n",
    "**否则**，请按照以下步骤操作：\n",
    "\n",
    "在Cloud Console中，转到[创建服务帐户密钥](https://console.cloud.google.com/apis/credentials/serviceaccountkey)页面。\n",
    "\n",
    "1. **点击创建服务帐户**。\n",
    "\n",
    "2. 在**服务帐户名称**字段中输入一个名称，然后点击**创建**。\n",
    "\n",
    "3. 在**向该服务帐户授予项目访问权限**部分，点击角色下拉列表。在过滤框中键入\"Vertex AI\"，选择**Vertex AI管理员**。在过滤框中键入\"Storage Object Admin\"，选择**Storage Object Admin**。\n",
    "\n",
    "4. 点击创建。一个包含您密钥的JSON文件将下载到本地环境。\n",
    "\n",
    "5. 在下面的单元格中将您的服务帐户密钥路径输入为GOOGLE_APPLICATION_CREDENTIALS变量，并运行该单元格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2b72272258fc"
   },
   "outputs": [],
   "source": [
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# If on Vertex AI Workbench, then don't execute this code\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\") and not os.getenv(\n",
    "    \"DL_ANACONDA_HOME\"\n",
    "):\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\"):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bucket:mbsdk"
   },
   "source": [
    "### 创建一个云存储桶\n",
    "\n",
    "**无论你使用哪种笔记本环境，下面的步骤都是必需的。**\n",
    "\n",
    "在初始化用于 Python 的 Vertex SDK 时，您需要指定一个云存储暂存桶。暂存桶是您的数据集和模型资源在不同会话中保留的地方。\n",
    "\n",
    "请在下方设置您的云存储桶的名称。桶的名称必须在所有 Google Cloud 项目中全局唯一，包括您的组织之外的项目。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bucket"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"[your-bucket-name]\"  # @param {type:\"string\"}\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "autoset_bucket"
   },
   "outputs": [],
   "source": [
    "if BUCKET_URI == \"\" or BUCKET_URI is None or BUCKET_URI == \"gs://[your-bucket-name]\":\n",
    "    BUCKET_NAME = PROJECT_ID + \"aip-\" + TIMESTAMP\n",
    "    BUCKET_URI = \"gs://\" + BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_bucket"
   },
   "source": [
    "只有在您的存储桶尚未存在时才运行以下单元格以创建您的云存储存储桶。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_bucket"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "validate_bucket"
   },
   "source": [
    "最后，通过检查云存储桶的内容来验证访问权限。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "validate_bucket"
   },
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_vars"
   },
   "source": [
    "### 设置变量\n",
    "\n",
    "接下来，设置一些在教程中使用的变量。\n",
    "### 导入库和定义常量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aiplatform\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk"
   },
   "source": [
    "### 初始化Vertex AI SDK for Python\n",
    "\n",
    "为您的项目和相应的存储桶初始化Python版本的Vertex AI SDK。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "accelerators:training,cpu,prediction,cpu,mbsdk"
   },
   "source": [
    "设置硬件加速器\n",
    "\n",
    "您可以为训练和预测设置硬件加速器。\n",
    "\n",
    "将变量`DEPLOY_GPU/DEPLOY_NGPU`设置为使用支持GPU的容器映像以及分配给虚拟机实例（VM）的GPU数量。例如，要使用一个带有4个Nvidia Telsa K80 GPU分配给每个VM的GPU容器映像，您应该指定：\n",
    "\n",
    "    (aip.AcceleratorType.NVIDIA_TESLA_K80, 4)\n",
    "\n",
    "否则，指定`(None, None)`以使用一个在CPU上运行的容器映像。\n",
    "\n",
    "了解有关您所在地区的[硬件加速器支持](https://cloud.google.com/vertex-ai/docs/general/locations#accelerators)。\n",
    "\n",
    "了解有关[机器类型的GPU兼容性](https://cloud.google.com/vertex-ai/docs/training/configure-compute#gpu-compatibility-table)的更多信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "accelerators:training,cpu,prediction,cpu,mbsdk"
   },
   "outputs": [],
   "source": [
    "if os.getenv(\"IS_TESTING_DEPLOY_GPU\"):\n",
    "    DEPLOY_GPU, DEPLOY_NGPU = (\n",
    "        aiplatform.gapic.AcceleratorType.NVIDIA_TESLA_K80,\n",
    "        int(os.getenv(\"IS_TESTING_DEPLOY_GPU\")),\n",
    "    )\n",
    "else:\n",
    "    DEPLOY_GPU, DEPLOY_NGPU = (aiplatform.gapic.AcceleratorType.NVIDIA_TESLA_K80, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "container:training,prediction"
   },
   "source": [
    "设置预构建的容器\n",
    "\n",
    "为预测设置预构建的Docker容器镜像。\n",
    "\n",
    "有关最新列表，请参阅[用于预测的预构建容器](https://cloud.google.com/ai-platform-unified/docs/predictions/pre-built-containers)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "container:training,prediction"
   },
   "outputs": [],
   "source": [
    "if os.getenv(\"IS_TESTING_TF\"):\n",
    "    TF = os.getenv(\"IS_TESTING_TF\")\n",
    "else:\n",
    "    TF = \"2.5\".replace(\".\", \"-\")\n",
    "\n",
    "GPU_VERSION = \"tf2-gpu.{}\".format(TF)\n",
    "CPU_VERSION = \"tf2-cpu.{}\".format(TF)\n",
    "\n",
    "DEPLOY_IMAGE_GPU = \"{}-docker.pkg.dev/vertex-ai/prediction/{}:latest\".format(\n",
    "    REGION.split(\"-\")[0], GPU_VERSION\n",
    ")\n",
    "\n",
    "DEPLOY_IMAGE_CPU = \"{}-docker.pkg.dev/vertex-ai/prediction/{}:latest\".format(\n",
    "    REGION.split(\"-\")[0], CPU_VERSION\n",
    ")\n",
    "\n",
    "print(\"Deployment:\", DEPLOY_IMAGE_GPU, DEPLOY_IMAGE_CPU, DEPLOY_GPU, DEPLOY_NGPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "machine:training"
   },
   "source": [
    "#### 设置机器类型\n",
    "\n",
    "接下来，设置用于预测的机器类型。\n",
    "\n",
    "- 将变量`DEPLOY_COMPUTE`设置为配置将用于预测的VM的计算资源。\n",
    " - `机器类型`\n",
    "     - `n1-standard`：每个vCPU 3.75GB的内存。\n",
    "     - `n1-highmem`：每个vCPU 6.5GB的内存\n",
    "     - `n1-highcpu`：每个vCPU 0.9GB的内存\n",
    " - `vCPUs`：数量为\\[2, 4, 8, 16, 32, 64, 96\\]\n",
    "\n",
    "*注意：您也可以使用n2和e2机器类型进行训练和部署，但它们不支持GPU。*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "machine:training"
   },
   "outputs": [],
   "source": [
    "if os.getenv(\"IS_TESTING_DEPLOY_MACHINE\"):\n",
    "    MACHINE_TYPE = os.getenv(\"IS_TESTING_DEPLOY_MACHINE\")\n",
    "else:\n",
    "    MACHINE_TYPE = \"n1-standard\"\n",
    "\n",
    "VCPU = \"4\"\n",
    "DEPLOY_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
    "print(\"Train machine type\", DEPLOY_COMPUTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8128b8ff025"
   },
   "source": [
    "## 从TensorFlow Hub获取预训练模型\n",
    "\n",
    "为了演示目的，本教程使用了从TensorFlow Hub（TFHub）获取的预训练模型，然后将其上传到`Vertex AI Model`资源。一旦您拥有了`Vertex AI Model`资源，该模型就可以部署到`Vertex AI Endpoint`资源。\n",
    "\n",
    "### 下载预训练模型\n",
    "\n",
    "首先，您可以从TensorFlow Hub下载预训练模型。该模型将作为TF.Keras层进行下载。为了完成模型，在本示例中，您将使用下载的TFHub模型作为层创建一个`Sequential()`模型，并指定模型的输入形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c55fa4c826f7"
   },
   "outputs": [],
   "source": [
    "tfhub_model = tf.keras.Sequential(\n",
    "    [hub.KerasLayer(\"https://tfhub.dev/google/imagenet/resnet_v2_101/classification/5\")]\n",
    ")\n",
    "\n",
    "tfhub_model.build([None, 224, 224, 3])\n",
    "\n",
    "tfhub_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63de49055083"
   },
   "source": [
    "### 保存模型文件\n",
    "\n",
    "在这一步，模型已经在内存中。接下来，您可以将模型文件保存到云存储位置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "64618c713db9"
   },
   "outputs": [],
   "source": [
    "MODEL_DIR = BUCKET_URI + \"/model\"\n",
    "tfhub_model.save(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8ce91147c93"
   },
   "source": [
    "将TensorFlow Hub模型上传到`Vertex AI Model`资源\n",
    "\n",
    "最后，您将来自TFHub模型和服务函数的模型文件上传到`Vertex AI Model`资源中。\n",
    "\n",
    "*注意:* 当您将模型文件上传到`Vertex AI Model`资源时，需要指定相应的部署容器镜像。在本例中，您将使用仅支持CPU的部署容器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ad61e1429512"
   },
   "outputs": [],
   "source": [
    "model = aiplatform.Model.upload(\n",
    "    display_name=\"example_\" + TIMESTAMP,\n",
    "    artifact_uri=MODEL_DIR,\n",
    "    serving_container_image_uri=DEPLOY_IMAGE_CPU,\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "628de0914ba1"
   },
   "source": [
    "创建`Endpoint`资源\n",
    "\n",
    "您可以使用`Endpoint.create()`方法创建`Endpoint`资源。 至少，您需要为endpoint指定显示名称。 可选的，您可以指定项目和位置（区域）； 否则，设置将继承您使用`init()`方法初始化Vertex AI SDK时设置的值。\n",
    "\n",
    "在此示例中，指定了以下参数：\n",
    "\n",
    "- `display_name`：`Endpoint`资源的人类可读名称。\n",
    "- `project`：您的项目ID。\n",
    "- `location`：您的区域。\n",
    "- `labels`：（可选）以键/值对形式定义`Endpoint`的用户自定义元数据。\n",
    "\n",
    "此方法将返回一个`Endpoint`对象。\n",
    "\n",
    "了解更多关于[Vertex AI Endpoints](https://cloud.google.com/vertex-ai/docs/predictions/deploy-model-api)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ea443f9593b"
   },
   "outputs": [],
   "source": [
    "endpoint = aiplatform.Endpoint.create(\n",
    "    display_name=\"example_\" + TIMESTAMP,\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    labels={\"your_key\": \"your_value\"},\n",
    ")\n",
    "\n",
    "print(endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ca3fa3f6a894"
   },
   "source": [
    "将`Model`资源部署到一个`Endpoint`资源。\n",
    "\n",
    "您可以将一个或多个`Vertex AI Model`资源实例部署到同一个端点。每个被部署的`Vertex AI Model`资源都将拥有其自己的服务二进制的部署容器。\n",
    "\n",
    "*注意:* 在前一个步骤中上传模型文件到`Vertex AI Model`资源时，您已经指定了TFHub模型的部署容器。\n",
    "\n",
    "### 扩展\n",
    "\n",
    "`Vertex AI Endpoint`资源支持三种类型的扩展:\n",
    "\n",
    "- 无扩展: 服务二进制部署到单个VM实例。\n",
    "- 手动扩展: 服务二进制部署到固定数量的多个VM实例。\n",
    "- 自动扩展: 根据负载情况，服务二进制部署到不同数量的VM实例。\n",
    "\n",
    "### 无扩展\n",
    "\n",
    "在下一个例子中，您将把`Vertex AI Model`资源部署到一个`Vertex AI Endpoint`资源，没有任何扩展 -- 也就是说，单个VM (节点)实例。换句话说，当模型被部署时，将配置一个单个VM实例并保持配置，直到模型被取消部署。\n",
    "\n",
    "在这个示例中，您将使用指定的最少参数部署模型，如下所示:\n",
    "\n",
    "- `model`: 要部署的`Model`资源。\n",
    "- `machine_type`: 每个VM实例的机器类型。\n",
    "- `deployed_model_displayed_name`: 部署模型实例的人类可读名称。\n",
    "\n",
    "对于无扩展，单个VM实例在模型部署期间被配置。由于准备资源的要求，这可能需要几分钟。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4e93b034a72f"
   },
   "outputs": [],
   "source": [
    "response = endpoint.deploy(\n",
    "    model=model,\n",
    "    deployed_model_display_name=\"example_\" + TIMESTAMP,\n",
    "    machine_type=DEPLOY_COMPUTE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83f002e40547"
   },
   "source": [
    "#### 显示缩放配置\n",
    "\n",
    "一旦您的模型部署完成，您可以查询`Endpoint`资源，使用属性`endpoint.gca_resource.deployed_models`来检索已部署模型的缩放配置。\n",
    "\n",
    "由于一个`Endpoint`资源可能有多个已部署模型，`deployed_models`属性会返回一个列表，每个已部署模型对应一个条目。在这个例子中，只有一个已部署模型，您可以通过`deployed_models[0]`来获取列表中的第一个条目，并显示属性`dedicated_resources`，该属性会返回机器类型和最小/最大节点数以进行缩放。对于不需要缩放的情况，最小/最大节点数将被设为一。\n",
    "\n",
    "*注意：* 已部署模型标识符是指已部署模型的实例，而不是模型资源标识符。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "86d795232426"
   },
   "outputs": [],
   "source": [
    "print(endpoint.gca_resource.deployed_models[0].dedicated_resources)\n",
    "\n",
    "deployed_model_id = endpoint.gca_resource.deployed_models[0].id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "undeploy_model:mbsdk"
   },
   "source": [
    "解除模型\n",
    "\n",
    "当您完成预测后，您可以从 `Endpoint` 资源中解除模型。这将取消所有计算资源并停止部署模型的计费。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "undeploy_model:mbsdk"
   },
   "outputs": [],
   "source": [
    "endpoint.undeploy(deployed_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ae12f1afe50"
   },
   "source": [
    "### 手动扩展\n",
    "\n",
    "在下面的示例中，您将`Vertex AI Model`资源部署到`Vertex AI Endpoint`资源，用于手动扩展--一个固定数量（大于1）的VM实例。换句话说，当模型被部署时，固定数量的VM实例被预配并保持预配状态直到模型被取消部署。\n",
    "\n",
    "在这个示例中，您以最小数量的指定参数部署模型，如下所示：\n",
    "\n",
    "- `model`: 要部署的`Model`资源。\n",
    "- `machine_type`: 每个VM实例的机器类型。\n",
    "- `deployed_model_displayed_name`: 部署模型实例的人类可读名称。\n",
    "- `min_replica_count`: 要预配的最小VM实例（节点）数量。\n",
    "- `max_replica_count`: 要预配的最大VM实例（节点）数量。\n",
    "\n",
    "对于手动扩展，固定数量的VM实例在模型部署期间被预配。\n",
    "\n",
    "*注意:* 对于手动扩展，节点的最小和最大数量被设置为相同的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cd5df385d405"
   },
   "outputs": [],
   "source": [
    "MIN_NODES = MAX_NODES = 2\n",
    "\n",
    "response = endpoint.deploy(\n",
    "    model=model,\n",
    "    deployed_model_display_name=\"example_\" + TIMESTAMP,\n",
    "    machine_type=DEPLOY_COMPUTE,\n",
    "    min_replica_count=MIN_NODES,\n",
    "    max_replica_count=MAX_NODES,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cc28de118414"
   },
   "source": [
    "显示缩放配置\n",
    "\n",
    "在这个示例中，有一个单独部署的模型，您可以通过列表中的第一个条目`deployed_models[0]`检索到缩放配置。然后显示`dedicated_resources`属性，这将返回机器类型和最小/最大节点数量以进行缩放。对于手动缩放，最小/最大节点将被设置为相同的值，大于1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f5e6ddbc8a35"
   },
   "outputs": [],
   "source": [
    "print(endpoint.gca_resource.deployed_models[0].dedicated_resources)\n",
    "\n",
    "deployed_model_id = endpoint.gca_resource.deployed_models[0].id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "undeploy_model:mbsdk"
   },
   "source": [
    "#### 卸载模型\n",
    "\n",
    "当您完成预测时，您可以从`Endpoint`资源中卸载模型。这将取消所有计算资源并停止为部署模型计费。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "undeploy_model:mbsdk"
   },
   "outputs": [],
   "source": [
    "endpoint.undeploy(deployed_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1d7ac6107f1"
   },
   "source": [
    "### 自动缩放\n",
    "\n",
    "在下一个示例中，您将将`Vertex AI Model`资源部署到`Vertex AI Endpoint`资源以进行自动缩放，即变量数量（大于1）的VM实例。换句话说，当模型部署时，将提供最小数量的VM实例。随着负载的变化，提供的实例数量可能动态增加到最大VM实例数量，并减少到最小数量的VM实例。提供的VM实例数量永远不会少于最小值或多于最大值。\n",
    "\n",
    "在此示例中，您将使用指定参数的最少量部署模型，如下所示：\n",
    "\n",
    "- `model`：要部署的`Model`资源。\n",
    "- `machine_type`：每个VM实例的机器类型。\n",
    "- `deployed_model_displayed_name`：部署模型实例的人类可读名称。\n",
    "- `min_replica_count`：要提供的VM实例（节点）的最小数量。\n",
    "- `max_replica_count`：要提供的VM实例（节点）的最大数量。\n",
    "\n",
    "对于自动缩放，在部署模型时将提供最小数量的VM实例。\n",
    "\n",
    "*注意：*对于自动缩放，最小节点数必须设置为大于零的值。换句话说，至少会有一个VM实例被提供。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "277de42a6475"
   },
   "outputs": [],
   "source": [
    "MIN_NODES = 1\n",
    "MAX_NODES = 2\n",
    "\n",
    "response = endpoint.deploy(\n",
    "    model=model,\n",
    "    deployed_model_display_name=\"example_\" + TIMESTAMP,\n",
    "    machine_type=DEPLOY_COMPUTE,\n",
    "    min_replica_count=MIN_NODES,\n",
    "    max_replica_count=MAX_NODES,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9fefa8f69569"
   },
   "source": [
    "#### 显示缩放配置\n",
    "\n",
    "在这个例子中，有一个已部署的模型，并且您可以将缩放配置作为列表中的第一个条目进行检索：`deployed_models[0]`。然后，您可以显示`dedicated_resources`属性，该属性将返回机器类型和缩放的最小/最大节点数。对于自动缩放，最大节点数将被设置为大于最小节点数的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d9fae1534f4e"
   },
   "outputs": [],
   "source": [
    "print(endpoint.gca_resource.deployed_models[0].dedicated_resources)\n",
    "\n",
    "deployed_model_id = endpoint.gca_resource.deployed_models[0].id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "undeploy_model:mbsdk"
   },
   "source": [
    "#### 卸载模型\n",
    "\n",
    "当您完成预测时，您可以从 `Endpoint` 资源中卸载模型。这将取消所有计算资源并停止部署模型的计费。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "undeploy_model:mbsdk"
   },
   "outputs": [],
   "source": [
    "endpoint.undeploy(deployed_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34b243986832"
   },
   "source": [
    "### 设置缩放阈值\n",
    "\n",
    "`Endpoint`资源支持基于两个指标的自动缩放: CPU利用率和GPU工作循环。这两个指标通过计算每个部署模型的平均利用率来衡量。一旦利用率指标超过某个阈值一段时间，VM实例（节点）的数量会相应地调整上升或下降。\n",
    "\n",
    "\n",
    "#### CPU阈值\n",
    "\n",
    "在先前的示例中，部署的VM实例仅带有CPU -- 即没有硬件加速器。默认情况下（自动缩放中），CPU利用率指标设置为60%。在部署模型时，可以指定参数`autoscaling_target_cpu_utilization`来设置非默认值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b60fa50468d3"
   },
   "outputs": [],
   "source": [
    "MIN_NODES = 1\n",
    "MAX_NODES = 4\n",
    "\n",
    "response = endpoint.deploy(\n",
    "    model=model,\n",
    "    deployed_model_display_name=\"example_\" + TIMESTAMP,\n",
    "    machine_type=DEPLOY_COMPUTE,\n",
    "    min_replica_count=MIN_NODES,\n",
    "    max_replica_count=MAX_NODES,\n",
    "    autoscaling_target_cpu_utilization=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85541ea27277"
   },
   "source": [
    "显示缩放配置\n",
    "\n",
    "在这个例子中，有一个部署的模型，您可以将缩放配置作为列表中的第一个条目检索出来：`deployed_models[0]`。然后，您可以显示`dedicated_resources`属性，它将返回机器类型和可扩展的节点的最小/最大数量，以及CPU利用率的目标值：`autoscaling_metric_specs`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "51df17fdd0fd"
   },
   "outputs": [],
   "source": [
    "print(endpoint.gca_resource.deployed_models[0].dedicated_resources)\n",
    "\n",
    "deployed_model_id = endpoint.gca_resource.deployed_models[0].id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "undeploy_model:mbsdk"
   },
   "source": [
    "取消模型部署\n",
    "\n",
    "当你完成预测时，你可以从`Endpoint`资源中取消模型部署。这将取消所有计算资源并停止为部署的模型计费。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "undeploy_model:mbsdk"
   },
   "outputs": [],
   "source": [
    "endpoint.undeploy(deployed_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "93a66905f829"
   },
   "source": [
    "### 上传适用于 GPU 部署图像的 TensorFlow Hub 模型\n",
    "\n",
    "接下来，您可以上传第二个实例的 TensorFlow Hub 模型作为`Model`资源 -- 但是相应的服务容器支持 GPU。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "35020c3e4474"
   },
   "outputs": [],
   "source": [
    "model_gpu = aiplatform.Model.upload(\n",
    "    display_name=\"example_\" + TIMESTAMP,\n",
    "    artifact_uri=MODEL_DIR,\n",
    "    serving_container_image_uri=DEPLOY_IMAGE_GPU,\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1992d1913c53"
   },
   "source": [
    "GPU阈值\n",
    "\n",
    "在此示例中，部署的 VM 实例被配置为使用硬件加速器（即 GPUs），通过指定以下参数：\n",
    "\n",
    "- `accelerator_type`: 硬件（例如 GPU）加速器的类型。\n",
    "- `accelerator_count`: 每个预置 VM 实例的硬件加速器数量。\n",
    "\n",
    "支持的 GPU 类型和数量特定于机器类型和区域。\n",
    "\n",
    "了解更多关于[每个机器类型的 GPU 类型和数量](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute)。\n",
    "\n",
    "了解更多关于[每个区域可用的 GPU 类型](https://cloud.google.com/vertex-ai/docs/general/locations#accelerators)。\n",
    "\n",
    "默认情况下（在自动扩展中），GPU 利用率指标设置为 60%。在部署模型时，指定参数 `autoscaling_target_accelerator_duty_cycle` 来设置非默认值。\n",
    "\n",
    "在提供服务时，如果 CPU 利用率或 GPU 负载周期超出或低于某个阈值一段时间，那么将触发自动扩展。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e6e268a79068"
   },
   "outputs": [],
   "source": [
    "MIN_NODES = 1\n",
    "MAX_NODES = 2\n",
    "\n",
    "response = endpoint.deploy(\n",
    "    model=model_gpu,\n",
    "    deployed_model_display_name=\"example_\" + TIMESTAMP,\n",
    "    machine_type=DEPLOY_COMPUTE,\n",
    "    accelerator_type=DEPLOY_GPU.name,\n",
    "    accelerator_count=DEPLOY_NGPU,\n",
    "    min_replica_count=MIN_NODES,\n",
    "    max_replica_count=MAX_NODES,\n",
    "    autoscaling_target_accelerator_duty_cycle=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9fbf52386a0e"
   },
   "source": [
    "#### 显示缩放配置\n",
    "\n",
    "在此示例中，有一个已部署的模型，您可以将缩放配置作为列表中的第一个条目检索：`deployed_models[0]`。然后显示`dedicated_resources`属性，该属性将返回机器类型和可缩放的节点的最小/最大数量，以及GPU工作周期的目标值：`autoscaling_metric_specs`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6aab057ece6a"
   },
   "outputs": [],
   "source": [
    "print(endpoint.gca_resource.deployed_models[0].dedicated_resources)\n",
    "\n",
    "deployed_model_id = endpoint.gca_resource.deployed_models[0].id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68b48039d9ca"
   },
   "source": [
    "### 将多个模型部署到“Endpoint”资源\n",
    "\n",
    "接下来，您将两个模型部署到同一个“Endpoint”资源，并在它们之间分配预测请求流量。一个模型将使用GPU，占流量的80%，另一个将使用CPU，占流量的20%。\n",
    "\n",
    "您已经将GPU版本的模型部署到“Endpoint”资源中。在这个例子中，您将向同一个“Endpoint”资源添加第二个模型实例 - CPU版本，并指定模型之间的流量分配。在这个例子中，`traffic_split`参数被指定如下：\n",
    "\n",
    "- `\"0\": 20`：被部署的模型（默认ID为0）将接收20%的流量。\n",
    "- `deployed_model_id: 80`：现有部署的模型（通过其部署的模型ID指定）将接收80%的流量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c84a81996ea4"
   },
   "outputs": [],
   "source": [
    "response = endpoint.deploy(\n",
    "    model=model,\n",
    "    deployed_model_display_name=\"example_\" + TIMESTAMP,\n",
    "    machine_type=DEPLOY_COMPUTE,\n",
    "    min_replica_count=MIN_NODES,\n",
    "    max_replica_count=MAX_NODES,\n",
    "    autoscaling_target_cpu_utilization=50,\n",
    "    traffic_split={\"0\": 20, deployed_model_id: 80},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b4d2cff3650d"
   },
   "source": [
    "#### 显示缩放配置\n",
    "\n",
    "在这个例子中，有两个部署模型，分别是CPU版本和GPU版本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a9293577fbad"
   },
   "outputs": [],
   "source": [
    "print(endpoint.gca_resource.deployed_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "undeploy_model:mbsdk"
   },
   "source": [
    "解除部署模型\n",
    "\n",
    "当您完成预测工作后，您将从“端点”资源中解除部署所有模型。这将取消所有计算资源，并停止对已部署模型的计费。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "undeploy_model:mbsdk"
   },
   "outputs": [],
   "source": [
    "endpoint.undeploy_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_delete:mbsdk"
   },
   "source": [
    "#### 删除模型实例\n",
    "\n",
    "方法'delete（）'将删除模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model_delete:mbsdk"
   },
   "outputs": [],
   "source": [
    "model.delete()\n",
    "model_gpu.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "endpoint_delete:mbsdk"
   },
   "source": [
    "删除端点\n",
    "\n",
    "方法'delete()'会删除端点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "endpoint_delete:mbsdk"
   },
   "outputs": [],
   "source": [
    "endpoint.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleanup"
   },
   "source": [
    "清理工作\n",
    "\n",
    "要清理此项目中使用的所有Google Cloud资源，您可以[删除用于教程的Google Cloud项目](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects)。\n",
    "\n",
    "否则，您可以删除在本教程中创建的单个资源。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cleanup"
   },
   "outputs": [],
   "source": [
    "# Set this to true only if you'd like to delete your bucket\n",
    "delete_bucket = True\n",
    "\n",
    "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
    "    ! gsutil rm -r $BUCKET_URI"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "get_started_with_autoscaling.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
