{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bfe9394d8cd"
   },
   "source": [
    "这本笔记本是由[Jesus Chavez](https://github.com/jchavezar)编写的笔记本的更新版本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title:generic,gcp"
   },
   "source": [
    "在Google云平台上进行端到端机器学习：MLOps阶段2：实验：使用DASK开始分布式训练\n",
    "\n",
    "在Colab中运行\n",
    "\n",
    "在GitHub上查看\n",
    "\n",
    "在Vertex AI Workbench中打开"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3baf7d4e1b84"
   },
   "source": [
    "## 概述\n",
    "\n",
    "本教程演示了如何使用`Vertex AI Training`为XGBoost模型进行分布式训练。本教程包括了使用DASK支持XGBoost和Scikit-learn模型的分布式训练，以及使用Flask作为自定义服务容器的Web服务器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9402cfbdc2d"
   },
   "source": [
    "### 目标\n",
    "\n",
    "在本教程中，您将学习如何使用`Vertex AI Training`来进行XGBoost模型的分布式训练，使用OSS包DASK。此外，您还将学习如何使用Flask Web服务器构建和部署自定义Serving容器。\n",
    "\n",
    "本教程使用以下谷歌云机器学习服务和资源：\n",
    "\n",
    "- `Vertex AI Training`\n",
    "- `Vertex AI Prediction`\n",
    "- `Vertex AI Model` 资源\n",
    "- `Vertex AI Endpoint` 资源\n",
    "\n",
    "执行的步骤包括：\n",
    "\n",
    "- 使用DASK构建XGBoost训练脚本进行分布式训练。\n",
    "- 构建自定义的训练容器。\n",
    "- 配置分布式自定义训练作业。\n",
    "- 执行自定义训练作业。\n",
    "- 使用Flask构建自定义Serving容器。\n",
    "- 将训练好的XGBoost模型上传为`Vertex AI Model`资源。\n",
    "- 创建一个`Vertex AI Endpoint`资源。\n",
    "- 将`Vertex AI Model`资源部署到`Vertex AI Endpoint`资源。\n",
    "- 进行预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset:iris,lcn"
   },
   "source": [
    "数据集\n",
    "\n",
    "本教程使用的数据集是[森林覆盖类型](https://archive.ics.uci.edu/ml/datasets/covertype)。该数据集的版本以 CSV 格式存储在一个公共云存储桶中。该数据集仅从地图变量预测森林覆盖类型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "costs"
   },
   "source": [
    "###  费用\n",
    "\n",
    "本教程使用 Google Cloud 的可计费组件:\n",
    "\n",
    "* Vertex AI\n",
    "* 云存储\n",
    "\n",
    "了解 [Vertex AI 的定价](https://cloud.google.com/vertex-ai/pricing) 和 [云存储的定价](https://cloud.google.com/storage/pricing)，并使用 [定价计算器](https://cloud.google.com/products/calculator/) 根据您的预期使用情况生成费用估算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_aip"
   },
   "source": [
    "## 安装\n",
    "\n",
    "安装以下软件包以执行此笔记本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_aip"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Vertex AI Workbench Notebook product has specific requirements\n",
    "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\")\n",
    "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
    "    \"/opt/deeplearning/metadata/env_version\"\n",
    ")\n",
    "\n",
    "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_WORKBENCH_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\"\n",
    "\n",
    "! pip3 install --upgrade google-cloud-aiplatform $USER_FLAG -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhq5zEbGg0XX"
   },
   "source": [
    "### 重新启动内核\n",
    "\n",
    "在安装了额外的软件包之后，您需要重新启动笔记本内核，这样它才能找到这些软件包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EzrelQZ22IZj"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "before_you_begin"
   },
   "source": [
    "## 开始之前\n",
    "\n",
    "### 设置您的Google Cloud项目\n",
    "\n",
    "**无论您使用什么笔记本环境，都需要按照以下步骤操作。**\n",
    "\n",
    "1. [选择或创建Google Cloud项目](https://console.cloud.google.com/cloud-resource-manager)。您首次创建账户时，可获得300美元的免费信贷，可用于计算/存储成本。\n",
    "\n",
    "2. [确保您的项目已启用计费功能。](https://cloud.google.com/billing/docs/how-to/modify-project)\n",
    "\n",
    "3. [启用以下API: Vertex AI API、Compute Engine API和Cloud Storage。](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,compute_component,storage-component.googleapis.com)\n",
    "\n",
    "4. 如果您在本地运行此笔记本，则需要安装[Cloud SDK]((https://cloud.google.com/sdk))。\n",
    "\n",
    "5. 在下面的单元格中输入您的项目ID。然后运行该单元格，确保Cloud SDK在本笔记本的所有命令中使用正确的项目。\n",
    "\n",
    "**注意**：Jupyter运行以`!`为前缀的行作为shell命令，并插入以`$`为前缀的Python变量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "project_id"
   },
   "source": [
    "设定您的项目ID\n",
    "\n",
    "**如果您不知道您的项目ID**，您可以使用`gcloud`来获取您的项目ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_project_id"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "autoset_project_id"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID:\", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_gcloud_project_id"
   },
   "outputs": [],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "区域\n",
    "\n",
    "您还可以更改“REGION”变量，该变量用于本笔记本其余部分的操作。以下是Vertex AI支持的区域。我们建议您选择离您最近的区域。\n",
    "\n",
    "- 美洲：`us-central1`\n",
    "- 欧洲：`europe-west4`\n",
    "- 亚太：`asia-east1`\n",
    "\n",
    "您可能无法在Vertex AI中使用多区域存储桶进行训练。并非所有区域都为所有Vertex AI服务提供支持。\n",
    "\n",
    "了解更多关于[Vertex AI地区](https://cloud.google.com/vertex-ai/docs/general/locations)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "region"
   },
   "outputs": [],
   "source": [
    "REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
    "\n",
    "if REGION == \"[your-region]\":\n",
    "    REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "timestamp"
   },
   "source": [
    "时间戳\n",
    "\n",
    "如果您正在进行直播教程会话，您可能正在使用一个共享的测试账户或项目。为避免在创建的资源上发生名称冲突，您应该为每个实例会话创建一个时间戳，并将时间戳附加到您在本教程中创建的资源名称上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "timestamp"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcp_authenticate"
   },
   "source": [
    "### 认证您的Google Cloud帐户\n",
    "\n",
    "**如果您正在使用Vertex AI Workbench笔记本**，您的环境已经通过身份验证。跳过此步骤。\n",
    "\n",
    "**如果您正在使用Colab**，请运行下面的单元格，并在提示时按照说明通过oAuth认证您的帐户。\n",
    "\n",
    "**否则**，请按照以下步骤操作：\n",
    "\n",
    "在Cloud控制台中，转到[创建服务帐户密钥](https://console.cloud.google.com/apis/credentials/serviceaccountkey)页面。\n",
    "\n",
    "**点击Create service account**。\n",
    "\n",
    "在**服务帐户名称**字段中输入一个名称，并点击**创建**。\n",
    "\n",
    "在**授予该服务帐户对项目的访问权限**部分，点击角色下拉列表。在过滤框中键入“Vertex”，并选择**Vertex管理员**。在过滤框中键入“Storage Object Admin”，并选择**Storage Object Admin**。\n",
    "\n",
    "点击创建。一个包含您密钥的JSON文件将下载到您的本地环境中。\n",
    "\n",
    "将您的服务帐户密钥路径输入为下面单元格中的GOOGLE_APPLICATION_CREDENTIALS变量，并运行该单元格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gcp_authenticate"
   },
   "outputs": [],
   "source": [
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# If on Vertex AI Workbench, then don't execute this code\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\") and not os.getenv(\n",
    "    \"DL_ANACONDA_HOME\"\n",
    "):\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\"):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bucket:mbsdk"
   },
   "source": [
    "创建云存储桶\n",
    "\n",
    "**无论您使用哪种笔记本环境，都需要以下步骤。**\n",
    "\n",
    "当您初始化用于Python的Vertex AI SDK时，需要指定一个云存储暂存桶。暂存桶是您的数据集和模型资源相关数据在会话中保留的地方。\n",
    "\n",
    "在下方设置您的云存储桶名称。存储桶名称必须在所有谷歌云项目中全局唯一，包括您组织之外的项目。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bucket"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"[your-bucket-name]\"  # @param {type:\"string\"}\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "autoset_bucket"
   },
   "outputs": [],
   "source": [
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"[your-bucket-name]\":\n",
    "    BUCKET_NAME = PROJECT_ID + \"aip-\" + TIMESTAMP\n",
    "    BUCKET_URI = \"gs://\" + BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_bucket"
   },
   "source": [
    "只有当您的存储桶尚不存在时：运行以下单元格以创建您的云存储存储桶。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_bucket"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "validate_bucket"
   },
   "source": [
    "最后，通过检查Cloud Storage桶的内容来验证访问权限。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "validate_bucket"
   },
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gar_enable_api"
   },
   "source": [
    "### 启用Artifact Registry API\n",
    "\n",
    "您必须为您的项目启用Artifact Registry API服务。\n",
    "\n",
    "了解更多关于[启用服务](https://cloud.google.com/artifact-registry/docs/enable-service)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gar_enable_api"
   },
   "outputs": [],
   "source": [
    "! gcloud services enable artifactregistry.googleapis.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_vars"
   },
   "source": [
    "### 设置变量\n",
    "\n",
    "接下来，设置一些在本教程中使用的变量。\n",
    "### 导入库并定义常量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk"
   },
   "source": [
    "### 初始化用于 Python 的 Vertex AI SDK\n",
    "\n",
    "为您的项目和相应的存储桶初始化用于 Python 的 Vertex AI SDK。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2259110f1d46"
   },
   "source": [
    "## DASK简介\n",
    "\n",
    "摘自XGBoost DASK文档。\n",
    "\n",
    "### 什么是DASK?\n",
    "\n",
    "```\n",
    "Dask是一个基于Python的并行计算库。Dask允许轻松管理分布式工作节点，并擅长处理大规模的分布式数据科学工作流。XGBoost中的实现源自dask-xgboost，具有一些扩展功能和不同的接口。\n",
    "```\n",
    "\n",
    "### XGBoost DASK概述\n",
    "\n",
    "```\n",
    "一个dask集群由三个不同组件组成：一个集中式调度器，一个或多个工作节点，以及一个或多个充当用户操作入口的客户端。当与dask一起使用XGBoost时，需要从客户端调用XGBoost dask接口。以下是一个简单的示例，演示在dask集群上运行XGBoost的基本用法。\n",
    "```\n",
    "\n",
    "了解更多关于[XGBoost DASK](https://xgboost.readthedocs.io/en/stable/tutorials/dask.html)\n",
    "\n",
    "了解更多关于[DASK](dask.org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xgboost_intro"
   },
   "source": [
    "## XGBoost培训介绍\n",
    "\n",
    "一旦您训练了一个XGBoost模型，您将希望将其保存在云存储位置，以便随后上传到`Vertex AI Model`资源。\n",
    "XGBoost软件包没有支持将模型保存到云存储位置。相反，您将执行以下步骤将其保存到云存储位置。\n",
    "\n",
    "1. 将内存中的模型保存到本地文件系统（例如，model.bst）。\n",
    "2. 使用`google.cloud.storage`将本地副本复制到指定的云存储位置。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "examine_training_package:xgboost"
   },
   "source": [
    "### 检查培训包\n",
    "\n",
    "#### 包布局\n",
    "\n",
    "在开始培训之前，您将查看如何组装用于自定义训练作业的Python包。解压缩后，该包包含以下目录/文件布局。\n",
    "\n",
    "- PKG-INFO\n",
    "- README.md\n",
    "- trainer\n",
    "  - \\_\\_init\\_\\_.py\n",
    "  - task.py\n",
    "\n",
    "文件`trainer/task.py`是用于执行自定义训练作业的Python脚本。*注意*，在工作池规范中引用它时，您将目录斜杠替换为点号（`trainer.task`），并删除文件后缀（`.py`）。\n",
    "\n",
    "#### 包装配\n",
    "\n",
    "在下面的单元格中，您将组装培训包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f4wS4eISox9V"
   },
   "outputs": [],
   "source": [
    "# Make folder for Python training script\n",
    "! rm -rf custom prediction\n",
    "! mkdir custom prediction\n",
    "\n",
    "# Add package information\n",
    "! touch custom/README.md\n",
    "\n",
    "pkg_info = \"Metadata-Version: 1.0\\n\\nName: cover_type classification\\n\\nVersion: 0.0.0\\n\\nSummary: Demostration training script\\n\\nHome-page: www.google.com\\n\\nAuthor: Google\\n\\nAuthor-email: aferlitsch@google.com\\n\\nLicense: Public\\n\\nDescription: Demo\\n\\nPlatform: Vertex\"\n",
    "! echo \"$pkg_info\" > custom/PKG-INFO\n",
    "\n",
    "# Make the training subfolder\n",
    "! mkdir custom/trainer\n",
    "! touch custom/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ab4eeb45d1c"
   },
   "source": [
    "## 使用Dask + CUDA（GPU）构建训练脚本\n",
    "\n",
    "接下来，您将编写自定义训练脚本，使用DASK进行分布式训练来训练XGBoost模型。\n",
    "\n",
    "- `args`：传递给训练脚本的参数：\n",
    "  - `dataset-source`：包含训练数据的CSV文件的云存储位置。\n",
    "  - `model_dir`：用于存储模型构件的云存储位置。\n",
    "  - `model_name`：模型的文件名。\n",
    "  - `--num-gpu-per-worker`：对于调度程序，每个worker的GPU数量。您还需要在`run()`方法中进一步设置分配给作业的相同数量的GPU的`accelerator_count`。\n",
    "  - `--threads-per-worker`：为了提高效率，您应该将每个worker的线程数设置为GPU的数量。\n",
    "- `get_scheduler_info()`：获取用于设置分布式集群的VM/进程调度相关信息。\n",
    "- `using_quantile_device_dmatrix()`：执行分布式训练：\n",
    "  - 注意`client`是用于分布式训练的集群控制器。\n",
    "  - 使用`dask_cudf.read_csv()`从CSV文件中读取数据集。\n",
    "  - 将数据集拆分并预处理为训练/评估集。\n",
    "  - 使用`dxgb.DaskDeviceQuantileDMatrix()`加载用于分布式训练的数据集。\n",
    "  - 使用`xgb.dask.train()`进行分布式训练。\n",
    "- `saved_model()`：将模型和评估指标保存到指定的云存储位置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ba73b24cd497"
   },
   "outputs": [],
   "source": [
    "%%writefile custom/trainer/task.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import logging\n",
    "import dask_cudf\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import time\n",
    "from google.cloud import storage\n",
    "\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client\n",
    "from dask.distributed import wait\n",
    "from dask import array as da\n",
    "from xgboost import dask as dxgb\n",
    "from xgboost.dask import DaskDMatrix\n",
    "from dask.utils import parse_bytes\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    '--dataset-source', dest='dataset',\n",
    "    type=str,\n",
    "    help='Dataset.')\n",
    "parser.add_argument(\n",
    "    '--model-dir',\n",
    "    default=os.getenv('AIP_MODEL_DIR'),\n",
    "    help='GCS location to export models')\n",
    "parser.add_argument(\n",
    "    '--model-name',\n",
    "    default=\"custom-train\",\n",
    "    help='The name of your saved model')\n",
    "parser.add_argument(\n",
    "    '--num-gpu-per-worker', type=str, help='num of workers',\n",
    "    default=2)\n",
    "parser.add_argument(\n",
    "    '--threads-per-worker', type=str, help='num of threads per worker',\n",
    "    default=4)\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "def save_model(model_dir):\n",
    "    logging.info(f\"Reading input job_dir: {model_dir}\")\n",
    "    model_dir = model_dir.split(\"/\")\n",
    "    bucket_name = model_dir[2]\n",
    "    object_prefix = \"/\".join(model_dir[3:]).rstrip(\"/\")\n",
    "    logging.info(f\"Reading object_prefix: {object_prefix}\")\n",
    "\n",
    "    if object_prefix:\n",
    "        model_path = '{}/{}'.format(object_prefix, \"xgboost\")\n",
    "    else:\n",
    "        model_path = '{}'.format(\"xgboost\")\n",
    "            \n",
    "    logging.info(f\"The model path is {model_path}\")\n",
    "    bucket = storage.Client().bucket(bucket_name)    \n",
    "    local_path = os.path.join(\"/tmp\", \"xgboost\")\n",
    "    \n",
    "    files = [f for f in os.listdir(local_path) if os.path.isfile(os.path.join(local_path, f))]\n",
    "    for file in files:\n",
    "        local_file = os.path.join(local_path, file)\n",
    "        blob = bucket.blob(\"/\".join([model_path, file]))\n",
    "        blob.upload_from_filename(local_file)\n",
    "    logging.info(local_file)\n",
    "    logging.info(f\"gs://{bucket_name}/{model_path}\")\n",
    "    logging.info(f\"Saved model files in gs://{bucket_name}/{model_path}\")\n",
    "\n",
    "        \n",
    "def using_quantile_device_dmatrix(client: Client, \n",
    "                                  dataset_source: str, \n",
    "                                  model_dir: str, \n",
    "                                  model_name: str):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    logging.info(f\"Importing dataset {dataset_source}\")\n",
    "    df = dask_cudf.read_csv(dataset_source)\n",
    "\n",
    "    logging.info(\"Cleaning and standarizing dataset\")\n",
    "    df = df.dropna() \n",
    "\n",
    "    logging.info(\"Splitting dataset\")\n",
    "    df_train, df_eval = df.random_split([0.8, 0.2], random_state=123)\n",
    "    df_train_features= df_train.drop('Cover_Type', axis=1)\n",
    "    df_eval_features= df_eval.drop('Cover_Type', axis=1)\n",
    "    df_train_labels = df_train.pop('Cover_Type')\n",
    "    df_eval_labels = df_eval.pop('Cover_Type')\n",
    "\n",
    "    logging.info(\"Train Dataset for dask\")\n",
    "    dtrain = dxgb.DaskDeviceQuantileDMatrix(client, df_train_features, df_train_labels)\n",
    "    \n",
    "    logging.info(\"Eval Dataset for dask\")\n",
    "    dvalid = dxgb.DaskDeviceQuantileDMatrix(client, df_eval_features, df_eval_labels)\n",
    "    logging.info(\"[INFO]: ------ QuantileDMatrix is formed in {} seconds ---\".format((time.time() - start_time)))\n",
    "\n",
    "    del df_train_features\n",
    "    del df_train_labels\n",
    "    del df_eval_features\n",
    "    del df_eval_labels \n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    logging.info(\"Training\")\n",
    "    logging.info(f\"XGBoost version: {xgb.__version__}\")\n",
    "    output = xgb.dask.train(\n",
    "        client,\n",
    "        {\n",
    "            \"verbosity\": 2, \n",
    "            \"tree_method\": \"gpu_hist\", \n",
    "            \"objective\": \"multi:softprob\",\n",
    "            \"eval_metric\": [\"mlogloss\"],\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"gamma\": 0.9,\n",
    "            \"subsample\": 0.5,\n",
    "            \"max_depth\": 9,\n",
    "            \"num_class\": 8\n",
    "        },\n",
    "        dtrain,\n",
    "        num_boost_round=10,\n",
    "        evals=[(dvalid, \"valid1\")],\n",
    "        early_stopping_rounds=5\n",
    "    ) \n",
    "    print(\"[INFO]: ------ Training is completed in {} seconds ---\".format((time.time() - start_time)))\n",
    "\n",
    "    # Saving models and exporting performance metrics\n",
    "    \n",
    "    df_eval_metrics = pd.DataFrame(output[\"history\"][\"valid1\"])\n",
    "    model = output[\"booster\"]\n",
    "    best_model = model[: model.best_iteration]\n",
    "    print(f\"Best model: {best_model}\")\n",
    "    \n",
    "    temp_dir = \"/tmp/xgboost\"\n",
    "    os.mkdir(temp_dir)\n",
    "    best_model.save_model(\"{}/{}\".format(temp_dir, model_name))\n",
    "    df_eval_metrics.to_json(\"{}/all_results.json\".format(temp_dir))\n",
    "\n",
    "    save_model(model_dir)\n",
    "        \n",
    "def get_scheduler_info():\n",
    "    scheduler_ip =  subprocess.check_output(['hostname','--all-ip-addresses'])\n",
    "    scheduler_ip = scheduler_ip.decode('UTF-8').split()[0]\n",
    "    scheduler_port = '8786'\n",
    "    scheduler_uri = '{}:{}'.format(scheduler_ip, scheduler_port)\n",
    "    return scheduler_ip, scheduler_uri\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"Creating dask cluster\")\n",
    "    \n",
    "    sched_ip, sched_uri = get_scheduler_info()\n",
    "    \n",
    "    print(f\"Sched_ip and Sched_uri, {sched_ip}, {sched_uri}\")\n",
    "\n",
    "    print(\"[INFO]: ------ LocalCUDACluster is being formed \")\n",
    "    \n",
    "    with LocalCUDACluster(\n",
    "        ip=sched_ip,\n",
    "        n_workers=int(args.num_gpu_per_worker), \n",
    "        threads_per_worker=int(args.threads_per_worker) \n",
    "    ) as cluster:\n",
    "        with Client(cluster) as client:\n",
    "            print('[INFO]: ------ Calling main function ')\n",
    "            using_quantile_device_dmatrix(client, \n",
    "                                          dataset_source=args.dataset, \n",
    "                                          model_dir=args.model_dir, \n",
    "                                          model_name=args.model_name\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "26a173ba89d6"
   },
   "source": [
    "### 构建自定义训练容器\n",
    "\n",
    "接下来，您将构建一个支持GPU CUDA的用于训练XGBoost模型的自定义（Docker）容器。作为基础镜像，您将使用带有CUDA支持的Rapids AI镜像。然后，您将在该镜像中安装XGBoost、Google Cloud Fuse和Cloud Storage的软件包。\n",
    "\n",
    "*备注：*目前，Vertex AI不支持具有GPU支持的XGBoost的预构建容器（仅支持CPU）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1c8198a33c48"
   },
   "outputs": [],
   "source": [
    "%%writefile custom/Dockerfile\n",
    "\n",
    "FROM rapidsai/rapidsai-nightly:22.04-cuda11.2-base-ubuntu20.04-py3.9\n",
    "\n",
    "RUN pip install google.cloud[storage] \\\n",
    "  && pip install gcsfs \\\n",
    "  && pip install xgboost --upgrade\n",
    "\n",
    "COPY trainer trainer/\n",
    "\n",
    "ENTRYPOINT [\"python\", \"trainer/task.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gar_create_repo"
   },
   "source": [
    "## 创建一个私有的Docker仓库\n",
    "\n",
    "接下来，在Google Artifact Registry中创建您自己的Docker仓库。\n",
    "\n",
    "1. 运行`gcloud artifacts repositories create`命令，使用您的区域创建一个新的Docker仓库，描述为\"docker仓库\"。\n",
    "\n",
    "2. 运行`gcloud artifacts repositories list`命令，验证您的仓库是否已经创建。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gar_create_repo"
   },
   "outputs": [],
   "source": [
    "PRIVATE_REPO = \"my-docker-repo\"\n",
    "\n",
    "! gcloud artifacts repositories create {PRIVATE_REPO} --repository-format=docker --location={REGION} --description=\"Docker repository\"\n",
    "\n",
    "! gcloud artifacts repositories list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gar_auth"
   },
   "source": [
    "### 配置私有存储库的验证\n",
    "\n",
    "在推送或拉取容器图像之前，配置Docker使用`gcloud`命令行工具对您所在地区的`Artifact Registry`进行验证请求。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gar_auth"
   },
   "outputs": [],
   "source": [
    "! gcloud auth configure-docker {REGION}-docker.pkg.dev --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6882c48fb15"
   },
   "source": [
    "接下来，您将构建自定义培训（Docker）映像。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7b0aa42672d6"
   },
   "outputs": [],
   "source": [
    "TRAIN_IMAGE = (\n",
    "    f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{PRIVATE_REPO}/train_gpu_xgb:latest\"\n",
    ")\n",
    "\n",
    "! docker build custom -t $TRAIN_IMAGE\n",
    "! docker push $TRAIN_IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98e8ddfb3ee9"
   },
   "source": [
    "## 使用Flask构建serving容器。\n",
    "\n",
    "在本教程中，将使用自定义的serving容器来提供模型。您可以使用Flask构建用于健康和预测路由的HTTP服务器。\n",
    "\n",
    "*TODO: 解除项目ID的硬编码*  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "38de94ac40a3"
   },
   "outputs": [],
   "source": [
    "%%writefile prediction/app.py\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from flask import Flask, request, Response, jsonify\n",
    "from google.cloud import storage\n",
    "\n",
    "#client = storage.Client(project=os.environ['PROJECT_ID'])\n",
    "client = storage.Client(project='andy-1234-221921')\n",
    "\n",
    "# Model Download from gcs\n",
    "\n",
    "fname = \"model.json\"\n",
    "\n",
    "with open(fname, \"wb\") as model:\n",
    "    client.download_blob_to_file(\n",
    "        f\"{os.environ['AIP_STORAGE_URI']}/{fname}\", model\n",
    "    )\n",
    "\n",
    "# Loading model\n",
    "print(\"Loading model from: {}\".format(fname))\n",
    "model = xgb.Booster(model_file=fname)\n",
    "\n",
    "# Creation of the Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Flask route for Liveness checks\n",
    "@app.route(os.environ['AIP_HEALTH_ROUTE'])\n",
    "def isalive():\n",
    "    status_code = Response(status=200)\n",
    "    return status_code\n",
    "\n",
    "# Flask route for predictions\n",
    "@app.route(os.environ['AIP_PREDICT_ROUTE'],methods=['GET','POST'])\n",
    "def prediction():\n",
    "    _features = ['Id','Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n",
    "                          'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm','Horizontal_Distance_To_Fire_Points', 'Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3', \n",
    "                          'Wilderness_Area4', 'Soil_Type1', 'Soil_Type2', 'Soil_Type3', 'Soil_Type4', 'Soil_Type5', 'Soil_Type6', 'Soil_Type7', 'Soil_Type8', 'Soil_Type9',\n",
    "                          'Soil_Type10','Soil_Type11','Soil_Type12','Soil_Type13','Soil_Type14','Soil_Type15','Soil_Type16','Soil_Type17','Soil_Type18','Soil_Type19', \n",
    "                          'Soil_Type20', 'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24', 'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28', 'Soil_Type29',\n",
    "                          'Soil_Type30', 'Soil_Type31', 'Soil_Type32', 'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36', 'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40']\n",
    "    data = request.get_json(silent=True, force=True)\n",
    "    dmf = xgb.DMatrix(pd.DataFrame(data[\"instances\"], columns=_features))\n",
    "    response = pd.DataFrame(model.predict(dmf))\n",
    "    logging.info(f\"Response: {response}\")\n",
    "    return jsonify({\"Cover Type\": str(response.idxmax(axis=1)[0])})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True, host='0.0.0.0', port=8080)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1f71690e0d59"
   },
   "source": [
    "### 为您定制的容器构建软件包要求\n",
    "\n",
    "接下来，您需要为您的定制容器构建软件包要求文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "936121d30322"
   },
   "outputs": [],
   "source": [
    "%%writefile prediction/requirements.txt\n",
    "\n",
    "google-cloud-storage\n",
    "numpy\n",
    "pandas\n",
    "flask\n",
    "xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ac192063545"
   },
   "source": [
    "接下来，您需构建一个定制的（Docker）容器，用于从您部署的XGBoost模型提供预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dfaa2f71def4"
   },
   "outputs": [],
   "source": [
    "%%writefile prediction/Dockerfile\n",
    "\n",
    "FROM python:3.7-buster\n",
    "\n",
    "RUN mkdir my-model\n",
    "\n",
    "COPY app.py ./app.py\n",
    "COPY requirements.txt ./requirements.txt\n",
    "RUN pip install -r requirements.txt \n",
    "\n",
    "# Flask Env Variable\n",
    "ENV FLASK_APP=app\n",
    "\n",
    "# Expose port 8080\n",
    "EXPOSE 8080\n",
    "\n",
    "CMD flask run --host=0.0.0.0 --port=8080"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "445235be1721"
   },
   "source": [
    "自定义服务容器\n",
    "\n",
    "接下来，您将构建您的自定义预测（Docker）镜像。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "71f78292b55f"
   },
   "outputs": [],
   "source": [
    "DEPLOY_IMAGE = (\n",
    "    f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{PRIVATE_REPO}/predict_gpu_xgb:latest\"\n",
    ")\n",
    "\n",
    "! docker build prediction -t $DEPLOY_IMAGE\n",
    "! docker push $DEPLOY_IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tarball_training_script"
   },
   "source": [
    "将培训脚本存储到您的云存储桶中\n",
    "\n",
    "接下来，您将培训文件夹打包成压缩的tar文件，并将其存储在您的云存储桶中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dnmdycf6ox9X"
   },
   "outputs": [],
   "source": [
    "! rm -f custom.tar custom.tar.gz\n",
    "! tar cvf custom.tar custom\n",
    "! gzip custom.tar\n",
    "! gsutil cp custom.tar.gz $BUCKET_URI/trainer_covertype.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_custom_pp_training_job:mbsdk"
   },
   "source": [
    "### 创建并运行自定义训练任务\n",
    "\n",
    "要训练一个自定义模型，您需要执行两个步骤：1）创建一个自定义训练任务，2）运行这个任务。\n",
    "\n",
    "#### 创建自定义训练任务\n",
    "\n",
    "使用`CustomTrainingJob`类创建一个自定义训练任务，具有以下参数：\n",
    "\n",
    "- `display_name`：自定义训练任务的可读名称。\n",
    "- `container_uri`：训练容器镜像。\n",
    "\n",
    "- `command`：容器中要调用的命令（如解释器）和脚本。\n",
    "\n",
    "- `model_serving_container_image_uri`：在模型部署时要与之一起使用的对应的服务容器。\n",
    "\n",
    "*注意：* 容器内的解释器和要调用的脚本可以在容器中被覆盖（即ENTRYPOINT）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rVEMz1xqox9X"
   },
   "outputs": [],
   "source": [
    "DISPLAY_NAME = \"covertype_\" + TIMESTAMP\n",
    "\n",
    "job = aiplatform.CustomContainerTrainingJob(\n",
    "    display_name=\"DISPLAY_NAME\",\n",
    "    container_uri=TRAIN_IMAGE,\n",
    "    command=[\"python3\", \"trainer/task.py\"],\n",
    "    model_serving_container_image_uri=DEPLOY_IMAGE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run_custom_container_training_job:no_model"
   },
   "source": [
    "#### 运行自定义容器训练任务\n",
    "\n",
    "接下来，您通过调用`run()`方法来运行自定义作业，以启动训练任务，传入以下参数：\n",
    "\n",
    "- `model_display_name`：当训练任务完成时，模型工件将自动上传为指定显示名称的`Vertex AI模型`资源。\n",
    "- `args`：要传递给训练脚本的命令行参数：\n",
    "    - `dataset-source`：CSV数据集文件的Cloud Storage位置。\n",
    "    - `model-name`：模型工件的文件名。\n",
    "    - `num-gpu-per-worker`：每个VM实例（worker）的GPU数量。\n",
    "    - `threads-per-worker`：每个VM实例的训练进程线程数。\n",
    "- `replica_count`：VM实例的数量。\n",
    "- `machine_type`：每个VM实例的机器类型。\n",
    "- `accelerator_type`：GPU的类型，如果有的话。\n",
    "- `accelerator_count`：每个VM实例的GPU数量，如果有的话。\n",
    "- `base_output_dir`：保存模型工件的Cloud Storage位置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5eaec67248bf"
   },
   "outputs": [],
   "source": [
    "DATASET_FILE = \"gs://vtx-datasets-public/cover_type_4Mrows.csv\"\n",
    "\n",
    "MODEL_DIR = f\"{BUCKET_URI}\"\n",
    "\n",
    "NGPU = 4\n",
    "\n",
    "CMDARGS = [\n",
    "    \"--dataset-source\",\n",
    "    DATASET_FILE,\n",
    "    \"--model-name\",\n",
    "    \"model.json\",\n",
    "    \"--num-gpu-per-worker\",\n",
    "    str(NGPU),\n",
    "    \"--threads-per-worker\",\n",
    "    \"4\",\n",
    "]\n",
    "\n",
    "model = job.run(\n",
    "    model_display_name=\"covertype_\" + TIMESTAMP,\n",
    "    args=CMDARGS,\n",
    "    replica_count=1,\n",
    "    machine_type=\"n1-standard-4\",\n",
    "    accelerator_type=\"NVIDIA_TESLA_T4\",\n",
    "    accelerator_count=NGPU,\n",
    "    base_output_dir=MODEL_DIR,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "delete_job"
   },
   "source": [
    "删除自定义训练作业\n",
    "\n",
    "在训练作业完成后，您可以使用 `delete()` 方法删除训练作业。在完成之前，训练作业可以使用 `cancel()` 方法取消。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "delete_job"
   },
   "outputs": [],
   "source": [
    "job.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8c9e104ae3ac"
   },
   "source": [
    "显示已保存模型工件的位置\n",
    "\n",
    "接下来，您可以显示云存储位置的内容，您的训练脚本保存了训练模型的工件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "88cf7be4cd4e"
   },
   "outputs": [],
   "source": [
    "! gsutil ls {MODEL_DIR}/model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "92c9a5641b60"
   },
   "source": [
    "### 部署模型\n",
    "\n",
    "接下来，您可以使用`deploy()`方法将您的XGBoost模型+服务容器部署到`Vertex AI Endpoint`资源。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4082d39f001b"
   },
   "outputs": [],
   "source": [
    "endpoint = model.deploy(machine_type=\"n1-standard-4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6ad37e4520c"
   },
   "source": [
    "准备预测请求\n",
    "\n",
    "接下来，您准备一个预测请求。为了演示目的，您使用数据集中的第一行（示例）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "758edcec727f"
   },
   "outputs": [],
   "source": [
    "output = ! gsutil cat {DATASET_FILE} | head -n2\n",
    "\n",
    "print(output[1])\n",
    "\n",
    "import json\n",
    "\n",
    "instance = json.loads(output[1])\n",
    "print(instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "149556c3b776"
   },
   "source": [
    "### 进行预测\n",
    "\n",
    "BLAH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d486a0ddff75"
   },
   "outputs": [],
   "source": [
    "prediction = endpoint.predict(instances=[instance])\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e18637e0af66"
   },
   "source": [
    "解除部署并删除“端点”资源"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "659cf7dd8d33"
   },
   "outputs": [],
   "source": [
    "endpoint.undeploy_all(force=True)\n",
    "endpoint.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e591b61fd8f5"
   },
   "source": [
    "#### 删除 `Model` 资源\n",
    "\n",
    "您可以使用 delete() 方法删除一个模型资源。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "00fa6a7b4f24"
   },
   "outputs": [],
   "source": [
    "model.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8b6599395122"
   },
   "source": [
    "清理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e7ffef3306e5"
   },
   "outputs": [],
   "source": [
    "delete_bucket = True\n",
    "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
    "    ! gsutil rm -r {BUCKET_URI}\n",
    "\n",
    "! rm -rf custom prediction custom.tar.gz\n",
    "\n",
    "# TODO: delete repo, delete images"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "get_started_with_distributed_training_xgboost.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
