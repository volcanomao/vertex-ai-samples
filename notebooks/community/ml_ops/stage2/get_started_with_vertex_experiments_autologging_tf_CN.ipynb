{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "copyright"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "在GCP上进行端到端的ML：MLOps阶段2：使用Vertex AI实验开始自动记录TensorFlow模型\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage2/get_started_with_vertex_experiments_autologging_tf.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> 在Colab中运行\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage2/get_started_with_vertex_experiments_autologging_tf.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      在GitHub上查看\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/ml_ops/stage2/get_started_with_vertex_experiments_autologging_tf.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      在Vertex AI Workbench中打开\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "<br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "overview:automl"
   },
   "source": [
    "## 概述\n",
    "\n",
    "本教程演示了如何使用DIY代码使用`Vertex AI Experiments`来实现对实验参数和指标的自动记录。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "objective:automl,training,batch_prediction"
   },
   "source": [
    "### 目标\n",
    "\n",
    "在本教程中，您将学习如何创建一个用于训练 TensorFlow 模型的实验，并使用附带的自助（DIY）代码自动记录参数和指标。\n",
    "\n",
    "本教程使用以下 Google Cloud ML 服务和资源：\n",
    "\n",
    "- `Vertex AI Experiments`\n",
    "\n",
    "执行的步骤包括：\n",
    "\n",
    "- 构建自助记录代码。\n",
    "- 构建具有调用自助记录功能的 TensorFlow Sequential 模型的训练包。\n",
    "- 训练模型。\n",
    "- 查看实验结果。\n",
    "- 构建具有调用自助记录功能的 TensorFlow Functional 模型的训练包。\n",
    "- 比较实验结果。\n",
    "- 删除实验。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2739272aae1b"
   },
   "source": [
    "数据集\n",
    "\n",
    "本教程使用的数据集是[Boston房价数据集](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html)。该教程使用的数据集版本已经整合到TensorFlow中。训练好的模型可以预测房屋价格的中位数，单位为1千美元。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "costs"
   },
   "source": [
    "费用\n",
    "\n",
    "本教程使用 Google Cloud 的可计费组件：\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "了解 [Vertex AI 价格](https://cloud.google.com/vertex-ai/pricing) 和 [Cloud Storage 价格](https://cloud.google.com/storage/pricing)，并使用 [定价计算器](https://cloud.google.com/products/calculator/) 根据您的预期使用量生成费用估算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_local"
   },
   "source": [
    "### 设置本地开发环境\n",
    "\n",
    "如果您正在使用Colab或Vertex Workbench AI笔记本，您的环境已经满足运行此笔记本的所有要求。您可以跳过这一步。\n",
    "\n",
    "否则，请确保您的环境满足此笔记本的要求。您需要以下内容：\n",
    "\n",
    "- 云存储SDK\n",
    "- Git\n",
    "- Python 3\n",
    "- virtualenv\n",
    "- 在使用Python 3的虚拟环境中运行Jupyter笔记本\n",
    "\n",
    "[设置Python开发环境](https://cloud.google.com/python/setup)和[Jupyter安装指南](https://jupyter.org/install)提供了满足这些要求的详细说明。以下步骤提供了简要的说明：\n",
    "\n",
    "1. [安装和初始化SDK](https://cloud.google.com/sdk/docs/)。\n",
    "\n",
    "2. [安装Python 3](https://cloud.google.com/python/setup#installing_python)。\n",
    "\n",
    "3. [安装virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv)并创建一个使用Python 3的虚拟环境。激活虚拟环境。\n",
    "\n",
    "4. 要安装Jupyter，在终端shell中运行`pip3 install jupyter`。\n",
    "\n",
    "5. 要启动Jupyter，在终端shell中运行`jupyter notebook`。\n",
    "\n",
    "6. 在Jupyter Notebook仪表板中打开此笔记本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_aip:mbsdk"
   },
   "source": [
    "## 安装\n",
    "\n",
    "安装以下软件包以执行这个笔记本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Vertex AI Workbench Notebook product has specific requirements\n",
    "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\")\n",
    "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
    "    \"/opt/deeplearning/metadata/env_version\"\n",
    ")\n",
    "\n",
    "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_WORKBENCH_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\"\n",
    "\n",
    "! pip3 install --upgrade --quiet {USER_FLAG} google-cloud-aiplatform \\\n",
    "                                             tensorflow==2.5 \\\n",
    "                                             numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "restart"
   },
   "source": [
    "### 重新启动内核\n",
    "\n",
    "在安装了额外的包之后，您需要重新启动笔记本内核，以便它能够找到这些包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "restart"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "before_you_begin:nogpu"
   },
   "source": [
    "## 在开始之前\n",
    "\n",
    "### GPU运行时\n",
    "\n",
    "此教程不需要GPU运行时。\n",
    "\n",
    "### 设置您的Google Cloud项目\n",
    "\n",
    "**无论您使用什么笔记本环境，以下步骤都是必需的。**\n",
    "\n",
    "1. [选择或创建一个Google Cloud项目](https://console.cloud.google.com/cloud-resource-manager)。当您第一次创建帐户时，您将获得$300的免费信用，可用于支付计算/存储成本。\n",
    "\n",
    "2. [确保为您的项目启用了计费](https://cloud.google.com/billing/docs/how-to/modify-project)。\n",
    "\n",
    "3. [启用以下API：Vertex AI APIs、Compute Engine APIs和Cloud Storage](https://console.cloud.google.com/flows/enableapi?apiid=ml.googleapis.com,compute_component,storage-component.googleapis.com)。\n",
    "\n",
    "4. 如果您在本地运行此笔记本，您需要安装[Cloud SDK]。\n",
    "\n",
    "5. 在下面的单元格中输入您的项目ID。然后运行此单元格，以确保Cloud SDK在此笔记本中的所有命令中使用正确的项目。\n",
    "\n",
    "**注意**：Jupyter运行以`!`为前缀的行作为shell命令，并且会以`$`为前缀的Python变量进行内插操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "project_id"
   },
   "source": [
    "设置您的项目ID\n",
    "\n",
    "**如果您不知道您的项目ID**，您可以使用`gcloud`来获取您的项目ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_project_id"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "autoset_project_id"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID:\", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_gcloud_project_id"
   },
   "outputs": [],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "#### 区域\n",
    "\n",
    "您还可以更改 `REGION` 变量，该变量用于本笔记本的其余操作。以下是Vertex AI支持的区域。我们建议您选择离您最近的区域。\n",
    "\n",
    "- 美洲：`us-central1`\n",
    "- 欧洲：`europe-west4`\n",
    "- 亚太地区：`asia-east1`\n",
    "\n",
    "您不能在Vertex AI中使用多区域存储桶进行训练。并非所有区域都支持所有的Vertex AI服务。\n",
    "\n",
    "了解有关[Vertex AI区域](https://cloud.google.com/vertex-ai/docs/general/locations)的更多信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "region"
   },
   "outputs": [],
   "source": [
    "REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
    "\n",
    "if REGION == \"[your-region]\":\n",
    "    REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "timestamp"
   },
   "source": [
    "UUID\n",
    "\n",
    "如果您参加的是实时教程会话，您可能正在使用共享的测试账户或项目。为避免在创建的资源上发生名称冲突，您需要为每个实例会话创建一个UUID，并将其附加到本教程中创建的资源名称上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "timestamp"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "\n",
    "# Generate a uuid of a specifed length(default=8)\n",
    "def generate_uuid(length: int = 8) -> str:\n",
    "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
    "\n",
    "\n",
    "UUID = generate_uuid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcp_authenticate"
   },
   "source": [
    "### 验证您的谷歌云账户\n",
    "\n",
    "**如果您正在使用Vertex AI Workbench笔记本**，您的环境已经通过身份验证。\n",
    "\n",
    "**如果您正在使用Colab**，请运行下面的单元格，并按照提示进行oAuth身份验证。\n",
    "\n",
    "**否则**，请按照以下步骤操作：\n",
    "\n",
    "在Cloud Console中，转到[创建服务账号密钥](https://console.cloud.google.com/apis/credentials/serviceaccountkey)页面。\n",
    "\n",
    "**点击创建服务账号**。\n",
    "\n",
    "在**服务账号名称**字段中输入一个名称，然后点击**创建**。\n",
    "\n",
    "在**授予此服务账号对项目的访问权限**部分，点击角色下拉列表。在筛选框中键入\"Vertex\"，然后选择**Vertex管理员**。在筛选框中键入\"存储对象管理员\"，然后选择**存储对象管理员**。\n",
    "\n",
    "点击创建。包含您密钥的JSON文件将下载到您的本地环境中。\n",
    "\n",
    "在下面的单元格中将您的服务账号密钥路径输入为GOOGLE_APPLICATION_CREDENTIALS变量，并运行该单元格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gcp_authenticate"
   },
   "outputs": [],
   "source": [
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# If on Vertex AI Workbench, then don't execute this code\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\") and not os.getenv(\n",
    "    \"DL_ANACONDA_HOME\"\n",
    "):\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\"):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_vars"
   },
   "source": [
    "导入库并定义常量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "59963fb7178f"
   },
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aiplatform\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk"
   },
   "source": [
    "初始化用于您的项目和相应存储桶的 Python 的 Vertex AI SDK。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ae8f31c8c617"
   },
   "source": [
    "## TensorFlow Keras模型自动记录的DIY代码\n",
    "\n",
    "下面的代码使用Keras API 实现了对TensorFlow模型的自动记录。\n",
    "\n",
    "- `autologging()`: 初始化实验并使用堆注入来替换堆上的TF.keras `Sequential`和`Model`符号，分别用重定向包装类`VertexTFSequential`和`VertexTFModel`。\n",
    "\n",
    "- `VertexTFSequential`: tf.keras.Sequential类的子类。\n",
    "    - `compile()`: 覆盖了超类的方法。自动记录指定的超参数并调用底层的`compile()`方法。\n",
    "    - `fit()`: 覆盖了超类的方法。自动记录指定的超参数，调用底层的`fit()`方法，并记录结果指标。\n",
    "    - `evaluate()`: 覆盖了超类的方法。调用底层的`evaluate()`方法，并记录结果指标。\n",
    "- `VertexTFModel`: tf.keras.Model类的子类。\n",
    "- `VertexTFHelper`: 用于Sequential和Functional模型的通用记录方法的类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8eb012e5d7ef"
   },
   "outputs": [],
   "source": [
    "def autolog(\n",
    "    project: str = None,\n",
    "    location: str = None,\n",
    "    staging_bucket: str = None,\n",
    "    experiment: str = None,\n",
    "    run: str = None,\n",
    "    framework: str = \"tf\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Enable automatic logging of parameters and metrics in Vertex AI Experiments,\n",
    "    for corresponding framework.\n",
    "\n",
    "        project: The project ID\n",
    "        location : The region\n",
    "        staging_bucket: temporary bucket\n",
    "        experiment: The name of the experiment\n",
    "        run: The name of the run within the experiment\n",
    "        framework: The ML framework for which a model is being trained.\n",
    "    \"\"\"\n",
    "    # autologging\n",
    "    if framework == \"tf\":\n",
    "        try:\n",
    "            globals()[\"Sequential\"] = VertexTFSequential\n",
    "            if \"tf\" in globals():\n",
    "                tf.keras.Sequential = VertexTFSequential\n",
    "            if \"tensorflow\" in globals():\n",
    "                tensorflow.keras.Sequential = VertexTFSequential\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            globals()[\"Model\"] = VertexTFModel\n",
    "            if \"tf\" in globals():\n",
    "                tf.keras.Model = VertexTFModel\n",
    "            if \"tensorflow\" in globals():\n",
    "                tensorflow.keras.Model = VertexTFModel\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    if project:\n",
    "        aiplatform.init(\n",
    "            project=project, location=location, staging_bucket=staging_bucket\n",
    "        )\n",
    "\n",
    "    if experiment:\n",
    "        aiplatform.init(experiment=experiment)\n",
    "    if run:\n",
    "        aiplatform.start_run(run)\n",
    "\n",
    "\n",
    "class VertexTFSequential(tf.keras.Sequential):\n",
    "    \"\"\"\n",
    "    Sublass of the tensorflow.keras.Sequential model type. Overrides with pass thru of\n",
    "    underlying super class methods to automatically log parameters/metrics for Vertex AI experiments.\n",
    "\n",
    "        compile():\n",
    "        fit():\n",
    "        evaluate():\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        return super().__init__(layers)\n",
    "\n",
    "    def compile(\n",
    "        self,\n",
    "        optimizer=\"rmsprop\",\n",
    "        loss=None,\n",
    "        metrics=None,\n",
    "        loss_weights=None,\n",
    "        weighted_metrics=None,\n",
    "        run_eagerly=None,\n",
    "        steps_per_execution=None,\n",
    "    ):\n",
    "        try:\n",
    "            learning_rate = optimizer.learning_rate.numpy()\n",
    "            aiplatform.log_params({\"train.learning_rate\": float(learning_rate)})\n",
    "        except:\n",
    "            pass\n",
    "        return super().compile(\n",
    "            loss=loss,\n",
    "            optimizer=optimizer,\n",
    "            metrics=metrics,\n",
    "            loss_weights=loss_weights,\n",
    "            weighted_metrics=weighted_metrics,\n",
    "            run_eagerly=run_eagerly,\n",
    "            steps_per_execution=steps_per_execution,\n",
    "        )\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        x=None,\n",
    "        y=None,\n",
    "        batch_size=None,\n",
    "        epochs=1,\n",
    "        verbose=\"auto\",\n",
    "        callbacks=None,\n",
    "        validation_split=0.0,\n",
    "        validation_data=None,\n",
    "        shuffle=True,\n",
    "        class_weight=None,\n",
    "        sample_weight=None,\n",
    "        initial_epoch=0,\n",
    "        steps_per_epoch=None,\n",
    "        validation_steps=None,\n",
    "        validation_batch_size=None,\n",
    "        validation_freq=1,\n",
    "        max_queue_size=10,\n",
    "        workers=1,\n",
    "        use_multiprocessing=False,\n",
    "    ):\n",
    "        aiplatform.log_params({\"train.epochs\": int(epochs)})\n",
    "        if batch_size:\n",
    "            aiplatform.log_params({\"train.batch_size\": int(batch_size)})\n",
    "        if steps_per_epoch:\n",
    "            aiplatform.log_params({\"train.steps\": int(steps_per_epoch)})\n",
    "\n",
    "        history = super().fit(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            verbose=verbose,\n",
    "            callbacks=callbacks,\n",
    "            validation_split=validation_split,\n",
    "            validation_data=validation_data,\n",
    "            shuffle=shuffle,\n",
    "            class_weight=class_weight,\n",
    "            sample_weight=sample_weight,\n",
    "            initial_epoch=initial_epoch,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            validation_steps=validation_steps,\n",
    "            validation_batch_size=validation_batch_size,\n",
    "            validation_freq=validation_freq,\n",
    "            max_queue_size=max_queue_size,\n",
    "            workers=workers,\n",
    "            use_multiprocessing=use_multiprocessing,\n",
    "        )\n",
    "\n",
    "        TFHelper().model_size(self)\n",
    "\n",
    "        for key, val in history.history.items():\n",
    "            aiplatform.log_metrics({f\"train.{key}\": val[-1]})\n",
    "        return history\n",
    "\n",
    "    def evaluate(\n",
    "        self,\n",
    "        x=None,\n",
    "        y=None,\n",
    "        batch_size=None,\n",
    "        verbose=1,\n",
    "        sample_weight=None,\n",
    "        steps=None,\n",
    "        callbacks=None,\n",
    "        max_queue_size=10,\n",
    "        workers=1,\n",
    "        use_multiprocessing=False,\n",
    "        return_dict=False,\n",
    "    ):\n",
    "\n",
    "        metrics = super().evaluate(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            batch_size=batch_size,\n",
    "            verbose=verbose,\n",
    "            sample_weight=sample_weight,\n",
    "            steps=steps,\n",
    "            callbacks=callbacks,\n",
    "            max_queue_size=max_queue_size,\n",
    "            workers=workers,\n",
    "            use_multiprocessing=use_multiprocessing,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        aiplatform.log_metrics({\"eval.loss\": metrics[0]})\n",
    "        for _ in range(1, len(metrics)):\n",
    "            aiplatform.log_metrics({\"eval.metric\": metrics[_]})\n",
    "        return metrics\n",
    "\n",
    "\n",
    "class VertexTFModel(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Sublass of the tensorflow.keras.Model model type. Overrides with pass thru of\n",
    "    underlying super class methods to automatically log parameters/metrics for Vertex AI experiments.\n",
    "\n",
    "        compile():\n",
    "        fit():\n",
    "        evaluate():\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inputs, outputs):\n",
    "        return super().__init__(inputs, outputs)\n",
    "\n",
    "    def compile(\n",
    "        self,\n",
    "        optimizer=\"rmsprop\",\n",
    "        loss=None,\n",
    "        metrics=None,\n",
    "        loss_weights=None,\n",
    "        weighted_metrics=None,\n",
    "        run_eagerly=None,\n",
    "        steps_per_execution=None,\n",
    "    ):\n",
    "        try:\n",
    "            learning_rate = optimizer.learning_rate.numpy()\n",
    "            aiplatform.log_params({\"train.learning_rate\": float(learning_rate)})\n",
    "        except:\n",
    "            pass\n",
    "        return super().compile(\n",
    "            loss=loss,\n",
    "            optimizer=optimizer,\n",
    "            metrics=metrics,\n",
    "            loss_weights=loss_weights,\n",
    "            weighted_metrics=weighted_metrics,\n",
    "            run_eagerly=run_eagerly,\n",
    "            steps_per_execution=steps_per_execution,\n",
    "        )\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        x=None,\n",
    "        y=None,\n",
    "        batch_size=None,\n",
    "        epochs=1,\n",
    "        verbose=\"auto\",\n",
    "        callbacks=None,\n",
    "        validation_split=0.0,\n",
    "        validation_data=None,\n",
    "        shuffle=True,\n",
    "        class_weight=None,\n",
    "        sample_weight=None,\n",
    "        initial_epoch=0,\n",
    "        steps_per_epoch=None,\n",
    "        validation_steps=None,\n",
    "        validation_batch_size=None,\n",
    "        validation_freq=1,\n",
    "        max_queue_size=10,\n",
    "        workers=1,\n",
    "        use_multiprocessing=False,\n",
    "    ):\n",
    "        aiplatform.log_params({\"train.epochs\": int(epochs)})\n",
    "        if batch_size:\n",
    "            aiplatform.log_params({\"train.batch_size\": int(batch_size)})\n",
    "        if steps_per_epoch:\n",
    "            aiplatform.log_params({\"train.steps\": int(steps_per_epoch)})\n",
    "\n",
    "        history = super().fit(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            verbose=verbose,\n",
    "            callbacks=callbacks,\n",
    "            validation_split=validation_split,\n",
    "            validation_data=validation_data,\n",
    "            shuffle=shuffle,\n",
    "            class_weight=class_weight,\n",
    "            sample_weight=sample_weight,\n",
    "            initial_epoch=initial_epoch,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            validation_steps=validation_steps,\n",
    "            validation_batch_size=validation_batch_size,\n",
    "            validation_freq=validation_freq,\n",
    "            max_queue_size=max_queue_size,\n",
    "            workers=workers,\n",
    "            use_multiprocessing=use_multiprocessing,\n",
    "        )\n",
    "\n",
    "        TFHelper().model_size(self)\n",
    "\n",
    "        for key, val in history.history.items():\n",
    "            aiplatform.log_metrics({f\"train.{key}\": val[-1]})\n",
    "        return history\n",
    "\n",
    "    def evaluate(\n",
    "        self,\n",
    "        x=None,\n",
    "        y=None,\n",
    "        batch_size=None,\n",
    "        verbose=1,\n",
    "        sample_weight=None,\n",
    "        steps=None,\n",
    "        callbacks=None,\n",
    "        max_queue_size=10,\n",
    "        workers=1,\n",
    "        use_multiprocessing=False,\n",
    "        return_dict=False,\n",
    "    ):\n",
    "\n",
    "        metrics = super().evaluate(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            batch_size=batch_size,\n",
    "            verbose=verbose,\n",
    "            sample_weight=sample_weight,\n",
    "            steps=steps,\n",
    "            callbacks=callbacks,\n",
    "            max_queue_size=max_queue_size,\n",
    "            workers=workers,\n",
    "            use_multiprocessing=use_multiprocessing,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        aiplatform.log_metrics({\"eval.loss\": metrics[0]})\n",
    "        for _ in range(1, len(metrics)):\n",
    "            aiplatform.log_metrics({\"eval.metric\": metrics[_]})\n",
    "        return metrics\n",
    "\n",
    "\n",
    "class TFHelper(object):\n",
    "    def model_size(self, model):\n",
    "        \"\"\"\n",
    "        Get the memory footprint as measured by the number of weights\n",
    "        \"\"\"\n",
    "\n",
    "        def get_size(weights) -> int:\n",
    "            n = 0\n",
    "            for weight in weights:\n",
    "                try:\n",
    "                    n += len(weight)\n",
    "                    n += get_size(weight)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            return n\n",
    "\n",
    "        n = get_size(model.get_weights())\n",
    "        aiplatform.log_metrics({\"n_weights\": n})\n",
    "        return n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ce76826902c0"
   },
   "source": [
    "使用Vertex AI实验训练一个TensorFlow Sequential模型\n",
    "\n",
    "在下面的代码中，您会构建、训练和评估一个TensorFlow Sequential的表格模型。Python脚本包括以下调用以集成`Vertex AI实验`：\n",
    "\n",
    "- 命令行参数：参数`experiment`和`run`用来传递实验和运行名称。\n",
    "- `autologging()`: 初始化实验并进行堆注入。\n",
    "- `aiplatform.start_execution()`: 初始化一个上下文用于链接工件。\n",
    "- `aiplatform.end_run()`: 结束实验。\n",
    "\n",
    "*注意:* 初始化器`Sequential`会被堆注入重定向到`VertexTFSequential`。当对compile()、fit()和evaluate()方法进行后续调用时，它们将作为相应的`VertexTFSequential`方法执行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "427846783ed6"
   },
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = f\"myexperiment{UUID}\"\n",
    "RUN_NAME = \"run-1\"\n",
    "\n",
    "\n",
    "def make_dataset():\n",
    "\n",
    "    # Scaling Boston Housing data features\n",
    "    def scale(feature):\n",
    "        max = np.max(feature)\n",
    "        feature = (feature / max).astype(np.float)\n",
    "        return feature, max\n",
    "\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.boston_housing.load_data(\n",
    "        path=\"boston_housing.npz\", test_split=0.2, seed=113\n",
    "    )\n",
    "    params = []\n",
    "\n",
    "    for _ in range(13):\n",
    "        x_train[_], max = scale(x_train[_])\n",
    "        x_test[_], _ = scale(x_test[_])\n",
    "        params.append(max)\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "\n",
    "# Build the Keras model\n",
    "def build_and_compile_dnn_model(lr):\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.Dense(128, activation=\"relu\", input_shape=(13,)),\n",
    "            tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(1, activation=\"linear\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"mse\",\n",
    "        optimizer=tf.keras.optimizers.RMSprop(learning_rate=lr),\n",
    "        metrics=[tf.keras.metrics.RootMeanSquaredError()],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "# autologging\n",
    "autolog(experiment=EXPERIMENT_NAME, run=RUN_NAME)\n",
    "\n",
    "with aiplatform.start_execution(\n",
    "    schema_title=\"system.ContainerExecution\", display_name=\"example_training\"\n",
    ") as execution:\n",
    "    BATCH_SIZE = 16\n",
    "\n",
    "    model = build_and_compile_dnn_model(lr=0.01)\n",
    "\n",
    "    # Train the model\n",
    "    (x_train, y_train), (x_test, y_test) = make_dataset()\n",
    "    model.fit(x_train, y_train, epochs=10, batch_size=BATCH_SIZE)\n",
    "\n",
    "    model.evaluate(x_test, y_test)\n",
    "\n",
    "aiplatform.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5f40912e6500"
   },
   "source": [
    "#### 获取实验结果\n",
    "\n",
    "接下来，您可以使用实验名称作为参数传递给`get_experiment_df()`方法来获取实验结果作为pandas数据框。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7e9671712230"
   },
   "outputs": [],
   "source": [
    "experiment_df = aiplatform.get_experiment_df()\n",
    "experiment_df = experiment_df[experiment_df.experiment_name == EXPERIMENT_NAME]\n",
    "experiment_df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ce76826902c0"
   },
   "source": [
    "### 使用Vertex AI Experiments训练一个TensorFlow功能性模型\n",
    "\n",
    "在以下代码中，您将建立、训练和评估一个TensorFlow功能性表格模型。Python脚本包括以下调用以集成`Vertex AI Experiments`：\n",
    "\n",
    "- 命令行参数: 参数`experiment`和`run`用于传递实验和运行名称给实验。\n",
    "- `autologging()`: 初始化实验并进行堆注入。\n",
    "- `aiplatform.start_execution()`: 初始化一个上下文以链接工件。\n",
    "- `aiplatform.end_run()`: 结束实验。\n",
    "\n",
    "*注意:* 初始化器`Model`将通过堆注入重定向到`VertexTFModel`。当对compile()、fit()和evaluate()方法进行后续调用时，它们将作为相应的`VertexTFModel`方法执行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "427846783ed6"
   },
   "outputs": [],
   "source": [
    "RUN_NAME = \"run-2\"\n",
    "\n",
    "\n",
    "def make_dataset():\n",
    "\n",
    "    # Scaling Boston Housing data features\n",
    "    def scale(feature):\n",
    "        max = np.max(feature)\n",
    "        feature = (feature / max).astype(np.float)\n",
    "        return feature, max\n",
    "\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.boston_housing.load_data(\n",
    "        path=\"boston_housing.npz\", test_split=0.2, seed=113\n",
    "    )\n",
    "    params = []\n",
    "\n",
    "    for _ in range(13):\n",
    "        x_train[_], max = scale(x_train[_])\n",
    "        x_test[_], _ = scale(x_test[_])\n",
    "        params.append(max)\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "\n",
    "# Build the Keras model\n",
    "def build_and_compile_dnn_model(lr):\n",
    "    inputs = tf.keras.Input(shape=(13,))\n",
    "    x = tf.keras.layers.Dense(128, activation=\"relu\")(inputs)\n",
    "    x = tf.keras.layers.Dense(128, activation=\"relu\")(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation=\"linear\")(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"mse\",\n",
    "        optimizer=tf.keras.optimizers.RMSprop(learning_rate=lr),\n",
    "        metrics=[tf.keras.metrics.RootMeanSquaredError()],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "# autologging\n",
    "autolog(experiment=EXPERIMENT_NAME, run=RUN_NAME)\n",
    "\n",
    "with aiplatform.start_execution(\n",
    "    schema_title=\"system.ContainerExecution\", display_name=\"example_training\"\n",
    ") as execution:\n",
    "    BATCH_SIZE = 16\n",
    "\n",
    "    model = build_and_compile_dnn_model(lr=0.01)\n",
    "\n",
    "    # Train the model\n",
    "    (x_train, y_train), (x_test, y_test) = make_dataset()\n",
    "    model.fit(x_train, y_train, epochs=10, batch_size=BATCH_SIZE)\n",
    "\n",
    "    model.evaluate(x_test, y_test)\n",
    "\n",
    "aiplatform.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5f40912e6500"
   },
   "source": [
    "#### 获取实验结果\n",
    "\n",
    "接下来，您可以将实验名称作为参数传递给方法`get_experiment_df()`，以将实验结果获取为pandas dataframe。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7e9671712230"
   },
   "outputs": [],
   "source": [
    "experiment_df = aiplatform.get_experiment_df()\n",
    "experiment_df = experiment_df[experiment_df.experiment_name == EXPERIMENT_NAME]\n",
    "experiment_df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e508c159d712"
   },
   "source": [
    "由于实验是在培训脚本中创建的，要删除实验，您可以使用`list()`方法获取项目中所有实验，然后按实验名称进行过滤。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1a1b5fcbfde0"
   },
   "outputs": [],
   "source": [
    "experiments = aiplatform.Experiment.list()\n",
    "for experiment in experiments:\n",
    "    if experiment.name == EXPERIMENT_NAME:\n",
    "        experiment.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleanup:mbsdk"
   },
   "source": [
    "清理\n",
    "\n",
    "要清理此项目中使用的所有Google Cloud资源，您可以删除用于本教程的[Google Cloud项目](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects)。\n",
    "\n",
    "否则，您可以删除在本教程中创建的单个资源。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9eb897e0e728"
   },
   "outputs": [],
   "source": [
    "# There are no resources to cleanup"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "get_started_with_vertex_experiments_autologging_tf.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
