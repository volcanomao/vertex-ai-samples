{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "copyright"
   },
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title:generic,gcp"
   },
   "source": [
    "# GCP上的E2E ML：MLOps阶段2：实验\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage2/mlops_experimentation.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      在GitHub上查看\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/ml_ops/stage2/mlops_experimentation.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      在Vertex AI Workbench中打开\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "<br/><br/><br/>\n",
    "\n",
    "*注意：该笔记本不支持在Colab中执行*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "overview:mlops"
   },
   "source": [
    "## 概述\n",
    "\n",
    "本教程演示了如何在 Google Cloud 上使用 Vertex AI 进行端到端的生产环境 MLOps。本教程涵盖了阶段 2：实验。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "objective:mlops,stage2,tabular"
   },
   "source": [
    "### 目标\n",
    "\n",
    "在本教程中，您将创建一个MLOps阶段2：实验过程。\n",
    "\n",
    "本教程使用以下Vertex AI：\n",
    "\n",
    "- `Vertex AI Datasets`\n",
    "- `Vertex AI Models`\n",
    "- `Vertex AI AutoML`\n",
    "- `Vertex AI Training`\n",
    "- `Vertex AI TensorBoard`\n",
    "- `Vertex AI Vizier`\n",
    "- `Vertex AI Batch Prediction`\n",
    "\n",
    "执行的步骤包括：\n",
    "\n",
    "- 查看在阶段1创建的 `Dataset` 资源。\n",
    "- 在后台训练AutoML表格二元分类器模型。\n",
    "- 构建实验性模型架构。\n",
    "- 为 `Dataset` 资源构建自定义训练包。\n",
    "- 在本地测试自定义训练包。\n",
    "- 使用Vertex AI Training在云端测试自定义训练包。\n",
    "- 使用Vertex AI Vizier对模型训练进行超参数调整。\n",
    "- 使用Vertex AI Training训练自定义模型。\n",
    "- 为自定义模型添加用于在线/批处理预测的服务函数。\n",
    "- 使用服务函数测试自定义模型。\n",
    "- 使用Vertex AI Batch Prediction评估自定义模型。\n",
    "- 等待AutoML训练作业完成。\n",
    "- 使用相同的评估片段使用Vertex AI Batch Prediction评估AutoML模型。\n",
    "- 将AutoML模型的评估结果设置为基准。\n",
    "- 如果自定义模型的评估低于基准，请继续对自定义模型进行实验。\n",
    "- 如果自定义模型的评估高于基准，请将模型保存为第一个最佳模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "recommendation:mlops,stage2,tabular"
   },
   "source": [
    "### 推荐\n",
    "\n",
    "在谷歌云上进行端到端（E2E）MLOps实验时，建议采用以下关于结构化（表格）数据的最佳实践：\n",
    "\n",
    " - 使用AutoML确定基线评估。\n",
    " - 设计并构建模型架构。\n",
    "     - 将未经训练的模型架构上传为Vertex AI模型资源。\n",
    "\n",
    "\n",
    " - 构建一个可以在本地运行和作为Vertex AI训练作业运行的训练包。\n",
    "     - 将训练包分解为：数据、模型、训练和任务Python模块。\n",
    "     - 从Vertex AI数据集资源的用户元数据获取转换后的训练数据位置。\n",
    "     - 从Vertex AI模型资源获取模型工件的位置。\n",
    "     - 在训练包中包括初始化Vertex AI实验和相应运行。\n",
    "     - 记录实验的超参数和训练参数。\n",
    "     - 添加用于提前停止、TensorBoard和超参数调整的回调函数，其中超参数调整是一个命令行选项。\n",
    "\n",
    "\n",
    " - 用少量的时期在本地测试训练包。\n",
    " - 使用Vertex AI训练测试训练包。\n",
    " - 使用Vertex AI超参数调整进行超参数调整。\n",
    " - 使用Vertex AI训练对定制模型进行全面训练。\n",
    "     - 记录实验/运行的超参数值。\n",
    "\n",
    "\n",
    " - 评估定制模型。\n",
    "     - 单一评估切片，与AutoML相同的指标\n",
    "         - 将评估添加到训练包中，并在用于训练的云存储存储桶中返回结果文件\n",
    "     - 定制评估切片，定制指标\n",
    "         - 将定制评估切片作为AutoML和定制模型的Vertex AI批量预测\n",
    "         - 对批处理作业的结果执行定制指标\n",
    "\n",
    "\n",
    " - 将定制模型指标与AutoML基线进行比较\n",
    "     - 如果低于基线，则继续实验\n",
    "     - 如果高于基线，则将模型上传为新的基线，并将评估结果与模型保存。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset:bq,chicago,lbn"
   },
   "source": [
    "### 数据集\n",
    "\n",
    "本教程使用的数据集是[芝加哥出租车](https://www.kaggle.com/chicago/chicago-taxi-trips-bq)数据集。本教程中使用的数据集版本存储在一个公共BigQuery表中。训练好的模型可以预测一个人是否会给出租车费小费。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "costs"
   },
   "source": [
    "### 成本\n",
    "\n",
    "此教程使用 Google Cloud 的计费组件：\n",
    "\n",
    "* BigQuery\n",
    "* Vision API\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "了解[Vertex AI\n",
    "价格](https://cloud.google.com/vertex-ai/pricing)，[Vision API 价格](https://cloud.google.com/vision/pricing)，[BigQuery 价格](https://cloud.google.com/bigquery/pricing)，[Cloud Storage 价格](https://cloud.google.com/storage/pricing)，并使用[Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "根据您的预期使用量生成成本估算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_mlops"
   },
   "source": [
    "## 安装\n",
    "\n",
    "安装执行MLOps笔记本所需的软件包*仅需一次*。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_mlops"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Vertex AI Workbench Notebook product has specific requirements\n",
    "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\") and not os.getenv(\"VIRTUAL_ENV\")\n",
    "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
    "    \"/opt/deeplearning/metadata/env_version\"\n",
    ")\n",
    "\n",
    "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_WORKBENCH_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\"\n",
    "\n",
    "ONCE_ONLY = False\n",
    "if ONCE_ONLY:\n",
    "    ! pip3 install -U {USER_FLAG} -q tensorflow==2.5 \\\n",
    "                                     tensorflow-data-validation==1.2 \\\n",
    "                                     tensorflow-transform==1.2 \\\n",
    "                                     tensorflow-io==0.18 \n",
    "    \n",
    "    ! pip3 install --upgrade {USER_FLAG} -q google-cloud-aiplatform[tensorboard] \\\n",
    "                                            google-cloud-pipeline-components \\\n",
    "                                            google-cloud-bigquery \\\n",
    "                                            google-cloud-logging \\\n",
    "                                            apache-beam[gcp] \\\n",
    "                                            pyarrow \\\n",
    "                                            cloudml-hypertune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "restart"
   },
   "source": [
    "重新启动内核\n",
    "\n",
    "安装了额外的包之后，您需要重新启动笔记本内核，以便它可以找到这些包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "restart"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2721ef0202d9"
   },
   "source": [
    "## 开始之前\n",
    "\n",
    "### 设置您的 Google Cloud 项目\n",
    "\n",
    "**无论您的笔记本环境如何，以下步骤都是必须的。**\n",
    "\n",
    "1. [选择或创建一个 Google Cloud 项目](https://console.cloud.google.com/cloud-resource-manager)。当您首次创建帐户时，您将获得 $300 的免费信用额度用于计算/存储成本。\n",
    "\n",
    "2. [确保您的项目已启用计费](https://cloud.google.com/billing/docs/how-to/modify-project)。\n",
    "\n",
    "3. [启用 Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com)。\n",
    "\n",
    "4. 如果您在本地运行此笔记本，您需要安装 [Cloud SDK](https://cloud.google.com/sdk)。\n",
    "\n",
    "5. 在下面的单元格中输入您的项目 ID。然后运行该单元格，确保 Cloud SDK 为本笔记本中的所有命令使用正确的项目。\n",
    "\n",
    "**注意**：Jupyter 运行前缀为 `!` 的行作为 shell 命令，并将以 `$` 为前缀的 Python 变量插入这些命令中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "project_id"
   },
   "source": [
    "将您的项目ID设置为\n",
    "\n",
    "**如果您不知道您的项目ID**，您可以使用`gcloud`来获取您的项目ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_project_id"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "autoset_project_id"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID:\", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_gcloud_project_id"
   },
   "outputs": [],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "区域\n",
    "\n",
    "您还可以更改“REGION”变量，该变量用于本笔记本其余部分的操作。以下是Vertex AI支持的区域。我们建议您选择离您最近的区域。\n",
    "\n",
    "- 美洲：`us-central1`\n",
    "- 欧洲：`europe-west4`\n",
    "- 亚太地区：`asia-east1`\n",
    "\n",
    "您可能不能使用多区域存储桶来训练Vertex AI。并非所有区域都支持所有Vertex AI服务。\n",
    "\n",
    "了解更多关于[Vertex AI区域](https://cloud.google.com/vertex-ai/docs/general/locations)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "region"
   },
   "outputs": [],
   "source": [
    "REGION = \"[your-region]\"  # @param {type:\"string\"}\n",
    "if REGION == \"[your-region]\":\n",
    "    REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "timestamp"
   },
   "source": [
    "时间戳\n",
    "\n",
    "如果您正在进行实时教程会话，您可能正在使用共享测试账户或项目。为了避免在创建的资源上发生名称冲突，您为每个实例会话创建一个时间戳，并将时间戳附加到您在本教程中创建的资源的名称上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "timestamp"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "648aa9824ac6"
   },
   "source": [
    "### 验证您的谷歌云账户\n",
    "\n",
    "**如果您正在使用 Vertex AI Workbench 笔记本**，您的环境已经通过验证。跳过这一步。\n",
    "\n",
    "**如果您正在使用 Colab**，运行下面的单元格并按照提示进行身份验证，使用 oAuth。\n",
    "\n",
    "**否则**，请遵循以下步骤：\n",
    "\n",
    "1. 在 Cloud Console 中，转到 [**创建服务帐号密钥** 页面](https://console.cloud.google.com/apis/credentials/serviceaccountkey)。\n",
    "\n",
    "2. 点击 **创建服务帐号**。\n",
    "\n",
    "3. 在 **服务帐号名称** 字段中输入一个名称，然后点击 **创建**。\n",
    "\n",
    "4. 在 **授予该服务帐号对项目的访问权限** 部分，点击 **Role** 下拉列表。在过滤框中输入 \"Vertex AI\"，并选择 **Vertex AI 管理员**。在过滤框中输入 \"Storage Object Admin\"，并选择 **Storage Object Admin**。\n",
    "\n",
    "5. 点击 *创建*。包含您密钥的 JSON 文件将下载到您的本地环境中。\n",
    "\n",
    "6. 在下面的单元格中输入您的服务帐号密钥的路径作为`GOOGLE_APPLICATION_CREDENTIALS` 变量，并运行该单元格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "535223fa4b84"
   },
   "outputs": [],
   "source": [
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# If on Vertex AI Workbench, then don't execute this code\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\") and not os.getenv(\n",
    "    \"DL_ANACONDA_HOME\"\n",
    "):\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\"):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bucket:mbsdk"
   },
   "source": [
    "### 创建一个云存储桶\n",
    "\n",
    "**无论您使用的是哪种笔记本环境，下面的步骤都是必需的。**\n",
    "\n",
    "在初始化用于 Python 的 Vertex SDK 时，您需要指定一个云存储暂存桶。暂存桶是您的数据集和模型资源在各个会话中保留的地方。\n",
    "\n",
    "请在下面设置您的云存储桶的名称。存储桶的名称必须在所有谷歌云项目中是全局唯一的，包括您组织之外的项目。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bucket"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"gs://[your-bucket-name]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "autoset_bucket"
   },
   "outputs": [],
   "source": [
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"gs://[your-bucket-name]\":\n",
    "    BUCKET_NAME = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_bucket"
   },
   "source": [
    "只有在您的存储桶不存在时才运行以下单元格以创建您的云存储存储桶。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_bucket"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "validate_bucket"
   },
   "source": [
    "最后，通过检查其内容来验证对您的云存储存储桶的访问。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "validate_bucket"
   },
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "set_service_account"
   },
   "source": [
    "**服务账户**\n",
    "\n",
    "**如果你不知道你的服务账户**，尝试使用`gcloud`命令执行下面的第二个单元格来获取你的服务账户。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_service_account"
   },
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT = \"[your-service-account]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "autoset_service_account"
   },
   "outputs": [],
   "source": [
    "if (\n",
    "    SERVICE_ACCOUNT == \"\"\n",
    "    or SERVICE_ACCOUNT is None\n",
    "    or SERVICE_ACCOUNT == \"[your-service-account]\"\n",
    "):\n",
    "    # Get your service account from gcloud\n",
    "    if not IS_COLAB:\n",
    "        shell_output = !gcloud auth list 2>/dev/null\n",
    "        SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
    "\n",
    "    if IS_COLAB:\n",
    "        shell_output = ! gcloud projects describe  $PROJECT_ID\n",
    "        project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
    "        SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
    "\n",
    "    print(\"Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_vars"
   },
   "source": [
    "### 设置变量\n",
    "\n",
    "接下来，设置一些在整个教程中使用的变量。\n",
    "### 导入库并定义常量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_tf"
   },
   "source": [
    "#### 导入 TensorFlow\n",
    "\n",
    "将 TensorFlow 包导入到您的 Python 环境中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_tf"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_tft"
   },
   "source": [
    "#### 导入 TensorFlow Transform\n",
    "\n",
    "将 TensorFlow Transform（TFT）包导入到您的 Python 环境中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_tft"
   },
   "outputs": [],
   "source": [
    "import tensorflow_transform as tft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_tfdv"
   },
   "source": [
    "#### 导入 TensorFlow 数据验证\n",
    "\n",
    "将 TensorFlow 数据验证（TFDV）包导入您的 Python 环境中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_tfdv"
   },
   "outputs": [],
   "source": [
    "import tensorflow_data_validation as tfdv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "source": [
    "### 初始化用于 Python 的 Vertex AI SDK\n",
    "\n",
    "为您的项目和相应的存储桶初始化 Python 的 Vertex AI SDK。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "outputs": [],
   "source": [
    "aip.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "accelerators:training,prediction,ngpu,mbsdk"
   },
   "source": [
    "设置硬件加速器\n",
    "\n",
    "您可以为训练和预测设置硬件加速器。\n",
    "\n",
    "设置变量 `TRAIN_GPU/TRAIN_NGPU` 和 `DEPLOY_GPU/DEPLOY_NGPU` 来使用支持 GPU 的容器映像以及分配给虚拟机实例的 GPU 数量。例如，要使用一个支持 GPU 的容器映像，并为每个虚拟机分配 4 个 Nvidia Telsa K80 GPU，则可以指定：\n",
    "\n",
    "    (aip.AcceleratorType.NVIDIA_TESLA_K80, 4)\n",
    "\n",
    "\n",
    "否则，指定 `(None, None)` 来使用一个在 CPU 上运行的容器映像。\n",
    "\n",
    "了解更多关于 [您所在地区硬件加速器支持](https://cloud.google.com/vertex-ai/docs/general/locations#accelerators)。\n",
    "\n",
    "*注意*：TF 2.3 之前的版本在 GPU 支持上无法加载本教程中的自定义模型。这是一个已知问题，在 TF 2.3 中已修复。这是由生成在服务函数中的静态图操作引起的。如果您在自己的自定义模型上遇到此问题，请使用支持 GPU 的 TF 2.3 容器映像。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "accelerators:training,prediction,ngpu,mbsdk"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.getenv(\"IS_TESTING_TRAIN_GPU\"):\n",
    "    TRAIN_GPU, TRAIN_NGPU = (\n",
    "        aip.gapic.AcceleratorType.NVIDIA_TESLA_K80,\n",
    "        int(os.getenv(\"IS_TESTING_TRAIN_GPU\")),\n",
    "    )\n",
    "else:\n",
    "    TRAIN_GPU, TRAIN_NGPU = (aip.gapic.AcceleratorType.NVIDIA_TESLA_K80, 4)\n",
    "\n",
    "if os.getenv(\"IS_TESTING_DEPLOY_GPU\"):\n",
    "    DEPLOY_GPU, DEPLOY_NGPU = (\n",
    "        aip.gapic.AcceleratorType.NVIDIA_TESLA_K80,\n",
    "        int(os.getenv(\"IS_TESTING_DEPLOY_GPU\")),\n",
    "    )\n",
    "else:\n",
    "    DEPLOY_GPU, DEPLOY_NGPU = (None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "container:training,prediction"
   },
   "source": [
    "设置预先构建的容器\n",
    "\n",
    "设置用于训练和预测的预先构建的Docker容器镜像。\n",
    "\n",
    "有关最新列表，请查看[用于训练的预构建容器](https://cloud.google.com/ai-platform-unified/docs/training/pre-built-containers)。\n",
    "\n",
    "有关最新列表，请查看[用于预测的预构建容器](https://cloud.google.com/ai-platform-unified/docs/predictions/pre-built-containers)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "container:training,prediction"
   },
   "outputs": [],
   "source": [
    "if os.getenv(\"IS_TESTING_TF\"):\n",
    "    TF = os.getenv(\"IS_TESTING_TF\")\n",
    "else:\n",
    "    TF = \"2.5\".replace(\".\", \"-\")\n",
    "\n",
    "if TF[0] == \"2\":\n",
    "    if TRAIN_GPU:\n",
    "        TRAIN_VERSION = \"tf-gpu.{}\".format(TF)\n",
    "    else:\n",
    "        TRAIN_VERSION = \"tf-cpu.{}\".format(TF)\n",
    "    if DEPLOY_GPU:\n",
    "        DEPLOY_VERSION = \"tf2-gpu.{}\".format(TF)\n",
    "    else:\n",
    "        DEPLOY_VERSION = \"tf2-cpu.{}\".format(TF)\n",
    "else:\n",
    "    if TRAIN_GPU:\n",
    "        TRAIN_VERSION = \"tf-gpu.{}\".format(TF)\n",
    "    else:\n",
    "        TRAIN_VERSION = \"tf-cpu.{}\".format(TF)\n",
    "    if DEPLOY_GPU:\n",
    "        DEPLOY_VERSION = \"tf-gpu.{}\".format(TF)\n",
    "    else:\n",
    "        DEPLOY_VERSION = \"tf-cpu.{}\".format(TF)\n",
    "\n",
    "TRAIN_IMAGE = \"{}-docker.pkg.dev/vertex-ai/training/{}:latest\".format(\n",
    "    REGION.split(\"-\")[0], TRAIN_VERSION\n",
    ")\n",
    "DEPLOY_IMAGE = \"{}-docker.pkg.dev/vertex-ai/prediction/{}:latest\".format(\n",
    "    REGION.split(\"-\")[0], DEPLOY_VERSION\n",
    ")\n",
    "\n",
    "print(\"Training:\", TRAIN_IMAGE, TRAIN_GPU, TRAIN_NGPU)\n",
    "print(\"Deployment:\", DEPLOY_IMAGE, DEPLOY_GPU, DEPLOY_NGPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "machine:training,prediction"
   },
   "source": [
    "#### 设置机器类型\n",
    "\n",
    "接下来，设置用于训练和预测的机器类型。\n",
    "\n",
    "- 设置变量 `TRAIN_COMPUTE` 和 `DEPLOY_COMPUTE` 来配置用于训练和预测的虚拟机的计算资源。\n",
    "- `机器类型`\n",
    "     - `n1-standard`: 每个 vCPU 3.75GB 内存。\n",
    "     - `n1-highmem`: 每个 vCPU 6.5GB 内存。\n",
    "     - `n1-highcpu`: 每个 vCPU 0.9GB 内存。\n",
    "- `vCPUs`: \\[2, 4, 8, 16, 32, 64, 96 \\] 中的数字。\n",
    "\n",
    "*注: 以下机器类型不支持训练:*\n",
    "\n",
    "- `standard`: 2 vCPUs\n",
    "- `highcpu`: 2, 4 和 8 vCPUs\n",
    "\n",
    "*提示: 您也可以使用 n2 和 e2 机器类型进行训练和部署，但它们不支持 GPU*。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "machine:training,prediction"
   },
   "outputs": [],
   "source": [
    "if os.getenv(\"IS_TESTING_TRAIN_MACHINE\"):\n",
    "    MACHINE_TYPE = os.getenv(\"IS_TESTING_TRAIN_MACHINE\")\n",
    "else:\n",
    "    MACHINE_TYPE = \"n1-standard\"\n",
    "\n",
    "VCPU = \"4\"\n",
    "TRAIN_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
    "print(\"Train machine type\", TRAIN_COMPUTE)\n",
    "\n",
    "if os.getenv(\"IS_TESTING_DEPLOY_MACHINE\"):\n",
    "    MACHINE_TYPE = os.getenv(\"IS_TESTING_DEPLOY_MACHINE\")\n",
    "else:\n",
    "    MACHINE_TYPE = \"n1-standard\"\n",
    "\n",
    "VCPU = \"4\"\n",
    "DEPLOY_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
    "print(\"Deploy machine type\", DEPLOY_COMPUTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "find_dataset:bq"
   },
   "source": [
    "### 从阶段1检索数据集\n",
    "\n",
    "接下来，使用辅助函数`find_dataset()`检索在阶段1创建的数据集。该辅助函数会查找所有显示名称与指定前缀和导入格式（例如，bq）匹配的数据集。最后，它会按创建时间对匹配项进行排序并返回最新版本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "find_dataset:bq"
   },
   "outputs": [],
   "source": [
    "def find_dataset(display_name_prefix, import_format):\n",
    "    matches = []\n",
    "    datasets = aip.TabularDataset.list()\n",
    "    for dataset in datasets:\n",
    "        if dataset.display_name.startswith(display_name_prefix):\n",
    "            try:\n",
    "                if (\n",
    "                    \"bq\" == import_format\n",
    "                    and dataset.to_dict()[\"metadata\"][\"inputConfig\"][\"bigquerySource\"]\n",
    "                ):\n",
    "                    matches.append(dataset)\n",
    "                if (\n",
    "                    \"csv\" == import_format\n",
    "                    and dataset.to_dict()[\"metadata\"][\"inputConfig\"][\"gcsSource\"]\n",
    "                ):\n",
    "                    matches.append(dataset)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    create_time = None\n",
    "    for match in matches:\n",
    "        if create_time is None or match.create_time > create_time:\n",
    "            create_time = match.create_time\n",
    "            dataset = match\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "dataset = find_dataset(\"Chicago Taxi\", \"bq\")\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "load_dataset_user_metadata"
   },
   "source": [
    "加载数据集的用户元数据\n",
    "\n",
    "加载数据集的用户元数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_dataset_user_metadata"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "try:\n",
    "    with tf.io.gfile.GFile(\n",
    "        \"gs://\" + dataset.labels[\"user_metadata\"] + \"/metadata.jsonl\", \"r\"\n",
    "    ) as f:\n",
    "        metadata = json.load(f)\n",
    "\n",
    "    print(metadata)\n",
    "except:\n",
    "    print(\"no metadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_automl_pipeline:tabular,lbn"
   },
   "source": [
    "### 创建和运行训练管道\n",
    "\n",
    "要训练一个AutoML模型，您需要执行两个步骤：1）创建一个训练管道，2）运行该管道。\n",
    "\n",
    "#### 创建训练管道\n",
    "\n",
    "使用`AutoMLTabularTrainingJob`类创建一个AutoML训练管道，具有以下参数：\n",
    "\n",
    "- `display_name`：`TrainingJob`资源的人类可读名称。\n",
    "- `optimization_prediction_type`：为模型训练的任务类型。\n",
    "  - `classification`：一个表格分类模型。\n",
    "  - `regression`：一个表格回归模型。\n",
    "- `column_transformations`：（可选）应用于输入列的转换。\n",
    "- `optimization_objective`：要最小化或最大化的优化目标。\n",
    "  - 二元分类：\n",
    "    - `minimize-log-loss`\n",
    "    - `maximize-au-roc`\n",
    "    - `maximize-au-prc`\n",
    "    - `maximize-precision-at-recall`\n",
    "    - `maximize-recall-at-precision`\n",
    "  - 多类分类：\n",
    "    - `minimize-log-loss`\n",
    "  - 回归：\n",
    "    - `minimize-rmse`\n",
    "    - `minimize-mae`\n",
    "    - `minimize-rmsle`\n",
    "\n",
    "实例化的对象是训练管道的有向无环图（DAG）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_automl_pipeline:tabular,lbn"
   },
   "outputs": [],
   "source": [
    "dag = aip.AutoMLTabularTrainingJob(\n",
    "    display_name=\"chicago_\" + TIMESTAMP,\n",
    "    optimization_prediction_type=\"classification\",\n",
    "    optimization_objective=\"minimize-log-loss\",\n",
    ")\n",
    "\n",
    "print(dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run_automl_pipeline:async,tabular"
   },
   "source": [
    "运行训练管道\n",
    "\n",
    "接下来，您可以通过调用方法`run`来运行DAG以启动训练作业，需要传入以下参数：\n",
    "\n",
    "- `dataset`：用于训练模型的`Dataset`资源。\n",
    "- `model_display_name`：训练模型的可读名称。\n",
    "- `training_fraction_split`：用于训练的数据集百分比。\n",
    "- `test_fraction_split`：用于测试（留置数据）的数据集百分比。\n",
    "- `validation_fraction_split`：用于验证的数据集百分比。\n",
    "- `target_column`：要训练为标签的列的名称。\n",
    "- `budget_milli_node_hours`：（可选）以毫小时为单位指定的最大训练时间（1000 = 小时）。\n",
    "- `disable_early_stopping`：如果为`True`，则可能在服务认为无法进一步提高模型目标测量之前完成训练，并未使用整个预算。\n",
    "\n",
    "完成`run`方法后将返回`Model`资源。\n",
    "\n",
    "训练管道的执行将需要最多180分钟。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_automl_pipeline:async,tabular"
   },
   "outputs": [],
   "source": [
    "async_model = dag.run(\n",
    "    dataset=dataset,\n",
    "    model_display_name=\"chicago_\" + TIMESTAMP,\n",
    "    training_fraction_split=0.8,\n",
    "    validation_fraction_split=0.1,\n",
    "    test_fraction_split=0.1,\n",
    "    budget_milli_node_hours=8000,\n",
    "    disable_early_stopping=False,\n",
    "    target_column=\"tip_bin\",\n",
    "    sync=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "start_experiment"
   },
   "source": [
    "### 创建用于跟踪与训练相关的元数据的实验\n",
    "\n",
    "设置跟踪每个实验的参数（配置）和指标（结果）：\n",
    "\n",
    "- `aip.init()` - 创建一个实验实例\n",
    "- `aip.start_run()` - 跟踪实验中的特定运行。\n",
    "\n",
    "了解更多关于[Vertex AI ML Metadata简介](https://cloud.google.com/vertex-ai/docs/ml-metadata/introduction)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "start_experiment"
   },
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"chicago-\" + TIMESTAMP\n",
    "aip.init(experiment=EXPERIMENT_NAME)\n",
    "aip.start_run(\"run-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_tensorboard_instance"
   },
   "source": [
    "创建一个 Vertex AI TensorBoard 实例，以便在自定义模型训练中与 Vertex AI Training 一起使用 TensorBoard。\n",
    "\n",
    "了解更多关于 [开始使用 Vertex AI TensorBoard](https://cloud.google.com/vertex-ai/docs/experiments/tensorboard-overview)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_tensorboard_instance"
   },
   "outputs": [],
   "source": [
    "TENSORBOARD_DISPLAY_NAME = \"chicago_\" + TIMESTAMP\n",
    "tensorboard = aip.Tensorboard.create(display_name=TENSORBOARD_DISPLAY_NAME)\n",
    "tensorboard_resource_name = tensorboard.gca_resource.name\n",
    "print(\"TensorBoard resource name:\", tensorboard_resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_input_layer:tabular"
   },
   "source": [
    "### 为您的自定义模型创建输入层\n",
    "\n",
    "接下来，根据每个特征的数据类型为您的自定义表格模型创建输入层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_input_layer:tabular"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "\n",
    "def create_model_inputs(\n",
    "    numeric_features=None, categorical_features=None, embedding_features=None\n",
    "):\n",
    "    inputs = {}\n",
    "    for feature_name in numeric_features:\n",
    "        inputs[feature_name] = Input(name=feature_name, shape=[], dtype=tf.float32)\n",
    "    for feature_name in categorical_features:\n",
    "        inputs[feature_name] = Input(name=feature_name, shape=[], dtype=tf.int64)\n",
    "    for feature_name in embedding_features:\n",
    "        inputs[feature_name] = Input(name=feature_name, shape=[], dtype=tf.int64)\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "make_input_layer:tabular"
   },
   "outputs": [],
   "source": [
    "input_layers = create_model_inputs(\n",
    "    numeric_features=metadata[\"numeric_features\"],\n",
    "    categorical_features=metadata[\"categorical_features\"],\n",
    "    embedding_features=metadata[\"embedding_features\"],\n",
    ")\n",
    "\n",
    "print(input_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_binary_classifier:tabular"
   },
   "source": [
    "### 创建二分类器自定义模型\n",
    "\n",
    "接下来，您将创建您的二分类器自定义表格模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_binary_classifier:tabular"
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.layers import (Activation, Concatenate, Dense, Embedding,\n",
    "                                     experimental)\n",
    "\n",
    "\n",
    "def create_binary_classifier(\n",
    "    input_layers,\n",
    "    tft_output,\n",
    "    metaparams,\n",
    "    numeric_features,\n",
    "    categorical_features,\n",
    "    embedding_features,\n",
    "):\n",
    "    layers = []\n",
    "    for feature_name in input_layers:\n",
    "        if feature_name in embedding_features:\n",
    "            vocab_size = tft_output.vocabulary_size_by_name(feature_name)\n",
    "            embedding_size = int(sqrt(vocab_size))\n",
    "            embedding_output = Embedding(\n",
    "                input_dim=vocab_size + 1,\n",
    "                output_dim=embedding_size,\n",
    "                name=f\"{feature_name}_embedding\",\n",
    "            )(input_layers[feature_name])\n",
    "            layers.append(embedding_output)\n",
    "        elif feature_name in categorical_features:\n",
    "            vocab_size = tft_output.vocabulary_size_by_name(feature_name)\n",
    "            onehot_layer = experimental.preprocessing.CategoryEncoding(\n",
    "                num_tokens=vocab_size,\n",
    "                output_mode=\"binary\",\n",
    "                name=f\"{feature_name}_onehot\",\n",
    "            )(input_layers[feature_name])\n",
    "            layers.append(onehot_layer)\n",
    "        elif feature_name in numeric_features:\n",
    "            numeric_layer = tf.expand_dims(input_layers[feature_name], -1)\n",
    "            layers.append(numeric_layer)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    joined = Concatenate(name=\"combines_inputs\")(layers)\n",
    "    feedforward_output = Sequential(\n",
    "        [Dense(units, activation=\"relu\") for units in metaparams[\"hidden_units\"]],\n",
    "        name=\"feedforward_network\",\n",
    "    )(joined)\n",
    "    logits = Dense(units=1, name=\"logits\")(feedforward_output)\n",
    "    pred = Activation(\"sigmoid\")(logits)\n",
    "\n",
    "    model = Model(inputs=input_layers, outputs=[pred])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "make_binary_classifier:tabular"
   },
   "outputs": [],
   "source": [
    "TRANSFORM_ARTIFACTS_DIR = metadata[\"transform_artifacts_dir\"]\n",
    "tft_output = tft.TFTransformOutput(TRANSFORM_ARTIFACTS_DIR)\n",
    "\n",
    "metaparams = {\"hidden_units\": [128, 64]}\n",
    "aip.log_params(metaparams)\n",
    "\n",
    "model = create_binary_classifier(\n",
    "    input_layers,\n",
    "    tft_output,\n",
    "    metaparams,\n",
    "    numeric_features=metadata[\"numeric_features\"],\n",
    "    categorical_features=metadata[\"categorical_features\"],\n",
    "    embedding_features=metadata[\"embedding_features\"],\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "visualize_model"
   },
   "source": [
    "接下来，可视化自定义模型的架构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualize_model"
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes=True, show_dtype=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save_model:gcs"
   },
   "source": [
    "### 保存模型文件\n",
    "\n",
    "接下来，将模型文件保存到您的云存储桶中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_model:gcs"
   },
   "outputs": [],
   "source": [
    "MODEL_DIR = f\"{BUCKET_NAME}/base_model\"\n",
    "\n",
    "model.save(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upload_model:vertex,base_model"
   },
   "source": [
    "### 将本地模型上传到 Vertex AI 模型资源\n",
    "\n",
    "接下来，您将上传本地自定义模型构件到 Vertex AI，以便转换为托管的 Vertex AI 模型资源。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload_model:vertex,base_model"
   },
   "outputs": [],
   "source": [
    "vertex_custom_model = aip.Model.upload(\n",
    "    display_name=\"chicago_\" + TIMESTAMP,\n",
    "    artifact_uri=MODEL_DIR,\n",
    "    serving_container_image_uri=DEPLOY_IMAGE,\n",
    "    labels={\"base_model\": \"1\"},\n",
    "    sync=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "construct_training_package"
   },
   "source": [
    "### 构建培训包\n",
    "\n",
    "#### 包布局\n",
    "\n",
    "在开始培训之前，您应该了解如何为自定义培训任务组装一个Python包。解压后，该包包含以下目录/文件布局:\n",
    "\n",
    "- PKG-INFO\n",
    "- README.md\n",
    "- setup.cfg\n",
    "- setup.py\n",
    "- trainer\n",
    "  - \\_\\_init\\_\\_.py\n",
    "  - task.py\n",
    "  - 其他Python脚本\n",
    "\n",
    "文件`setup.cfg`和`setup.py`是将包安装到Docker映像的操作环境中的说明。\n",
    "\n",
    "文件`trainer/task.py`是执行自定义培训任务的Python脚本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "construct_training_package"
   },
   "outputs": [],
   "source": [
    "# Make folder for Python training script\n",
    "! rm -rf custom\n",
    "! mkdir custom\n",
    "\n",
    "# Add package information\n",
    "! touch custom/README.md\n",
    "\n",
    "setup_cfg = \"[egg_info]\\n\\ntag_build =\\n\\ntag_date = 0\"\n",
    "! echo \"$setup_cfg\" > custom/setup.cfg\n",
    "\n",
    "setup_py = \"import setuptools\\n\\nsetuptools.setup(\\n\\n    install_requires=[\\n\\n        'google-cloud-aiplatform',\\n\\n        'cloudml-hypertune',\\n\\n        'tensorflow_datasets==1.3.0',\\n\\n        'tensorflow==2.5',\\n\\n    'tensorflow_data_validation==1.2',\\n\\n    ],\\n\\n    packages=setuptools.find_packages())\"\n",
    "! echo \"$setup_py\" > custom/setup.py\n",
    "\n",
    "pkg_info = \"Metadata-Version: 1.0\\n\\nName: Chicago Taxi tabular binary classifier\\n\\nVersion: 0.0.0\\n\\nSummary: Demostration training script\\n\\nHome-page: www.google.com\\n\\nAuthor: Google\\n\\nAuthor-email: cdpe@google.com\\n\\nLicense: Public\\n\\nDescription: Demo\\n\\nPlatform: Vertex AI\"\n",
    "! echo \"$pkg_info\" > custom/PKG-INFO\n",
    "\n",
    "# Make the training subfolder\n",
    "! mkdir custom/trainer\n",
    "! touch custom/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "transform_feature_spec"
   },
   "source": [
    "获取预处理数据的特征规范\n",
    "\n",
    "接下来，为预处理数据创建特征规范。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "transform_feature_spec"
   },
   "outputs": [],
   "source": [
    "transform_feature_spec = tft_output.transformed_feature_spec()\n",
    "print(transform_feature_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "read_tfrecords_func"
   },
   "source": [
    "### 将转换后的数据加载到 tf.data.Dataset 中\n",
    "\n",
    "接下来，您将在 Cloud Storage 存储中加载 gzip TFRecords 到一个 `tf.data.Dataset` 生成器中。这些功能在使用 `Vertex Training` 训练自定义模型时会被重复使用，因此您可以将它们保存到 Python 训练包中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "read_tfrecords_func"
   },
   "outputs": [],
   "source": [
    "%%writefile custom/trainer/data.py\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def _gzip_reader_fn(filenames):\n",
    "    \"\"\"Small utility returning a record reader that can read gzip'ed files.\"\"\"\n",
    "    return tf.data.TFRecordDataset(filenames, compression_type=\"GZIP\")\n",
    "\n",
    "\n",
    "def get_dataset(file_pattern, feature_spec, label_column, batch_size=200):\n",
    "    \"\"\"Generates features and label for tuning/training.\n",
    "    Args:\n",
    "      file_pattern: input tfrecord file pattern.\n",
    "      feature_spec: a dictionary of feature specifications.\n",
    "      batch_size: representing the number of consecutive elements of returned\n",
    "        dataset to combine in a single batch\n",
    "    Returns:\n",
    "      A dataset that contains (features, indices) tuple where features is a\n",
    "        dictionary of Tensors, and indices is a single Tensor of label indices.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset = tf.data.experimental.make_batched_features_dataset(\n",
    "        file_pattern=file_pattern,\n",
    "        batch_size=batch_size,\n",
    "        features=feature_spec,\n",
    "        label_key=label_column,\n",
    "        reader=_gzip_reader_fn,\n",
    "        num_epochs=1,\n",
    "        drop_final_batch=True,\n",
    "    )\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "read_tfrecords"
   },
   "outputs": [],
   "source": [
    "from custom.trainer import data\n",
    "\n",
    "TRANSFORMED_DATA_PREFIX = metadata[\"transformed_data_prefix\"]\n",
    "LABEL_COLUMN = metadata[\"label_column\"]\n",
    "\n",
    "train_data_file_pattern = TRANSFORMED_DATA_PREFIX + \"/train/data-*.gz\"\n",
    "val_data_file_pattern = TRANSFORMED_DATA_PREFIX + \"/val/data-*.gz\"\n",
    "test_data_file_pattern = TRANSFORMED_DATA_PREFIX + \"/test/data-*.gz\"\n",
    "\n",
    "for input_features, target in data.get_dataset(\n",
    "    train_data_file_pattern, transform_feature_spec, LABEL_COLUMN, batch_size=3\n",
    ").take(1):\n",
    "    for key in input_features:\n",
    "        print(\n",
    "            f\"{key} {input_features[key].dtype}: {input_features[key].numpy().tolist()}\"\n",
    "        )\n",
    "    print(f\"target: {target.numpy().tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test_model_input"
   },
   "source": [
    "用转换后的输入测试模型架构。\n",
    "\n",
    "接下来，使用转换后的训练输入样本测试模型架构。\n",
    "\n",
    "*注意:*由于模型尚未训练，预测结果应该是随机的。由于这是一个二元分类器，预期预测结果约为0.5。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_model_input"
   },
   "outputs": [],
   "source": [
    "model(input_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train_model_func"
   },
   "source": [
    "## 开发和测试训练脚本\n",
    "\n",
    "在进行实验时，通常会先在本地开发和测试训练包，然后再转移到云端进行训练。\n",
    "\n",
    "### 创建训练脚本\n",
    "\n",
    "接下来，您需要编写Python脚本来编译和训练模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_model_func"
   },
   "outputs": [],
   "source": [
    "%%writefile custom/trainer/train.py\n",
    "\n",
    "from trainer import data\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "from hypertune import HyperTune\n",
    "\n",
    "def compile(model, hyperparams):\n",
    "    ''' Compile the model '''\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=hyperparams[\"learning_rate\"])\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "    metrics = [tf.keras.metrics.BinaryAccuracy(name=\"accuracy\")]\n",
    "\n",
    "    model.compile(optimizer=optimizer,loss=loss, metrics=metrics)\n",
    "    return model\n",
    "\n",
    "def warmup(\n",
    "    model,\n",
    "    hyperparams,\n",
    "    train_data_dir,\n",
    "    label_column,\n",
    "    transformed_feature_spec\n",
    "):\n",
    "    ''' Warmup the initialized model weights '''\n",
    "\n",
    "    train_dataset = data.get_dataset(\n",
    "        train_data_dir,\n",
    "        transformed_feature_spec,\n",
    "        label_column,\n",
    "        batch_size=hyperparams[\"batch_size\"],\n",
    "    )\n",
    "\n",
    "    lr_inc = (hyperparams['end_learning_rate'] - hyperparams['start_learning_rate']) / hyperparams['num_epochs']\n",
    "\n",
    "    def scheduler(epoch, lr):\n",
    "        if epoch == 0:\n",
    "            return hyperparams['start_learning_rate']\n",
    "        return lr + lr_inc\n",
    "\n",
    "\n",
    "    callbacks = [tf.keras.callbacks.LearningRateScheduler(scheduler)]\n",
    "\n",
    "    logging.info(\"Model warmup started...\")\n",
    "    history = model.fit(\n",
    "            train_dataset,\n",
    "            epochs=hyperparams[\"num_epochs\"],\n",
    "            steps_per_epoch=hyperparams[\"steps\"],\n",
    "            callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    logging.info(\"Model warmup completed.\")\n",
    "    return history\n",
    "\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    hyperparams,\n",
    "    train_data_dir,\n",
    "    val_data_dir,\n",
    "    label_column,\n",
    "    transformed_feature_spec,\n",
    "    log_dir,\n",
    "    tuning=False\n",
    "):\n",
    "    ''' Train the model '''\n",
    "\n",
    "    train_dataset = data.get_dataset(\n",
    "        train_data_dir,\n",
    "        transformed_feature_spec,\n",
    "        label_column,\n",
    "        batch_size=hyperparams[\"batch_size\"],\n",
    "    )\n",
    "\n",
    "    val_dataset = data.get_dataset(\n",
    "        val_data_dir,\n",
    "        transformed_feature_spec,\n",
    "        label_column,\n",
    "        batch_size=hyperparams[\"batch_size\"],\n",
    "    )\n",
    "\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=hyperparams[\"early_stop\"][\"monitor\"], patience=hyperparams[\"early_stop\"][\"patience\"], restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    callbacks = [early_stop]\n",
    "\n",
    "    if log_dir:\n",
    "        tensorboard = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "\n",
    "        callbacks = callbacks.append(tensorboard)\n",
    "\n",
    "    if tuning:\n",
    "        # Instantiate the HyperTune reporting object\n",
    "        hpt = HyperTune()\n",
    "\n",
    "        # Reporting callback\n",
    "        class HPTCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "            def on_epoch_end(self, epoch, logs=None):\n",
    "                hpt.report_hyperparameter_tuning_metric(\n",
    "                    hyperparameter_metric_tag='val_loss',\n",
    "                    metric_value=logs['val_loss'],\n",
    "                    global_step=epoch\n",
    "                )\n",
    "\n",
    "        if not callbacks:\n",
    "            callbacks = []\n",
    "        callbacks.append(HPTCallback())\n",
    "\n",
    "    logging.info(\"Model training started...\")\n",
    "    history = model.fit(\n",
    "            train_dataset,\n",
    "            epochs=hyperparams[\"num_epochs\"],\n",
    "            validation_data=val_dataset,\n",
    "            callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    logging.info(\"Model training completed.\")\n",
    "    return history\n",
    "\n",
    "def evaluate(\n",
    "    model,\n",
    "    hyperparams,\n",
    "    test_data_dir,\n",
    "    label_column,\n",
    "    transformed_feature_spec\n",
    "):\n",
    "    logging.info(\"Model evaluation started...\")\n",
    "    test_dataset = data.get_dataset(\n",
    "        test_data_dir,\n",
    "        transformed_feature_spec,\n",
    "        label_column,\n",
    "        hyperparams[\"batch_size\"],\n",
    "    )\n",
    "\n",
    "    evaluation_metrics = model.evaluate(test_dataset)\n",
    "    logging.info(\"Model evaluation completed.\")\n",
    "\n",
    "    return evaluation_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train_model_local"
   },
   "source": [
    "### 在本地训练模型\n",
    "\n",
    "接下来，通过仅训练几个时期来本地测试训练包：\n",
    "\n",
    "- `num_epochs`：传递给训练包的时期数。\n",
    "- `compile()`: 编译模型以进行训练。\n",
    "- `warmup()`: 预热初始化的模型权重。\n",
    "- `train()`: 训练模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_model_local"
   },
   "outputs": [],
   "source": [
    "os.chdir(\"custom\")\n",
    "\n",
    "import logging\n",
    "\n",
    "from trainer import train\n",
    "\n",
    "TENSORBOARD_LOG_DIR = \"./logs\"\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "hyperparams = {}\n",
    "hyperparams[\"learning_rate\"] = 0.01\n",
    "aip.log_params(hyperparams)\n",
    "\n",
    "train.compile(model, hyperparams)\n",
    "\n",
    "warmupparams = {}\n",
    "warmupparams[\"start_learning_rate\"] = 0.0001\n",
    "warmupparams[\"end_learning_rate\"] = 0.01\n",
    "warmupparams[\"num_epochs\"] = 4\n",
    "warmupparams[\"batch_size\"] = 64\n",
    "warmupparams[\"steps\"] = 50\n",
    "aip.log_params(warmupparams)\n",
    "\n",
    "train.warmup(\n",
    "    model, warmupparams, train_data_file_pattern, LABEL_COLUMN, transform_feature_spec\n",
    ")\n",
    "\n",
    "trainparams = {}\n",
    "trainparams[\"num_epochs\"] = 5\n",
    "trainparams[\"batch_size\"] = 64\n",
    "trainparams[\"early_stop\"] = {\"monitor\": \"val_loss\", \"patience\": 5}\n",
    "aip.log_params(trainparams)\n",
    "\n",
    "train.train(\n",
    "    model,\n",
    "    trainparams,\n",
    "    train_data_file_pattern,\n",
    "    val_data_file_pattern,\n",
    "    LABEL_COLUMN,\n",
    "    transform_feature_spec,\n",
    "    TENSORBOARD_LOG_DIR,\n",
    ")\n",
    "\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eval_model_local"
   },
   "source": [
    "### 在本地评估模型\n",
    "\n",
    "接下来，测试训练包的评估部分：\n",
    "\n",
    "- `evaluate()`: 评估模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eval_model_local"
   },
   "outputs": [],
   "source": [
    "os.chdir(\"custom\")\n",
    "\n",
    "from trainer import train\n",
    "\n",
    "evalparams = {}\n",
    "evalparams[\"batch_size\"] = 64\n",
    "\n",
    "metrics = {}\n",
    "metrics[\"loss\"], metrics[\"acc\"] = train.evaluate(\n",
    "    model, evalparams, test_data_file_pattern, LABEL_COLUMN, transform_feature_spec\n",
    ")\n",
    "print(\"ACC\", metrics[\"acc\"], \"LOSS\", metrics[\"loss\"])\n",
    "aip.log_metrics(metrics)\n",
    "\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_model_get"
   },
   "source": [
    "### 从 Vertex AI 检索模型\n",
    "\n",
    "接下来，创建 Python 脚本以从 Vertex AI 检索您的实验模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_model_get"
   },
   "outputs": [],
   "source": [
    "%%writefile custom/trainer/model.py\n",
    "\n",
    "import google.cloud.aiplatform as aip\n",
    "\n",
    "def get(model_id):\n",
    "    model = aip.Model(model_id)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_task_py"
   },
   "source": [
    "### 为 Python 培训包创建任务脚本\n",
    "\n",
    "接下来，您需要为驱动培训包创建 `task.py` 脚本。一些值得注意的步骤包括：\n",
    "\n",
    "- 命令行参数：\n",
    "    - `model-id`：在实验期间构建的 `Model` 资源的资源 ID。这是未经训练的模型架构。\n",
    "    - `dataset-id`：用于训练的 `Dataset` 资源的资源 ID。\n",
    "    - `experiment`：实验的名称。\n",
    "    - `run`：实验中的运行的名称。\n",
    "    - `tensorboard-logdir`：Vertex AI Tensorboard 的日志目录。\n",
    "\n",
    "\n",
    "- `get_data()`：\n",
    "    - 将 Dataset 资源加载到内存中。\n",
    "    - 从 Dataset 资源获取用户元数据。\n",
    "    - 从元数据中获取转化数据的位置、转化功能以及标签列的名称。\n",
    "\n",
    "\n",
    "- `get_model()`：\n",
    "    - 将 Model 资源加载到内存中。\n",
    "    - 获取模型架构的模型工件的位置。\n",
    "    - 加载模型架构。\n",
    "    - 编译模型。\n",
    "\n",
    "\n",
    "- `warmup_model()`：\n",
    "   - 对初始化的模型权重进行预热。\n",
    "\n",
    "\n",
    "- `train_model()`：\n",
    "    - 训练模型。\n",
    "\n",
    "\n",
    "- `evaluate_model()`：\n",
    "    - 评估模型。\n",
    "    - 将评估指标保存到 Cloud Storage 存储桶中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_task_py"
   },
   "outputs": [],
   "source": [
    "%%writefile custom/trainer/task.py\n",
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "import google.cloud.aiplatform as aip\n",
    "\n",
    "from trainer import data\n",
    "from trainer import model as model_\n",
    "from trainer import train\n",
    "try:\n",
    "    from trainer import serving\n",
    "except:\n",
    "    pass\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model-dir', dest='model_dir',\n",
    "                    default=os.getenv('AIP_MODEL_DIR'), type=str, help='Model dir.')\n",
    "parser.add_argument('--model-id', dest='model_id',\n",
    "                    default=None, type=str, help='Vertex Model ID.')\n",
    "parser.add_argument('--dataset-id', dest='dataset_id',\n",
    "                    default=None, type=str, help='Vertex Dataset ID.')\n",
    "parser.add_argument('--lr', dest='lr',\n",
    "                    default=0.001, type=float,\n",
    "                    help='Learning rate.')\n",
    "parser.add_argument('--start_lr', dest='start_lr',\n",
    "                    default=0.0001, type=float,\n",
    "                    help='Starting learning rate.')\n",
    "parser.add_argument('--epochs', dest='epochs',\n",
    "                    default=20, type=int,\n",
    "                    help='Number of epochs.')\n",
    "parser.add_argument('--steps', dest='steps',\n",
    "                    default=200, type=int,\n",
    "                    help='Number of steps per epoch.')\n",
    "parser.add_argument('--batch_size', dest='batch_size',\n",
    "                    default=16, type=int,\n",
    "                    help='Batch size.')\n",
    "parser.add_argument('--distribute', dest='distribute', type=str, default='single',\n",
    "                    help='distributed training strategy')\n",
    "parser.add_argument('--tensorboard-log-dir', dest='tensorboard_log_dir',\n",
    "                    default=os.getenv('AIP_TENSORBOARD_LOG_DIR'), type=str,\n",
    "                    help='Output file for tensorboard logs')\n",
    "parser.add_argument('--experiment', dest='experiment',\n",
    "                    default=None, type=str,\n",
    "                    help='Name of experiment')\n",
    "parser.add_argument('--project', dest='project',\n",
    "                    default=None, type=str,\n",
    "                    help='Name of project')\n",
    "parser.add_argument('--run', dest='run',\n",
    "                    default=None, type=str,\n",
    "                    help='Name of run in experiment')\n",
    "parser.add_argument('--evaluate', dest='evaluate',\n",
    "                    default=False, type=bool,\n",
    "                    help='Whether to perform evaluation')\n",
    "parser.add_argument('--serving', dest='serving',\n",
    "                    default=False, type=bool,\n",
    "                    help='Whether to attach the serving function')\n",
    "parser.add_argument('--tuning', dest='tuning',\n",
    "                    default=False, type=bool,\n",
    "                    help='Whether to perform hyperparameter tuning')\n",
    "parser.add_argument('--warmup', dest='warmup',\n",
    "                    default=False, type=bool,\n",
    "                    help='Whether to perform warmup weight initialization')\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "logging.info('DEVICES'  + str(device_lib.list_local_devices()))\n",
    "\n",
    "# Single Machine, single compute device\n",
    "if args.distribute == 'single':\n",
    "    if tf.test.is_gpu_available():\n",
    "        strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
    "    else:\n",
    "        strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
    "    logging.info(\"Single device training\")\n",
    "# Single Machine, multiple compute device\n",
    "elif args.distribute == 'mirrored':\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    logging.info(\"Mirrored Strategy distributed training\")\n",
    "# Multi Machine, multiple compute device\n",
    "elif args.distribute == 'multiworker':\n",
    "    strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
    "    logging.info(\"Multi-worker Strategy distributed training\")\n",
    "    logging.info('TF_CONFIG = {}'.format(os.environ.get('TF_CONFIG', 'Not found')))\n",
    "logging.info('num_replicas_in_sync = {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "# Initialize the run for this experiment\n",
    "if args.experiment:\n",
    "    logging.info(\"Initialize experiment: {}\".format(args.experiment))\n",
    "    aip.init(experiment=args.experiment, project=args.project)\n",
    "    aip.start_run(args.run)\n",
    "\n",
    "metadata = {}\n",
    "\n",
    "def get_data():\n",
    "    ''' Get the preprocessed training data '''\n",
    "    global train_data_file_pattern, val_data_file_pattern, test_data_file_pattern\n",
    "    global label_column, transform_feature_spec, metadata\n",
    "\n",
    "    dataset = aip.TabularDataset(args.dataset_id)\n",
    "    METADATA = 'gs://' + dataset.labels['user_metadata'] + \"/metadata.jsonl\"\n",
    "\n",
    "    with tf.io.gfile.GFile(METADATA, \"r\") as f:\n",
    "        metadata = json.load(f)\n",
    "\n",
    "    TRANSFORMED_DATA_PREFIX = metadata['transformed_data_prefix']\n",
    "    label_column = metadata['label_column']\n",
    "\n",
    "    train_data_file_pattern = TRANSFORMED_DATA_PREFIX + '/train/data-*.gz'\n",
    "    val_data_file_pattern = TRANSFORMED_DATA_PREFIX + '/val/data-*.gz'\n",
    "    test_data_file_pattern = TRANSFORMED_DATA_PREFIX + '/test/data-*.gz'\n",
    "\n",
    "    TRANSFORM_ARTIFACTS_DIR = metadata['transform_artifacts_dir']\n",
    "    tft_output = tft.TFTransformOutput(TRANSFORM_ARTIFACTS_DIR)\n",
    "    transform_feature_spec = tft_output.transformed_feature_spec()\n",
    "\n",
    "def get_model():\n",
    "    ''' Get the untrained model architecture '''\n",
    "    global model_artifacts\n",
    "\n",
    "    vertex_model = model_.get(args.model_id)\n",
    "    model_artifacts = vertex_model.gca_resource.artifact_uri\n",
    "    model = tf.keras.models.load_model(model_artifacts)\n",
    "\n",
    "    # Compile the model\n",
    "    hyperparams = {}\n",
    "    hyperparams[\"learning_rate\"] = args.lr\n",
    "    if args.experiment:\n",
    "        aip.log_params(hyperparams)\n",
    "\n",
    "    metadata.update(hyperparams)\n",
    "    with tf.io.gfile.GFile(os.path.join(args.model_dir, \"metrics.txt\"), \"w\") as f:\n",
    "        f.write(json.dumps(metadata))\n",
    "\n",
    "    train.compile(model, hyperparams)\n",
    "    return model\n",
    "\n",
    "def warmup_model(model):\n",
    "    ''' Warmup the initialized model weights '''\n",
    "    warmupparams = {}\n",
    "    warmupparams[\"num_epochs\"] = args.epochs\n",
    "    warmupparams[\"batch_size\"] = args.batch_size\n",
    "    warmupparams[\"steps\"] = args.steps\n",
    "    warmupparams[\"start_learning_rate\"] = args.start_lr\n",
    "    warmupparams[\"end_learning_rate\"] = args.lr\n",
    "\n",
    "    train.warmup(model, warmupparams, train_data_file_pattern, label_column, transform_feature_spec)\n",
    "    return model\n",
    "\n",
    "def train_model(model):\n",
    "    ''' Train the model '''\n",
    "    trainparams = {}\n",
    "    trainparams[\"num_epochs\"] = args.epochs\n",
    "    trainparams[\"batch_size\"] = args.batch_size\n",
    "    trainparams[\"early_stop\"] = {\"monitor\": \"val_loss\", \"patience\": 5}\n",
    "    if args.experiment:\n",
    "        aip.log_params(trainparams)\n",
    "\n",
    "    metadata.update(trainparams)\n",
    "    with tf.io.gfile.GFile(os.path.join(args.model_dir, \"metrics.txt\"), \"w\") as f:\n",
    "        f.write(json.dumps(metadata))\n",
    "\n",
    "    train.train(model, trainparams, train_data_file_pattern, val_data_file_pattern, label_column, transform_feature_spec, args.tensorboard_log_dir, args.tuning)\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model):\n",
    "    ''' Evaluate the model '''\n",
    "    evalparams = {}\n",
    "    evalparams[\"batch_size\"] = args.batch_size\n",
    "    metrics = train.evaluate(model, evalparams, test_data_file_pattern, label_column, transform_feature_spec)\n",
    "\n",
    "    metadata.update({'metrics': metrics})\n",
    "    with tf.io.gfile.GFile(os.path.join(args.model_dir, \"metrics.txt\"), \"w\") as f:\n",
    "        f.write(json.dumps(metadata))\n",
    "\n",
    "get_data()\n",
    "with strategy.scope():\n",
    "    model = get_model()\n",
    "\n",
    "if args.warmup:\n",
    "    model = warmup_model(model)\n",
    "else:\n",
    "    model = train_model(model)\n",
    "\n",
    "if args.evaluate:\n",
    "    evaluate_model(model)\n",
    "\n",
    "if args.serving:\n",
    "    logging.info('Save serving model to: ' + args.model_dir)\n",
    "    serving.construct_serving_model(\n",
    "        model=model,\n",
    "        serving_model_dir=args.model_dir,\n",
    "        metadata=metadata\n",
    "    )\n",
    "elif args.warmup:\n",
    "    logging.info('Save warmed up model to: ' + model_artifacts)\n",
    "    model.save(model_artifacts)\n",
    "else:\n",
    "    logging.info('Save trained model to: ' + args.model_dir)\n",
    "    model.save(args.model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test_package_locally"
   },
   "source": [
    "### 本地测试培训套餐\n",
    "\n",
    "接下来，只需进行几个周期的本地测试，测试您完成的培训套餐。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_package_locally"
   },
   "outputs": [],
   "source": [
    "DATASET_ID = dataset.resource_name\n",
    "MODEL_ID = vertex_custom_model.resource_name\n",
    "!cd custom; python3 -m trainer.task --model-id={MODEL_ID} --dataset-id={DATASET_ID} --experiment='chicago' --run='test' --project={PROJECT_ID} --epochs=5 --model-dir=/tmp --evaluate=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "warmup_base_model"
   },
   "source": [
    "热身训练\n",
    "\n",
    "现在你已经测试了训练脚本，你可以在基础模型上进行热身训练。热身训练是用来稳定权重初始化的。通过这样做，每次对模型架构进行训练和调整都将从相同稳定的权重初始化开始。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "warmup_base_model"
   },
   "outputs": [],
   "source": [
    "MODEL_DIR = f\"{BUCKET_NAME}/base_model\"\n",
    "\n",
    "!cd custom; python3 -m trainer.task --model-id={MODEL_ID} --dataset-id={DATASET_ID} --project={PROJECT_ID} --epochs=5 --steps=300 --batch_size=16 --lr=0.01 --start_lr=0.0001 --model-dir={MODEL_DIR} --warmup=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mirrored_intro"
   },
   "source": [
    "## 镜像策略\n",
    "\n",
    "在单个VM上进行训练时，可以选择在单个计算设备上训练，也可以选择在同一VM上的多个计算设备上训练。使用Vertex AI Distributed Training，您可以指定VM实例的计算设备数量和计算设备类型：CPU，GPU。\n",
    "\n",
    "Vertex AI Distributed Training支持TensorFlow模型的`tf.distribute.MirroredStrategy'。要在同一VM上多个计算设备上进行训练，您需要在Python训练脚本中执行以下额外步骤：\n",
    "\n",
    "1. 设置`tf.distribute.MirrorStrategy`\n",
    "2. 在`tf.distribute.MirrorStrategy`的范围内编译模型。*注意：*告诉MirroredStrategy要在计算设备之间镜像哪些变量。\n",
    "3. 将每个计算设备的批量大小增加到num_devices * batch size。\n",
    "\n",
    "在过渡期间，每个批次的分布以及对模型参数的更新将被同步。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_custom_pp_training_job:mbsdk"
   },
   "source": [
    "### 创建和运行自定义训练作业\n",
    "\n",
    "要训练一个自定义模型，您需要执行两个步骤：1）创建一个自定义训练作业，2）运行该作业。\n",
    "\n",
    "#### 创建自定义训练作业\n",
    "\n",
    "使用`CustomTrainingJob`类创建一个自定义训练作业，需要以下参数：\n",
    "\n",
    "- `display_name`：自定义训练作业的人类可读名称。\n",
    "- `container_uri`：训练容器镜像。\n",
    "- `python_package_gcs_uri`：Python训练包的位置，以tarball格式。\n",
    "- `python_module_name`：Python包中训练脚本的相对路径。\n",
    "- `model_serving_container_uri`：用于部署模型的容器镜像。\n",
    "\n",
    "*注意：* 没有requirements参数。您可以在Python包的`setup.py`脚本中指定任何需求。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_custom_pp_training_job:mbsdk"
   },
   "outputs": [],
   "source": [
    "DISPLAY_NAME = \"chicago_\" + TIMESTAMP\n",
    "\n",
    "job = aip.CustomPythonPackageTrainingJob(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    python_package_gcs_uri=f\"{BUCKET_NAME}/trainer_chicago.tar.gz\",\n",
    "    python_module_name=\"trainer.task\",\n",
    "    container_uri=TRAIN_IMAGE,\n",
    "    model_serving_container_image_uri=DEPLOY_IMAGE,\n",
    "    project=PROJECT_ID,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cleanup:trainer"
   },
   "outputs": [],
   "source": [
    "! rm -rf custom/logs\n",
    "! rm -rf custom/trainer/__pycache__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tarball_training_script"
   },
   "source": [
    "您可以将培训脚本存储在您的云存储桶中。\n",
    "\n",
    "接下来，您将培训文件夹打包成压缩的tar包，然后将其存储在您的云存储桶中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tarball_training_script"
   },
   "outputs": [],
   "source": [
    "! rm -f custom.tar custom.tar.gz\n",
    "! tar cvf custom.tar custom\n",
    "! gzip custom.tar\n",
    "! gsutil cp custom.tar.gz $BUCKET_NAME/trainer_chicago.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run_custom_pp_training_job:test"
   },
   "source": [
    "#### 运行自定义 Python 包训练作业\n",
    "\n",
    "接下来，您可以通过调用 `run()` 方法来运行自定义作业，从而开始训练作业。参数与运行 CustomTrainingJob 时相同。\n",
    "\n",
    "*注意:* 参数 `service_account` 被设置为使得初始化实验步骤 `aip.init(experiment=\"...\")` 必须具有权限访问 Vertex AI Metadata 存储库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_custom_pp_training_job:test"
   },
   "outputs": [],
   "source": [
    "MODEL_DIR = BUCKET_NAME + \"/testing\"\n",
    "\n",
    "CMDARGS = [\n",
    "    \"--epochs=5\",\n",
    "    \"--batch_size=16\",\n",
    "    \"--distribute=mirrored\",\n",
    "    \"--experiment=chicago\",\n",
    "    \"--run=test\",\n",
    "    \"--project=\" + PROJECT_ID,\n",
    "    \"--model-id=\" + MODEL_ID,\n",
    "    \"--dataset-id=\" + DATASET_ID,\n",
    "]\n",
    "\n",
    "model = job.run(\n",
    "    model_display_name=\"chicago_\" + TIMESTAMP,\n",
    "    args=CMDARGS,\n",
    "    replica_count=1,\n",
    "    machine_type=TRAIN_COMPUTE,\n",
    "    accelerator_type=TRAIN_GPU.name,\n",
    "    accelerator_count=TRAIN_NGPU,\n",
    "    base_output_dir=MODEL_DIR,\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    tensorboard=tensorboard_resource_name,\n",
    "    sync=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "delete_job"
   },
   "source": [
    "删除自定义训练作业\n",
    "\n",
    "在训练作业完成后，您可以使用方法 `delete()` 删除训练作业。在完成之前，可以使用方法 `cancel()` 取消训练作业。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "delete_job"
   },
   "outputs": [],
   "source": [
    "job.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_delete:mbsdk"
   },
   "source": [
    "删除模型\n",
    "\n",
    "`delete()` 方法将删除模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model_delete:mbsdk"
   },
   "outputs": [],
   "source": [
    "model.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hp_tuning"
   },
   "source": [
    "超参数调整\n",
    "\n",
    "接下来，您可以使用训练包进行超参数调整。训练包有一些新增内容，使得同一个包可以用于超参数调整、本地测试以及完整的云端训练：\n",
    "\n",
    "- 命令行：\n",
    "  - `tuning`：指示在训练过程中使用HyperTune服务作为回调。\n",
    "\n",
    "- `train()`：如果设置了tuning，将创建并添加一个回调到HyperTune服务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train_custom_job_machine_specification"
   },
   "source": [
    "准备您的机器规格\n",
    "\n",
    "现在为您的自定义训练工作定义机器规格。这告诉Vertex应为训练提供哪种类型的机器实例。\n",
    "- `machine_type`：要预留的GCP实例类型 -- 例如，n1-standard-8。\n",
    "- `accelerator_type`：硬件加速器的类型，如果有的话。在本教程中，如果您之前设置了变量`TRAIN_GPU != None`，则您正在使用GPU；否则将使用CPU。\n",
    "- `accelerator_count`：加速器的数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_custom_job_machine_specification"
   },
   "outputs": [],
   "source": [
    "if TRAIN_GPU:\n",
    "    machine_spec = {\n",
    "        \"machine_type\": TRAIN_COMPUTE,\n",
    "        \"accelerator_type\": TRAIN_GPU,\n",
    "        \"accelerator_count\": TRAIN_NGPU,\n",
    "    }\n",
    "else:\n",
    "    machine_spec = {\"machine_type\": TRAIN_COMPUTE, \"accelerator_count\": 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train_custom_job_disk_specification"
   },
   "source": [
    "### 准备您的磁盘规格\n",
    "\n",
    "（可选）现在为您的自定义训练作业定义磁盘规格。这告诉 Vertex 在培训中为每台机器实例提供何种类型和大小的磁盘。\n",
    "\n",
    "- `boot_disk_type`：SSD 或标准。SSD 更快，标准更便宜。默认为 SSD。\n",
    "- `boot_disk_size_gb`：磁盘大小（单位：GB）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_custom_job_disk_specification"
   },
   "outputs": [],
   "source": [
    "DISK_TYPE = \"pd-ssd\"  # [ pd-ssd, pd-standard]\n",
    "DISK_SIZE = 200  # GB\n",
    "\n",
    "disk_spec = {\"boot_disk_type\": DISK_TYPE, \"boot_disk_size_gb\": DISK_SIZE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "worker_pool_hpt"
   },
   "source": [
    "### 为超参数调整作业定义工作池规范\n",
    "\n",
    "接下来，定义工作池规范。请注意，我们计划调整学习率和批量大小，所以您不需要将它们作为命令行参数传递（已省略）。Vertex AI超参数调整服务将在试验期间为学习率和批量大小选择数值，并将它们作为命令行参数传递。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "worker_pool_hpt"
   },
   "outputs": [],
   "source": [
    "CMDARGS = [\n",
    "    \"--epochs=5\",\n",
    "    \"--distribute=mirrored\",\n",
    "    # \"--experiment=chicago\",\n",
    "    # \"--run=tune\",\n",
    "    # \"--project=\" + PROJECT_ID,\n",
    "    \"--model-id=\" + MODEL_ID,\n",
    "    \"--dataset-id=\" + DATASET_ID,\n",
    "    \"--tuning=True\",\n",
    "]\n",
    "\n",
    "worker_pool_spec = [\n",
    "    {\n",
    "        \"replica_count\": 1,\n",
    "        \"machine_spec\": machine_spec,\n",
    "        \"disk_spec\": disk_spec,\n",
    "        \"python_package_spec\": {\n",
    "            \"executor_image_uri\": TRAIN_IMAGE,\n",
    "            \"package_uris\": [BUCKET_NAME + \"/trainer_chicago.tar.gz\"],\n",
    "            \"python_module\": \"trainer.task\",\n",
    "            \"args\": CMDARGS,\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_custom_job:mbsdk"
   },
   "source": [
    "## 创建自定义作业\n",
    "\n",
    "使用类`CustomJob`创建自定义作业，例如用于超参数调整，具有以下参数：\n",
    "\n",
    "- `display_name`：自定义作业的人类可读名称。\n",
    "- `worker_pool_specs`：相应VM实例的规范。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_custom_job:mbsdk"
   },
   "outputs": [],
   "source": [
    "job = aip.CustomJob(\n",
    "    display_name=\"chicago_\" + TIMESTAMP, worker_pool_specs=worker_pool_spec\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_hpt_job:mbsdk"
   },
   "source": [
    "创建超参数调优作业\n",
    "\n",
    "使用类`HyperparameterTuningJob`来创建一个超参数调优作业，具有以下参数：\n",
    "\n",
    "- `display_name`：自定义作业的可读名称。\n",
    "- `custom_job`：此自定义作业的工作池规范适用于所有试验中创建的CustomJobs。\n",
    "- `metrics_spec`：要优化的指标。字典键是 metric_id，该 metric_id 由您的训练作业报告，字典值是指标的优化目标（'最小化'或'最大化'）。\n",
    "- `parameter_spec`：要优化的参数。字典键是 metric_id，作为命令行关键字参数传递到您的训练作业中，字典值是指标的参数规格。\n",
    "- `search_algorithm`：要使用的搜索算法：`grid`，`random`和`None`。如果指定`None`，则使用`Vizier`服务（贝叶斯）。\n",
    "- `max_trial_count`：要执行的最大试验次数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_hpt_job:stage2"
   },
   "outputs": [],
   "source": [
    "from google.cloud.aiplatform import hyperparameter_tuning as hpt\n",
    "\n",
    "hpt_job = aip.HyperparameterTuningJob(\n",
    "    display_name=\"chicago_\" + TIMESTAMP,\n",
    "    custom_job=job,\n",
    "    metric_spec={\n",
    "        \"val_loss\": \"minimize\",\n",
    "    },\n",
    "    parameter_spec={\n",
    "        \"lr\": hpt.DoubleParameterSpec(min=0.001, max=0.1, scale=\"log\"),\n",
    "        \"batch_size\": hpt.DiscreteParameterSpec([16, 32, 64, 128, 256], scale=\"linear\"),\n",
    "    },\n",
    "    search_algorithm=None,\n",
    "    max_trial_count=8,\n",
    "    parallel_trial_count=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run_hpt_job:mbsdk"
   },
   "source": [
    "运行超参数调整作业\n",
    "\n",
    "使用`run()`方法执行超参数调整作业。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_hpt_job:mbsdk"
   },
   "outputs": [],
   "source": [
    "hpt_job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "best_trial:mbsdk"
   },
   "source": [
    "现在看看哪个试验是最好的：### 最佳试验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "best_trial:mbsdk"
   },
   "outputs": [],
   "source": [
    "best = (None, None, None, 0.0)\n",
    "for trial in hpt_job.trials:\n",
    "    # Keep track of the best outcome\n",
    "    if float(trial.final_measurement.metrics[0].value) > best[3]:\n",
    "        try:\n",
    "            best = (\n",
    "                trial.id,\n",
    "                float(trial.parameters[0].value),\n",
    "                float(trial.parameters[1].value),\n",
    "                float(trial.final_measurement.metrics[0].value),\n",
    "            )\n",
    "        except:\n",
    "            best = (\n",
    "                trial.id,\n",
    "                float(trial.parameters[0].value),\n",
    "                None,\n",
    "                float(trial.final_measurement.metrics[0].value),\n",
    "            )\n",
    "\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "delete_hpt_job"
   },
   "source": [
    "### 删除超参数调整任务\n",
    "\n",
    "方法'delete()'将删除超参数调整任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "delete_hpt_job"
   },
   "outputs": [],
   "source": [
    "hpt_job.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save_hpt"
   },
   "source": [
    "保存最佳的超参数值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_hpt"
   },
   "outputs": [],
   "source": [
    "LR = best[2]\n",
    "BATCH_SIZE = int(best[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_custom_pp_training_job:mbsdk"
   },
   "source": [
    "### 创建和运行自定义训练作业\n",
    "\n",
    "要训练一个自定义模型，您需要执行两个步骤：1）创建一个自定义训练作业，2）运行该作业。\n",
    "\n",
    "#### 创建自定义训练作业\n",
    "\n",
    "使用`CustomTrainingJob`类创建一个自定义训练作业，其中包含以下参数：\n",
    "\n",
    "- `display_name`：自定义训练作业的可读名称。\n",
    "- `container_uri`：训练容器镜像。\n",
    "- `python_package_gcs_uri`：Python训练包的位置，以tarball方式存储。\n",
    "- `python_module_name`：Python包中训练脚本的相对路径。\n",
    "- `model_serving_container_uri`：用于部署模型的容器镜像。\n",
    "\n",
    "*注意：* 没有`requirements`参数。您可以在Python包的`setup.py`脚本中指定任何依赖项。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_custom_pp_training_job:mbsdk"
   },
   "outputs": [],
   "source": [
    "DISPLAY_NAME = \"chicago_\" + TIMESTAMP\n",
    "\n",
    "job = aip.CustomPythonPackageTrainingJob(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    python_package_gcs_uri=f\"{BUCKET_NAME}/trainer_chicago.tar.gz\",\n",
    "    python_module_name=\"trainer.task\",\n",
    "    container_uri=TRAIN_IMAGE,\n",
    "    model_serving_container_image_uri=DEPLOY_IMAGE,\n",
    "    project=PROJECT_ID,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run_custom_pp_training_job:full"
   },
   "source": [
    "运行自定义的Python包训练作业\n",
    "\n",
    "接下来，通过调用方法`run()`来运行自定义作业以启动训练作业。参数与运行CustomTrainingJob时相同。\n",
    "\n",
    "*注意:* 参数service_account 设置为使初始化实验步骤`aip.init(experiment=\"...\")`具有访问Vertex AI Metadata Store权限。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_custom_pp_training_job:full"
   },
   "outputs": [],
   "source": [
    "MODEL_DIR = BUCKET_NAME + \"/trained\"\n",
    "FULL_EPOCHS = 100\n",
    "\n",
    "CMDARGS = [\n",
    "    f\"--epochs={FULL_EPOCHS}\",\n",
    "    f\"--lr={LR}\",\n",
    "    f\"--batch_size={BATCH_SIZE}\",\n",
    "    \"--distribute=mirrored\",\n",
    "    \"--experiment=chicago\",\n",
    "    \"--run=full\",\n",
    "    \"--project=\" + PROJECT_ID,\n",
    "    \"--model-id=\" + MODEL_ID,\n",
    "    \"--dataset-id=\" + DATASET_ID,\n",
    "    \"--evaluate=True\",\n",
    "]\n",
    "\n",
    "model = job.run(\n",
    "    model_display_name=\"chicago_\" + TIMESTAMP,\n",
    "    args=CMDARGS,\n",
    "    replica_count=1,\n",
    "    machine_type=TRAIN_COMPUTE,\n",
    "    accelerator_type=TRAIN_GPU.name,\n",
    "    accelerator_count=TRAIN_NGPU,\n",
    "    base_output_dir=MODEL_DIR,\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    tensorboard=tensorboard_resource_name,\n",
    "    sync=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "delete_job"
   },
   "source": [
    "删除自定义训练作业\n",
    "\n",
    "在训练作业完成后，您可以使用`delete()`方法删除训练作业。在完成之前，可以使用`cancel()`方法取消训练作业。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "delete_job"
   },
   "outputs": [],
   "source": [
    "job.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "get_experiment"
   },
   "source": [
    "获取实验结果\n",
    "\n",
    "接下来，您可以将实验名称作为参数传递给方法 `get_experiment_df()`，以将实验结果获取为 pandas dataframe。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "get_experiment"
   },
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"chicago\"\n",
    "\n",
    "experiment_df = aip.get_experiment_df()\n",
    "experiment_df = experiment_df[experiment_df.experiment_name == EXPERIMENT_NAME]\n",
    "experiment_df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "review_builtin_metrics"
   },
   "source": [
    "## 审查定制模型评估结果\n",
    "\n",
    "接下来，您可以审查集成到培训包中的评估度量标准。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "review_builtin_metrics"
   },
   "outputs": [],
   "source": [
    "METRICS = MODEL_DIR + \"/model/metrics.txt\"\n",
    "! gsutil cat $METRICS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "delete_tensorboard"
   },
   "source": [
    "### 删除TensorBoard实例\n",
    "\n",
    "接下来，删除TensorBoard实例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "delete_tensorboard"
   },
   "outputs": [],
   "source": [
    "tensorboard.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "reload_model"
   },
   "outputs": [],
   "source": [
    "vertex_custom_model = model\n",
    "model = tf.keras.models.load_model(MODEL_DIR + \"/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "serving_function:chicago"
   },
   "source": [
    "## 添加一个serving功能\n",
    "\n",
    "接下来，您可以为在线和批处理预测向您的模型添加一个serving功能。这样可以将预测请求以原始格式（未经处理）发送，可以是序列化的TF.Example或JSONL对象。然后，serving功能将预处理预测请求转换为模型所期望的转换格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "serving_function:chicago"
   },
   "outputs": [],
   "source": [
    "%%writefile custom/trainer/serving.py\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_data_validation as tfdv\n",
    "import tensorflow_transform as tft\n",
    "import logging\n",
    "\n",
    "def _get_serve_features_fn(model, tft_output):\n",
    "    \"\"\"Returns a function that accept a dictionary of features and applies TFT.\"\"\"\n",
    "\n",
    "    model.tft_layer = tft_output.transform_features_layer()\n",
    "\n",
    "    @tf.function\n",
    "    def serve_features_fn(raw_features):\n",
    "        \"\"\"Returns the output to be used in the serving signature.\"\"\"\n",
    "\n",
    "        transformed_features = model.tft_layer(raw_features)\n",
    "        probabilities = model(transformed_features)\n",
    "        return {\"scores\": probabilities}\n",
    "\n",
    "\n",
    "    return serve_features_fn\n",
    "\n",
    "def _get_serve_tf_examples_fn(model, tft_output, feature_spec):\n",
    "    \"\"\"Returns a function that parses a serialized tf.Example and applies TFT.\"\"\"\n",
    "\n",
    "    model.tft_layer = tft_output.transform_features_layer()\n",
    "\n",
    "    @tf.function\n",
    "    def serve_tf_examples_fn(serialized_tf_examples):\n",
    "        \"\"\"Returns the output to be used in the serving signature.\"\"\"\n",
    "        for key in list(feature_spec.keys()):\n",
    "            if key not in features:\n",
    "                feature_spec.pop(key)\n",
    "\n",
    "        parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)\n",
    "\n",
    "        transformed_features = model.tft_layer(parsed_features)\n",
    "        probabilities = model(transformed_features)\n",
    "        return {\"scores\": probabilities}\n",
    "\n",
    "    return serve_tf_examples_fn\n",
    "\n",
    "def construct_serving_model(\n",
    "    model, serving_model_dir, metadata\n",
    "):\n",
    "    global features\n",
    "\n",
    "    schema_location = metadata['schema']\n",
    "    features = metadata['numeric_features'] + metadata['categorical_features'] + metadata['embedding_features']\n",
    "    print(\"FEATURES\", features)\n",
    "    tft_output_dir = metadata[\"transform_artifacts_dir\"]\n",
    "\n",
    "    schema = tfdv.load_schema_text(schema_location)\n",
    "    feature_spec = tft.tf_metadata.schema_utils.schema_as_feature_spec(schema).feature_spec\n",
    "\n",
    "    tft_output = tft.TFTransformOutput(tft_output_dir)\n",
    "\n",
    "    # Drop features that were not used in training\n",
    "    features_input_signature = {\n",
    "        feature_name: tf.TensorSpec(\n",
    "            shape=(None, 1), dtype=spec.dtype, name=feature_name\n",
    "        )\n",
    "        for feature_name, spec in feature_spec.items()\n",
    "        if feature_name in features\n",
    "    }\n",
    "\n",
    "    signatures = {\n",
    "        \"serving_default\": _get_serve_features_fn(\n",
    "            model, tft_output\n",
    "        ).get_concrete_function(features_input_signature),\n",
    "        \"serving_tf_example\": _get_serve_tf_examples_fn(\n",
    "            model, tft_output, feature_spec\n",
    "        ).get_concrete_function(\n",
    "            tf.TensorSpec(shape=[None], dtype=tf.string, name=\"examples\")\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    logging.info(\"Model saving started...\")\n",
    "    model.save(serving_model_dir, signatures=signatures)\n",
    "    logging.info(\"Model saving completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "construct_serving_model"
   },
   "source": [
    "### 构建服务模型\n",
    "\n",
    "现在构建服务模型并将服务模型存储到您的云存储桶中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "construct_serving_model"
   },
   "outputs": [],
   "source": [
    "os.chdir(\"custom\")\n",
    "\n",
    "from trainer import serving\n",
    "\n",
    "SERVING_MODEL_DIR = BUCKET_NAME + \"/serving_model\"\n",
    "\n",
    "serving.construct_serving_model(\n",
    "    model=model, serving_model_dir=SERVING_MODEL_DIR, metadata=metadata\n",
    ")\n",
    "\n",
    "serving_model = tf.keras.models.load_model(SERVING_MODEL_DIR)\n",
    "\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test_serving_model:tfrec"
   },
   "source": [
    "### 使用tf.Example数据在本地测试服务模型\n",
    "\n",
    "接下来，为tf.Example数据测试服务模型中的层接口。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_serving_model:tfrec"
   },
   "outputs": [],
   "source": [
    "EXPORTED_TFREC_PREFIX = metadata[\"exported_tfrec_prefix\"]\n",
    "file_names = tf.data.TFRecordDataset.list_files(\n",
    "    EXPORTED_TFREC_PREFIX + \"/data-*.tfrecord\"\n",
    ")\n",
    "for batch in tf.data.TFRecordDataset(file_names).batch(3).take(1):\n",
    "    predictions = serving_model.signatures[\"serving_tf_example\"](batch)\n",
    "    for key in predictions:\n",
    "        print(f\"{key}: {predictions[key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test_serving_model:jsonl,chicago"
   },
   "source": [
    "### 使用JSONL数据在本地测试服务模型\n",
    "\n",
    "接下来，测试服务模型中用于JSONL数据的层接口。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_serving_model:jsonl,chicago"
   },
   "outputs": [],
   "source": [
    "schema = tfdv.load_schema_text(metadata[\"schema\"])\n",
    "feature_spec = tft.tf_metadata.schema_utils.schema_as_feature_spec(schema).feature_spec\n",
    "\n",
    "instance = {\n",
    "    \"dropoff_grid\": \"POINT(-87.6 41.9)\",\n",
    "    \"euclidean\": 2064.2696,\n",
    "    \"loc_cross\": \"\",\n",
    "    \"payment_type\": \"Credit Card\",\n",
    "    \"pickup_grid\": \"POINT(-87.6 41.9)\",\n",
    "    \"trip_miles\": 1.37,\n",
    "    \"trip_day\": 12,\n",
    "    \"trip_hour\": 6,\n",
    "    \"trip_month\": 2,\n",
    "    \"trip_day_of_week\": 4,\n",
    "    \"trip_seconds\": 555,\n",
    "}\n",
    "\n",
    "for feature_name in instance:\n",
    "    dtype = feature_spec[feature_name].dtype\n",
    "    instance[feature_name] = tf.constant([[instance[feature_name]]], dtype)\n",
    "\n",
    "predictions = serving_model.signatures[\"serving_default\"](**instance)\n",
    "for key in predictions:\n",
    "    print(f\"{key}: {predictions[key].numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upload_serving_model:vertex,labels"
   },
   "source": [
    "### 将服务模型上传到 Vertex AI 模型资源\n",
    "\n",
    "接下来，您将上传您的服务定制模型工件到 Vertex AI，以转换为托管的 Vertex AI 模型资源。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload_serving_model:vertex,labels"
   },
   "outputs": [],
   "source": [
    "vertex_serving_model = aip.Model.upload(\n",
    "    display_name=\"chicago_\" + TIMESTAMP,\n",
    "    artifact_uri=SERVING_MODEL_DIR,\n",
    "    serving_container_image_uri=DEPLOY_IMAGE,\n",
    "    labels={\"user_metadata\": BUCKET_NAME[5:]},\n",
    "    sync=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluate_serving_model"
   },
   "source": [
    "### 评估服务模型\n",
    "\n",
    "接下来，使用评估（测试）切片来评估服务模型。为了进行苹果对苹果的比较，您可以为自定义模型和AutoML模型使用相同的评估切片。由于您的评估切片和指标可能是自定义的，我们建议：\n",
    "\n",
    "- 将每个评估切片作为 Vertex AI 批量预测作业发送。\n",
    "- 使用自定义评估脚本来评估批量预测作业的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluate_serving_model"
   },
   "outputs": [],
   "source": [
    "SERVING_OUTPUT_DATA_DIR = BUCKET_NAME + \"/batch_eval\"\n",
    "EXPORTED_JSONL_PREFIX = metadata[\"exported_jsonl_prefix\"]\n",
    "\n",
    "MIN_NODES = 1\n",
    "MAX_NODES = 1\n",
    "\n",
    "job = vertex_serving_model.batch_predict(\n",
    "    instances_format=\"jsonl\",\n",
    "    predictions_format=\"jsonl\",\n",
    "    job_display_name=\"chicago_\" + TIMESTAMP,\n",
    "    gcs_source=EXPORTED_JSONL_PREFIX + \"*.jsonl\",\n",
    "    gcs_destination_prefix=SERVING_OUTPUT_DATA_DIR,\n",
    "    model_parameters=None,\n",
    "    machine_type=DEPLOY_COMPUTE,\n",
    "    accelerator_type=DEPLOY_GPU,\n",
    "    accelerator_count=DEPLOY_NGPU,\n",
    "    starting_replica_count=MIN_NODES,\n",
    "    max_replica_count=MAX_NODES,\n",
    "    sync=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "custom_eval_script"
   },
   "source": [
    "### 执行自定义评估指标\n",
    "\n",
    "在批处理作业完成后，您将结果和目标标签输入到您的自定义评估脚本中。为了演示目的，我们只显示批处理预测的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "custom_eval_script"
   },
   "outputs": [],
   "source": [
    "batch_dir = ! gsutil ls $SERVING_OUTPUT_DATA_DIR\n",
    "batch_dir = batch_dir[0]\n",
    "outputs = ! gsutil ls $batch_dir\n",
    "errors = outputs[0]\n",
    "results = outputs[1]\n",
    "print(\"errors\")\n",
    "! gsutil cat $errors\n",
    "print(\"results\")\n",
    "! gsutil cat $results | head -n10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_model:async"
   },
   "outputs": [],
   "source": [
    "model = async_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "automl_job_wait:mbsdk"
   },
   "source": [
    "### 等待AutoML训练作业完成\n",
    "\n",
    "接下来，等待AutoML训练作业完成。或者，可以在`run()`方法中将参数`sync`设置为`True`，以阻塞直到AutoML训练作业完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "automl_job_wait:mbsdk"
   },
   "outputs": [],
   "source": [
    "model.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluate_the_model:mbsdk"
   },
   "source": [
    "## 检查模型评估分数\n",
    "\n",
    "在模型训练完成后，您可以使用`list_model_evaluations（）`方法来查看其评估分数。该方法将返回每个评估分片的迭代器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluate_the_model:mbsdk"
   },
   "outputs": [],
   "source": [
    "model_evaluations = model.list_model_evaluations()\n",
    "\n",
    "for model_evaluation in model_evaluations:\n",
    "    print(model_evaluation.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "custom_vs_automl_compare"
   },
   "source": [
    "最后，根据以下步骤，您可以决定当前实验是否产生了比AutoML基准更好的自定义模型：\n",
    "- 比较自定义模型和AutoML模型在每个评估切片上的评估结果。\n",
    "- 根据您的业务目的对结果进行加权。\n",
    "- 综合考虑结果并确定自定义模型是否更好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "store_model_metadata"
   },
   "source": [
    "### 存储自定义模型的店铺评估结果\n",
    "\n",
    "接下来，您可以使用标签字段来存储包含自定义指标信息的用户元数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "store_model_metadata"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "metadata = {}\n",
    "metadata[\"train_eval_metrics\"] = METRICS\n",
    "metadata[\"custom_eval_metrics\"] = \"[you-fill-this-in]\"\n",
    "\n",
    "with tf.io.gfile.GFile(\"gs://\" + BUCKET_NAME[5:] + \"/metadata.jsonl\", \"w\") as f:\n",
    "    json.dump(metadata, f)\n",
    "\n",
    "!gsutil cat $BUCKET_NAME/metadata.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleanup:mbsdk"
   },
   "source": [
    "清理\n",
    "\n",
    "要清理此项目中使用的所有 Google 云资源，您可以删除用于本教程的 [Google 云项目](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects)。\n",
    "\n",
    "否则，您可以删除本教程中创建的各个资源：\n",
    "\n",
    "- 数据集\n",
    "- 流水线\n",
    "- 模型\n",
    "- 端点\n",
    "- AutoML 训练作业\n",
    "- 批处理作业\n",
    "- 自定义作业\n",
    "- 超参数调优作业\n",
    "- 云存储存储桶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cleanup:stage2"
   },
   "outputs": [],
   "source": [
    "delete_all = False\n",
    "\n",
    "if delete_all:\n",
    "    # Delete the dataset using the Vertex dataset object\n",
    "    try:\n",
    "        if \"dataset\" in globals():\n",
    "            dataset.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the model using the Vertex model object\n",
    "    try:\n",
    "        if \"model\" in globals():\n",
    "            model.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    if \"BUCKET_NAME\" in globals():\n",
    "        ! gsutil rm -r $BUCKET_NAME"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "mlops_experimentation.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
