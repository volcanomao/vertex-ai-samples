{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "copyright"
   },
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title:generic,gcp"
   },
   "source": [
    "# GCP上的端到端机器学习： MLOps阶段1：数据管理\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage1/mlops_data_management.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      在GitHub上查看\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/ml_ops/stage1/mlops_data_management.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      在Vertex AI Workbench中打开\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "<br/><br/><br/>\n",
    "\n",
    "*注意：此笔记本不支持在Colab中执行*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "overview:mlops"
   },
   "source": [
    "## 概述\n",
    "\n",
    "本教程演示如何在Google Cloud上生产环境中使用Vertex AI进行端到端MLOps。本教程涵盖阶段1：数据管理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "objective:mlops,stage1,tabular"
   },
   "source": [
    "### 目标\n",
    "\n",
    "在本教程中，您将创建一个 MLOps 阶段 1：数据管理流程。\n",
    "\n",
    "本教程使用以下 Vertex AI 和数据分析服务：\n",
    "\n",
    "- `Vertex AI 数据集`\n",
    "- `BigQuery`\n",
    "- `Dataflow`\n",
    "\n",
    "执行的步骤包括：\n",
    "\n",
    "- 探索和可视化数据。\n",
    "- 从`BigQuery`表创建一个 Vertex AI `数据集` 资源 -- 用于 AutoML 训练。\n",
    "- 将数据集的副本提取到 Cloud 存储中的 CSV 文件。\n",
    "- 从 CSV 文件创建一个 Vertex AI `数据集` 资源 -- 作为 AutoML 训练的另一种选择。\n",
    "- 将`BigQuery`数据集的样本读取到 dataframe 中。\n",
    "- 使用 TensorFlow 数据验证从 dataframe 中的样本生成统计数据和数据架构。\n",
    "- 使用 TensorFlow 数据验证从数据架构生成 TFRecord 特征规范。\n",
    "- 使用`Dataflow`预处理部分`BigQuery`数据 -- 用于自定义训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "recommendation:mlops,stage1,tabular"
   },
   "source": [
    "### 推荐\n",
    "\n",
    "在谷歌云上进行端到端MLOps数据管理时，建议使用以下结构化（表格）数据的最佳实践：\n",
    "\n",
    "- 对于大量数据，请使用BigQuery表。否则，请使用存储在云存储中的CSV文件。\n",
    "- 在CSV文件中存储大量数据时，请将数据分片为每个分片的10,000行。\n",
    "- 使用Vertex AI的`TabularDataset`创建受管数据集。\n",
    "- 使用`Dataflow`对数据进行预处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset:bq,chicago,lbn"
   },
   "source": [
    "### 数据集\n",
    "\n",
    "本教程使用的数据集是[芝加哥出租车](https://www.kaggle.com/chicago/chicago-taxi-trips-bq)。本教程中使用的数据集版本存储在一个公共的BigQuery表中。经过训练的模型预测某人是否会为出租车服务留下小费。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9e483012a752"
   },
   "source": [
    "成本\n",
    "本教程使用了Google Cloud的计费组件：\n",
    "\n",
    "- Vertex AI\n",
    "- Cloud Storage\n",
    "- BigQuery\n",
    "- Dataflow\n",
    "\n",
    "了解[Vertex AI定价](https://cloud.google.com/vertex-ai/pricing)，[Cloud Storage定价](https://cloud.google.com/storage/pricing)，[BigQuery定价](https://cloud.google.com/bigquery/pricing)，以及[Dataflow定价](https://cloud.google.com/dataflow/pricing)，并使用[Pricing Calculator](https://cloud.google.com/products/calculator/)根据您预计的使用情况生成成本估算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_mlops"
   },
   "source": [
    "安装\n",
    "\n",
    "仅需安装一次用于执行 MLOps 笔记本的软件包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_mlops"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Vertex AI Workbench Notebook product has specific requirements\n",
    "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\") and not os.getenv(\"VIRTUAL_ENV\")\n",
    "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
    "    \"/opt/deeplearning/metadata/env_version\"\n",
    ")\n",
    "\n",
    "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_WORKBENCH_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\"\n",
    "\n",
    "ONCE_ONLY = False\n",
    "if ONCE_ONLY:\n",
    "    ! pip3 install -U {USER_FLAG} -q tensorflow==2.5 \\\n",
    "                                     tensorflow-data-validation==1.2 \\\n",
    "                                     tensorflow-transform==1.2 \\\n",
    "                                     tensorflow-io==0.18 \n",
    "    \n",
    "    ! pip3 install --upgrade {USER_FLAG} -q google-cloud-aiplatform[tensorboard] \\\n",
    "                                            google-cloud-pipeline-components \\\n",
    "                                            google-cloud-bigquery \\\n",
    "                                            google-cloud-logging \\\n",
    "                                            apache-beam[gcp] \\\n",
    "                                            pyarrow \\\n",
    "                                            cloudml-hypertune\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "restart"
   },
   "source": [
    "重新启动内核\n",
    "\n",
    "安装了额外的包之后，您需要重新启动笔记本内核，以便它可以找到这些包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "restart"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84cd83853240"
   },
   "source": [
    "开始之前\n",
    "\n",
    "设置您的Google Cloud项目\n",
    "\n",
    "**无论您使用哪种笔记本环境，下面的步骤都是必需的。**\n",
    "\n",
    "1. [选择或创建一个Google Cloud项目](https://console.cloud.google.com/cloud-resource-manager)。当您第一次创建帐户时，您将获得$300的免费信用额度用于计算/存储成本。\n",
    "\n",
    "1. [确保您的项目已启用计费](https://cloud.google.com/billing/docs/how-to/modify-project)。\n",
    "\n",
    "1. [启用Vertex AI、BigQuery、计算引擎和云存储API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,bigquery,compute_component,storage_component)。\n",
    "\n",
    "1. 如果您在本地运行此笔记本，您需要安装[Cloud SDK](https://cloud.google.com/sdk)。\n",
    "\n",
    "1. 在下面的单元格中输入您的项目ID。然后运行该单元格，确保Cloud SDK在本笔记本中的所有命令中使用正确的项目。\n",
    "\n",
    "**注意**: Jupyter会将以`!`开头的行视为shell命令，并将以`$`开头的Python变量插入这些命令中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "project_id"
   },
   "source": [
    "设置您的项目ID\n",
    "\n",
    "**如果您不知道您的项目ID**，您可以使用`gcloud`来获取您的项目ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_project_id"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "autoset_project_id"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID:\", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_gcloud_project_id"
   },
   "outputs": [],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "区域\n",
    "\n",
    "您还可以更改“REGION”变量，该变量用于笔记本的其余部分的操作。以下是Vertex AI支持的区域。我们建议您选择最接近您的区域。\n",
    "\n",
    "- 美洲：`us-central1`\n",
    "- 欧洲：`europe-west4`\n",
    "- 亚太：`asia-east1`\n",
    "\n",
    "您可能无法在Vertex AI进行训练时使用多区域存储桶。并非所有区域都支持所有Vertex AI服务。\n",
    "\n",
    "了解有关[Vertex AI区域](https://cloud.google.com/vertex-ai/docs/general/locations)的更多信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "region"
   },
   "outputs": [],
   "source": [
    "REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
    "\n",
    "if REGION == \"[your-region]\":\n",
    "    REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "timestamp"
   },
   "source": [
    "时间戳\n",
    "\n",
    "如果您正在进行实时教程会话，您可能正在使用共享的测试帐户或项目。为了避免用户在创建的资源之间发生名称冲突，您为每个实例会话创建一个时间戳，并将时间戳附加到您在本教程中创建的资源名称上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "timestamp"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77c385f0db59"
   },
   "source": [
    "### 验证您的Google Cloud账户\n",
    "\n",
    "**如果您正在使用Vertex AI Workbench笔记本**，您的环境已经通过验证。\n",
    "\n",
    "**如果您正在使用Colab**，运行下方的单元格，并按照提示进行身份验证，通过oAuth授权您的账户。\n",
    "\n",
    "**否则**，请按照以下步骤操作：\n",
    "\n",
    "在Cloud控制台中，前往[创建服务帐户密钥](https://console.cloud.google.com/apis/credentials/serviceaccountkey)页面。\n",
    "\n",
    "1. **点击创建服务帐户**。\n",
    "\n",
    "2. 在**服务帐户名称**字段中输入一个名称，然后点击**创建**。\n",
    "\n",
    "3. 在**将此服务帐户授权访问项目**部分，点击角色下拉列表。在过滤框中输入\"Vertex AI\"，选择**Vertex AI管理员**。在过滤框中输入\"Storage Object Admin\"，选择**Storage Object Admin**。\n",
    "\n",
    "4. 点击创建。包含您密钥的JSON文件将下载到您的本地环境。\n",
    "\n",
    "5. 在下方的单元格中，将您的服务帐户密钥路径输入为GOOGLE_APPLICATION_CREDENTIALS变量，然后运行该单元格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "535223fa4b84"
   },
   "outputs": [],
   "source": [
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# If on Vertex AI Workbench, then don't execute this code\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\") and not os.getenv(\n",
    "    \"DL_ANACONDA_HOME\"\n",
    "):\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\"):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bucket:custom"
   },
   "source": [
    "### 创建一个云存储桶\n",
    "\n",
    "**无论您使用什么笔记本环境，都需要执行以下步骤。**\n",
    "\n",
    "当您使用 Vertex AI SDK 提交一个自定义训练任务时，您需要上传一个包含训练代码的 Python 软件包到一个云存储桶中。Vertex AI 将从该软件包中运行代码。在本教程中，Vertex AI 还会将您的任务的训练模型保存在同一个存储桶中。您可以随后基于这个输出创建一个 `Endpoint` 资源，以便用于提供在线预测。\n",
    "\n",
    "在下面设置您的云存储桶的名称。存储桶的名称在所有 Google Cloud 项目中必须是全局唯一的，包括您所在组织之外的项目。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bucket"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"[your-bucket-name]\"  # @param {type:\"string\"}\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "autoset_bucket"
   },
   "outputs": [],
   "source": [
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"[your-bucket-name]\":\n",
    "    BUCKET_NAME = PROJECT_ID + \"aip-\" + TIMESTAMP\n",
    "    BUCKET_URI = \"gs://\" + BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_bucket"
   },
   "source": [
    "只有在您的存储桶尚不存在时：运行以下单元格以创建您的云存储存储桶。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_bucket"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "validate_bucket"
   },
   "source": [
    "最后，通过查看其内容来验证对Cloud Storage存储桶的访问。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "validate_bucket"
   },
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_vars"
   },
   "source": [
    "### 设置变量\n",
    "\n",
    "接下来，设置一些在整个教程中使用的变量。\n",
    "### 导入库并定义常量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_bq"
   },
   "source": [
    "导入BigQuery\n",
    "\n",
    "将BigQuery包导入到您的Python环境中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_bq"
   },
   "outputs": [],
   "source": [
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_beam"
   },
   "source": [
    "#### 导入Apache Beam\n",
    "\n",
    "将Apache Beam包导入到您的Python环境中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_beam"
   },
   "outputs": [],
   "source": [
    "import apache_beam as beam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_tf"
   },
   "source": [
    "导入TensorFlow\n",
    "\n",
    "将TensorFlow包导入您的Python环境中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_tf"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_tfdv"
   },
   "source": [
    "导入TensorFlow数据验证（TFDV）包到您的Python环境中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_tfdv"
   },
   "outputs": [],
   "source": [
    "import tensorflow_data_validation as tfdv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_tft"
   },
   "source": [
    "#### 导入TensorFlow Transform\n",
    "\n",
    "将TensorFlow Transform（TFT）包导入到您的Python环境中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_tft"
   },
   "outputs": [],
   "source": [
    "import tensorflow_transform as tft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk"
   },
   "source": [
    "### 初始化Python的Vertex AI SDK\n",
    "\n",
    "为您的项目和对应的存储桶初始化Python的Vertex AI SDK。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "aip.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_bq"
   },
   "source": [
    "创建BigQuery客户端。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_bq"
   },
   "outputs": [],
   "source": [
    "bqclient = bigquery.Client(project=PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "recommendation:dataset,tabular"
   },
   "source": [
    "数据集\n",
    "\n",
    "接下来，您将看看创建受管数据集的选项：\n",
    "\n",
    "* `BigQuery`：创建一个 Vertex `TabularDataset` 资源。\n",
    "* `CSV`：创建一个 Vertex `TabularDataset` 资源。\n",
    "* `TFRecords`：在云存储上自行管理数据集。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_file:u_dataset,bq"
   },
   "source": [
    "#### BigQuery培训数据的位置。\n",
    "\n",
    "现在将变量`IMPORT_FILE`设置为BigQuery中数据表的位置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_file:chicago,bq,lbn"
   },
   "outputs": [],
   "source": [
    "IMPORT_FILE = \"bq://bigquery-public-data.chicago_taxi_trips.taxi_trips\"\n",
    "BQ_TABLE = \"bigquery-public-data.chicago_taxi_trips.taxi_trips\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "explore_bq:chicago"
   },
   "source": [
    "### 探索 BigQuery 数据集\n",
    "\n",
    "探索 BigQuery 表格的内容：\n",
    "\n",
    "- 获取所有来自 2015 年的示例\n",
    "- 按照一周中的日期排序\n",
    "- 计算每周各天的示例数量."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "explore_bq:chicago"
   },
   "outputs": [],
   "source": [
    "query = \"\"\"SELECT\n",
    "    CAST(EXTRACT(DAYOFWEEK FROM trip_start_timestamp) AS string) AS trip_dayofweek,\n",
    "    FORMAT_DATE('%A',cast(trip_start_timestamp as date)) AS trip_dayname,\n",
    "    COUNT(*) as trip_count,\n",
    "FROM `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
    "WHERE\n",
    "    EXTRACT(YEAR FROM trip_start_timestamp) = 2015\n",
    "GROUP BY\n",
    "    trip_dayofweek,\n",
    "    trip_dayname\n",
    "ORDER BY\n",
    "    trip_dayofweek\"\"\"\n",
    "\n",
    "_ = bqclient.query(query)\n",
    "rows = _.result()\n",
    "dataframe = rows.to_dataframe()\n",
    "print(dataframe.head(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_bq:chicago"
   },
   "outputs": [],
   "source": [
    "dataframe.plot(kind=\"bar\", x=\"trip_dayname\", y=\"trip_count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bq_copy:chicago"
   },
   "source": [
    "### 创建BigQuery表的私有副本\n",
    "\n",
    "接下来，您可以创建BigQuery表的私有副本：\n",
    "- 选择列的子集\n",
    "- 选择行的子集（限制）\n",
    "- 设置条件（WHERE）\n",
    "- 对地理位置坐标进行特征工程\n",
    "- 预先拆分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bq_copy:chicago"
   },
   "outputs": [],
   "source": [
    "BQ_DATASET = BQ_TABLE.split(\".\")[1]\n",
    "BQ_TABLE_COPY = f\"{PROJECT_ID}.{BQ_DATASET}.taxi_trips\"\n",
    "LIMIT = 300000\n",
    "YEAR = 2020\n",
    "\n",
    "# First, create the dataset entry\n",
    "dataset = bigquery.Dataset(f\"{PROJECT_ID}.{BQ_DATASET}\")\n",
    "dataset.location = \"US\"\n",
    "dataset = bqclient.create_dataset(dataset, timeout=30)\n",
    "\n",
    "query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE `{BQ_TABLE_COPY}`\n",
    "AS (\n",
    "    WITH\n",
    "      taxitrips AS (\n",
    "      SELECT\n",
    "        trip_start_timestamp,\n",
    "        trip_seconds,\n",
    "        trip_miles,\n",
    "        payment_type,\n",
    "        pickup_longitude,\n",
    "        pickup_latitude,\n",
    "        dropoff_longitude,\n",
    "        dropoff_latitude,\n",
    "        tips,\n",
    "        fare\n",
    "      FROM\n",
    "        `{BQ_TABLE}`\n",
    "      WHERE pickup_longitude IS NOT NULL\n",
    "      AND pickup_latitude IS NOT NULL\n",
    "      AND dropoff_longitude IS NOT NULL\n",
    "      AND dropoff_latitude IS NOT NULL\n",
    "      AND trip_miles > 0\n",
    "      AND trip_seconds > 0\n",
    "      AND fare > 0\n",
    "      AND EXTRACT(YEAR FROM trip_start_timestamp) = {YEAR}\n",
    "    )\n",
    "\n",
    "    SELECT\n",
    "      EXTRACT(MONTH from trip_start_timestamp) as trip_month,\n",
    "      EXTRACT(DAY from trip_start_timestamp) as trip_day,\n",
    "      EXTRACT(DAYOFWEEK from trip_start_timestamp) as trip_day_of_week,\n",
    "      EXTRACT(HOUR from trip_start_timestamp) as trip_hour,\n",
    "      CAST(trip_seconds AS FLOAT64) as trip_seconds,\n",
    "      trip_miles,\n",
    "      payment_type,\n",
    "      ST_AsText(\n",
    "          ST_SnapToGrid(ST_GeogPoint(pickup_longitude, pickup_latitude), 0.1)\n",
    "      ) AS pickup_grid,\n",
    "      ST_AsText(\n",
    "          ST_SnapToGrid(ST_GeogPoint(dropoff_longitude, dropoff_latitude), 0.1)\n",
    "      ) AS dropoff_grid,\n",
    "      ST_Distance(\n",
    "          ST_GeogPoint(pickup_longitude, pickup_latitude),\n",
    "          ST_GeogPoint(dropoff_longitude, dropoff_latitude)\n",
    "      ) AS euclidean,\n",
    "      CONCAT(\n",
    "          ST_AsText(ST_SnapToGrid(ST_GeogPoint(pickup_longitude,\n",
    "              pickup_latitude), 0.1)),\n",
    "          ST_AsText(ST_SnapToGrid(ST_GeogPoint(dropoff_longitude,\n",
    "              dropoff_latitude), 0.1))\n",
    "      ) AS loc_cross,\n",
    "      IF((tips/fare >= 0.2), 1, 0) AS tip_bin,\n",
    "    FROM\n",
    "      taxitrips\n",
    "    LIMIT {LIMIT}\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "response = bqclient.query(query)\n",
    "_ = response.result()\n",
    "\n",
    "BQ_TABLE = BQ_TABLE_COPY\n",
    "IMPORT_FILE = f\"bq://{BQ_TABLE_COPY}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_dataset:tabular,bq,lbn"
   },
   "source": [
    "### 创建数据集\n",
    "\n",
    "#### BigQuery 输入数据\n",
    "\n",
    "接下来，使用 `TabularDataset` 类的 `create` 方法创建 `Dataset` 资源，该方法需要以下参数：\n",
    "\n",
    "- `display_name`：`Dataset` 资源的人类可读名称。\n",
    "- `bq_source`：将数据项从 BigQuery 表导入到 `Dataset` 资源中。\n",
    "- `labels`：用户定义的元数据。在此示例中，您可以存储包含用户定义数据的 Cloud Storage 存储桶的位置。\n",
    "\n",
    "了解更多关于 [TabularDataset from BigQuery table](https://cloud.google.com/vertex-ai/docs/datasets/create-dataset-api#aiplatform_create_dataset_tabular_bigquery_sample-python)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_dataset:tabular,bq,lbn"
   },
   "outputs": [],
   "source": [
    "dataset = aip.TabularDataset.create(\n",
    "    display_name=\"Chicago Taxi\" + \"_\" + TIMESTAMP,\n",
    "    bq_source=[IMPORT_FILE],\n",
    "    labels={\"user_metadata\": BUCKET_NAME},\n",
    ")\n",
    "\n",
    "label_column = \"tip_bin\"\n",
    "\n",
    "print(dataset.resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bq_to_dataframe:all"
   },
   "source": [
    "### 读取 BigQuery 数据集到 pandas dataframe\n",
    "\n",
    "接下来，您可以使用 BigQuery `list_rows()` 和 `to_dataframe()` 方法，将数据集的样本读取到 pandas dataframe 中，具体步骤如下：\n",
    "\n",
    "- `list_rows()`: 对指定表执行查询并返回查询结果的行迭代器。可选指定：\n",
    "  - `selected_fields`: 要返回的字段（列）的子集。\n",
    "  - `max_results`: 要返回的最大行数。与 SQL LIMIT 命令相同。\n",
    "\n",
    "- `rows.to_dataframe()`: 调用行迭代器并将数据读取到 pandas dataframe 中。\n",
    "\n",
    "了解更多关于[将 BigQuery 表加载到 dataframe 中的内容](https://cloud.google.com/bigquery/docs/bigquery-storage-python-pandas)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bq_to_dataframe:all"
   },
   "outputs": [],
   "source": [
    "# Download a table.\n",
    "table = bigquery.TableReference.from_string(BQ_TABLE)\n",
    "\n",
    "rows = bqclient.list_rows(table, max_results=300000)\n",
    "\n",
    "dataframe = rows.to_dataframe()\n",
    "print(dataframe.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfdv_stats:dataframe"
   },
   "source": [
    "### 生成数据集统计\n",
    "\n",
    "#### 数据框输入数据\n",
    "\n",
    "使用 TensorFlow Data Validation（TFDV）包对数据集生成统计数据。使用`generate_statistics_from_dataframe()`方法，并设置以下参数：\n",
    "\n",
    "- `dataframe`：存储在内存中的pandas数据框中的数据集。\n",
    "- `stats_options`：已选统计选项：\n",
    "  - `label_feature`：要预测的列。\n",
    "  - `sample_rate`：抽样率。如果指定，则对样本进行统计。\n",
    "  - `num_top_values`：要保留的最常见特征值数量。\n",
    "\n",
    "了解有关[TensorFlow Data Validation（TFDV）](https://www.tensorflow.org/tfx/data_validation/get_started)的信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tfdv_stats:dataframe"
   },
   "outputs": [],
   "source": [
    "stats = tfdv.generate_statistics_from_dataframe(\n",
    "    dataframe=dataframe,\n",
    "    stats_options=tfdv.StatsOptions(\n",
    "        label_feature=\"tip_bin\", sample_rate=1, num_top_values=50\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfdv_visualize_stats"
   },
   "source": [
    "### 可视化数据集统计信息\n",
    "\n",
    "可以使用TFDV的`visualize_statistics()`方法来显示数据集统计信息的可视化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tfdv_visualize_stats"
   },
   "outputs": [],
   "source": [
    "tfdv.visualize_statistics(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stats_extract_features"
   },
   "source": [
    "### 从统计数据中提取特征分组\n",
    "\n",
    "接下来，您可以从统计数据中提取特征名称和数据类型，然后将这些特征分为：\n",
    "\n",
    "- 数值特征：浮点数\n",
    "- 分类特征：字符串、整数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "stats_extract_features"
   },
   "outputs": [],
   "source": [
    "NUMERIC_FEATURES = []\n",
    "CATEGORICAL_FEATURES = []\n",
    "for _ in range(len(stats.datasets[0].features)):\n",
    "    if stats.datasets[0].features[_].path.step[0] == label_column:\n",
    "        continue\n",
    "    if stats.datasets[0].features[_].type == 0:  # int\n",
    "        CATEGORICAL_FEATURES.append(stats.datasets[0].features[_].path.step[0])\n",
    "    elif stats.datasets[0].features[_].type == 1:  # float\n",
    "        NUMERIC_FEATURES.append(stats.datasets[0].features[_].path.step[0])\n",
    "    elif stats.datasets[0].features[_].type == 2:  # string\n",
    "        CATEGORICAL_FEATURES.append(stats.datasets[0].features[_].path.step[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "retain_feature_info:tabular"
   },
   "source": [
    "### 保留特征列信息\n",
    "\n",
    "接下来，您需要保留数据集中特征列的信息。在此示例中，您将将这些用户定义的元数据作为 JSON 文件添加到与数据集关联的 Cloud Storage 存储桶中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "retain_feature_info:tabular"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "metadata = {\n",
    "    \"label_column\": label_column,\n",
    "    \"numeric_features\": NUMERIC_FEATURES,\n",
    "    \"categorical_features\": CATEGORICAL_FEATURES,\n",
    "}\n",
    "with tf.io.gfile.GFile(\n",
    "    \"gs://\" + dataset.labels[\"user_metadata\"] + \"/metadata.jsonl\", \"w\"\n",
    ") as f:\n",
    "    json.dump(metadata, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfdv_stats:write"
   },
   "source": [
    "保留数据集的统计信息\n",
    "\n",
    "接下来，您将数据集的统计信息写入数据集的云存储桶，并保留统计文件的云存储位置。在这个例子中，您将其添加到存储在数据集云存储桶中的此数据集的用户定义元数据中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tfdv_stats:write"
   },
   "outputs": [],
   "source": [
    "STATISTICS_SCHEMA = BUCKET_URI + \"/statistics.jsonl\"\n",
    "\n",
    "tfdv.write_stats_text(stats, BUCKET_URI + \"/statistics.jsonl\")\n",
    "\n",
    "with tf.io.gfile.GFile(\n",
    "    \"gs://\" + dataset.labels[\"user_metadata\"] + \"/metadata.jsonl\", \"r\"\n",
    ") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "metadata[\"statistics\"] = STATISTICS_SCHEMA\n",
    "with tf.io.gfile.GFile(\n",
    "    \"gs://\" + dataset.labels[\"user_metadata\"] + \"/metadata.jsonl\", \"w\"\n",
    ") as f:\n",
    "    json.dump(metadata, f)\n",
    "\n",
    "! gsutil cat $BUCKET_URI/metadata.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfdv_schema"
   },
   "source": [
    "### 生成原始数据模式\n",
    "\n",
    "使用 TensorFlow 数据验证（TFDV）包在数据集上生成数据模式。使用 `infer_schema()` 方法，并设置以下参数：\n",
    "\n",
    "- `statistics`：TFDV 生成的统计信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tfdv_schema"
   },
   "outputs": [],
   "source": [
    "schema = tfdv.infer_schema(statistics=stats)\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfdv_schema:save"
   },
   "source": [
    "将数据集的模式保存到云存储\n",
    "\n",
    "接下来，您将数据集的模式编写到数据集的云存储存储桶中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tfdv_schema:save"
   },
   "outputs": [],
   "source": [
    "SCHEMA_LOCATION = BUCKET_URI + \"/schema.txt\"\n",
    "\n",
    "# When running Apache Beam directly (file is directly accessed)\n",
    "tfdv.write_schema_text(output_path=SCHEMA_LOCATION, schema=schema)\n",
    "# When running with Dataflow (file is uploaded to worker pool)\n",
    "tfdv.write_schema_text(output_path=\"schema.txt\", schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfdv_schema:write"
   },
   "source": [
    "保留数据集的模式\n",
    "\n",
    "接下来，您保留模式文件的云存储位置。在这个示例中，您将其添加到存储在数据集的云存储桶中的用户定义的元数据中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tfdv_schema:write"
   },
   "outputs": [],
   "source": [
    "with tf.io.gfile.GFile(\n",
    "    \"gs://\" + dataset.labels[\"user_metadata\"] + \"/metadata.jsonl\", \"r\"\n",
    ") as f:\n",
    "    metadata = json.load(f)\n",
    "metadata[\"schema\"] = SCHEMA_LOCATION\n",
    "\n",
    "with tf.io.gfile.GFile(\n",
    "    \"gs://\" + dataset.labels[\"user_metadata\"] + \"/metadata.jsonl\", \"w\"\n",
    ") as f:\n",
    "    json.dump(metadata, f)\n",
    "\n",
    "! gsutil cat $BUCKET_URI/metadata.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tft_feature_spec"
   },
   "source": [
    "生成特征规范，与TFRecords兼容，在具有TensorFlow Transform (TFT)包的数据集上。使用`schema_as_feature_spec()`方法，具有以下参数：\n",
    "\n",
    "- `schema`：由TFDV生成的数据模式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tft_feature_spec"
   },
   "outputs": [],
   "source": [
    "feature_spec = tft.tf_metadata.schema_utils.schema_as_feature_spec(schema).feature_spec\n",
    "\n",
    "print(feature_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataflow_setup:transform"
   },
   "source": [
    "准备数据流作业的软件包要求。\n",
    "\n",
    "在运行数据流作业之前，您需要为执行作业的工作池指定软件包要求。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dataflow_setup:transform"
   },
   "outputs": [],
   "source": [
    "%%writefile setup.py\n",
    "import setuptools\n",
    "\n",
    "REQUIRED_PACKAGES = [\n",
    "    \"google-cloud-aiplatform\",\n",
    "    \"tensorflow-transform==1.2.0\",\n",
    "    \"tensorflow-data-validation==1.2.0\",\n",
    "]\n",
    "\n",
    "setuptools.setup(\n",
    "    name=\"executor\",\n",
    "    version=\"0.0.1\",\n",
    "    install_requires=REQUIRED_PACKAGES,\n",
    "    packages=setuptools.find_packages(),\n",
    "    include_package_data=True,\n",
    "    package_data={\"./\": [\"schema.txt\"]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataflow:preprocess,chicago"
   },
   "source": [
    "创建预处理函数\n",
    "\n",
    "接下来，您将创建一个特定于数据集的预处理函数。在这个例子中，您将预处理函数写入一个单独的Python模块，并添加一个__init__.py以使其看起来像一个包。为什么？当您在Dataflow中运行Apache Beam管道时，您的脚本会在一个或多个工作程序上运行。预处理函数在与管道不同的工作程序中运行，因此不包含管道的运行时，如全局变量的值。要解决这个问题，您需要将所有依赖项和值硬编码到预处理包中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dataflow:preprocess,chicago"
   },
   "outputs": [],
   "source": [
    "! rm -rf src\n",
    "! mkdir src\n",
    "! touch src/__init__.py\n",
    "\n",
    "with open(\"src/features.py\", \"w\") as f:\n",
    "    f.write(\"import tensorflow as tf\\n\")\n",
    "    f.write(\"import tensorflow_transform as tft\\n\")\n",
    "\n",
    "    f.write(\"def preprocessing_fn(inputs):\\n\")\n",
    "    f.write(\"\toutputs = {}\\n\")\n",
    "    f.write(\"\tfor key in inputs.keys():\\n\")\n",
    "    f.write(f\"\t\tif key in {NUMERIC_FEATURES}:\\n\")\n",
    "    f.write(\"\t\t\toutputs[key] = tft.scale_to_z_score(inputs[key])\\n\")\n",
    "    f.write(f\"\t\telif key in {CATEGORICAL_FEATURES}:\\n\")\n",
    "    f.write(\"\t\t\toutputs[key] = tft.compute_and_apply_vocabulary(\\n\")\n",
    "    f.write(\"\t\t\t\tinputs[key],\\n\")\n",
    "    f.write(\"\t\t\t\tnum_oov_buckets=1,\\n\")\n",
    "    f.write(\"\t\t\t\tvocab_filename=key,\\n\")\n",
    "    f.write(\"\t\t\t)\\n\")\n",
    "    f.write(\"\t\telse:\\n\")\n",
    "    f.write(\"\t\t\toutputs[key] = inputs[key]\\n\")\n",
    "    f.write(\"\t\toutputs[key] = tf.squeeze(outputs[key], -1)\\n\")\n",
    "    f.write(\"\treturn outputs\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataflow:transform,bq,chicago"
   },
   "source": [
    "### 使用Dataflow预处理数据\n",
    "\n",
    "#### 数据预处理\n",
    "\n",
    "接下来，您将使用Dataflow预处理数据。在这个例子中，您将查询BigQuery表，并将示例拆分成训练、验证和测试（评估）数据集，并对特征列进行预处理：\n",
    "\n",
    "- `Numeric`：使用 `tft.scale_to_z_score` 重新缩放值。\n",
    "- `Categorical`：使用 `tft.compute_and_apply_vocabulary` 将其编码为分类列。\n",
    "\n",
    "除了经过预处理（转换）的数据之外，还会生成原始版本的测试数据，分别以tf.Example和JSONL格式存储。转换工件也会被保存，以供后续的服务功能将原始数据转换为经过转换的数据。\n",
    "\n",
    "总而言之，生成的输出包括：\n",
    "\n",
    "- 经过转换的训练数据\n",
    "- 经过转换的验证数据\n",
    "- 经过转换的测试数据\n",
    "- 以JSONL格式存储的原始测试数据\n",
    "- 以tf.Example格式存储的原始测试数据\n",
    "- 转换函数工件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dataflow:transform,bq,chicago"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow_transform.beam as tft_beam\n",
    "from src import features\n",
    "\n",
    "RUNNER = \"DataflowRunner\"  # DirectRunner for local running w/o Dataflow\n",
    "\n",
    "\n",
    "def parse_bq_record(bq_record):\n",
    "    \"\"\"Parses a bq_record to a dictionary.\"\"\"\n",
    "    output = {}\n",
    "    for key in bq_record:\n",
    "        output[key] = [bq_record[key]]\n",
    "    return output\n",
    "\n",
    "\n",
    "def split_dataset(bq_row, num_partitions, ratio):\n",
    "    \"\"\"Returns a partition number for a given bq_row.\"\"\"\n",
    "    import json\n",
    "\n",
    "    assert num_partitions == len(ratio)\n",
    "    bucket = sum(map(ord, json.dumps(bq_row))) % sum(ratio)\n",
    "    total = 0\n",
    "    for i, part in enumerate(ratio):\n",
    "        total += part\n",
    "        if bucket < total:\n",
    "            return i\n",
    "    return len(ratio) - 1\n",
    "\n",
    "\n",
    "def convert_to_jsonl(data, label=None):\n",
    "    \"\"\"Converts a parsed record to JSON\"\"\"\n",
    "    import json\n",
    "\n",
    "    if label:\n",
    "        del data[label]\n",
    "    return json.dumps(data)\n",
    "\n",
    "\n",
    "def run_pipeline(args):\n",
    "    \"\"\"Runs a Beam pipeline to split the dataset\"\"\"\n",
    "\n",
    "    pipeline_options = beam.pipeline.PipelineOptions(flags=[], **args)\n",
    "\n",
    "    raw_data_query = args[\"raw_data_query\"]\n",
    "    label = args[\"label\"]\n",
    "    transformed_data_prefix = args[\"transformed_data_prefix\"]\n",
    "    transform_artifact_dir = args[\"transform_artifact_dir\"]\n",
    "    exported_jsonl_prefix = args[\"exported_jsonl_prefix\"]\n",
    "    exported_tfrec_prefix = args[\"exported_tfrec_prefix\"]\n",
    "    temp_location = args[\"temp_location\"]\n",
    "    project = args[\"project\"]\n",
    "\n",
    "    schema = tfdv.load_schema_text(SCHEMA_LOCATION)\n",
    "    feature_spec = tft.tf_metadata.schema_utils.schema_as_feature_spec(\n",
    "        schema\n",
    "    ).feature_spec\n",
    "\n",
    "    raw_metadata = tft.tf_metadata.dataset_metadata.DatasetMetadata(\n",
    "        tft.tf_metadata.schema_utils.schema_from_feature_spec(feature_spec)\n",
    "    )\n",
    "\n",
    "    with beam.Pipeline(options=pipeline_options) as pipeline:\n",
    "        with tft_beam.Context(temp_location):\n",
    "\n",
    "            # Read raw BigQuery data.\n",
    "            raw_train_data, raw_val_data, raw_test_data = (\n",
    "                pipeline\n",
    "                | \"Read Raw Data\"\n",
    "                >> beam.io.ReadFromBigQuery(\n",
    "                    query=raw_data_query,\n",
    "                    project=project,\n",
    "                    use_standard_sql=True,\n",
    "                )\n",
    "                | \"Parse Data\" >> beam.Map(parse_bq_record)\n",
    "                | \"Split\" >> beam.Partition(split_dataset, 3, ratio=[8, 1, 1])\n",
    "            )\n",
    "\n",
    "            # Create a train_dataset from the data and schema.\n",
    "            raw_train_dataset = (raw_train_data, raw_metadata)\n",
    "\n",
    "            # Analyze and transform raw_train_dataset to produced transformed_train_dataset and transform_fn.\n",
    "            transformed_train_dataset, transform_fn = (\n",
    "                raw_train_dataset\n",
    "                | \"Analyze & Transform\"\n",
    "                >> tft_beam.AnalyzeAndTransformDataset(features.preprocessing_fn)\n",
    "            )\n",
    "\n",
    "            # Get data and schema separately from the transformed_dataset.\n",
    "            transformed_train_data, transformed_metadata = transformed_train_dataset\n",
    "\n",
    "            # write transformed train data.\n",
    "            _ = (\n",
    "                transformed_train_data\n",
    "                | \"Write Transformed Train Data\"\n",
    "                >> beam.io.tfrecordio.WriteToTFRecord(\n",
    "                    file_path_prefix=os.path.join(\n",
    "                        transformed_data_prefix, \"train/data\"\n",
    "                    ),\n",
    "                    file_name_suffix=\".gz\",\n",
    "                    coder=tft.coders.ExampleProtoCoder(transformed_metadata.schema),\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Create a val_dataset from the data and schema.\n",
    "            raw_val_dataset = (raw_val_data, raw_metadata)\n",
    "\n",
    "            # Transform raw_val_dataset to produced transformed_val_dataset using transform_fn.\n",
    "            transformed_val_dataset = (\n",
    "                raw_val_dataset,\n",
    "                transform_fn,\n",
    "            ) | \"Transform Validation Data\" >> tft_beam.TransformDataset()\n",
    "\n",
    "            # Get data from the transformed_val_dataset.\n",
    "            transformed_val_data, _ = transformed_val_dataset\n",
    "\n",
    "            # write transformed val data.\n",
    "            _ = (\n",
    "                transformed_val_data\n",
    "                | \"Write Transformed Validation Data\"\n",
    "                >> beam.io.tfrecordio.WriteToTFRecord(\n",
    "                    file_path_prefix=os.path.join(transformed_data_prefix, \"val/data\"),\n",
    "                    file_name_suffix=\".gz\",\n",
    "                    coder=tft.coders.ExampleProtoCoder(transformed_metadata.schema),\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Create a test_dataset from the data and schema.\n",
    "            raw_test_dataset = (raw_test_data, raw_metadata)\n",
    "\n",
    "            # Transform raw_test_dataset to produced transformed_test_dataset using transform_fn.\n",
    "            transformed_test_dataset = (\n",
    "                raw_test_dataset,\n",
    "                transform_fn,\n",
    "            ) | \"Transform Test Data\" >> tft_beam.TransformDataset()\n",
    "\n",
    "            # Get data from the transformed_test_dataset.\n",
    "            transformed_test_data, _ = transformed_test_dataset\n",
    "\n",
    "            # write transformed test data.\n",
    "            _ = (\n",
    "                transformed_test_data\n",
    "                | \"Write Transformed Test Data\"\n",
    "                >> beam.io.tfrecordio.WriteToTFRecord(\n",
    "                    file_path_prefix=os.path.join(transformed_data_prefix, \"test/data\"),\n",
    "                    file_name_suffix=\".gz\",\n",
    "                    coder=tft.coders.ExampleProtoCoder(transformed_metadata.schema),\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Write transform_fn.\n",
    "            _ = transform_fn | \"Write Transform Artifacts\" >> tft_beam.WriteTransformFn(\n",
    "                transform_artifact_dir\n",
    "            )\n",
    "\n",
    "            # Write raw test data to GCS as TF Records\n",
    "            _ = (\n",
    "                raw_test_data\n",
    "                | \"Write TF Test Data\"\n",
    "                >> beam.io.tfrecordio.WriteToTFRecord(\n",
    "                    file_path_prefix=os.path.join(exported_tfrec_prefix, \"data\"),\n",
    "                    file_name_suffix=\".tfrecord\",\n",
    "                    coder=tft.coders.ExampleProtoCoder(raw_metadata.schema),\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Convert raw test data to JSON (for batch prediction)\n",
    "            json_test_data = (raw_test_data) | \"Convert Batch Test Data\" >> beam.Map(\n",
    "                convert_to_jsonl, label=label\n",
    "            )\n",
    "\n",
    "            # Write raw test data to GCS as JSONL files.\n",
    "            _ = json_test_data | \"Write JSONL Test Data\" >> beam.io.WriteToText(\n",
    "                file_path_prefix=exported_jsonl_prefix, file_name_suffix=\".jsonl\"\n",
    "            )\n",
    "\n",
    "\n",
    "EXPORTED_JSONL_PREFIX = os.path.join(BUCKET_URI, \"exported_data/jsonl\")\n",
    "EXPORTED_TFREC_PREFIX = os.path.join(BUCKET_URI, \"exported_data/tfrec\")\n",
    "TRANSFORMED_DATA_PREFIX = os.path.join(BUCKET_URI, \"transformed_data\")\n",
    "TRANSFORM_ARTIFACTS_DIR = os.path.join(BUCKET_URI, \"transformed_artifacts\")\n",
    "\n",
    "QUERY_STRING = \"SELECT * FROM {} LIMIT 300000\".format(BQ_TABLE)\n",
    "JOB_NAME = \"chicago\" + TIMESTAMP\n",
    "\n",
    "args = {\n",
    "    \"runner\": RUNNER,\n",
    "    \"raw_data_query\": QUERY_STRING,\n",
    "    \"label\": label_column,\n",
    "    \"transformed_data_prefix\": TRANSFORMED_DATA_PREFIX,\n",
    "    \"transform_artifact_dir\": TRANSFORM_ARTIFACTS_DIR,\n",
    "    \"exported_jsonl_prefix\": EXPORTED_JSONL_PREFIX,\n",
    "    \"exported_tfrec_prefix\": EXPORTED_TFREC_PREFIX,\n",
    "    \"temp_location\": os.path.join(BUCKET_URI, \"temp\"),\n",
    "    \"project\": PROJECT_ID,\n",
    "    \"region\": REGION,\n",
    "    \"setup_file\": \"./setup.py\",\n",
    "}\n",
    "\n",
    "print(\"Data preprocessing started...\")\n",
    "run_pipeline(args)\n",
    "print(\"Data preprocessing completed.\")\n",
    "\n",
    "! gsutil ls $TRANSFORMED_DATA_PREFIX/train\n",
    "! gsutil ls $TRANSFORMED_DATA_PREFIX/val\n",
    "! gsutil ls $TRANSFORMED_DATA_PREFIX/test\n",
    "! gsutil ls $TRANSFORM_ARTIFACTS_DIR\n",
    "! gsutil ls {EXPORTED_JSONL_PREFIX}*\n",
    "! gsutil ls $EXPORTED_TFREC_PREFIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataflow:transform,retain"
   },
   "source": [
    "#### 保留转换后数据集的引用\n",
    "\n",
    "接下来，您需要保留转换后的数据集与数据集的Cloud Storage位置的关联。在本例中，您将其添加到存储在数据集的Cloud Storage存储桶中的用户定义的元数据中。\n",
    "\n",
    "在数据转换过程中，转换函数计算了每个特征的唯一出现次数。一些分类值（字符串，整数）可能具有很多唯一值。在这种情况下，最好通过将其从分类变为嵌入特征来降低它们的维度。\n",
    "\n",
    "该代码使用了一个经验法则，即嵌入大小应该是唯一值的平方根。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dataflow:transform,retain"
   },
   "outputs": [],
   "source": [
    "with tf.io.gfile.GFile(\n",
    "    \"gs://\" + dataset.labels[\"user_metadata\"] + \"/metadata.jsonl\", \"r\"\n",
    ") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "tft_output = tft.TFTransformOutput(TRANSFORM_ARTIFACTS_DIR)\n",
    "\n",
    "CATEGORICAL_FEATURES = []\n",
    "EMBEDDING_FEATURES = []\n",
    "categorical_features = metadata[\"categorical_features\"]\n",
    "for feature in categorical_features:\n",
    "    unique = tft_output.vocabulary_size_by_name(feature)\n",
    "    if unique > 10:\n",
    "        EMBEDDING_FEATURES.append(feature)\n",
    "        print(\"Convert to embedding\", feature, unique)\n",
    "    else:\n",
    "        CATEGORICAL_FEATURES.append(feature)\n",
    "\n",
    "metadata[\"categorical_features\"] = CATEGORICAL_FEATURES\n",
    "metadata[\"embedding_features\"] = EMBEDDING_FEATURES\n",
    "\n",
    "metadata[\"transformed_data_prefix\"] = TRANSFORMED_DATA_PREFIX\n",
    "metadata[\"transform_artifacts_dir\"] = TRANSFORM_ARTIFACTS_DIR\n",
    "metadata[\"exported_jsonl_prefix\"] = EXPORTED_JSONL_PREFIX\n",
    "metadata[\"exported_tfrec_prefix\"] = EXPORTED_TFREC_PREFIX\n",
    "with tf.io.gfile.GFile(\n",
    "    \"gs://\" + dataset.labels[\"user_metadata\"] + \"/metadata.jsonl\", \"w\"\n",
    ") as f:\n",
    "    json.dump(metadata, f)\n",
    "\n",
    "! gsutil cat $BUCKET_URI/metadata.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleanup:mbsdk"
   },
   "source": [
    "清理\n",
    "\n",
    "为了清理此项目中使用的所有Google Cloud资源，您可以删除用于本教程的[Google Cloud项目](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects)。\n",
    "\n",
    "否则，您可以删除在本教程中创建的各个资源。\n",
    "\n",
    "*注意:* stage2/mlops_experimentation 依赖于由stage1笔记本创建的资源。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cleanup:stage1"
   },
   "outputs": [],
   "source": [
    "delete_all = False\n",
    "\n",
    "if delete_all:\n",
    "    # Delete the dataset using the Vertex dataset object\n",
    "    try:\n",
    "        if \"dataset\" in globals():\n",
    "            dataset.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    if \"BUCKET_URI\" in globals():\n",
    "        ! gsutil rm -r $BUCKET_URI"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "mlops_data_management.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
