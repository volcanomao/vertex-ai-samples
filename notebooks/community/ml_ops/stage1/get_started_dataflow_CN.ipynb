{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "copyright"
   },
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title:generic,gcp"
   },
   "source": [
    "# GCP上的端到端ML：MLOps第1阶段：数据管理：开始使用Dataflow\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage1/get_started_dataflow.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      在GitHub上查看\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "        <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage1/get_started_dataflow.ipynb\">\n",
    "        <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\\\" alt=\"Colab logo\"> 在Colab中运行\n",
    "        </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/ml_ops/stage1/get_started_dataflow.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      在Vertex AI Workbench中打开\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "<br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "overview:mlops"
   },
   "source": [
    "## 概述\n",
    "\n",
    "本教程演示了如何在Google Cloud上使用Vertex AI进行端到端MLOps生产环境。本教程涵盖了阶段1：数据管理：开始使用Dataflow。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "objective:mlops,stage1,get_started_dataflow"
   },
   "source": [
    "### 目标\n",
    "\n",
    "在本教程中，您将学习如何在`Vertex AI`中使用`Dataflow`进行培训。\n",
    "\n",
    "本教程使用以下谷歌云ML服务:\n",
    "\n",
    "- `Dataflow`\n",
    "- `BigQuery数据集`\n",
    "\n",
    "执行的步骤包括:\n",
    "\n",
    "- 数据的离线预处理:\n",
    "    - 串行 - 无Dataflow\n",
    "    - 并行 - 使用Dataflow\n",
    "- 上游数据的预处理:\n",
    "    - 表格数据\n",
    "    - 图像数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "recommendation:mlops,stage1,dataflow"
   },
   "source": [
    "### 推荐\n",
    "\n",
    "在 Google Cloud 上进行端到端 MLOps 时，以下是预处理和训练自定义模型期间数据馈送的最佳实践：\n",
    "\n",
    "#### 预处理\n",
    "\n",
    "数据预处理可以是：\n",
    "\n",
    "- 离线：数据在训练之前被预处理并存储。\n",
    "    - 小数据集：当有新数据时重新处理并存储。\n",
    "- 上游：数据在馈送模型之前在模型上游被预处理。\n",
    "    - 在 CPU 上训练。\n",
    "- 下游：数据在馈送模型之前在模型下游被预处理。\n",
    "    - 在硬件加速器上训练（例如，GPU/TPU）。\n",
    "\n",
    "#### 模型馈送\n",
    "\n",
    "数据用于模型馈送可以是：\n",
    "\n",
    "- 内存中：小数据集。\n",
    "- 从磁盘读取：大数据集，快速训练。\n",
    "- 从磁盘读取的 `Dataflow`：大规模数据集，扩展训练。\n",
    "\n",
    "#### AutoML\n",
    "\n",
    "对于 AutoML 训练，预处理和模型馈送会自动处理。\n",
    "\n",
    "另外对于 AutoML 表格模型训练，您可以重新配置默认的预处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset:gsod,lrg"
   },
   "source": [
    "### 数据集\n",
    "\n",
    "本教程使用的数据集是来自[BigQuery公共数据集](https://cloud.google.com/bigquery/public-data)的GSOD数据集。您使用的数据集版本只使用年份、月份和日期字段来预测每日平均温度（mean_temp）的值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9e483012a752"
   },
   "source": [
    "###  费用\n",
    "本教程使用 Google Cloud 的计费组件：\n",
    "\n",
    "- Vertex AI\n",
    "- Cloud Storage\n",
    "- BigQuery\n",
    "- Dataflow\n",
    "\n",
    "了解[Vertex AI 定价](https://cloud.google.com/vertex-ai/pricing)、[Cloud Storage 定价](https://cloud.google.com/storage/pricing)、[BigQuery 定价](https://cloud.google.com/bigquery/pricing) 和[Dataflow 定价](https://cloud.google.com/dataflow/pricing)，并使用[定价计算器](https://cloud.google.com/products/calculator/)根据您的预计使用量生成成本估算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_mlops"
   },
   "source": [
    "安装以下软件包以执行此笔记本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_mlops"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Vertex AI Workbench Notebook product has specific requirements\n",
    "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\") and not os.getenv(\"VIRTUAL_ENV\")\n",
    "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
    "    \"/opt/deeplearning/metadata/env_version\"\n",
    ")\n",
    "\n",
    "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_WORKBENCH_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\"\n",
    "\n",
    "extra_pkgs = \"tensorflow==2.5 tensorflow-data-validation==1.2 tensorflow-transform==1.2 \\\n",
    "              tensorflow-io==0.18 pyarrow pandas apache-beam[gcp] google-cloud-bigquery\"\n",
    "! pip3 install --upgrade --quiet {USER_FLAG} google-cloud-aiplatform $extra_pkgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "restart"
   },
   "source": [
    "重新启动内核\n",
    "\n",
    "安装完额外的包后，您需要重新启动笔记本内核，以便它可以找到这些包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "restart"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fc8fb52b5cca"
   },
   "source": [
    "常见设置\n",
    "\n",
    "现在，执行笔记本教程的常见设置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "001a0fcd5d78"
   },
   "outputs": [],
   "source": [
    "# Common code setup for notebook tutorials\n",
    "\n",
    "! wget https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/ml_ops/setup.py -O setup.py\n",
    "\n",
    "%run setup.py --bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d809f07a8935"
   },
   "outputs": [],
   "source": [
    "# Other Common setup instructions for notebook tutorials\n",
    "\n",
    "! wget https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/ml_ops/setup.md -O setup.md\n",
    "\n",
    "%load setup.md "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_vars"
   },
   "source": [
    "### 设置变量\n",
    "\n",
    "接下来，设置一些在教程中使用的变量。\n",
    "### 导入库并定义常量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_beam"
   },
   "source": [
    "#### 导入Apache Beam\n",
    "\n",
    "将Apache Beam包导入到您的Python环境中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_beam"
   },
   "outputs": [],
   "source": [
    "import apache_beam as beam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_bq"
   },
   "source": [
    "#### 导入 BigQuery\n",
    "\n",
    "将 BigQuery 包导入到您的 Python 环境中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_bq"
   },
   "outputs": [],
   "source": [
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_pandas"
   },
   "source": [
    "#### 导入pandas\n",
    "\n",
    "将pandas包导入到您的Python环境中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_pandas"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_numpy"
   },
   "source": [
    "导入numpy\n",
    "\n",
    "将numpy包导入到您的Python环境中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_numpy"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_tfdv"
   },
   "source": [
    "导入 TensorFlow 数据验证\n",
    "\n",
    "将 TensorFlow 数据验证（TFDV）包导入到您的Python环境中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_tfdv"
   },
   "outputs": [],
   "source": [
    "import tensorflow_data_validation as tfdv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_tft"
   },
   "source": [
    "#### 导入TensorFlow变换\n",
    "\n",
    "在您的Python环境中导入TensorFlow变换（TFT）包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_tft"
   },
   "outputs": [],
   "source": [
    "import tensorflow_transform as tft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk,region"
   },
   "source": [
    "### 初始化Python的Vertex AI SDK\n",
    "\n",
    "为您的项目和相应的存储桶初始化Python的Vertex AI SDK。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_aip:mbsdk,region"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_bq"
   },
   "source": [
    "创建BigQuery客户端。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_bq"
   },
   "outputs": [],
   "source": [
    "bqclient = bigquery.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "offline_preprocess:bq"
   },
   "source": [
    "## 使用 pandas dataframe 对 BigQuery 表进行离线预处理数据\n",
    "\n",
    "- 离线：在训练之前，BigQuery 表在内存中进行预处理并存储。\n",
    "\n",
    "    - 将表格数据提取到 pandas dataframe 中。\n",
    "    - 在 dataframe 中逐列对数据进行预处理。\n",
    "    - 将预处理后的 dataframe 写入新的 BigQuery 表中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_file:gsod,bq,lrg"
   },
   "outputs": [],
   "source": [
    "IMPORT_FILE = \"bq://bigquery-public-data.samples.gsod\"\n",
    "BQ_TABLE = \"bigquery-public-data.samples.gsod\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bq_to_dataframe:gsod"
   },
   "source": [
    "### 将BigQuery数据集读入pandas dataframe\n",
    "\n",
    "接下来，您可以使用BigQuery的`list_rows()`和`to_dataframe()`方法，将数据集的样本读入pandas dataframe，具体如下：\n",
    "\n",
    "- `list_rows()`: 在指定表上执行查询并返回一个行迭代器以获取查询结果。可选指定：\n",
    "   - `selected_fields`: 要返回的字段（列）的子集。\n",
    "   - `max_results`: 要返回的行数的最大值。与SQL LIMIT命令相同。\n",
    "\n",
    "- `rows.to_dataframe()`: 调用行迭代器并将数据读入pandas dataframe。\n",
    "\n",
    "了解更多关于[将BigQuery表加载到dataframe](https://cloud.google.com/bigquery/docs/bigquery-storage-python-pandas)的信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bq_to_dataframe:gsod"
   },
   "outputs": [],
   "source": [
    "# Download a table.\n",
    "table = bigquery.TableReference.from_string(\"bigquery-public-data.samples.gsod\")\n",
    "\n",
    "rows = bqclient.list_rows(\n",
    "    table,\n",
    "    max_results=500,\n",
    "    selected_fields=[\n",
    "        bigquery.SchemaField(\"station_number\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"year\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"month\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"day\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"mean_temp\", \"FLOAT\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "dataframe = rows.to_dataframe()\n",
    "print(dataframe.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataframe_transform:gsod"
   },
   "source": [
    "### 在pandas dataframe中转换数据。\n",
    "\n",
    "接下来，你需要对dataframe中的数据进行预处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dataframe_transform:gsod"
   },
   "outputs": [],
   "source": [
    "dataframe[\"station_number\"] = pd.to_numeric(dataframe[\"station_number\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqml_create_dataset"
   },
   "source": [
    "### 创建BQ数据集资源\n",
    "\n",
    "首先，在您的项目中创建一个空的数据集资源。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bqml_create_dataset"
   },
   "outputs": [],
   "source": [
    "BQ_MY_DATASET = 'samples'\n",
    "BQ_MY_TABLE = 'gsod'\n",
    "! bq --location=US mk -d \\\n",
    "$PROJECT_ID:$BQ_MY_DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dataframe_to_bq:transformed,gsod"
   },
   "outputs": [],
   "source": [
    "job_config = bigquery.LoadJobConfig(\n",
    "    # Specify a (partial) schema. All columns are always written to the\n",
    "    # table. The schema is used to assist in data type definitions.\n",
    "    schema=[\n",
    "        bigquery.SchemaField(\"station_number\", \"FLOAT\"),  # <-- after one hot encoding\n",
    "        bigquery.SchemaField(\"year\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"month\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"day\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"mean_temp\", \"FLOAT\"),\n",
    "    ],\n",
    "    # Optionally, set the write disposition. BigQuery appends loaded rows\n",
    "    # to an existing table by default, but with WRITE_TRUNCATE write\n",
    "    # disposition it replaces the table with the loaded data.\n",
    "    write_disposition=\"WRITE_TRUNCATE\",\n",
    ")\n",
    "\n",
    "NEW_BQ_TABLE = f\"{PROJECT_ID}.samples.gsod_transformed\"\n",
    "\n",
    "job = bqclient.load_table_from_dataframe(\n",
    "    dataframe, NEW_BQ_TABLE, job_config=job_config\n",
    ")  # Make an API request.\n",
    "job.result()  # Wait for the job to complete.\n",
    "\n",
    "table = bqclient.get_table(NEW_BQ_TABLE)  # Make an API request.\n",
    "print(\n",
    "    \"Loaded {} rows and {} columns to {}\".format(\n",
    "        table.num_rows, len(table.schema), NEW_BQ_TABLE\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upstream_preprocess:image"
   },
   "source": [
    "## 使用 tf.data.Dataset 生成器对上游数据进行预处理\n",
    "\n",
    "### 图像数据\n",
    "\n",
    "- 上游数据：数据在模型训练时被提前进行预处理。\n",
    "\n",
    "    - 定义预处理函数：\n",
    "        - 输入：未处理的张量批量\n",
    "        - 输出：预处理后的张量批量\n",
    "    - 使用 tf.data.Dataset 的 `map()` 方法将预处理函数映射到生成器的输出中。\n",
    "\n",
    "在本例中：\n",
    "\n",
    "- 将 CIFAR10 数据集加载到内存中作为 numpy 数组。\n",
    "- 为内存中的 CIFAR10 数据集创建一个 tf.data.Dataset 生成器。*注意*：将像素数据转换为 FLOAT32 类型，以便与将像素数据输出为 FLOAT32 类型的预处理函数兼容。\n",
    "- 定义一个预处理函数来通过 1/255.0 对像素数据进行重新缩放。\n",
    "- 将预处理函数映射到生成器中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upstream_preprocess:image"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "tf_dataset = tf.data.Dataset.from_tensor_slices((x_train.astype(np.float32), y_train))\n",
    "\n",
    "print(\"Before preprocessing\")\n",
    "for batch in tf_dataset:\n",
    "    print(batch)\n",
    "    break\n",
    "\n",
    "\n",
    "def preprocess_fn(inputs, labels):\n",
    "    inputs /= 255.0\n",
    "    return tf.cast(inputs, tf.float32), labels\n",
    "\n",
    "\n",
    "tf_dataset = tf_dataset.map(preprocess_fn)\n",
    "\n",
    "print(\"After preprocessing\")\n",
    "for batch in tf_dataset:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upstream_preprocess:tabular"
   },
   "source": [
    "## 通过tf.data.Dataset生成器进行上游预处理数据\n",
    "\n",
    "### 表格数据\n",
    "\n",
    "- 上游: 在数据被用于训练之前，数据被模型上游预处理。\n",
    "\n",
    "    - 定义预处理函数:\n",
    "        - 输入: 未加工的张量批次\n",
    "        - 输出: 预处理的张量批次\n",
    "    - 使用tf.data.Dataset `map()` 方法将预处理函数映射到生成器输出。\n",
    "\n",
    "在这个例子中:\n",
    "\n",
    "- 为波士顿房屋数据创建tf.data.Dataset生成器。\n",
    "- 在预处理前迭代一个批次。\n",
    "- 定义预处理函数，将所有特征缩放在0到1之间。\n",
    "- 将预处理函数映射到数据集。\n",
    "- 通过数据集迭代一次。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upstream_preprocess:tabular"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import boston_housing\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = boston_housing.load_data()\n",
    "\n",
    "tf_dataset = tf.data.Dataset.from_tensor_slices((x_train.astype(np.float32), y_train))\n",
    "\n",
    "print(\"Before preprocessing\")\n",
    "for batch in tf_dataset:\n",
    "    print(batch)\n",
    "    break\n",
    "\n",
    "\n",
    "def preprocessing_fn(inputs, labels):\n",
    "    inputs = tft.scale_to_0_1(inputs)\n",
    "    return tf.cast(inputs, tf.float32), labels\n",
    "\n",
    "\n",
    "tf_dataset = tf_dataset.map(preprocessing_fn)\n",
    "\n",
    "print(\"After preprocessing\")\n",
    "for batch in tf_dataset:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "offline_preprocess:dataflow"
   },
   "source": [
    "## 数据流离线预处理\n",
    "\n",
    "- 从BigQuery表生成数据架构。\n",
    "- 定义Beam管道来：\n",
    "    - 将BigQuery表中的数据分割成训练集和评估集。\n",
    "    - 使用数据架构将数据集编码为TFRecords。\n",
    "    - 将TFRecords保存为压缩文件到Cloud Storage。\n",
    "- 运行管道。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bq_to_dataframe:gsod"
   },
   "source": [
    "### 将BigQuery数据集读取到pandas dataframe中\n",
    "\n",
    "接下来，您可以使用BigQuery `list_rows()` 和 `to_dataframe()` 方法从数据集中读取一部分数据到pandas dataframe中，具体步骤如下：\n",
    "\n",
    "- `list_rows()`: 在指定表上执行查询并返回一个行迭代器来获取查询结果。您可以选择指定以下参数：\n",
    " - `selected_fields`: 要返回的字段（列）的子集。\n",
    " - `max_results`: 要返回的最大行数。类似于SQL的LIMIT命令。\n",
    "\n",
    "- `rows.to_dataframe()`: 调用行迭代器并将数据读取到pandas dataframe中。\n",
    "\n",
    "了解更多关于[将BigQuery表加载到dataframe中](https://cloud.google.com/bigquery/docs/bigquery-storage-python-pandas)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bq_to_dataframe:gsod"
   },
   "outputs": [],
   "source": [
    "# Download a table.\n",
    "table = bigquery.TableReference.from_string(\"bigquery-public-data.samples.gsod\")\n",
    "\n",
    "rows = bqclient.list_rows(\n",
    "    table,\n",
    "    max_results=500,\n",
    "    selected_fields=[\n",
    "        bigquery.SchemaField(\"station_number\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"year\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"month\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"day\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"mean_temp\", \"FLOAT\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "dataframe = rows.to_dataframe()\n",
    "print(dataframe.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfdv_stats:dataframe"
   },
   "source": [
    "### 生成数据集统计信息\n",
    "\n",
    "#### Dataframe 输入数据\n",
    "\n",
    "使用 TensorFlow Data Validation (TFDV) 包对数据集生成统计信息。使用 `generate_statistics_from_dataframe()` 方法，其中包括以下参数：\n",
    "\n",
    "- `dataframe`: 存储在内存中的 pandas dataframe 格式的数据集。\n",
    "- `stats_options`: 选择的统计选项:\n",
    "  - `label_feature`: 要预测的列。\n",
    "  - `sample_rate`: 采样率。如果指定了该参数，将在采样数据上计算统计信息。\n",
    "  - `num_top_values`: 对于字符串类型特征，保留的最常见特征值数量。\n",
    "\n",
    "了解有关[TensorFlow Data Validation (TFDV)](https://www.tensorflow.org/tfx/data_validation/get_started)的信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tfdv_stats:dataframe"
   },
   "outputs": [],
   "source": [
    "stats = tfdv.generate_statistics_from_dataframe(\n",
    "    dataframe=dataframe,\n",
    "    stats_options=tfdv.StatsOptions(\n",
    "        label_feature=\"mean_temp\", sample_rate=1, num_top_values=50\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfdv_schema"
   },
   "source": [
    "生成原始数据架构\n",
    "\n",
    "使用 TensorFlow Data Validation (TFDV) 包在数据集上生成数据架构。使用 `infer_schema()` 方法，使用以下参数：\n",
    "\n",
    "- `statistics`: TFDV 生成的统计数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tfdv_schema"
   },
   "outputs": [],
   "source": [
    "schema = tfdv.infer_schema(statistics=stats)\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfdv_schema:save"
   },
   "source": [
    "#### 将数据集的架构保存到云存储\n",
    "\n",
    "接下来，将数据集的架构写入数据集的云存储存储桶中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tfdv_schema:save"
   },
   "outputs": [],
   "source": [
    "SCHEMA_LOCATION = BUCKET_URI + \"/schema.txt\"\n",
    "\n",
    "# When running Apache Beam directly (file is directly accessed)\n",
    "tfdv.write_schema_text(output_path=SCHEMA_LOCATION, schema=schema)\n",
    "# When running with Dataflow (file is uploaded to worker pool)\n",
    "tfdv.write_schema_text(output_path=\"schema.txt\", schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataflow_setup:transform"
   },
   "source": [
    "#### 为Dataflow作业准备软件包要求。\n",
    "\n",
    "在您运行Dataflow作业之前，您需要为将执行作业的工作池指定软件包要求。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dataflow_setup:transform"
   },
   "outputs": [],
   "source": [
    "%%writefile setup.py\n",
    "import setuptools\n",
    "\n",
    "REQUIRED_PACKAGES = [\n",
    "    \"google-cloud-aiplatform==1.4.2\",\n",
    "    \"tensorflow-transform==1.2.0\",\n",
    "    \"tensorflow-data-validation==1.2.0\",\n",
    "]\n",
    "\n",
    "setuptools.setup(\n",
    "    name=\"executor\",\n",
    "    version=\"0.0.1\",\n",
    "    install_requires=REQUIRED_PACKAGES,\n",
    "    packages=setuptools.find_packages(),\n",
    "    include_package_data=True,\n",
    "    package_data={\"./\": [\"schema.txt\"]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataflow:split,bq,gsod"
   },
   "source": [
    "### 使用Dataflow对数据进行预处理\n",
    "\n",
    "#### 数据集拆分\n",
    "\n",
    "接下来，您将使用Dataflow对数据进行预处理。在本例中，您将查询BigQuery表并将示例拆分为训练和评估数据集。出于便利起见，数据集中的示例数量被限制为500个。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dataflow:split,bq,gsod"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow_transform.beam as tft_beam\n",
    "\n",
    "RUNNER = \"DataflowRunner\"  # DirectRunner for local running w/o Dataflow\n",
    "\n",
    "\n",
    "def parse_bq_record(bq_record):\n",
    "    \"\"\"Parses a bq_record to a dictionary.\"\"\"\n",
    "    output = {}\n",
    "    for key in bq_record:\n",
    "        output[key] = [bq_record[key]]\n",
    "    return output\n",
    "\n",
    "\n",
    "def split_dataset(bq_row, num_partitions, ratio):\n",
    "    \"\"\"Returns a partition number for a given bq_row.\"\"\"\n",
    "    import json\n",
    "\n",
    "    assert num_partitions == len(ratio)\n",
    "    bucket = sum(map(ord, json.dumps(bq_row))) % sum(ratio)\n",
    "    total = 0\n",
    "    for i, part in enumerate(ratio):\n",
    "        total += part\n",
    "        if bucket < total:\n",
    "            return i\n",
    "    return len(ratio) - 1\n",
    "\n",
    "\n",
    "def run_pipeline(args):\n",
    "    \"\"\"Runs a Beam pipeline to split the dataset\"\"\"\n",
    "\n",
    "    pipeline_options = beam.pipeline.PipelineOptions(flags=[], **args)\n",
    "\n",
    "    raw_data_query = args[\"raw_data_query\"]\n",
    "    exported_data_prefix = args[\"exported_data_prefix\"]\n",
    "    temp_location = args[\"temp_location\"]\n",
    "    project = args[\"project\"]\n",
    "\n",
    "    schema = tfdv.load_schema_text(SCHEMA_LOCATION)\n",
    "\n",
    "    with beam.Pipeline(options=pipeline_options) as pipeline:\n",
    "        with tft_beam.Context(temp_location):\n",
    "\n",
    "            # Read raw BigQuery data.\n",
    "            raw_train_data, raw_eval_data = (\n",
    "                pipeline\n",
    "                | \"Read Raw Data\"\n",
    "                >> beam.io.ReadFromBigQuery(\n",
    "                    query=raw_data_query,\n",
    "                    project=project,\n",
    "                    use_standard_sql=True,\n",
    "                )\n",
    "                | \"Parse Data\" >> beam.Map(parse_bq_record)\n",
    "                | \"Split\" >> beam.Partition(split_dataset, 2, ratio=[8, 2])\n",
    "            )\n",
    "\n",
    "            _ = (\n",
    "                raw_train_data\n",
    "                | \"Write Raw Train Data\"\n",
    "                >> beam.io.tfrecordio.WriteToTFRecord(\n",
    "                    file_path_prefix=os.path.join(exported_data_prefix, \"train/\"),\n",
    "                    file_name_suffix=\".gz\",\n",
    "                    coder=tft.coders.ExampleProtoCoder(schema),\n",
    "                )\n",
    "            )\n",
    "\n",
    "            _ = (\n",
    "                raw_eval_data\n",
    "                | \"Write Raw Eval Data\"\n",
    "                >> beam.io.tfrecordio.WriteToTFRecord(\n",
    "                    file_path_prefix=os.path.join(exported_data_prefix, \"eval/\"),\n",
    "                    file_name_suffix=\".gz\",\n",
    "                    coder=tft.coders.ExampleProtoCoder(schema),\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "EXPORTED_DATA_PREFIX = os.path.join(BUCKET_URI, \"exported_data\")\n",
    "\n",
    "QUERY_STRING = \"SELECT {},{} FROM {} LIMIT 500\".format(\n",
    "    \"CAST(station_number as STRING) AS station_number,year,month,day\",\n",
    "    \"mean_temp\",\n",
    "    IMPORT_FILE[5:],\n",
    ")\n",
    "JOB_NAME = \"gsod\" + TIMESTAMP\n",
    "\n",
    "args = {\n",
    "    \"runner\": RUNNER,\n",
    "    \"raw_data_query\": QUERY_STRING,\n",
    "    \"exported_data_prefix\": EXPORTED_DATA_PREFIX,\n",
    "    \"temp_location\": os.path.join(BUCKET_URI, \"temp\"),\n",
    "    \"project\": PROJECT_ID,\n",
    "    \"region\": REGION,\n",
    "    \"setup_file\": \"./setup.py\",\n",
    "}\n",
    "\n",
    "print(\"Data preprocessing started...\")\n",
    "run_pipeline(args)\n",
    "print(\"Data preprocessing completed.\")\n",
    "\n",
    "! gsutil ls $EXPORTED_DATA_PREFIX/train\n",
    "! gsutil ls $EXPORTED_DATA_PREFIX/eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleanup:mbsdk"
   },
   "source": [
    "清理\n",
    "\n",
    "要清理此项目中使用的所有谷歌云资源，您可以[删除用于本教程的谷歌云项目](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects)。\n",
    "\n",
    "否则，您可以删除您在本教程中创建的各个资源。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cleanup:mbsdk"
   },
   "outputs": [],
   "source": [
    "delete_storage = False\n",
    "\n",
    "if delete_storage or os.getenv(\"IS_TESTING\"):\n",
    "    if \"BUCKET_URI\" in globals():\n",
    "        ! gsutil rm -r $BUCKET_URI"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "get_started_dataflow.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
